<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Clustering - AIML documents</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Clustering";
        var mkdocs_page_input_path = "MachineLearning/UnsupervisedLearning/Clustering.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Machine Learning</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Supervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/Classification.html">Classification</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/CrossValidation.html">Cross Validation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../SupervisedLearning/TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/DecisionTreeRegression.html">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/RegressionModels/RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../SupervisedLearning/NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" >Unsupervised Learning</a>
    <ul class="current">
                <li class="toctree-l3"><a class="reference internal" href="overview.html">Overview</a>
                </li>
                <li class="toctree-l3 current"><a class="reference internal current" href="#">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Deep Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Natural Language Processing(NLP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/nlpdetails.html">NLP Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html" class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">AIML</li>
          <li class="breadcrumb-item">Machine Learning</li>
          <li class="breadcrumb-item">Unsupervised Learning</li>
      <li class="breadcrumb-item active">Clustering</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 style="color:red;">✅ Clustering</h2>

<h3 style="color:blue;">📌 What is K means Clustering?</h3>

<p>K-Means Clustering is an Unsupervised Machine Learning algorithm which groups unlabeled dataset into different clusters. It is used to organize data into <strong>groups based on their similarity</strong>.</p>
<h3 style="color:blue;">📌 Understanding K-means Clustering</h3>

<p><img alt="alt text" src="../images/UL2.png" /></p>
<p><img alt="alt text" src="../images/UL3.png" /></p>
<p>For example online store uses K-Means to group customers based on purchase frequency and spending creating segments like Budget Shoppers, Frequent Buyers and Big Spenders for personalised marketing.</p>
<ul>
<li>
<p>The algorithm works by first randomly picking some central points called centroids and each data point is then assigned to the closest centroid forming a cluster.</p>
</li>
<li>
<p>After all the points are assigned to a cluster the centroids are updated by finding the average position of the points in each cluster.</p>
</li>
<li>
<p>This process repeats until the centroids stop changing forming clusters.</p>
</li>
<li>
<p>The goal of clustering is to divide the data points into clusters so that similar data points belong to same group.</p>
</li>
</ul>
<h3 style="color:blue;">📌 How k-means clustering works?</h3>

<p>We are given a data set of items with certain features and values for these features like a vector.</p>
<p>The task is to categorize those items into groups. To achieve this we will use the K-means algorithm. 'K' in the name of the algorithm represents the number of groups/clusters we want to classify our items into.</p>
<p><img alt="alt text" src="../images/UL4.png" /></p>
<p>The algorithm will categorize the items into k groups or clusters of similarity.To calculate that similarity we will use the <strong>Euclidean distance</strong> as a measurement. The algorithm works as follows:  </p>
<ol>
<li>
<p>First we randomly initialize <strong>k points</strong> called <strong>means</strong> or <strong>cluster centroids</strong>.</p>
</li>
<li>
<p>We categorize each item to its <strong>closest mean</strong> and we <strong>update the mean's coordinates</strong>, which are the <strong>averages of the items categorized in that cluster</strong> so far.</p>
</li>
<li>
<p>We repeat the process for a <strong>given number of iterations</strong> and at the end, we have our clusters.</p>
</li>
</ol>
<p><strong>In K-Means:</strong></p>
<ul>
<li>
<p>You start with <strong>some initial cluster centers (means)</strong>.</p>
</li>
<li>
<p>These <strong>means are just points in your feature space</strong>.</p>
</li>
<li>
<p>There are <strong>two common ways</strong> to choose them:</p>
<ol>
<li>
<p>Pick random <strong>data points</strong> as means.</p>
</li>
<li>
<p>Pick random <strong>values</strong> within the <strong>range of the dataset</strong>.</p>
</li>
</ol>
</li>
</ul>
<p><strong>Suppose you have this dataset:</strong></p>
<table>
<thead>
<tr>
<th>Customer ID</th>
<th>Age (x1)</th>
<th>Income (x2 in \$K)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>25</td>
<td>40</td>
</tr>
<tr>
<td>2</td>
<td>30</td>
<td>45</td>
</tr>
<tr>
<td>3</td>
<td>35</td>
<td>50</td>
</tr>
<tr>
<td>4</td>
<td>60</td>
<td>100</td>
</tr>
<tr>
<td>5</td>
<td>65</td>
<td>105</td>
</tr>
</tbody>
</table>
<p>This is a <strong>2D dataset</strong> (Age and Income).</p>
<p>✅ <strong>Method 1: Initialize means using random data points</strong></p>
<p>We randomly choose <strong>k=2</strong> actual rows as our initial centers:</p>
<ul>
<li>
<p>Mean 1 = (25, 40)</p>
</li>
<li>
<p>Mean 2 = (60, 100)</p>
</li>
</ul>
<p>These are <strong>real customers</strong>, just chosen as starting points.</p>
<p>✅ <strong>Method 2: Initialize means using random values within feature ranges</strong></p>
<p>Here, we use the <strong>min and max</strong> of each feature:</p>
<ul>
<li>
<p><strong>Age:</strong> min = 25, max = 65 ⇒ range = [25, 65]</p>
</li>
<li>
<p><strong>Income:</strong> min = 40, max = 105 ⇒ range = [40, 105]</p>
</li>
</ul>
<p>Now randomly generate any values within this box:</p>
<ul>
<li>
<p><strong>Mean 1</strong> = (30.5, 80.2) ← random numbers between 25–65 and 40–105</p>
</li>
<li>
<p><strong>Mean 2</strong> = (58.3, 45.0)</p>
</li>
</ul>
<p>🤔 <strong>Why do we need different ways?</strong></p>
<ul>
<li>
<p><strong>Random points from the dataset:</strong> safer, avoids outliers.</p>
</li>
<li>
<p><strong>Random values in the feature space:</strong> more flexible, but risky if the range has irrelevant areas (e.g., outliers can skew the range).</p>
</li>
</ul>
<p>🔁 <strong>After Initialization</strong></p>
<p>No matter how you initialize:</p>
<ul>
<li>
<p>K-Means will <strong>iteratively adjust the means</strong> by:</p>
<ul>
<li>
<p>Assigning each point to the nearest mean.</p>
</li>
<li>
<p>Recomputing each mean as the <strong>average of its assigned points</strong>.</p>
</li>
</ul>
</li>
<li>
<p>The algorithm stops when the means stop changing significantly.</p>
</li>
</ul>
<p><strong>Summary:</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Example</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pick random data points</td>
<td>(25, 40), (60, 100)</td>
<td>Easy, safe</td>
</tr>
<tr>
<td>Pick random values in range</td>
<td>(30.5, 80.2), (58.3, 45.0)</td>
<td>More flexible, but riskier</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">📌 Example: K-Means Initialization Methods with Visualization</h3>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="c1"># Sample dataset: [Age, Income]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">45</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mi">105</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Function to initialize centroids using method 1 (random points from data)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_random_points</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="kp">indices</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="kp">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="kp">indices</span><span class="p">]</span>

<span class="c1"># Function to initialize centroids using method 2 (random values within range)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_random_values</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">mins</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="kp">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">maxs</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="kp">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="kp">uniform</span><span class="p">(</span><span class="n">mins</span><span class="p">,</span> <span class="n">maxs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span>

<span class="c1"># Initialize</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="kp">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># for reproducibility</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">means_method1</span> <span class="o">=</span> <span class="n">init_random_points</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">means_method2</span> <span class="o">=</span> <span class="n">init_random_values</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data Points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">means_method1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">means_method1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Method 1: Data Points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">means_method2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">means_method2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;P&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Method 2: Random Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Income ($K)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;K-Means Initialization Methods&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="alt text" src="../images/UL5.png" /></p>
<p>📊 <strong>What You'll See:</strong></p>
<ul>
<li>
<p><strong>Blue dots</strong> = original customer data (Age vs. Income)</p>
</li>
<li>
<p><strong>Green 'X' markers</strong> = initial means chosen from actual data points</p>
</li>
<li>
<p><strong>Red 'P' markers</strong> = initial means from <strong>random values inside feature range</strong></p>
</li>
</ul>
<p>This will help you visually compare how these two initialization strategies place the cluster centers.</p>
<h3 style="color:blue;">📌 Euclidean Distance</h3>

<p><strong>Euclidean Distance</strong> is defined as the distance between two points in Euclidean space.To find the distance between two points, the length of the line segment that connects the two points should be measured.</p>
<p>Euclidean distance is like <strong>measuring the straightest and shortest path between two points</strong>.</p>
<p>Imagine you have a string and you stretch it tight between two points on a map; the length of that string is the Euclidean distance. It tells you how far apart the two points are without any turns or bends, just like a bird would fly directly from one spot to another.</p>
<p>This metric is based on the <strong>Pythagorean theorem</strong> and is widely utilized in various fields such as machine learning, data analysis, computer vision, and more.</p>
<p><strong>Euclidean Distance Formula</strong></p>
<p>Consider two points (x1, y1) and (x2, y2) in a 2-dimensional space; the Euclidean Distance between them is given by using the formula:</p>
<p><img alt="alt text" src="../images/UL6.png" /></p>
<p>Where,</p>
<ul>
<li>
<p>d is Euclidean Distance,</p>
</li>
<li>
<p>(x1, y1) is the Coordinate of the first point,</p>
</li>
<li>
<p>(x2, y2) is the Coordinate of the second point.</p>
</li>
</ul>
<p><strong>Euclidean Distance in 3D</strong></p>
<p>If the two points (x1, y1, z1) and (x2, y2, z2) are in a 3-dimensional space, the Euclidean Distance between them is given by using the formula:</p>
<p><img alt="alt text" src="../images/UL7.png" /></p>
<p>Where,</p>
<ul>
<li>
<p>d is Euclidean Distance,</p>
</li>
<li>
<p>(x1, y1, z1) is the Coordinate of the first point,</p>
</li>
<li>
<p>(x2, y2, z2) is the Coordinate of the second point.</p>
</li>
</ul>
<p><strong>Euclidean Distance in nD</strong></p>
<p>In general, the Euclidean Distance formula between two points (x11, x12, x13, ...., x1n) and (x21, x22, x23, ...., x2n) in an n-dimensional space is given by the formula:</p>
<p><img alt="alt text" src="../images/UL8.png" /></p>
<p>Where,</p>
<ul>
<li>
<p>i Ranges from 1 to n,</p>
</li>
<li>
<p>d is Euclidean distance,</p>
</li>
<li>
<p>(x11, x12, x13, ...., x1n) is the Coordinate of the First Point,</p>
</li>
<li>
<p>(x21, x22, x23, ...., x2n) is the Coordinate of the Second Point.</p>
</li>
</ul>
<p><strong>Euclidean Distance Formula Derivation</strong></p>
<p>Euclidean Distance Formula is derived by following the steps added below:</p>
<ul>
<li>
<p><strong>Step 1:</strong> Let us consider two points, A (x1, y1) and B (x2, y2), and d is the distance between the two points.</p>
</li>
<li>
<p><strong>Step 2:</strong> Join the points using a straight line (AB).</p>
</li>
<li>
<p><strong>Step 3:</strong> Now, let us construct a right-angled triangle whose hypotenuse is AB, as shown in the figure below.</p>
</li>
</ul>
<p><img alt="alt text" src="../images/UL9.png" /></p>
<p><strong>Step 4:</strong> Now, using Pythagoras theorem we know that,</p>
<p><img alt="alt text" src="../images/UL10.png" /></p>
<p><strong>Note:</strong>
Selecting the right number of clusters is important for meaningful segmentation to do this we use <strong>Elbow Method for optimal value of k in KMeans</strong> which is a graphical tool used to determine the optimal number of clusters (k) in K-means.</p>
<h3 style="color:blue;">📌 Elbow Method for optimal value of k in KMeans</h3>

<p>Choosing the optimal number of clusters is a crucial step in any unsupervised learning algorithm.</p>
<p>Since we don’t have predefined cluster counts in unsupervised learning, we need a systematic approach to determine the best k value. The <strong>Elbow Method</strong> is a popular technique used for this purpose in K-Means clustering.</p>
<h3 style="color:blue;">📌 Elbow Method in K-Means Clustering</h3>

<p>In K-Means clustering, we start by randomly initializing k clusters and iteratively adjusting these clusters until they stabilize at an equilibrium point. However, before we can do this, we need to decide how many clusters (k) we should use.</p>
<p>The Elbow Method helps us find this optimal k value. Here’s how it works:</p>
<ol>
<li>
<p>We iterate over a range of k values, typically from 1 to n (where n is a hyper-parameter you choose).</p>
</li>
<li>
<p>For each k, we calculate the <strong>Within-Cluster Sum of Squares (WCSS)</strong>.</p>
</li>
</ol>
<p><img alt="alt text" src="../images/UL11.png" /></p>
<p><strong>The Elbow Point: Optimal k Value</strong></p>
<p>The Elbow Method works in below steps:</p>
<ul>
<li>
<p><strong>We calculate a distance measure called WCSS (Within-Cluster Sum of Squares)</strong>. This tells us how spread out the data points are within each cluster.</p>
</li>
<li>
<p><strong>We try different k values (number of clusters).</strong> For each k, we run KMeans and calculate the WCSS.</p>
</li>
<li>
<p><strong>We plot a graph with k on the X-axis and WCSS on the Y-axis.</strong></p>
</li>
<li>
<p><strong>Identifying the Elbow Point:</strong> As we increase kkk, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an "elbow" shape in the graph.</p>
<ul>
<li>
<p><strong>Before the elbow:</strong> Increasing kkk significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability.</p>
</li>
<li>
<p><strong>After the elbow:</strong> Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting.</p>
</li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../images/UL12.png" /></p>
<p>The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. This "elbow" point suggests the optimal number of clusters.</p>
<h3 style="color:blue;">📌 Understanding Distortion and Inertia in K-Means Clustering</h3>

<p>In K-Means clustering, we aim to group similar data points together. To evaluate the quality of these groupings, we use two key metrics: <strong>Distortion</strong> and <strong>Inertia</strong>.</p>
<p><strong>1. Distortion</strong></p>
<p>Distortion measures the average squared distance between each data point and its assigned cluster center. It's a measure of how well the clusters represent the data. A lower distortion value indicates better clustering.</p>
<p><img alt="alt text" src="../images/UL13.png" /></p>
<p><strong>2. Inertia</strong></p>
<p>Inertia is the sum of squared distances of each data point to its closest cluster center. It's essentially the total squared error of the clustering. Like distortion, a lower inertia value suggests better clustering.</p>
<p><img alt="alt text" src="../images/UL14.png" /></p>
<p>In the Elbow Method, we calculate the distortion or inertia for different values of k (number of clusters). We then plot these values to identify the "elbow point", where the rate of decrease in distortion or inertia starts to slow down. This elbow point often indicates the optimal number of clusters.</p>
<p><strong>A Lower Distortion or Inertia is Generally Better</strong></p>
<p><strong>A lower distortion or inertia implies that the data points are more closely grouped around their respective cluster centers.</strong> However, it's important to balance this with the number of clusters. Too few clusters might not capture the underlying structure of the data, while too many clusters can lead to overfitting.</p>
<p>By understanding distortion and inertia, we can effectively evaluate the quality of K-Means clustering and select the optimal number of clusters.</p>
<h3 style="color:blue;">📌 Implementation of Elbow Method</h3>

<p>In this section, we will demonstrate how to implement the Elbow Method to determine the optimal number of clusters (k) using Python's Scikit-learn library. We will create a random dataset, apply K-means clustering, calculate the Within-Cluster Sum of Squares (WCSS) for different values of k, and visualize the results to determine the optimal number of clusters.</p>
<h3 style="color:blue;">📌 Step 1: Importing the required libraries</h3>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.spatial.distance</span><span class="w"> </span><span class="kn">import</span> <span class="n">cdist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
</code></pre></div>

<h3 style="color:blue;">📌 Step 2: Creating and Visualizing the data</h3>

<p>We will create a random array and visualize its distribution</p>
<div class="codehilite"><pre><span></span><code>#<span class="w"> </span><span class="nv">Creating</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">dataset</span>
<span class="nv">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="mi">3</span>,<span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">6</span>,
<span class="w">               </span><span class="mi">7</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">9</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">9</span>,<span class="w"> </span><span class="mi">9</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">4</span>,<span class="w"> </span><span class="mi">4</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">4</span>]<span class="ss">)</span>
<span class="nv">x2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="mi">5</span>,<span class="w"> </span><span class="mi">4</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">7</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">7</span>,
<span class="w">               </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">9</span>,<span class="w"> </span><span class="mi">10</span>,<span class="w"> </span><span class="mi">9</span>,<span class="w"> </span><span class="mi">10</span>]<span class="ss">)</span>
<span class="nv">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span><span class="nv">list</span><span class="ss">(</span><span class="nv">zip</span><span class="ss">(</span><span class="nv">x1</span>,<span class="w"> </span><span class="nv">x2</span><span class="ss">)))</span>.<span class="nv">reshape</span><span class="ss">(</span><span class="nv">len</span><span class="ss">(</span><span class="nv">x1</span><span class="ss">)</span>,<span class="w"> </span><span class="mi">2</span><span class="ss">)</span>

#<span class="w"> </span><span class="nv">Visualizing</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">data</span>
<span class="nv">plt</span>.<span class="nv">scatter</span><span class="ss">(</span><span class="nv">x1</span>,<span class="w"> </span><span class="nv">x2</span>,<span class="w"> </span><span class="nv">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">xlim</span><span class="ss">(</span>[<span class="mi">0</span>,<span class="w"> </span><span class="mi">10</span>]<span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">ylim</span><span class="ss">(</span>[<span class="mi">0</span>,<span class="w"> </span><span class="mi">10</span>]<span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">title</span><span class="ss">(</span><span class="s1">&#39;Dataset Visualization&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">xlabel</span><span class="ss">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">ylabel</span><span class="ss">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="k">show</span><span class="ss">()</span>
</code></pre></div>

<p><img alt="alt text" src="../images/UL15.png" /></p>
<p>From the above visualization, we can see that the optimal number of clusters should be around 3. But visualizing the data alone cannot always give the right answer. Hence we demonstrate the following steps.</p>
<h3 style="color:blue;">📌 Step 3: Building the Clustering Model and Calculating Distortion and Inertia</h3>

<p>In this step, we will fit the K-means model for different values of k (number of clusters) and calculate both the distortion and inertia for each value.</p>
<div class="codehilite"><pre><span></span><code><span class="n">distortions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="n">inertias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="n">mapping1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span>
<span class="n">mapping2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span>
<span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>

<span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">K</span><span class="p">:</span>
<span class="w">    </span><span class="n">kmeanModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="w">    </span><span class="n">distortions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">kmeanModel</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;euclidean&#39;</span><span class="p">),</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">)</span>

<span class="w">    </span><span class="n">inertias</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeanModel</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="w">    </span><span class="n">mapping1</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distortions</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span>
<span class="w">    </span><span class="n">mapping2</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inertias</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span>
</code></pre></div>

<h3 style="color:blue;">📌 Step 4: Tabulating and Visualizing the Results</h3>

<p><strong>a) Displaying Distortion Values</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;Distortion values:&quot;</span><span class="ss">)</span>
<span class="k">for</span><span class="w"> </span><span class="nv">key</span>,<span class="w"> </span><span class="nv">val</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">mapping1</span>.<span class="nv">items</span><span class="ss">()</span>:
<span class="w">    </span><span class="nv">print</span><span class="ss">(</span><span class="nv">f</span><span class="s1">&#39;{key} : {val}&#39;</span><span class="ss">)</span>

<span class="nv">plt</span>.<span class="nv">plot</span><span class="ss">(</span><span class="nv">K</span>,<span class="w"> </span><span class="nv">distortions</span>,<span class="w"> </span><span class="s1">&#39;bx-&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">xlabel</span><span class="ss">(</span><span class="s1">&#39;Number of Clusters (k)&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">ylabel</span><span class="ss">(</span><span class="s1">&#39;Distortion&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="nv">title</span><span class="ss">(</span><span class="s1">&#39;The Elbow Method using Distortion&#39;</span><span class="ss">)</span>
<span class="nv">plt</span>.<span class="k">show</span><span class="ss">()</span>
</code></pre></div>

<p><img alt="alt text" src="../images/UL16.png" /></p>
<p><strong>b) Displaying Inertia Values:</strong></p>
<p><img alt="alt text" src="../images/UL17.png" /></p>
<h3 style="color:blue;">📌 Step 5: Clustered Data Points For Different k Values</h3>

<p>We will plot images of data points clustered for different values of k. For this, we will apply the k-means algorithm on the dataset by iterating on a range of k values.</p>
<div class="codehilite"><pre><span></span><code>k_range = range(1, 5)

for k in k_range:
    kmeans = KMeans(n_clusters=k, init=&#39;k-means++&#39;, random_state=42)
    y_kmeans = kmeans.fit_predict(X)

    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=&#39;viridis&#39;, marker=&#39;o&#39;, edgecolor=&#39;k&#39;, s=100)
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                s=300, c=&#39;red&#39;, label=&#39;Centroids&#39;, edgecolor=&#39;k&#39;)
    plt.title(f&#39;K-means Clustering (k={k})&#39;)
    plt.xlabel(&#39;Feature 1&#39;)
    plt.ylabel(&#39;Feature 2&#39;)
    plt.legend()
    plt.grid()
    plt.show()
</code></pre></div>

<p><img alt="alt text" src="../images/UL18.png" /></p>
<p><img alt="alt text" src="../images/UL19.png" /></p>
<p><img alt="alt text" src="../images/UL20.png" /></p>
<p><img alt="alt text" src="../images/UL21.png" /></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>
<p>The Elbow Method helps you choose the optimal number of clusters (k) in KMeans clustering.</p>
</li>
<li>
<p>It analyzes how adding more clusters (increasing k) affects the spread of data points within each cluster (WCSS).</p>
</li>
<li>
<p>The k value corresponding to the "elbow" in the WCSS vs k graph is considered the optimal choice.</p>
</li>
<li>
<p>The Elbow Method provides a good starting point, but consider your specific data and goals when finalizing k.</p>
</li>
</ul>
<h3 style="color:blue;">📊 Use Case: Customer Segmentation for Marketing</h3>

<p>🎯 <strong>Objective</strong></p>
<p>A retail company wants to segment its customers based on purchasing behavior so that it can:</p>
<ul>
<li>
<p>Run personalized marketing campaigns,</p>
</li>
<li>
<p>Identify high-value customers,</p>
</li>
<li>
<p>Improve customer retention.</p>
</li>
</ul>
<p>📁 <strong>Dataset</strong></p>
<p>[Kaggle][https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis]</p>
<p>📘 <strong>Dataset Overview</strong></p>
<p>This dataset, available at Kaggle as Customer Personality Analysis by imakash3011, includes <strong>2,240 customer records</strong> with <strong>29 features</strong>, covering demographic info, household characteristics, spending on products, and campaign responses</p>
<p><strong>Key feature groups:</strong></p>
<ul>
<li>
<p><strong>People:</strong> <code>ID, Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome, Dt_Customer, Recency, Complain</code></p>
</li>
<li>
<p><strong>Product Purchases (last 2 years):</strong> <code>MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds</code></p>
</li>
<li>
<p><strong>Promotion Behavior:</strong> <code>NumDealsPurchases, AcceptedCmp1–AcceptedCmp5, Response</code></p>
</li>
<li>
<p><strong>Purchasing Channels:</strong> <code>NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth</code></p>
</li>
</ul>
<p><strong>🧠 Step-by-Step: Build K-Means Real-world Model</strong></p>
<p><strong>1. Load &amp; Clean Data</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;marketing_campaign.csv&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># Handle missing income values by median or capping</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p><strong>2. Feature Engineering</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">year</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Year_Birth&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Total_Expenses&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;MntWines&#39;</span><span class="p">,</span> <span class="s1">&#39;MntFruits&#39;</span><span class="p">,</span> <span class="s1">&#39;MntMeatProducts&#39;</span><span class="p">,</span>
                            <span class="s1">&#39;MntFishProducts&#39;</span><span class="p">,</span> <span class="s1">&#39;MntSweetProducts&#39;</span><span class="p">,</span> <span class="s1">&#39;MntGoldProds&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Total_Accepted_Cmp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;AcceptedCmp1&#39;</span><span class="p">,</span><span class="s1">&#39;AcceptedCmp2&#39;</span><span class="p">,</span><span class="s1">&#39;AcceptedCmp3&#39;</span><span class="p">,</span><span class="s1">&#39;AcceptedCmp4&#39;</span><span class="p">,</span><span class="s1">&#39;AcceptedCmp5&#39;</span><span class="p">,</span><span class="s1">&#39;Response&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p><strong>3. Select and Scale Features</strong></p>
<p>Choose relevant variables for segmentation:</p>
<div class="codehilite"><pre><span></span><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Recency&#39;</span><span class="p">,</span> <span class="s1">&#39;Total_Expenses&#39;</span><span class="p">,</span> <span class="s1">&#39;Total_Accepted_Cmp&#39;</span><span class="p">,</span> <span class="s1">&#39;NumWebPurchases&#39;</span><span class="p">,</span> <span class="s1">&#39;NumStorePurchases&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p><strong>4. Determine Number of Clusters</strong></p>
<p>Plot the Elbow Curve or compute silhouette scores to choose optimal k, typically k=2 to 4</p>
<p><strong>5. Run K-Means Clustering</strong></p>
<p><strong>✅ Real-Time Use Case: Customer Segmentation using K-Means</strong></p>
<p><strong>Business Scenario:</strong></p>
<p>A retail company wants to segment its customers based on demographics, spending behavior, and tenure to run personalized marketing campaigns.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Step 1: Install required packages</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">seaborn</span>

<span class="c1"># Step 2: Import libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># Step 3: Load data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;marketing_campaign.csv&quot;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Step 4: Preprocess</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Dt_Customer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Dt_Customer&#39;</span><span class="p">],</span> <span class="n">dayfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Customer_Tenure&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="s2">&quot;2025-01-01&quot;</span><span class="p">)</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Dt_Customer&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">days</span>

<span class="c1"># Drop unnecessary columns</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;Dt_Customer&#39;</span><span class="p">,</span> <span class="s1">&#39;Z_CostContact&#39;</span><span class="p">,</span> <span class="s1">&#39;Z_Revenue&#39;</span><span class="p">])</span>

<span class="c1"># One-hot encode categorical</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Education&#39;</span><span class="p">,</span> <span class="s1">&#39;Marital_Status&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Handle missing values</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)</span>
<span class="n">df_imputed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Standardize features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_imputed</span><span class="p">)</span>

<span class="c1"># Step 5: Fit K-Means</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Step 6: Label clusters (optional &amp; domain-driven)</span>
<span class="n">cluster_labels</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;High Income, High Spend&#39;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;Low Income, Less Spend&#39;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;Middle Income, Average Spend&#39;</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;Senior Customers&#39;</span>
<span class="p">}</span>
<span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;Segment_Label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">)</span>

<span class="c1"># Step 7: Visualize clusters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;Income&#39;</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;MntWines&#39;</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">df_imputed</span><span class="p">[</span><span class="s1">&#39;Segment_Label&#39;</span><span class="p">],</span>
    <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;Set2&#39;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Customer Segmentation using K-Means&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Annual Income&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Wine Spend&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="alt text" src="../images/UL22.png" /></p>
<p><strong>🧠 Insights You Can Derive:</strong></p>
<ul>
<li>
<p><strong>High Income, High Spend:</strong> Likely premium customers to upsell.</p>
</li>
<li>
<p><strong>Low Income, Less Spend:</strong> Might benefit from discount-based offers.</p>
</li>
<li>
<p><strong>Middle Income:</strong> Core customer base with moderate loyalty.</p>
</li>
<li>
<p><strong>Senior Customers:</strong> Segment by age and target with legacy brand values.</p>
</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="Pca.html" class="btn btn-neutral float-right" title="Principal Component Analysis(PCA)">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="overview.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="Pca.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
