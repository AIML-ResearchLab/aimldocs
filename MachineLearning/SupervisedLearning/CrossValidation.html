<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Cross Validation - AIML documents</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Cross Validation";
        var mkdocs_page_input_path = "MachineLearning/SupervisedLearning/CrossValidation.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Machine Learning</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../Overview.html">Overview</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" >Supervised Learning</a>
    <ul class="current">
                <li class="toctree-l3"><a class="reference internal" href="Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Classification.html">Classification</a>
                </li>
                <li class="toctree-l3 current"><a class="reference internal current" href="#">Cross Validation</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/DecisionTreeRegression.html">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RegressionModels/RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Unsupervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../UnsupervisedLearning/overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../UnsupervisedLearning/Clustering.html">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../UnsupervisedLearning/Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Deep Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Natural Language Processing(NLP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/nlpdetails.html">NLP Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html" class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">AIML</li>
          <li class="breadcrumb-item">Machine Learning</li>
          <li class="breadcrumb-item">Supervised Learning</li>
      <li class="breadcrumb-item active">Cross Validation</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 style="color:red;">✅ Cross Validation in Machine Learning</h2>

<h3 style="color:blue;">📌 What is Cross Validation in Machine Learning?</h3>

<p>Cross-validation is a technique used to check how well a machine learning model performs on unseen data.
It splits the data into several parts, trains the model on some parts and tests it on the remaining part repeating this process multiple times. Finally the results from each validation step are averaged to produce a more accurate estimate of the model's performance.</p>
<p>The main purpose of cross validation is to prevent <strong>overfitting</strong>. If you want to make sure your machine learning model is not just memorizing the training data but is capable of adapting to real-world data cross-validation is a commonly used technique.</p>
<p><img alt="alt text" src="../images/Cross-validation.svg" /></p>
<p>In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. Let’s load the iris data set to fit a linear support vector machine on it:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">svm</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="kp">shape</span>
</code></pre></div>

<p>We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:</p>
<div class="codehilite"><pre><span></span><code>X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=0)

X_train.shape, y_train.shape
X_test.shape, y_test.shape

clf = svm.SVC(kernel=&#39;linear&#39;, C=1).fit(X_train, y_train)
clf.score(X_test, y_test)
</code></pre></div>

<h2 id="what-is-c-in-svm">🔹 What is C in SVM?<a class="headerlink" href="#what-is-c-in-svm" title="Permanent link">#</a></h2>
<p><code>C</code>is the <strong>regularization parameter</strong> in SVM.</p>
<p>It controls the trade-off between:</p>
<ul>
<li>
<p><strong>Having a wide margin (simpler model, better generalization)</strong></p>
</li>
<li>
<p><strong>Classifying training points correctly (lower training error)</strong></p>
</li>
</ul>
<h2 id="intuition-margin-vs-misclassification">🔸 Intuition (Margin vs Misclassification)<a class="headerlink" href="#intuition-margin-vs-misclassification" title="Permanent link">#</a></h2>
<ul>
<li>
<p><strong>Large C (e.g., C=1000):</strong></p>
</li>
<li>
<p>The model <strong>tries very hard</strong> to classify all training points correctly.</p>
</li>
<li>
<p><strong>Narrow margin</strong>, less tolerance for misclassification.</p>
</li>
<li>
<p>Risk of <strong>overfitting</strong> (memorizes training data, may fail on unseen data).</p>
</li>
<li>
<p><strong>Small C (e.g., C=0.01):</strong></p>
</li>
<li>
<p>The model <strong>allows some misclassifications</strong>.</p>
</li>
<li>
<p><strong>Wider margin,</strong> focuses more on generalization.</p>
</li>
<li>
<p>Risk of <strong>underfitting</strong> (too simple, misses patterns).</p>
</li>
</ul>
<p>When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance.To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</p>
<p>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</p>
<p>A solution to this problem is a procedure called <strong>cross-validation</strong> (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:</p>
<ul>
<li>
<p>A model is trained using <code>k -1</code> of the folds as training data;</p>
</li>
<li>
<p>the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).</p>
</li>
</ul>
<p>The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</p>
<h2 id="computing-cross-validated-metrics">Computing cross-validated metrics<a class="headerlink" href="#computing-cross-validated-metrics" title="Permanent link">#</a></h2>
<p>The simplest way to use cross-validation is to call the <code>cross_val_score</code> helper function on the estimator and the dataset.</p>
<p>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span>
</code></pre></div>

<p>The mean score and the standard deviation are hence given by:</p>
<div class="codehilite"><pre><span></span><code>print(&quot;%0.2f accuracy with a standard deviation of %0.2f&quot; % (scores.mean(), scores.std()))
0.98 accuracy with a standard deviation of 0.02
</code></pre></div>

<p>By default, the score computed at each CV iteration is the <code>score</code> method of the estimator. It is possible to change this by using the scoring parameter:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1_macro&#39;</span><span class="p">)</span>
<span class="n">scores</span>
</code></pre></div>

<h2 id="string-name-scorers">String name scorers<a class="headerlink" href="#string-name-scorers" title="Permanent link">#</a></h2>
<h1 id="scikit-learn-scoring-reference">Scikit-learn Scoring Reference<a class="headerlink" href="#scikit-learn-scoring-reference" title="Permanent link">#</a></h1>
<h2 id="classification">Classification<a class="headerlink" href="#classification" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th>Scoring String Name</th>
<th>Function</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>accuracy</code></td>
<td><code>metrics.accuracy_score</code></td>
<td></td>
</tr>
<tr>
<td><code>balanced_accuracy</code></td>
<td><code>metrics.balanced_accuracy_score</code></td>
<td></td>
</tr>
<tr>
<td><code>top_k_accuracy</code></td>
<td><code>metrics.top_k_accuracy_score</code></td>
<td></td>
</tr>
<tr>
<td><code>average_precision</code></td>
<td><code>metrics.average_precision_score</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_brier_score</code></td>
<td><code>metrics.brier_score_loss</code></td>
<td></td>
</tr>
<tr>
<td><code>f1</code></td>
<td><code>metrics.f1_score</code></td>
<td>for binary targets</td>
</tr>
<tr>
<td><code>f1_micro</code></td>
<td><code>metrics.f1_score</code></td>
<td>micro-averaged</td>
</tr>
<tr>
<td><code>f1_macro</code></td>
<td><code>metrics.f1_score</code></td>
<td>macro-averaged</td>
</tr>
<tr>
<td><code>f1_weighted</code></td>
<td><code>metrics.f1_score</code></td>
<td>weighted average</td>
</tr>
<tr>
<td><code>f1_samples</code></td>
<td><code>metrics.f1_score</code></td>
<td>by multilabel sample</td>
</tr>
<tr>
<td><code>neg_log_loss</code></td>
<td><code>metrics.log_loss</code></td>
<td>requires <code>predict_proba</code> support</td>
</tr>
<tr>
<td><code>precision</code>, etc.</td>
<td><code>metrics.precision_score</code></td>
<td>suffixes apply as with <code>f1</code></td>
</tr>
<tr>
<td><code>recall</code>, etc.</td>
<td><code>metrics.recall_score</code></td>
<td>suffixes apply as with <code>f1</code></td>
</tr>
<tr>
<td><code>jaccard</code>, etc.</td>
<td><code>metrics.jaccard_score</code></td>
<td>suffixes apply as with <code>f1</code></td>
</tr>
<tr>
<td><code>roc_auc</code></td>
<td><code>metrics.roc_auc_score</code></td>
<td></td>
</tr>
<tr>
<td><code>roc_auc_ovr</code></td>
<td><code>metrics.roc_auc_score</code></td>
<td></td>
</tr>
<tr>
<td><code>roc_auc_ovo</code></td>
<td><code>metrics.roc_auc_score</code></td>
<td></td>
</tr>
<tr>
<td><code>roc_auc_ovr_weighted</code></td>
<td><code>metrics.roc_auc_score</code></td>
<td></td>
</tr>
<tr>
<td><code>roc_auc_ovo_weighted</code></td>
<td><code>metrics.roc_auc_score</code></td>
<td></td>
</tr>
<tr>
<td><code>d2_log_loss_score</code></td>
<td><code>metrics.d2_log_loss_score</code></td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="clustering">Clustering<a class="headerlink" href="#clustering" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th>Scoring String Name</th>
<th>Function</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>adjusted_mutual_info_score</code></td>
<td><code>metrics.adjusted_mutual_info_score</code></td>
<td></td>
</tr>
<tr>
<td><code>adjusted_rand_score</code></td>
<td><code>metrics.adjusted_rand_score</code></td>
<td></td>
</tr>
<tr>
<td><code>completeness_score</code></td>
<td><code>metrics.completeness_score</code></td>
<td></td>
</tr>
<tr>
<td><code>fowlkes_mallows_score</code></td>
<td><code>metrics.fowlkes_mallows_score</code></td>
<td></td>
</tr>
<tr>
<td><code>homogeneity_score</code></td>
<td><code>metrics.homogeneity_score</code></td>
<td></td>
</tr>
<tr>
<td><code>mutual_info_score</code></td>
<td><code>metrics.mutual_info_score</code></td>
<td></td>
</tr>
<tr>
<td><code>normalized_mutual_info_score</code></td>
<td><code>metrics.normalized_mutual_info_score</code></td>
<td></td>
</tr>
<tr>
<td><code>rand_score</code></td>
<td><code>metrics.rand_score</code></td>
<td></td>
</tr>
<tr>
<td><code>v_measure_score</code></td>
<td><code>metrics.v_measure_score</code></td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="regression">Regression<a class="headerlink" href="#regression" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th>Scoring String Name</th>
<th>Function</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>explained_variance</code></td>
<td><code>metrics.explained_variance_score</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_max_error</code></td>
<td><code>metrics.max_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_absolute_error</code></td>
<td><code>metrics.mean_absolute_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_squared_error</code></td>
<td><code>metrics.mean_squared_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_root_mean_squared_error</code></td>
<td><code>metrics.root_mean_squared_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_squared_log_error</code></td>
<td><code>metrics.mean_squared_log_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_root_mean_squared_log_error</code></td>
<td><code>metrics.root_mean_squared_log_error</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_median_absolute_error</code></td>
<td><code>metrics.median_absolute_error</code></td>
<td></td>
</tr>
<tr>
<td><code>r2</code></td>
<td><code>metrics.r2_score</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_poisson_deviance</code></td>
<td><code>metrics.mean_poisson_deviance</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_gamma_deviance</code></td>
<td><code>metrics.mean_gamma_deviance</code></td>
<td></td>
</tr>
<tr>
<td><code>neg_mean_absolute_percentage_error</code></td>
<td><code>metrics.mean_absolute_percentage_error</code></td>
<td></td>
</tr>
<tr>
<td><code>d2_absolute_error_score</code></td>
<td><code>metrics.d2_absolute_error_score</code></td>
<td></td>
</tr>
</tbody>
</table>
<p>In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.</p>
<p>When the cv argument is an integer, cross_val_score uses the <code>KFold</code> or <code>StratifiedKFold</code> strategies by default</p>
<p>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">ShuffleSplit</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
</code></pre></div>

<h3 style="color:blue;">📌 Types of Cross-Validation</h3>

<p>There are several types of cross validation techniques which are as follows:</p>
<p><strong>1. Holdout Validation</strong></p>
<p>In Holdout Validation we perform training on the 50% of the given dataset and rest 50% is used for the testing purpose. It's a simple and quick way to evaluate a model. The major drawback of this method is that we perform training on the 50% of the dataset, it may possible that the remaining 50% of the data contains some important information which we are leaving while training our model that can lead to higher bias.</p>
<p><strong>Process:</strong> Split dataset into training and test sets (commonly 70:30 or 80:20).</p>
<p><strong>Pros:</strong> Simple, fast.</p>
<p><strong>Cons:</strong> High variance (depends heavily on split).</p>
<p><strong>When to use:</strong> Very large datasets where k-fold isn’t necessary.</p>
<p><strong>2. LOOCV (Leave One Out Cross Validation)</strong></p>
<p>In this method we perform training on the whole dataset but leaves only one data-point of the available dataset and then iterates for each data-point. In LOOCV the model is trained on n−1 samples and tested on the one omitted sample repeating this process for each data point in the dataset. It has some advantages as well as disadvantages also.</p>
<ul>
<li>
<p>An advantage of using this method is that we make use of all data points and hence it is low bias.</p>
</li>
<li>
<p>The major drawback of this method is that it leads to higher variation in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation.</p>
</li>
<li>
<p>Another drawback is it takes a lot of execution time as it iterates over the number of data points we have.</p>
</li>
</ul>
<p><strong>Process:</strong> k = number of samples. Train on all except one sample, test on the one sample. Repeat for each sample.</p>
<p><strong>Pros:</strong> Almost unbiased performance estimate.</p>
<p><strong>Cons:</strong> Extremely computationally expensive for large datasets; can have high variance.</p>
<p><strong>3. Stratified Cross-Validation</strong></p>
<p>It is a technique used in machine learning to ensure that each fold of the cross-validation process maintains the same class distribution as the entire dataset. This is particularly important when dealing with imbalanced datasets where certain classes may be under represented. In this method:</p>
<ul>
<li>
<p>The dataset is divided into k folds while maintaining the proportion of classes in each fold.</p>
</li>
<li>
<p>During each iteration, one-fold is used for testing and the remaining folds are used for training.</p>
</li>
<li>
<p>The process is repeated k times with each fold serving as the test set exactly once.</p>
</li>
</ul>
<p>Stratified Cross-Validation is essential when dealing with classification problems where maintaining the balance of class distribution is crucial for the model to generalize well to unseen data.</p>
<p><strong>4. K-Fold Cross Validation</strong></p>
<p>In K-Fold Cross Validation we split the dataset into k number of subsets known as folds then we perform training on the all the subsets but leave one (k-1) subset for the evaluation of the trained model. In this method, we iterate k times with a different subset reserved for testing purpose each time.</p>
<p><strong>Note:</strong> <code>It is always suggested that the value of k should be 10 as the lower value of k takes towards validation and higher value of k leads to LOOCV method.</code></p>
<p><strong>Example of K Fold Cross Validation</strong></p>
<p>The diagram below shows an example of the training subsets and evaluation subsets generated in k-fold cross-validation. Here we have total 25 instances. In first iteration we use the first 20 percent of data for evaluation and the remaining 80 percent for training like [1-5] testing and [5-25] training while in the second iteration we use the second subset of 20 percent for evaluation and the remaining three subsets of the data for training like [5-10] testing and [1-5 and 10-25] training and so on.</p>
<p><img alt="alt text" src="../images/crossvalidation1.png" /></p>
<table>
<thead>
<tr>
<th>Iteration</th>
<th>Training Set Observations</th>
<th>Testing Set Observations</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>[5-24]</td>
<td>[0-4]</td>
</tr>
<tr>
<td>2</td>
<td>[0-4, 10-24]</td>
<td>[5-9]</td>
</tr>
<tr>
<td>3</td>
<td>[0-9, 15-24]</td>
<td>[10-14]</td>
</tr>
<tr>
<td>4</td>
<td>[0-14, 20-24]</td>
<td>[15-19]</td>
</tr>
<tr>
<td>5</td>
<td>[0-19]</td>
<td>[20-24]</td>
</tr>
</tbody>
</table>
<p>Each iteration uses different subsets for testing and training, ensuring that all data points are used for both training and testing.</p>
<p><strong>5. Time Series Cross-Validation (Rolling/Expanding Window)</strong></p>
<p><strong>Process:</strong></p>
<ul>
<li>
<p>Train on first chunk, validate on next chunk.</p>
</li>
<li>
<p>Move the window forward in time.</p>
</li>
</ul>
<p><strong>Pros:</strong> Preserves temporal order, avoids leakage from future.</p>
<p><strong>Cons:</strong> Less training data in early folds.</p>
<p><strong>Use case:</strong> Forecasting, stock prices, sensor data.</p>
<h3 style="color:blue;">📌 Comparison between K-Fold Cross-Validation and Hold Out Method</h3>

<p>K-Fold Cross-Validation and Hold Out Method are widely used technique and sometimes they are confusing so here is the quick comparison between them:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>K-Fold Cross-Validation</th>
<th>Hold-Out Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Definition</strong></td>
<td>The dataset is divided into 'k' subsets (folds). Each fold gets a turn to be the test set while the others are used for training.</td>
<td>The dataset is split into two sets: one for training and one for testing.</td>
</tr>
<tr>
<td><strong>Training Sets</strong></td>
<td>✅ The model is trained 'k' times, each time on a different training subset.</td>
<td>⚠️ The model is trained once on the training set.</td>
</tr>
<tr>
<td><strong>Testing Sets</strong></td>
<td>✅ The model is tested 'k' times, each time on a different test subset.</td>
<td>⚠️ The model is tested once on the test set.</td>
</tr>
<tr>
<td><strong>Bias</strong></td>
<td>✅ Less biased due to multiple splits and testing.</td>
<td>❌ Can have higher bias due to a single split.</td>
</tr>
<tr>
<td><strong>Variance</strong></td>
<td>✅ Lower variance, as it tests on multiple splits.</td>
<td>❌ Higher variance, as results depend on the single split.</td>
</tr>
<tr>
<td><strong>Computation Cost</strong></td>
<td>❌ High, as the model is trained and tested 'k' times.</td>
<td>✅ Low, as the model is trained and tested only once.</td>
</tr>
<tr>
<td><strong>Use in Model Selection</strong></td>
<td>✅ Better for tuning and evaluating model performance due to reduced bias.</td>
<td>❌ Less reliable for model selection, as it might give inconsistent results.</td>
</tr>
<tr>
<td><strong>Data Utilization</strong></td>
<td>✅ The entire dataset is used for both training and testing.</td>
<td>❌ Only a portion of the data is used for testing, so some data is not used for validation.</td>
</tr>
<tr>
<td><strong>Suitability for Small Datasets</strong></td>
<td>✅ Preferred for small datasets, as it maximizes data usage.</td>
<td>❌ Less ideal for small datasets, as a significant portion is held out for testing.</td>
</tr>
<tr>
<td><strong>Risk of Overfitting</strong></td>
<td>✅ Less prone to overfitting due to multiple training and testing cycles.</td>
<td>❌ Higher risk of overfitting as the model is trained on one set.</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">📌 Advantages and Disadvantages of Cross Validation</h3>

<p><strong>Advantages:</strong></p>
<ol>
<li>
<p><strong>Overcoming Overfitting:</strong> Cross validation helps to prevent overfitting by providing a more robust estimate of the model's performance on unseen data.</p>
</li>
<li>
<p><strong>Model Selection:</strong> Cross validation is used to compare different models and select the one that performs the best on average.</p>
</li>
<li>
<p><strong>Hyperparameter tuning:</strong> This is used to optimize the hyperparameters of a model such as the regularization parameter by selecting the values that result in the best performance on the validation set.</p>
</li>
<li>
<p><strong>Data Efficient:</strong> It allow the use of all the available data for both training and validation making it more data-efficient method compared to traditional validation techniques.</p>
</li>
</ol>
<p><strong>Disadvantages:</strong></p>
<ol>
<li>
<p><strong>Computationally Expensive:</strong> It can be computationally expensive especially when the number of folds is large or when the model is complex and requires a long time to train.</p>
</li>
<li>
<p><strong>Time-Consuming:</strong> It can be time-consuming especially when there are many hyperparameters to tune or when multiple models need to be compared.</p>
</li>
<li>
<p><strong>Bias-Variance Tradeoff:</strong> The choice of the number of folds in cross validation can impact the bias-variance tradeoff i.e too few folds may result in high bias while too many folds may result in high variance.</p>
</li>
</ol>
<h3 style="color:blue;">📌 Python implementation for k fold cross-validation</h3>

<p><strong>Step 1: Importing necessary libraries</strong></p>
<p><a href="https://www.geeksforgeeks.org/machine-learning/learning-model-building-scikit-learn-python-machine-learning-library/">import from scikit learn.</a></p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
</code></pre></div>

<p><strong>Step 2: Loading the dataset</strong></p>
<p>let's use the iris dataset which is a multi-class classification in-built dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="n">iris</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</code></pre></div>

<p><strong>Step 3: Creating SVM classifier</strong></p>
<p><a href="https://www.geeksforgeeks.org/machine-learning/kernel-trick-in-support-vector-classification/">SVC is a Support Vector Classification model from scikit-learn.</a></p>
<div class="codehilite"><pre><span></span><code>svm_classifier = SVC(kernel=&#39;linear&#39;)
</code></pre></div>

<p><strong>Step 4: Defining the number of folds for cross-validation</strong></p>
<p>Here we will be using 5 folds.</p>
<div class="codehilite"><pre><span></span><code>num_folds = 5
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
</code></pre></div>

<p><strong>Step 5: Performing k-fold cross-validation</strong></p>
<div class="codehilite"><pre><span></span><code>cross_val_results = cross_val_score(svm_classifier, X, y, cv=kf)
</code></pre></div>

<p><strong>Step 6: Evaluation metrics</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cross-Validation Results (Accuracy):&quot;</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">cross_val_results</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;  Fold {i}: {result * 100:.2f}%&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Mean Accuracy: {cross_val_results.mean()* 100:.2f}%&#39;</span><span class="p">)</span>
</code></pre></div>

<p><img alt="alt text" src="../images/crossvalidation2.png" /></p>
<p>The output shows the accuracy scores from each of the 5 folds in the K-fold cross-validation process. The mean accuracy is the average of these individual scores which is approximately 97.33% indicating the model's overall performance across all the folds.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="Classification.html" class="btn btn-neutral float-left" title="Classification"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="HyperparameterTuning.html" class="btn btn-neutral float-right" title="Hyperparameter Tuning">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="Classification.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="HyperparameterTuning.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
