<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Decision Tree Regression - AIML documents</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Decision Tree Regression";
        var mkdocs_page_input_path = "MachineLearning/SupervisedLearning/RegressionModels/DecisionTreeRegression.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Machine Learning</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../../Overview.html">Overview</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" >Supervised Learning</a>
    <ul class="current">
                <li class="toctree-l3"><a class="reference internal" href="../Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Classification.html">Classification</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../CrossValidation.html">Cross Validation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3 current"><a class="reference internal current" >Regression Models</a>
    <ul class="current">
                <li class="toctree-l4"><a class="reference internal" href="SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4 current"><a class="reference internal current" href="#">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Unsupervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../UnsupervisedLearning/overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../UnsupervisedLearning/Clustering.html">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../UnsupervisedLearning/Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Deep Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../../DeepLearning/Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../DeepLearning/Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../../DeepLearning/Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../DeepLearning/Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Natural Language Processing(NLP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../NLP/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../NLP/nlpdetails.html">NLP Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../index.html" class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">AIML</li>
          <li class="breadcrumb-item">Machine Learning</li>
          <li class="breadcrumb-item">Supervised Learning</li>
          <li class="breadcrumb-item">Regression Models</li>
      <li class="breadcrumb-item active">Decision Tree Regression</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 style="color:red;">✅ Decision Tree Regression</h2>

<h3 style="color:blue;">📌 What is Decision Tree Regression?</h3>

<p>A Decision Tree helps us to make decisions by mapping out different choices and their possible outcomes.It’s used in machine learning for tasks like classification and prediction.</p>
<p><img alt="alt text" src="../../images/DTR1.png" /></p>
<p><img alt="alt text" src="../../images/DTR2.png" /></p>
<p><img alt="alt text" src="../../images/DTR3.png" /></p>
<p><img alt="alt text" src="../../images/DTR4.png" /></p>
<p>A Decision Tree helps us make decisions by showing different options and how they are related. It has a tree-like structure that starts with one main question called the root node which represents the entire dataset. From there, the tree branches out into different possibilities based on features in the data.</p>
<ul>
<li>
<p><strong>Root Node:</strong> Starting point representing the whole dataset.</p>
</li>
<li>
<p><strong>Branches:</strong> Lines connecting nodes showing the flow from one decision to another.</p>
</li>
<li>
<p><strong>Internal Nodes:</strong> Points where decisions are made based on data features.</p>
</li>
<li>
<p><strong>Leaf Nodes:</strong> End points of the tree where the final decision or prediction is made.</p>
</li>
</ul>
<p><img alt="alt text" src="../../images/DTR5.png" /></p>
<p>A Decision Tree also helps with decision-making by showing possible outcomes clearly. By looking at the "branches" we can quickly compare options and figure out the best choice.</p>
<p>There are mainly two types of Decision Trees based on the target variable:</p>
<ol>
<li>
<p><strong>Classification Trees:</strong> Used for predicting categorical outcomes like spam or not spam. These trees split the data based on features to classify data into predefined categories.</p>
</li>
<li>
<p><strong>Regression Trees:</strong> Used for predicting continuous outcomes like predicting house prices. Instead of assigning categories, it provides numerical predictions based on the input features.</p>
</li>
</ol>
<h3 style="color:blue;">📌 How Decision Trees Work?</h3>

<ol>
<li><strong>1. Start with the Root Node:</strong> It begins with a main question at the root node which is derived from the dataset’s features.</li>
</ol>
<p><strong>2. Ask Yes/No Questions:</strong> From the root, the tree asks a series of yes/no questions to split the data into subsets based on specific attributes.</p>
<p><strong>3. Branching Based on Answers:</strong> Each question leads to different branches:</p>
<ul>
<li>
<p>If the answer is yes, the tree follows one path.</p>
</li>
<li>
<p>If the answer is no, the tree follows another path.</p>
</li>
</ul>
<p><strong>4. Continue Splitting:</strong> This branching continues through further decisions helps in reducing the data down step-by-step.</p>
<p><strong>5. Reach the Leaf Node:</strong> The process ends when there are no more useful questions to ask leading to the leaf node where the final decision or prediction is made.</p>
<p>Let’s look at a simple example to understand how it works. Imagine we need to decide whether to drink coffee based on the time of day and how tired we feel. The tree first checks the time:</p>
<p><strong>1. In the morning: It asks “Tired?”</strong></p>
<ul>
<li>
<p>If yes, the tree suggests drinking coffee.</p>
</li>
<li>
<p>If no, it says no coffee is needed.</p>
</li>
<li>
<p>In the afternoon: It asks again “Tired?”</p>
</li>
<li>
<p>If yes, it suggests drinking coffee.</p>
</li>
<li>
<p>If no, no coffee is needed.</p>
</li>
</ul>
<p><img alt="alt text" src="../../images/DTR6.png" /></p>
<h3 style="color:blue;">📌 Splitting Criteria in Decision Trees</h3>

<p>In a Decision Tree, the process of splitting data at each node is important. The splitting criteria finds the best feature to split the data on. Common splitting criteria include <strong>Gini Impurity</strong> and <strong>Entropy</strong>.</p>
<ul>
<li>
<p><strong>Gini Impurity:</strong> This criterion measures how "impure" a node is. The <strong>lower the Gini Impurity</strong> the better the feature splits the data into distinct categories.</p>
</li>
<li>
<p><strong>Entropy:</strong> This measures the amount of uncertainty or disorder in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.</p>
</li>
</ul>
<p>These criteria help decide which features are useful for making the best split at each decision point in the tree.</p>
<h3 style="color:blue;">📌 Gini Impurity and Entropy in Decision Tree</h3>

<p><strong>Gini Index</strong></p>
<ul>
<li>
<p>The <strong>Gini Index</strong> is the additional approach to dividing a decision tree.</p>
</li>
<li>
<p><strong>Purity</strong> and <strong>impurity</strong> in a junction are the primary focus of the Entropy and Information Gain framework.</p>
</li>
<li>
<p>The Gini Index, also known as Impurity, calculates the likelihood that somehow a randomly picked instance would be erroneously cataloged. </p>
</li>
</ul>
<p><strong>1. Gini Impurity</strong></p>
<p><img alt="alt text" src="../../images/DTR7.png" /></p>
<p><img alt="alt text" src="../../images/DTR8.png" /></p>
<p><strong>2. Entropy (Information Gain)</strong></p>
<p><img alt="alt text" src="../../images/DTR9.png" /></p>
<p><img alt="alt text" src="../../images/DTR10.png" /></p>
<p><strong>3. Comparison</strong></p>
<p><img alt="alt text" src="../../images/DTR11.png" /></p>
<h3 style="color:blue;">📌 Example Dataset: Weather & Play Tennis</h3>

<table>
<thead>
<tr>
<th>ID</th>
<th>Outlook</th>
<th>PlayTennis</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Sunny</td>
<td>No</td>
</tr>
<tr>
<td>2</td>
<td>Sunny</td>
<td>No</td>
</tr>
<tr>
<td>3</td>
<td>Overcast</td>
<td>Yes</td>
</tr>
<tr>
<td>4</td>
<td>Rain</td>
<td>Yes</td>
</tr>
<tr>
<td>5</td>
<td>Rain</td>
<td>Yes</td>
</tr>
<tr>
<td>6</td>
<td>Rain</td>
<td>No</td>
</tr>
<tr>
<td>7</td>
<td>Overcast</td>
<td>Yes</td>
</tr>
<tr>
<td>8</td>
<td>Sunny</td>
<td>No</td>
</tr>
<tr>
<td>9</td>
<td>Sunny</td>
<td>Yes</td>
</tr>
<tr>
<td>10</td>
<td>Rain</td>
<td>Yes</td>
</tr>
<tr>
<td>11</td>
<td>Sunny</td>
<td>Yes</td>
</tr>
<tr>
<td>12</td>
<td>Overcast</td>
<td>Yes</td>
</tr>
<tr>
<td>13</td>
<td>Overcast</td>
<td>Yes</td>
</tr>
<tr>
<td>14</td>
<td>Rain</td>
<td>No</td>
</tr>
</tbody>
</table>
<p><strong>Step 1 – Parent Node Calculation (Before Split)</strong></p>
<p><img alt="alt text" src="../../images/DTR12.png" /></p>
<p><strong>Step 2 – Split on "Outlook"</strong></p>
<p><img alt="alt text" src="../../images/DTR13.png" /></p>
<p><strong>Overcast Group</strong></p>
<p><img alt="alt text" src="../../images/DTR14.png" /></p>
<p><strong>Rain Group</strong></p>
<p><img alt="alt text" src="../../images/DTR15.png" /></p>
<p><strong>Step 3 – Weighted Average After Split</strong></p>
<p><img alt="alt text" src="../../images/DTR16.png" /></p>
<p><strong>Step 4 – Information Gain (for Entropy)</strong></p>
<p><img alt="alt text" src="../../images/DTR17.png" /></p>
<p><strong>Step 4 – Information Gain (for Entropy)</strong></p>
<p><img alt="alt text" src="../../images/DTR18.png" /></p>
<p><strong>✅ Summary Table</strong></p>
<table>
<thead>
<tr>
<th>Group</th>
<th>Yes</th>
<th>No</th>
<th>Gini</th>
<th>Entropy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parent</td>
<td>9</td>
<td>5</td>
<td>0.46</td>
<td>0.94</td>
</tr>
<tr>
<td>Sunny</td>
<td>2</td>
<td>3</td>
<td>0.48</td>
<td>0.971</td>
</tr>
<tr>
<td>Overcast</td>
<td>4</td>
<td>0</td>
<td>0.00</td>
<td>0.000</td>
</tr>
<tr>
<td>Rain</td>
<td>3</td>
<td>2</td>
<td>0.48</td>
<td>0.971</td>
</tr>
<tr>
<td><strong>Weighted Avg</strong></td>
<td>—</td>
<td>—</td>
<td>0.343</td>
<td>0.692</td>
</tr>
</tbody>
</table>
<p><img alt="alt text" src="../../images/DTR19.png" /></p>
<p><img alt="alt text" src="../../images/DTR20.png" /></p>
<p><strong>Entropy v/s Gini Impurity:</strong> Now we have learned about Gini Impurity and Entropy and how it actually works. Also, we have seen how we can calculate Gini Impurity/Entropy for a split/feature. But the major question that arises here is why do we need to have both methods for computation and which is better. </p>
<p><img alt="alt text" src="../../images/DTR21.png" /></p>
<p>The internal workings of both methods are similar, as they are used for computing the impurity of features after each split. <strong>However, Gini Impurity is generally more computationally efficient than entropy. The graph of entropy increases up to 1 and then starts decreasing, while Gini Impurity only goes up to 0.5 before decreasing, thus requiring less computational power. The range of entropy is from 0 to (log2C), whereas the range of Gini Impurity is from 0 to 0.5 (for binary classification)</strong>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Note</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">range</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Gini</span><span class="w"> </span><span class="n">Impurity</span><span class="w"> </span><span class="o">[</span><span class="mi">0</span><span class="o">,</span><span class="mf">0.5</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">classification</span><span class="w"> </span><span class="n">problem</span><span class="o">.</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">multi</span><span class="o">-</span><span class="kd">class</span><span class="w"> </span><span class="n">classification</span><span class="o">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">calculate</span><span class="w"> </span><span class="n">range</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="k">this</span><span class="w"> </span><span class="n">method</span><span class="o">:</span>
</code></pre></div>

<p><img alt="alt text" src="../../images/DTR22.png" /></p>
<p>However, the main reason for Gini Impurity's computational advantage is that it does not involve logarithmic functions, which are more computationally intensive. Therefore, Gini Impurity is often considered more efficient compared to entropy for selecting the best features.</p>
<p><strong>Pruning in Decision Trees</strong></p>
<ul>
<li>
<p>Pruning is an important technique used to prevent overfitting in Decision Trees. Overfitting occurs when a tree becomes too deep and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.</p>
</li>
<li>
<p>This technique reduces the complexity of the tree by removing branches that have little predictive power. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.</p>
</li>
<li>
<p>It is useful when a Decision Tree is too deep and starts to capture noise in the data.</p>
</li>
</ul>
<p><strong>Advantages of Decision Trees</strong></p>
<ul>
<li>
<p><strong>Easy to Understand:</strong> Decision Trees are visual which makes it easy to follow the decision-making process.</p>
</li>
<li>
<p><strong>Versatility:</strong> Can be used for both classification and regression problems.</p>
</li>
<li>
<p><strong>No Need for Feature Scaling:</strong> Unlike many machine learning models, it don’t require us to scale or normalize our data.</p>
</li>
<li>
<p><strong>Handles Non-linear Relationships:</strong> It capture complex, non-linear relationships between features and outcomes effectively.</p>
</li>
<li>
<p><strong>Interpretability:</strong> The tree structure is easy to interpret helps in allowing users to understand the reasoning behind each decision.</p>
</li>
<li>
<p><strong>Handles Missing Data:</strong> It can handle missing values by using strategies like assigning the most common value or ignoring missing data during splits.</p>
</li>
</ul>
<p><strong>Disadvantages of Decision Trees</strong></p>
<ul>
<li>
<p><strong>Overfitting:</strong> They can overfit the training data if they are too deep which means they memorize the data instead of learning general patterns. This leads to poor performance on unseen data.</p>
</li>
<li>
<p><strong>Instability:</strong> It can be unstable which means that small changes in the data may lead to significant differences in the tree structure and predictions.</p>
</li>
<li>
<p><strong>Bias towards Features with Many Categories:</strong> It can become biased toward features with many distinct values which focuses too much on them and potentially missing other important features which can reduce prediction accuracy.</p>
</li>
<li>
<p><strong>Difficulty in Capturing Complex Interactions:</strong> Decision Trees may struggle to capture complex interactions between features which helps in making them less effective for certain types of data.</p>
</li>
<li>
<p><strong>Computationally Expensive for Large Datasets:</strong> For large datasets, building and pruning a Decision Tree can be computationally intensive, especially as the tree depth increases.</p>
</li>
</ul>
<p><strong>Applications of Decision Trees</strong></p>
<p>Decision Trees are used across various fields due to their simplicity, interpretability and versatility lets see some key applications:</p>
<ol>
<li>
<p><strong>Loan Approval in Banking:</strong> Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.</p>
</li>
<li>
<p><strong>Medical Diagnosis:</strong> In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.</p>
</li>
<li>
<p><strong>Predicting Exam Results in Education:</strong> Educational institutions use to predict whether a student will pass or fail based on factors like attendance, study time and past grades. This helps teachers identify at-risk students and offer targeted support.</p>
</li>
<li>
<p><strong>Customer Churn Prediction:</strong> Companies use Decision Trees to predict whether a customer will leave or stay based on behavior patterns, purchase history, and interactions. This allows businesses to take proactive steps to retain customers.</p>
</li>
<li>
<p><strong>Fraud Detection:</strong> In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud. By analyzing past transaction data and patterns, Decision Trees can identify suspicious activities and flag them for further investigation.</p>
</li>
</ol>
<p><strong>Example:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="kp">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Decision Tree Regression Implementation ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 1. LOAD AND EXPLORE DATASET</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. Loading California Housing Dataset...&quot;</span><span class="p">)</span>
<span class="c1"># Load the California housing dataset - perfect for regression</span>
<span class="n">california_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">california_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">california_housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">california_housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MedHouseValue&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target variable range: $</span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="kp">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> - $</span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="kp">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (in hundreds of thousands)&quot;</span><span class="p">)</span>

<span class="c1"># Manually define feature descriptions to avoid parsing issues</span>
<span class="n">feature_info</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;MedInc&#39;</span><span class="p">:</span> <span class="s1">&#39;Median income in block group (in tens of thousands of USD)&#39;</span><span class="p">,</span>
    <span class="s1">&#39;HouseAge&#39;</span><span class="p">:</span> <span class="s1">&#39;Median house age in years&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AveRooms&#39;</span><span class="p">:</span> <span class="s1">&#39;Average number of rooms per household&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AveBedrms&#39;</span><span class="p">:</span> <span class="s1">&#39;Average number of bedrooms per household&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Population&#39;</span><span class="p">:</span> <span class="s1">&#39;Population of the block group&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AveOccup&#39;</span><span class="p">:</span> <span class="s1">&#39;Average number of household members&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Latitude&#39;</span><span class="p">:</span> <span class="s1">&#39;Block group latitude&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Longitude&#39;</span><span class="p">:</span> <span class="s1">&#39;Block group longitude&#39;</span>
<span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Feature descriptions:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">description</span> <span class="ow">in</span> <span class="n">feature_info</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># Display basic statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dataset Info:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="kp">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Target variable statistics:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="kp">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># 2. DATA PREPROCESSING</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. Data Preprocessing...&quot;</span><span class="p">)</span>

<span class="c1"># Check for missing values</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing values in features: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="kp">sum</span><span class="p">()</span><span class="o">.</span><span class="kp">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing values in target: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="kp">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Feature correlation analysis</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Feature correlation with target:&quot;</span><span class="p">)</span>
<span class="n">correlations</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corrwith</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">correlations</span><span class="o">.</span><span class="kp">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># 3. TRAIN-TEST SPLIT</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. Splitting data into train and test sets...&quot;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="kp">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training set size: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test set size: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>

<span class="c1"># 4. MODEL TRAINING WITH HYPERPARAMETER TUNING</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. Training Decision Tree Regressor with Hyperparameter Tuning...&quot;</span><span class="p">)</span>

<span class="c1"># Define hyperparameter grid for optimization</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sqrt&#39;</span><span class="p">,</span> <span class="s1">&#39;log2&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="c1"># &#39;auto&#39; was removed in recent sklearn versions</span>
<span class="p">}</span>

<span class="c1"># Initialize the regressor</span>
<span class="n">dt_regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Perform grid search with cross-validation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Performing Grid Search with 5-fold Cross Validation...&quot;</span><span class="p">)</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">dt_regressor</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="c1"># Fit the grid search</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Get the best model</span>
<span class="n">best_dt</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best parameters: </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best cross-validation score (negative MSE): </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Train a simple model for comparison</span>
<span class="n">simple_dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">simple_dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 5. MODEL EVALUATION</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. Model Evaluation...&quot;</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_train_pred_best</span> <span class="o">=</span> <span class="n">best_dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_pred_best</span> <span class="o">=</span> <span class="n">best_dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_train_pred_simple</span> <span class="o">=</span> <span class="n">simple_dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_pred_simple</span> <span class="o">=</span> <span class="n">simple_dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate metrics for both models</span>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">dataset_type</span><span class="p">):</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> - </span><span class="si">{</span><span class="n">dataset_type</span><span class="si">}</span><span class="s2"> Set Metrics:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Root Mean Squared Error (RMSE): </span><span class="si">{</span><span class="n">rmse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R² Score: </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">r2</span>

<span class="c1"># Evaluate both models</span>
<span class="n">calculate_metrics</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_best</span><span class="p">,</span> <span class="s2">&quot;Best Tuned Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">calculate_metrics</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_best</span><span class="p">,</span> <span class="s2">&quot;Best Tuned Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">calculate_metrics</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_simple</span><span class="p">,</span> <span class="s2">&quot;Simple Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">calculate_metrics</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_simple</span><span class="p">,</span> <span class="s2">&quot;Simple Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Test&quot;</span><span class="p">)</span>

<span class="c1"># Cross-validation scores</span>
<span class="n">cv_scores_best</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">best_dt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
<span class="n">cv_scores_simple</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">simple_dt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cross-validation RMSE (Best Model): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">cv_scores_best</span><span class="o">.</span><span class="kp">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (+/- </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">cv_scores_best</span><span class="o">.</span><span class="kp">std</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cross-validation RMSE (Simple Model): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">cv_scores_simple</span><span class="o">.</span><span class="kp">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (+/- </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">cv_scores_simple</span><span class="o">.</span><span class="kp">std</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># 6. FEATURE IMPORTANCE ANALYSIS</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. Feature Importance Analysis...&quot;</span><span class="p">)</span>
<span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="s1">&#39;importance_best&#39;</span><span class="p">:</span> <span class="n">best_dt</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span>
    <span class="s1">&#39;importance_simple&#39;</span><span class="p">:</span> <span class="n">simple_dt</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="p">})</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;importance_best&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top 5 Most Important Features (Best Model):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># 7. VISUALIZATIONS</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">7. Generating Visualizations...&quot;</span><span class="p">)</span>

<span class="c1"># Set up the plotting style</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;default&#39;</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

<span class="c1"># Plot 1: Actual vs Predicted (Best Model)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_best</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">y_test</span><span class="o">.</span><span class="kp">min</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="kp">max</span><span class="p">()],</span> <span class="p">[</span><span class="n">y_test</span><span class="o">.</span><span class="kp">min</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="kp">max</span><span class="p">()],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Actual Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted Values (Best Model)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 2: Residuals Plot (Best Model)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y_test</span> <span class="o">-</span> <span class="n">y_test_pred_best</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test_pred_best</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Residuals Plot (Best Model)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 3: Feature Importance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">feature_importance</span><span class="p">[</span><span class="s1">&#39;feature&#39;</span><span class="p">],</span> <span class="n">feature_importance</span><span class="p">[</span><span class="s1">&#39;importance_best&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Importance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Feature Importance (Best Model)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>

<span class="c1"># Plot 4: Prediction Distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_test_pred_best</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;House Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution: Actual vs Predicted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 5: Model Comparison (MSE)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Simple Model</span><span class="se">\n</span><span class="s1">(max_depth=5)&#39;</span><span class="p">,</span> <span class="s1">&#39;Best Tuned Model&#39;</span><span class="p">]</span>
<span class="n">train_mse</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_simple</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_best</span><span class="p">)]</span>
<span class="n">test_mse</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_simple</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_best</span><span class="p">)]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">))</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">train_mse</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train MSE&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test MSE&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Comparison: MSE&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 6: Learning Curve (Tree Depth)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">depths</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
    <span class="n">dt_temp</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">dt_temp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_scores</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dt_temp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
    <span class="n">test_scores</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_temp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">depths</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training MSE&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">depths</span><span class="p">,</span> <span class="n">test_scores</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test MSE&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Tree Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Curve: Effect of Tree Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 7: Correlation Heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="kp">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.2f&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Feature Correlation Heatmap&#39;</span><span class="p">)</span>

<span class="c1"># Plot 8: Cross-validation Scores</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Fold&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
    <span class="s1">&#39;Best Model&#39;</span><span class="p">:</span> <span class="o">-</span><span class="n">cv_scores_best</span><span class="p">,</span>
    <span class="s1">&#39;Simple Model&#39;</span><span class="p">:</span> <span class="o">-</span><span class="n">cv_scores_simple</span>
<span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;Fold&#39;</span><span class="p">],</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;Best Model&#39;</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Best Model&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;Fold&#39;</span><span class="p">],</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;Simple Model&#39;</span><span class="p">],</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Simple Model&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;CV Fold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cross-Validation Performance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 9: Tree Visualization (Simple Model)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">simple_dt</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decision Tree Structure (Simplified View)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 8. DETAILED ANALYSIS AND INSIGHTS</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">8. Model Analysis and Insights...&quot;</span><span class="p">)</span>

<span class="c1"># Analyze overfitting</span>
<span class="n">train_r2_best</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_best</span><span class="p">)</span>
<span class="n">test_r2_best</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_best</span><span class="p">)</span>
<span class="n">train_r2_simple</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred_simple</span><span class="p">)</span>
<span class="n">test_r2_simple</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred_simple</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Overfitting Analysis:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best Model - Train R²: </span><span class="si">{</span><span class="n">train_r2_best</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test R²: </span><span class="si">{</span><span class="n">test_r2_best</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Simple Model - Train R²: </span><span class="si">{</span><span class="n">train_r2_simple</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test R²: </span><span class="si">{</span><span class="n">test_r2_simple</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">overfitting_best</span> <span class="o">=</span> <span class="n">train_r2_best</span> <span class="o">-</span> <span class="n">test_r2_best</span>
<span class="n">overfitting_simple</span> <span class="o">=</span> <span class="n">train_r2_simple</span> <span class="o">-</span> <span class="n">test_r2_simple</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Overfitting Gap (Best): </span><span class="si">{</span><span class="n">overfitting_best</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Overfitting Gap (Simple): </span><span class="si">{</span><span class="n">overfitting_simple</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Model complexity analysis</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Model Complexity:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best Model - Tree Depth: </span><span class="si">{</span><span class="n">best_dt</span><span class="o">.</span><span class="n">get_depth</span><span class="p">()</span><span class="si">}</span><span class="s2">, Leaves: </span><span class="si">{</span><span class="n">best_dt</span><span class="o">.</span><span class="n">get_n_leaves</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Simple Model - Tree Depth: </span><span class="si">{</span><span class="n">simple_dt</span><span class="o">.</span><span class="n">get_depth</span><span class="p">()</span><span class="si">}</span><span class="s2">, Leaves: </span><span class="si">{</span><span class="n">simple_dt</span><span class="o">.</span><span class="n">get_n_leaves</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Feature importance insights</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Key Insights:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;• Most important feature: </span><span class="si">{</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;importance_best&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;• Least important feature: </span><span class="si">{</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;importance_best&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Performance summary</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final Model Performance Summary:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;• Best model RMSE on test set: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test_pred_best</span><span class="p">))</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (hundreds of thousands)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;• This represents an average prediction error of ~$</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test_pred_best</span><span class="p">))</span><span class="o">*</span><span class="mi">100000</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;• Model explains </span><span class="si">{</span><span class="n">test_r2_best</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> of the variance in house prices&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Analysis Complete ===&quot;</span><span class="p">)</span>
</code></pre></div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="SupportVectorRegression.html" class="btn btn-neutral float-left" title="Support Vector Regression (SVR)"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="RandomForestRegression.html" class="btn btn-neutral float-right" title="Random Forest Regression">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="SupportVectorRegression.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="RandomForestRegression.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
