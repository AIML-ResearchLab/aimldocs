<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>NLP Details - AIML documents</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "NLP Details";
        var mkdocs_page_input_path = "NLP/nlpdetails.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Machine Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../MachineLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Supervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Classification.html">Classification</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/CrossValidation.html">Cross Validation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/DecisionTreeRegression.html">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Unsupervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/Clustering.html">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Deep Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../DeepLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../DeepLearning/Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../DeepLearning/Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../DeepLearning/Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../DeepLearning/Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../DeepLearning/Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Natural Language Processing(NLP)</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="overview.html">Overview</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="#">NLP Details</a>
    <ul class="current">
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">AIML</li>
          <li class="breadcrumb-item">Natural Language Processing(NLP)</li>
      <li class="breadcrumb-item active">NLP Details</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 style="color:red;">✅ Natural Language Processing (NLP)</h2>

<p><img alt="alt text" src="images/nlp19.png" /></p>
<h3 style="color:blue;">📌 Architecture Details Explanation </h3>

<h1 id="end-to-end-nlp-text-classification-pipeline">End-to-End NLP Text Classification Pipeline<a class="headerlink" href="#end-to-end-nlp-text-classification-pipeline" title="Permanent link">#</a></h1>
<p><strong>Project</strong>: End-to-End NLP Text Classification (Preprocessing → Training → Deployment)</p>
<p><strong>Purpose</strong>: A production-oriented pipeline for building, evaluating, and deploying text classification models. This repository includes preprocessing pipelines, model training with cross-validation and hyperparameter tuning, model serialization, and a simple inference API.</p>
<hr />
<h3 style="color:blue;">📌 Table of Contents </h3>

<ul>
<li><a href="#architecture-overview">Architecture Overview</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#folder-structure">Folder Structure</a></li>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#data-preprocessing-steps">Data Preprocessing Steps</a></li>
<li><a href="#model-training--evaluation">Model Training &amp; Evaluation</a></li>
<li><a href="#deployment--inference">Deployment &amp; Inference</a></li>
<li><a href="#model-monitoring--mlops">Model Monitoring &amp; MLOps</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#license">License</a></li>
</ul>
<hr />
<h3 style="color:blue;">📌 Architecture Overview </h3>

<p>The pipeline consists of:
1. <strong>Data ingestion &amp; EDA</strong> — ingest raw text, visualize (word clouds), and inspect class balance.<br />
2. <strong>Preprocessing</strong> — lowercasing, URL/email removal, tokenization, punctuation/digit handling, stopword removal, lemmatization/stemming.<br />
3. <strong>Feature extraction</strong> — TF-IDF/BoW with n-gram support or contextual embeddings (BERT).<br />
4. <strong>Model training</strong> — classifiers with StratifiedKFold CV and hyperparameter search.<br />
5. <strong>Evaluation</strong> — accuracy, precision, recall, F1 (macro/weighted), ROC-AUC, PR-AUC, MCC.<br />
6. <strong>Deployment</strong> — packaged pipeline served via FastAPI (ensures same preprocessing).<br />
7. <strong>Monitoring</strong> — model versioning (MLflow/DVC), drift detection, performance tracking.</p>
<hr />
<h3 style="color:blue;">📌 Features </h3>

<ul>
<li>Reproducible preprocessing pipeline</li>
<li>TF-IDF and n-gram support</li>
<li>Stratified K-Fold cross-validation</li>
<li>Grid/Randomized hyperparameter search</li>
<li>Model serialization and simple API for inference</li>
<li>Guidance for monitoring and model versioning</li>
</ul>
<hr />
<h3 style="color:blue;">📌 Folder Structure </h3>

<div class="codehilite"><pre><span></span><code>.
├──<span class="w"> </span><span class="nv">data</span><span class="o">/</span>
│<span class="w">   </span>├──<span class="w"> </span><span class="nv">raw</span><span class="o">/</span><span class="w">                 </span>#<span class="w"> </span><span class="nv">raw</span><span class="w"> </span><span class="nv">datasets</span><span class="w"> </span><span class="ss">(</span><span class="k">do</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">edit</span><span class="ss">)</span>
│<span class="w">   </span>└──<span class="w"> </span><span class="nv">processed</span><span class="o">/</span><span class="w">           </span>#<span class="w"> </span><span class="nv">processed</span><span class="w"> </span><span class="nv">datasets</span>
├──<span class="w"> </span><span class="nv">notebooks</span><span class="o">/</span><span class="w">               </span>#<span class="w"> </span><span class="nv">EDA</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">experiments</span>
├──<span class="w"> </span><span class="nv">src</span><span class="o">/</span>
│<span class="w">   </span>├──<span class="w"> </span><span class="nv">preprocess</span>.<span class="nv">py</span><span class="w">        </span>#<span class="w"> </span><span class="nv">cleaning</span>,<span class="w"> </span><span class="nv">tokenizer</span>,<span class="w"> </span><span class="nv">lemmatizer</span>
│<span class="w">   </span>├──<span class="w"> </span><span class="nv">features</span>.<span class="nv">py</span><span class="w">          </span>#<span class="w"> </span><span class="nv">vectorizers</span>,<span class="w"> </span><span class="nv">feature</span><span class="w"> </span><span class="nv">selection</span>
│<span class="w">   </span>├──<span class="w"> </span><span class="nv">models</span>.<span class="nv">py</span><span class="w">            </span>#<span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">training</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">CV</span>
│<span class="w">   </span>└──<span class="w"> </span><span class="nv">serve</span>.<span class="nv">py</span><span class="w">             </span>#<span class="w"> </span><span class="nv">FastAPI</span><span class="w"> </span><span class="nv">app</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">inference</span>
├──<span class="w"> </span><span class="nv">models</span><span class="o">/</span><span class="w">                  </span>#<span class="w"> </span><span class="nv">saved</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">artifacts</span><span class="w"> </span><span class="ss">(</span>.<span class="nv">pkl</span>,<span class="w"> </span><span class="nv">ONNX</span><span class="ss">)</span>
├──<span class="w"> </span><span class="nv">requirements</span>.<span class="nv">txt</span>
├──<span class="w"> </span><span class="nv">Dockerfile</span><span class="w">               </span>#<span class="w"> </span><span class="nv">optional</span><span class="w"> </span><span class="nv">containerization</span>
├──<span class="w"> </span><span class="nv">README</span>.<span class="nv">md</span>
└──<span class="w"> </span><span class="nv">tests</span><span class="o">/</span><span class="w">                   </span>#<span class="w"> </span><span class="nv">unit</span><span class="w"> </span><span class="nv">tests</span>
</code></pre></div>

<hr />
<h3 style="color:blue;">📌 Requirements </h3>

<p>Create a virtual environment and install dependencies:</p>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</code></pre></div>

<p><code>requirements.txt</code> should include:</p>
<div class="codehilite"><pre><span></span><code>scikit-learn
pandas
numpy
nltk
joblib
fastapi
uvicorn
wordcloud
matplotlib
imblearn
xgboost
lightgbm
shap
mlflow
</code></pre></div>

<hr />
<h3 style="color:blue;">📌 Quickstart </h3>

<ol>
<li><strong>Prepare data</strong>: put <code>train.csv</code> into <code>data/raw/</code> with columns <code>id</code>, <code>text</code>, <code>label</code>.</li>
<li><strong>Run preprocessing + feature build</strong>:</li>
</ol>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>src/preprocess.py<span class="w"> </span>--input<span class="w"> </span>data/raw/train.csv<span class="w"> </span>--output<span class="w"> </span>data/processed/train_clean.csv
</code></pre></div>

<ol>
<li><strong>Train model</strong>:</li>
</ol>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>src/models.py<span class="w"> </span>--train<span class="w"> </span>data/processed/train_clean.csv<span class="w"> </span>--model-out<span class="w"> </span>models/best_model.pkl
</code></pre></div>

<ol>
<li><strong>Serve model (local)</strong>:</li>
</ol>
<div class="codehilite"><pre><span></span><code>uvicorn<span class="w"> </span>src.serve:app<span class="w"> </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span>
<span class="c1"># POST to http://localhost:8000/predict with {&quot;text&quot;: &quot;your text here&quot;}</span>
</code></pre></div>

<hr />
<h3 style="color:blue;">📌 Data Preprocessing Steps </h3>

<ol>
<li>Lowercase</li>
<li>Remove URLs, emails, mentions</li>
<li>Remove non-alphanumeric characters (configurable)</li>
<li>Tokenize</li>
<li>Remove stopwords (configurable language)</li>
<li>Lemmatize (preferred) or stem</li>
<li>Optional: numeric/emoji special handling, class balancing (SMOTE for embeddings)</li>
<li>Save processed data to <code>data/processed/</code></li>
</ol>
<hr />
<h3 style="color:blue;">📌 Model Training & Evaluation </h3>

<ul>
<li>Use <code>StratifiedKFold(n_splits=5, shuffle=True)</code> to split data.</li>
<li>Use <code>Pipeline</code> from <code>sklearn</code> to chain preprocess → vectorize → classifier.</li>
<li>Use <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> with scoring <code>f1_weighted</code> (or domain-specific metric).</li>
<li>Log runs and artifacts to MLflow:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">mlflow</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic_regression&#39;</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">)</span>
<span class="n">mlflow</span><span class="o">.</span><span class="n">end_run</span><span class="p">()</span>
</code></pre></div>

<hr />
<h3 style="color:blue;">📌 Deployment & Inference </h3>

<ul>
<li>Ensure the <strong>same pipeline</strong> used in training is saved and served.</li>
<li>Example using FastAPI: <code>src/serve.py</code> loads <code>models/best_model.pkl</code> and exposes <code>/predict</code>.</li>
<li>Consider containerizing:</li>
</ul>
<div class="codehilite"><pre><span></span><code>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>nlp-classifier:latest<span class="w"> </span>.
docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span><span class="m">8000</span>:8000<span class="w"> </span>nlp-classifier:latest
</code></pre></div>

<hr />
<h3 style="color:blue;">📌 Model Monitoring & MLOps </h3>

<ul>
<li>Track model versions and datasets with MLflow / DVC.</li>
<li>Monitor:</li>
<li>Prediction distribution (class drift)</li>
<li>Input feature drift</li>
<li>Model performance on a labeled validation set</li>
<li>Set alerts for drift thresholds; trigger retraining pipeline.</li>
</ul>
<hr />
<h3 style="color:blue;">📌 Best Practices </h3>

<ul>
<li>Use Pipelines to avoid leakage.</li>
<li>Keep preprocessing deterministic and versioned.</li>
<li>For heavy models, choose batch inference for non-critical latency tasks.</li>
<li>Use feature selection if TF-IDF dims explode.</li>
<li>Use SHAP for model explainability.</li>
</ul>
<h3 style="color:blue;">📌 Text Normalization</h3>
<p>Steps Required for Text normalization.</p>
<ul>
<li>
<p>Input text String</p>
</li>
<li>
<p>Convert all letters of the string to one case(either lower or upper case)</p>
</li>
<li>
<p>If numbers are essential to convert to words else remove all numbers</p>
</li>
<li>
<p>Remove punctuations, other formalities of grammar</p>
</li>
<li>
<p>Remove white spaces</p>
</li>
<li>
<p>Remove stop words</p>
</li>
<li>
<p>And any other computations</p>
</li>
</ul>
<p>Text normalization with above-mentioned steps, every step can be done in some ways. So we will discuss each and everything in this whole process.</p>
<p><strong>Text String</strong></p>
<div class="codehilite"><pre><span></span><code>#<span class="w"> </span><span class="nv">input</span><span class="w"> </span><span class="nv">string</span><span class="w"> </span>
<span class="nv">string</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2&#39;s end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).&quot;</span>
<span class="nv">print</span><span class="ss">(</span><span class="nv">string</span><span class="ss">)</span>
</code></pre></div>

<p><strong>Case Conversion (Lower Case)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> convert to lower case
lower_string = string.lower()
print(lower_string)
</code></pre></div>

<p><strong>Removing Numbers</strong>
Remove numbers if they're not relevant to your analyses. Usually, regular expressions are used to remove numbers.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># import regex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="c1"># input string </span>
<span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2&#39;s end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).&quot;</span>

<span class="c1"># convert to lower case</span>
<span class="n">lower_string</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># remove numbers</span>
<span class="n">no_number_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">lower_string</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">no_number_string</span><span class="p">)</span>
</code></pre></div>

<p><strong>Removing punctuation</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># import regex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="c1"># input string </span>
<span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2&#39;s end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).&quot;</span>

<span class="c1"># convert to lower case</span>
<span class="n">lower_string</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># remove numbers</span>
<span class="n">no_number_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">lower_string</span><span class="p">)</span>

<span class="c1"># remove all punctuation except words and space</span>
<span class="n">no_punc_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">no_number_string</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">no_punc_string</span><span class="p">)</span>
</code></pre></div>

<p><strong>Removing White space</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># import regex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="c1"># input string </span>
<span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2&#39;s end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).&quot;</span>

<span class="c1"># convert to lower case</span>
<span class="n">lower_string</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># remove numbers</span>
<span class="n">no_number_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">lower_string</span><span class="p">)</span>

<span class="c1"># remove all punctuation except words and space</span>
<span class="n">no_punc_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">no_number_string</span><span class="p">)</span> 

<span class="c1"># remove white spaces</span>
<span class="n">no_wspace_string</span> <span class="o">=</span> <span class="n">no_punc_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">no_wspace_string</span><span class="p">)</span>
</code></pre></div>

<p><strong>Removing Stop Words</strong></p>
<p>Stop words” are the foremost common words during a language like “the”, “a”, “on”, “is”, “all”. These words don't carry important meaning and are usually faraway from texts. It is possible to get rid of stop words using tongue Toolkit (NLTK), a set of libraries and programs for symbolic and statistical tongue processing.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># download stopwords</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>

<span class="c1"># import nltk for stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>

<span class="c1"># assign string</span>
<span class="n">no_wspace_string</span><span class="o">=</span><span class="s1">&#39;python  released in  was a major revision of the language that is not completely backward compatible and much python  code does not run unmodified on python  with python s endoflife only python x and later are supported with older versions still supporting eg windows  and old installers not restricted to bit windows&#39;</span>

<span class="c1"># convert string to list of words</span>
<span class="n">lst_string</span> <span class="o">=</span> <span class="p">[</span><span class="n">no_wspace_string</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lst_string</span><span class="p">)</span>

<span class="c1"># remove stopwords</span>
<span class="n">no_stpwords_string</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst_string</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
        <span class="n">no_stpwords_string</span> <span class="o">+=</span> <span class="n">i</span><span class="o">+</span><span class="s1">&#39; &#39;</span>

<span class="c1"># removing last space</span>
<span class="n">no_stpwords_string</span> <span class="o">=</span> <span class="n">no_stpwords_string</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">no_stpwords_string</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># import regex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="c1"># download stopwords</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>

<span class="c1"># import nltk for stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>


<span class="c1"># input string </span>
<span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2&#39;s end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).&quot;</span>

<span class="c1"># convert to lower case</span>
<span class="n">lower_string</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># remove numbers</span>
<span class="n">no_number_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">lower_string</span><span class="p">)</span>

<span class="c1"># remove all punctuation except words and space</span>
<span class="n">no_punc_string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">no_number_string</span><span class="p">)</span> 

<span class="c1"># remove white spaces</span>
<span class="n">no_wspace_string</span> <span class="o">=</span> <span class="n">no_punc_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="n">no_wspace_string</span>

<span class="c1"># convert string to list of words</span>
<span class="n">lst_string</span> <span class="o">=</span> <span class="p">[</span><span class="n">no_wspace_string</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lst_string</span><span class="p">)</span>

<span class="c1"># remove stopwords</span>
<span class="n">no_stpwords_string</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst_string</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
        <span class="n">no_stpwords_string</span> <span class="o">+=</span> <span class="n">i</span><span class="o">+</span><span class="s1">&#39; &#39;</span>

<span class="c1"># removing last space</span>
<span class="n">no_stpwords_string</span> <span class="o">=</span> <span class="n">no_stpwords_string</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">no_stpwords_string</span><span class="p">)</span>
</code></pre></div>

<h3 style="color:blue;">📌 Tokenization</h3>
<p>Tokenization is a process of splitting text into smaller units called tokens.</p>
<p>Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a stream of text into smaller units called tokens. These tokens can range from individual characters to full words or phrases, Based on how detailed it needs to be. By converting text into these manageable chunks, machines can more effectively analyze and understand human language.</p>
<p><img alt="alt text" src="images/nlp2.png" /></p>
<p><img alt="alt text" src="images/nlp3.png" /></p>
<p><img alt="alt text" src="images/nlp4.png" /></p>
<h3 style="color:blue;">📌 Types of Tokenization</h3>

<p><img alt="alt text" src="images/nlp5.png" /></p>
<p><img alt="alt text" src="images/nlp6.png" /></p>
<ul>
<li>
<p><strong>Word Tokenization:</strong>This is the most common method where text is divided into individual words. It works well for languages with clear word boundaries, like English.</p>
</li>
<li>
<p><strong>Character Tokenization:</strong>In this method, text is split into individual characters. This is particularly useful for languages without clear word boundaries or for tasks that require a detailed analysis, such as spelling correction.</p>
</li>
<li>
<p><strong>Subword Tokenization:</strong>Sub-word tokenization strikes a balance between word and character tokenization by breaking down text into units that are larger than a single character but smaller than a full word.</p>
</li>
<li>
<p><strong>SentenceTokenization:</strong> Sentence tokenization is also a common technique used to make a division of paragraphs or large set of sentences into separated sentences as tokens.</p>
</li>
<li>
<p><strong>N-gram Tokenization</strong>N-gram tokenization splits words into fixed-sized chunks (size = n) of data</p>
</li>
</ul>
<h3 style="color:blue;">📌 Tokenization Challenges</h3>

<p><img alt="alt text" src="images/nlp7.png" /></p>
<p><strong>Implementing Tokenization</strong></p>
<ul>
<li>
<p>NLTK (Natural Language Toolkit)</p>
</li>
<li>
<p>SpaCy</p>
</li>
<li>
<p>BERT Tokenizer</p>
</li>
<li>
<p>Byte-Pair Encoding (BPE)</p>
</li>
<li>
<p>Sentence Piece</p>
</li>
</ul>
<h3 style="color:blue;">📌 Lemmatization</h3>
<p>Lemmatization reduces words to their base or root form.</p>
<p>Lemmatization is an important text pre-processing technique in Natural Language Processing (NLP) that reduces words to their base form known as a "lemma." For example, the lemma of "running" is "run" and "better" becomes "good." Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word. This makes lemmatization more accurate as it avoids generating non-dictionary words.</p>
<p><img alt="alt text" src="images/nlp8.png" /></p>
<p>It is used for:</p>
<ul>
<li>
<p><strong>Improves accuracy:</strong> It ensures words with similar meanings like "running" and "ran" are treated as the same.</p>
</li>
<li>
<p><strong>Reduced Data Redundancy:</strong> By reducing words to their base forms, it reduces redundancy in the dataset. This leads to smaller datasets which makes it easier to handle and process large amounts of text for analysis or training machine learning models.</p>
</li>
<li>
<p><strong>Better NLP Model Performance:</strong> By treating all similar word as same, it improves the performance of NLP models by making text more consistent. For example, treating "running," "ran" and "runs" as the same word improves the model's understanding of context and meaning.</p>
</li>
</ul>
<h3 style="color:blue;">📌 Lemmatization Techniques</h3>

<p>There are different techniques to perform lemmatization each with its own advantages and use cases:</p>
<p><strong>1. Rule Based Lemmatization</strong></p>
<p>In rule-based lemmatization, predefined rules are applied to a word to remove suffixes and get the root form. This approach works well for regular words but may not handle irregularities well.</p>
<p><strong>For example:</strong></p>
<p><strong>Rule:</strong> </p>
<div class="codehilite"><pre><span></span><code><span class="n">For</span><span class="w"> </span><span class="n">regular</span><span class="w"> </span><span class="n">verbs</span><span class="w"> </span><span class="n">ending</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="s">&quot;-ed,&quot;</span><span class="w"> </span><span class="n">remove</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s">&quot;-ed&quot;</span><span class="w"> </span><span class="n">suffix</span><span class="p">.</span>

<span class="nl">Example:</span><span class="w"> </span><span class="s">&quot;walked&quot;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="s">&quot;walk&quot;</span>
</code></pre></div>

<p>While this method is simple and interpretable, it doesn't account for irregular word forms like "better" which should be lemmatized to "good".</p>
<p><strong>2. Dictionary-Based Lemmatization</strong></p>
<p>It uses a predefined dictionary or lexicon such as WordNet to look up the base form of a word. This method is more accurate than rule-based lemmatization because it accounts for exceptions and irregular words.</p>
<p><strong>For example:</strong></p>
<ul>
<li>'running' -&gt; 'run'</li>
<li>'better' -&gt; 'good'</li>
<li>'went' -&gt; 'go</li>
</ul>
<p>"I was running to become a better athlete and then I went home," -&gt; "I was run to become a good athlete and then I go home."</p>
<p>By using dictionaries like WordNet this method can handle a range of words effectively, especially in languages with well-established dictionaries.</p>
<p><strong>3. Machine Learning-Based Lemmatization</strong></p>
<p>It uses algorithms trained on large datasets to automatically identify the base form of words. This approach is highly flexible and can handle irregular words and linguistic nuances better than the rule-based and dictionary-based methods.</p>
<p><strong>For example:</strong></p>
<p>A trained model may deduce that “went” corresponds to “go” even though the suffix removal rule doesn’t apply. Similarly, for 'happier' the model deduces 'happy' as the lemma. </p>
<p>Machine learning-based lemmatizers are more adaptive and can generalize across different word forms which makes them ideal for complex tasks involving diverse vocabularies.</p>
<p><strong>Step 1: Installing NLTK and Downloading Necessary Resources</strong></p>
<p>In Python, the NLTK library provides an easy and efficient way to implement lemmatization. First, we need to install the NLTK library and download the necessary datasets like WordNet and the punkt tokenizer.</p>
<div class="codehilite"><pre><span></span><code><span class="sx">!pip install nltk</span>
</code></pre></div>

<p>Now lets import the library and download the necessary datasets.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt_tab&#39;</span><span class="p">)</span>      
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>    
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;omw-1.4&#39;</span><span class="p">)</span> 
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger_eng&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Step 2: Lemmatizing Text with NLTK</strong></p>
<p>Now we can tokenize the text and apply lemmatization using NLTK's WordNetLemmatizer.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The cats were running faster than the dogs.&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">lemmatized_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Lemmatized Words: </span><span class="si">{</span><span class="n">lemmatized_words</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Step 3: Improving Lemmatization with Part of Speech (POS) Tagging</strong></p>
<p>To improve the accuracy of lemmatization, it’s important to specify the correct Part of Speech (POS) for each word. By default, NLTK assumes that words are nouns when no POS tag is provided. However, it can be more accurate if we specify the correct POS tag for each word.</p>
<p><strong>For example:</strong></p>
<ul>
<li>
<p>"running" (as a verb) should be lemmatized to "run".</p>
</li>
<li>
<p>"better" (as an adjective) should be lemmatized to "good".</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk</span><span class="w"> </span><span class="kn">import</span> <span class="n">pos_tag</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;The children are running towards a better place.&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="n">tagged_tokens</span> <span class="o">=</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_wordnet_pos</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;J&#39;</span><span class="p">):</span>  
        <span class="k">return</span> <span class="s1">&#39;a&#39;</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">):</span>  
        <span class="k">return</span> <span class="s1">&#39;v&#39;</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">):</span>  
        <span class="k">return</span> <span class="s1">&#39;n&#39;</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;R&#39;</span><span class="p">):</span>  
        <span class="k">return</span> <span class="s1">&#39;r&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;n&#39;</span>  

<span class="n">lemmatized_sentence</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tagged_tokens</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;are&#39;</span> <span class="ow">or</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;am&#39;</span><span class="p">]:</span>
        <span class="n">lemmatized_sentence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>  
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lemmatized_sentence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">get_wordnet_pos</span><span class="p">(</span><span class="n">tag</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Sentence: &quot;</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemmatized Sentence: &quot;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lemmatized_sentence</span><span class="p">))</span>
</code></pre></div>

<h3 style="color:blue;">📌 Stemming</h3>
<p>Stemming reduces works to their root by removing suffixes. Types of stemmers include:</p>
<p>Stemming is an important text-processing technique that reduces words to their base or root form by removing prefixes and suffixes. This process standardizes words which helps to improve the efficiency and effectiveness of various natural language processing (NLP) tasks.</p>
<p><img alt="alt text" src="images/nlp9.png" /></p>
<p><img alt="alt text" src="images/nlp10.png" /></p>
<p><img alt="alt text" src="images/nlp11.png" /></p>
<p><img alt="alt text" src="images/nlp12.png" /></p>
<p>In NLP, stemming simplifies words to their most basic form, making it easier to analyze and process text. For example, "chocolates" becomes "chocolate" and "retrieval" becomes "retrieve". This is important in the early stages of NLP tasks where words are extracted from a document and tokenized (broken into individual words).</p>
<p>It helps in tasks such as <code>text classification</code>, <code>information retrieval</code> and <code>text summarization</code> by reducing words to a base form. While it is effective, it can sometimes introduce drawbacks including potential inaccuracies and a reduction in text readability.</p>
<p>Examples of stemming for the word "like":</p>
<div class="codehilite"><pre><span></span><code>&quot;likes&quot; → &quot;like&quot;
&quot;liked&quot; → &quot;like&quot;
&quot;likely&quot; → &quot;like&quot;
&quot;liking&quot; → &quot;like&quot;
</code></pre></div>

<ul>
<li><strong>Porter Stemmer</strong>
   Porter's Stemmer is one of the most popular and widely used stemming algorithms. Proposed in 1980 by Martin Porter, this stemmer works by applying a series of rules to remove common suffixes from English words. It is well-known for its simplicity, speed and reliability. However, the stemmed output is not guaranteed to be a meaningful word and its applications are limited to the English language.</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>
<p>'agreed' → 'agree'</p>
</li>
<li>
<p><strong>Rule:</strong> If the word has a suffix <strong>EED</strong> (with at least one vowel and consonant) remove the suffix and change it to <strong>EE</strong>.</p>
</li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>
<p>Very fast and efficient.</p>
</li>
<li>
<p>Commonly used for tasks like information retrieval and text mining.</p>
</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>
<p>Outputs may not always be real words.</p>
</li>
<li>
<p>Limited to English words.</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem</span><span class="w"> </span><span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;running&quot;</span><span class="p">,</span> <span class="s2">&quot;jumps&quot;</span><span class="p">,</span> <span class="s2">&quot;happily&quot;</span><span class="p">,</span> <span class="s2">&quot;running&quot;</span><span class="p">,</span> <span class="s2">&quot;happily&quot;</span><span class="p">]</span>

<span class="n">stemmed_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">porter_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original words:&quot;</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stemmed words:&quot;</span><span class="p">,</span> <span class="n">stemmed_words</span><span class="p">)</span>
</code></pre></div>

<ul>
<li><strong>Snowball Stemmer</strong>
   The Snowball Stemmer is an enhanced version of the Porter Stemmer which was introduced by Martin Porter as well. It is referred to as Porter2 and is faster and more aggressive than its predecessor. One of the key advantages of this is that it supports multiple languages, making it a multilingual stemmer.</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>
<p>'running' → 'run'</p>
</li>
<li>
<p>'quickly' → 'quick'</p>
</li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>
<p>More efficient than Porter Stemmer.</p>
</li>
<li>
<p>Supports multiple languages.</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem</span><span class="w"> </span><span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">words_to_stem</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;running&#39;</span><span class="p">,</span> <span class="s1">&#39;jumped&#39;</span><span class="p">,</span> <span class="s1">&#39;happily&#39;</span><span class="p">,</span> <span class="s1">&#39;quickly&#39;</span><span class="p">,</span> <span class="s1">&#39;foxes&#39;</span><span class="p">]</span>

<span class="n">stemmed_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_to_stem</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original words:&quot;</span><span class="p">,</span> <span class="n">words_to_stem</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stemmed words:&quot;</span><span class="p">,</span> <span class="n">stemmed_words</span><span class="p">)</span>
</code></pre></div>

<h3 style="color:blue;">📌 Stopword removal</h3>
<p>Stopword removal is a process to remove common words from the document.
Natural language processing tasks often involve filtering out commonly occurring words that provide no or very little semantic value to text analysis. These words are known as stopwords include articles, prepositions and pronouns like "the", "and", "is" and "in". While they seem insignificant, proper stopword handling can dramatically impact the performance and accuracy of NLP applications.</p>
<p><strong>When to Remove Stopwords</strong>
The decision to remove stopwords depends heavily on the specific NLP task at hand:</p>
<p><strong>Tasks that benefit from stopword removal:</strong></p>
<ul>
<li>
<p>Text classification and sentiment analysis</p>
</li>
<li>
<p>Information retrieval and search engines</p>
</li>
<li>
<p>Topic modelling and clustering</p>
</li>
<li>
<p>Keyword extraction</p>
</li>
</ul>
<p><strong>Tasks that require preserving stopwords:</strong></p>
<ul>
<li>
<p>Machine translation (maintains grammatical structure)</p>
</li>
<li>
<p>Text summarization (preserves sentence coherence)</p>
</li>
<li>
<p>Question-answering systems (syntactic relationships matter)</p>
</li>
<li>
<p>Grammar checking and parsing</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>

<span class="c1"># Sample text</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This is a sample sentence showing stopword removal.&quot;</span>

<span class="c1"># Get English stopwords and tokenize</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

<span class="c1"># Remove stopwords</span>
<span class="n">filtered_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original:&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Filtered:&quot;</span><span class="p">,</span> <span class="n">filtered_tokens</span><span class="p">)</span>
</code></pre></div>

<h3 style="color:blue;">📌 Parts of Speech (POS) Tagging</h3>
<p>Parts of Speech (POS) Tagging assigns a part of speech to each word in sentence based on definition and context.</p>
<ul>
<li><strong>Parts of Speech (POS) Tagging</strong></li>
</ul>
<h2 style="color:red;">✅ Text representation Techniques</h2>
<p>It converts textual data into numerical vectors that are processed by the following methods:</p>
<h3 style="color:blue;">📌 Parts of Speech (POS) Tagging</h3>

<ul>
<li>
<p><strong>One-Hot Encoding</strong></p>
</li>
<li>
<p><strong>Bag of Words (BOW)</strong></p>
</li>
</ul>
<p>In Natural Language Processing (NLP) text data needs to be converted into numbers so that machine learning algorithms can understand it. One common method to do this is Bag of Words (BoW) model. It turns text like sentence, paragraph or document into a collection of words and counts how often each word appears but ignoring the order of the words. It does not consider the order of the words or their grammar but focuses on counting how often each word appears in the text.</p>
<p>This makes it useful for tasks like text classification, sentiment analysis and clustering.</p>
<p><strong>Key Components of BoW</strong></p>
<ul>
<li>
<p><strong>Vocabulary:</strong> It is a list of all unique words from the entire dataset. Each word in the vocabulary corresponds to a feature in the model.</p>
</li>
<li>
<p><strong>Document Representation:</strong> Each document is represented as a vector where each element shows the frequency of the words from the vocabulary in that document. The frequency of each word is used as a feature for the model.</p>
</li>
</ul>
<h3 style="color:blue;">📌 Steps to Implement the Bag of Words (BoW) Model</h3>

<p><strong>Step 1: Preprocessing the Text</strong></p>
<p>Before applying the BoW model, we need to preprocess the text. This includes:</p>
<ul>
<li>
<p>Converting the text to lowercase</p>
</li>
<li>
<p>Removing non-word characters</p>
</li>
<li>
<p>Removing extra spaces</p>
</li>
</ul>
<p>Lets consider a sample text for this implementation:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Beans</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">trying</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">explain</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">somebody</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">were</span><span class="w"> </span><span class="nx">flying</span><span class="w"> </span><span class="k">in</span><span class="p">,</span><span class="w"> </span><span class="nx">that</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">corn</span><span class="p">.</span><span class="w"> </span><span class="nx">That</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">beans</span><span class="p">.</span><span class="w"> </span><span class="nx">And</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">were</span><span class="w"> </span><span class="nx">very</span><span class="w"> </span><span class="nx">impressed</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">my</span><span class="w"> </span><span class="nx">agricultural</span><span class="w"> </span><span class="nx">knowledge</span><span class="p">.</span><span class="w"> </span><span class="nx">Please</span><span class="w"> </span><span class="nx">give</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">Amaury</span><span class="w"> </span><span class="nx">once</span><span class="w"> </span><span class="nx">again</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">outstanding</span><span class="w"> </span><span class="nx">introduction</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">bunch</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">good</span><span class="w"> </span><span class="nx">friends</span><span class="w"> </span><span class="nx">here</span><span class="w"> </span><span class="nx">today</span><span class="p">,</span><span class="w"> </span><span class="nx">including</span><span class="w"> </span><span class="nx">somebody</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">served</span><span class="w"> </span><span class="nx">with</span><span class="p">,</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">finest</span><span class="w"> </span><span class="nx">senators</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">country</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">we</span><span class="err">&#39;</span><span class="nx">re</span><span class="w"> </span><span class="nx">lucky</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">him</span><span class="p">,</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">Senator</span><span class="p">,</span><span class="w"> </span><span class="nx">Dick</span><span class="w"> </span><span class="nx">Durbin</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">here</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">also</span><span class="w"> </span><span class="nx">noticed</span><span class="p">,</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">way</span><span class="p">,</span><span class="w"> </span><span class="nx">former</span><span class="w"> </span><span class="nx">Governor</span><span class="w"> </span><span class="nx">Edgar</span><span class="w"> </span><span class="nx">here</span><span class="p">,</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">haven</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">seen</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">long</span><span class="w"> </span><span class="nx">time</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">somehow</span><span class="w"> </span><span class="nx">he</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">aged</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">have</span><span class="p">.</span><span class="w"> </span><span class="nx">And</span><span class="w"> </span><span class="nx">it</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">great</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">see</span><span class="w"> </span><span class="nx">you</span><span class="p">,</span><span class="w"> </span><span class="nx">Governor</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">thank</span><span class="w"> </span><span class="nx">President</span><span class="w"> </span><span class="nx">Killeen</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">everybody</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">U</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">System</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">making</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="nx">possible</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">me</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">here</span><span class="w"> </span><span class="nx">today</span><span class="p">.</span><span class="w"> </span><span class="nx">And</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">am</span><span class="w"> </span><span class="nx">deeply</span><span class="w"> </span><span class="nx">honored</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Paul</span><span class="w"> </span><span class="nx">Douglas</span><span class="w"> </span><span class="nx">Award</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">being</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">me</span><span class="p">.</span><span class="w"> </span><span class="nx">He</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">somebody</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">path</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">outstanding</span><span class="w"> </span><span class="nx">public</span><span class="w"> </span><span class="nx">service</span><span class="w"> </span><span class="nx">here</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Illinois</span><span class="p">.</span><span class="w"> </span><span class="nx">Now</span><span class="p">,</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">start</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">addressing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">elephant</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">room</span><span class="p">.</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">people</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">still</span><span class="w"> </span><span class="nx">wondering</span><span class="w"> </span><span class="nx">why</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">didn</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">speak</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">commencement</span><span class="p">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Beans. I was trying to explain to somebody as we were flying in, that&#39;s corn.  That&#39;s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we&#39;re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven&#39;t seen in a long time, and somehow he has not aged and I have. And it&#39;s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn&#39;t speak at the commencement.&quot;&quot;&quot;</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)):</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\W&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Step 2: Counting Word Frequencies</strong></p>
<p>In this step, we count the frequency of each word in the preprocessed text. We will store these counts in a pandas DataFrame to view them easily in a tabular format.</p>
<ul>
<li>
<p>We initialize a dictionary to hold our word counts.</p>
</li>
<li>
<p>Then, we tokenize each sentence into words.</p>
</li>
<li>
<p>For each word, we check if it exists in our dictionary. If it does, we increment its count. If it doesn’t, we add it to the dictionary with a count of 1.</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">word2count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span>

<span class="k">for</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">dataset</span><span class="p">:</span>
<span class="w">    </span><span class="n">words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nltk</span><span class="p">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="k">data</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">words</span><span class="p">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">word2count</span><span class="p">:</span>
<span class="w">            </span><span class="n">word2count</span><span class="o">[</span><span class="n">word</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="k">else</span><span class="err">:</span>
<span class="w">            </span><span class="n">word2count</span><span class="o">[</span><span class="n">word</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>

<span class="n">stop_words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>

<span class="n">filtered_word2count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="nl">word</span><span class="p">:</span><span class="w"> </span><span class="nf">count</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="nf">count</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">word2count</span><span class="p">.</span><span class="n">items</span><span class="p">()</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">stop_words</span><span class="err">}</span>

<span class="n">word_freq_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">list</span><span class="p">(</span><span class="n">filtered_word2count</span><span class="p">.</span><span class="n">items</span><span class="p">()),</span><span class="w"> </span><span class="n">columns</span><span class="o">=[</span><span class="n">&#39;Word&#39;, &#39;Frequency&#39;</span><span class="o">]</span><span class="p">)</span>

<span class="n">word_freq_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">word_freq_df</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="k">by</span><span class="o">=</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">ascending</span><span class="o">=</span><span class="k">False</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">word_freq_df</span><span class="p">)</span>
</code></pre></div>

<p><strong>Step 3: Selecting the Most Frequent Words</strong></p>
<p>Now that we have counted the word frequencies, we will select the top N most frequent words (e.g top 10) to be used in the BoW model. We can visualize these frequent words using a bar chart to understand the distribution of words in our dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">heapq</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">freq_words</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">word2count</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">word2count</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Top 10 frequent words: </span><span class="si">{</span><span class="n">freq_words</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">top_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">words</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">top_words</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Top 10 Most Frequent Words&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Words&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><strong>Step 4: Building the Bag of Words (BoW) Model</strong>
Now we will build the Bag of Words (BoW) model. This model is represented as a binary matrix where each row corresponds to a sentence and each column represents one of the top N frequent words. A 1 in the matrix shows that the word is present in the sentence and a 0 shows its absence.</p>
<p>We will use a heatmap to visualize this binary matrix where green shows the presence of a word (1) and red shows its absence (0).</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">freq_words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">vector</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vector</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlGn&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">freq_words</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Sentence </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bag of Words Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Frequent Words&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sentences&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><strong>Step 5: Visualizing Word Frequencies with a Word Cloud</strong></p>
<p>Finally, we can create a Word Cloud to visually represent the word frequencies. In a word cloud, the size of each word is proportional to its frequency which makes it easy to identify the most common words at a glance.</p>
<ul>
<li><strong>Term Frequency-Inverse Document Frequency (TF-IDF)</strong></li>
</ul>
<p>TF-IDF (Term Frequency–Inverse Document Frequency) is a statistical method used in natural language processing and information retrieval to evaluate how important a word is to a document in relation to a larger collection of documents. TF-IDF combines two components:</p>
<p><strong>1. Term Frequency (TF):</strong> Measures how often a word appears in a document. A higher frequency suggests greater importance. If a term appears frequently in a document, it is likely relevant to the document’s content.</p>
<p><img alt="alt text" src="images/nlp13.png" /></p>
<p><strong>2. Inverse Document Frequency (IDF):</strong> Reduces the weight of common words across multiple documents while increasing the weight of rare words. If a term appears in fewer documents, it is more likely to be meaningful and specific.</p>
<p><img alt="alt text" src="images/nlp14.png" /></p>
<p>This balance allows TF-IDF to highlight terms that are both frequent within a specific document and distinctive across the text document, making it a useful tool for tasks like search ranking, text classification and keyword extraction.</p>
<p><strong>Converting Text into vectors with TF-IDF</strong></p>
<p>Let's take an example where we have a corpus (a collection of documents) with three documents and our goal is to calculate the TF-IDF score for specific terms in these documents.</p>
<ol>
<li><strong>Document 1:</strong> "The cat sat on the mat."</li>
<li><strong>Document 2:</strong> "The dog played in the park."</li>
<li><strong>Document 3:</strong> "Cats and dogs are great pets."</li>
</ol>
<p>Our goal is to calculate the TF-IDF score for specific terms in these documents. Let’s focus on the word <strong>"cat"</strong> and see how TF-IDF evaluates its importance.</p>
<p><strong>Step 1: Calculate Term Frequency (TF)</strong></p>
<p><strong>For Document 1:</strong></p>
<ul>
<li>
<p>The word "cat" appears 1 time.</p>
</li>
<li>
<p>The total number of terms in Document 1 is 6 ("the", "cat", "sat", "on", "the", "mat").</p>
</li>
<li>
<p>So, TF(cat,Document 1) = 1/6</p>
</li>
</ul>
<p><strong>For Document 2:</strong></p>
<ul>
<li>
<p>The word "cat" does not appear.</p>
</li>
<li>
<p>So, TF(cat,Document 2)=0.</p>
</li>
</ul>
<p><strong>For Document 3:</strong></p>
<ul>
<li>
<p>The word "cat" appears 1 time.</p>
</li>
<li>
<p>The total number of terms in Document 3 is 6 ("cats", "and", "dogs", "are", "great", "pets").
So TF (cat,Document 3)=1/6</p>
</li>
</ul>
<p>In Document 1 and Document 3 the word "cat" has the same TF score. This means it appears with the same relative frequency in both documents. In Document 2 the TF score is 0 because the word "cat" does not appear.</p>
<p><strong>Step 2: Calculate Inverse Document Frequency (IDF)</strong></p>
<ul>
<li>
<p>Total number of documents in the corpus (D): 3</p>
</li>
<li>
<p>Number of documents containing the term "cat": 2 (Document 1 and Document 3).</p>
</li>
</ul>
<p><img alt="alt text" src="images/nlp15.png" /></p>
<p><strong>Step 3: Calculate TF-IDF</strong></p>
<p>The TF-IDF score for "cat" is 0.029 in Document 1 and Document 3 and 0 in Document 2 that reflects both the frequency of the term in the document (TF) and its rarity across the corpus (IDF).</p>
<p>The TF-IDF score is the product of TF and IDF:</p>
<p><img alt="alt text" src="images/nlp16.png" /></p>
<h3 style="color:blue;">📌 Implementing TF-IDF in Python</h3>

<p><strong>Step 1: Import modules</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TfidfVectorizer</span>
</code></pre></div>

<p><strong>Step 2: Collect strings from documents and create a corpus</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nv">d0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;Geeks for geeks&#39;</span>
<span class="nv">d1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;Geeks&#39;</span>
<span class="nv">d2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;r2j&#39;</span>
<span class="nv">string</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[<span class="nv">d0</span>,<span class="w"> </span><span class="nv">d1</span>,<span class="w"> </span><span class="nv">d2</span>]
</code></pre></div>

<p><strong>Step 3: Get TF-IDF values</strong></p>
<p>Here we are using TfidfVectorizer() from scikit learn to perform tf-idf and apply on our courpus using fit_transform.</p>
<div class="codehilite"><pre><span></span><code>tfidf = TfidfVectorizer()
result = tfidf.fit_transform(string)
</code></pre></div>

<p><strong>Step 4: Display IDF values</strong></p>
<div class="codehilite"><pre><span></span><code>print(&#39;\nidf values:&#39;)
for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):
    print(ele1, &#39;:&#39;, ele2)
</code></pre></div>

<p><img alt="alt text" src="images/nlp17.png" /></p>
<p><strong>Step 5: Display TF-IDF values along with indexing</strong></p>
<div class="codehilite"><pre><span></span><code>print(&#39;\nWord indexes:&#39;)
print(tfidf.vocabulary_)
print(&#39;\ntf-idf value:&#39;)
print(result)
print(&#39;\ntf-idf values in matrix form:&#39;)
print(result.toarray())
</code></pre></div>

<p><img alt="alt text" src="images/nlp18.png" /></p>
<p><img alt="alt text" src="./images/image.png" /></p>
<ul>
<li>
<p><strong>N-Gram Language Modeling with NLTK</strong></p>
</li>
<li>
<p><strong>Latent Semantic Analysis (LSA)</strong></p>
</li>
<li>
<p><strong>Latent Dirichlet Allocation (LDA)</strong></p>
</li>
</ul>
<h2 style="color:red;">✅ Text Embedding Techniques</h2>
<p>It refers to methods that create dense vector representations of text, capturing semantic meaning including advanced approaches like:</p>
<h3 style="color:blue;">📌 Word Embedding</h3>

<ul>
<li>
<p><strong>Word2Vec (SkipGram, Continuous Bag of Words - CBOW)</strong></p>
</li>
<li>
<p><strong>GloVe (Global Vectors for Word Representation)</strong></p>
</li>
<li>
<p><strong>fastText</strong></p>
</li>
</ul>
<h3 style="color:blue;">📌 Pre-Trained Embedding</h3>

<ul>
<li>
<p><strong>ELMo (Embeddings from Language Models)</strong></p>
</li>
<li>
<p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong></p>
</li>
</ul>
<h3 style="color:blue;">📌 Word Embedding</h3>

<ul>
<li><strong>Doc2Vec</strong></li>
</ul>
<h3 style="color:blue;">📌 Advanced Embeddings</h3>

<ul>
<li>
<p><strong>RoBERTa</strong></p>
</li>
<li>
<p><strong>DistilBERT</strong></p>
</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../RAG/rag.html" class="btn btn-neutral float-right" title="RAG">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="overview.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../RAG/rag.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
