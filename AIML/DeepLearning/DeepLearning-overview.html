<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>DeepLearning overview - AIML documents</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "DeepLearning overview";
        var mkdocs_page_input_path = "AIML/DeepLearning/DeepLearning-overview.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Machine Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Supervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/Classification.html">Classification</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/CrossValidation.html">Cross Validation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/DecisionTreeRegression.html">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/RegressionModels/RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Unsupervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/UnsupervisedLearning/overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/UnsupervisedLearning/Clustering.html">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/UnsupervisedLearning/Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Deep Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../../DeepLearning/Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../DeepLearning/Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Natural Language Processing(NLP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../NLP/nlpdetails.html">NLP Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">DeepLearning overview</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="deep-learning"><span style="color:red"><strong>Deep Learning</strong></span><a class="headerlink" href="#deep-learning" title="Permanent link">#</a></h2>
<ul>
<li>To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence.</li>
<li>At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center.</li>
<li>Broadly speaking, deep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow.</li>
<li>Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons.</li>
<li>The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs.</li>
<li>We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer.</li>
</ul>
<p><img alt="aiml" src="img/Picture21.png" /> </p>
<ul>
<li>Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems.</li>
<li>In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.</li>
<li>Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning.</li>
<li>Machine learning involves computer intelligence that doesn’t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. </li>
<li>
<p>Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra.</p>
</li>
<li>
<p>There are two broad classes of machine learning methods:</p>
<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
</ul>
</li>
</ul>
<p>In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.</p>
<p>For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.</p>
<p>With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.</p>
<p>While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.</p>
<h2 id="why-is-deep-learning-important"><strong>Why is Deep Learning Important?</strong><a class="headerlink" href="#why-is-deep-learning-important" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture22.png" /></p>
<p>Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks.</p>
<p>Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”</p>
<p>Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.</p>
<p>What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”</p>
<p>Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.</p>
<h2 id="deep-learning-microservices"><strong>Deep Learning Microservices</strong><a class="headerlink" href="#deep-learning-microservices" title="Permanent link">#</a></h2>
<p>Here’s a quick overview of some deep learning use cases and microservices.</p>
<p>Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what’s in the image. DeepFilter is a style transfer service for applying artistic filters to images.</p>
<p>The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google’s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.</p>
<h2 id="open-source-deep-learning-frameworks"><strong>Open Source Deep Learning Frameworks</strong><a class="headerlink" href="#open-source-deep-learning-frameworks" title="Permanent link">#</a></h2>
<p>Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here’s an overview of each:</p>
<p><strong>DL4J:</strong></p>
<ul>
<li>JVM-based</li>
<li>Distrubted</li>
<li>Integrates with Hadoop and Spark</li>
</ul>
<p><strong>Theano:</strong></p>
<ul>
<li>Very popular in Academia</li>
<li>Fairly low level</li>
<li>Interfaced with via Python and Numpy</li>
</ul>
<p><strong>Torch:</strong></p>
<ul>
<li>Lua based</li>
<li>In house versions used by Facebook and Twitter</li>
<li>Contains pretrained models</li>
</ul>
<p><strong>TensorFlow:</strong></p>
<ul>
<li>Google written successor to Theano</li>
<li>Interfaced with via Python and Numpy</li>
<li>Highly parallel</li>
<li>Can be somewhat slow for certain problem sets</li>
</ul>
<p><strong>Caffe:</strong></p>
<ul>
<li>Not general purpose. Focuses on machine-vision problems</li>
<li>Implemented in C++ and is very fast</li>
<li>Not easily extensible</li>
<li>Has a Python interface</li>
</ul>
<h2 id="mcculloch-and-pitts-neuron"><strong>McCulloch and Pitts Neuron</strong><a class="headerlink" href="#mcculloch-and-pitts-neuron" title="Permanent link">#</a></h2>
<p>In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:</p>
<ol>
<li>A set of <strong>weights</strong> corresponding to synapses (inputs)</li>
<li>An <strong>adder</strong> for summing input signals; analogous to cell membrane that collects charge</li>
<li>An <strong>activation function</strong> for determining when the neuron fires, based on accumulated input</li>
</ol>
<p><img alt="aiml" src="img/Picture23.png" /></p>
<p>A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a network can they perform useful work.</p>
<p>Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.</p>
<h2 id="perceptron"><strong>Perceptron</strong><a class="headerlink" href="#perceptron" title="Permanent link">#</a></h2>
<p>A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.</p>
<p>Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.</p>
<p>The number of inputs and outputs are determined by the data. Weights are stored as a N x K matrix, with N observations and K neurons, with specifying the weight on the ith observation on the jth neuron.</p>
<p><img alt="aiml" src="img/Picture24.png" /></p>
<p><img alt="aiml" src="img/Picture25.png" /></p>
<p><img alt="aiml" src="img/Picture26.png" /></p>
<h2 id="learning-with-perceptrons"><strong>Learning with Perceptrons</strong><a class="headerlink" href="#learning-with-perceptrons" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture27.png" /></p>
<h2 id="example-logical-functions"><strong>Example: Logical functions</strong><a class="headerlink" href="#example-logical-functions" title="Permanent link">#</a></h2>
<p>Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables x1 and x2, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables.</p>
<div class="codehilite"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ipywidgets</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVG</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>AND = pd.DataFrame({&#39;x1&#39;: (0,0,1,1), &#39;x2&#39;: (0,1,0,1), &#39;y&#39;: (0,0,0,1)})
AND

x1  x2  y
0   0   0   0
1   0   1   0
2   1   0   0
3   1   1   1
</code></pre></div>

<p>First, we need to initialize weights to small, random values (can be positive and negative).</p>
<div class="codehilite"><pre><span></span><code>w = np.random.randn(3)*1e-4
</code></pre></div>

<p>Then, a simple activation function for calculating g(h):</p>
<div class="codehilite"><pre><span></span><code>g = lambda inputs, weights: np.where(np.dot(inputs, weights)&gt;0, 1, 0)
</code></pre></div>

<p>Finally, a training function that iterates the learning algorithm, returning the adapted weights.</p>
<div class="codehilite"><pre><span></span><code>def train(inputs, targets, weights, eta, n_iterations):

    # Add the inputs that match the bias node
    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]

    for n in range(n_iterations):

        activations = g(inputs, weights);
        weights -= eta*np.dot(np.transpose(inputs), activations - targets)

    return(weights)
</code></pre></div>

<p>Let's test it first on the AND function.</p>
<div class="codehilite"><pre><span></span><code>inputs = AND[[&#39;x1&#39;,&#39;x2&#39;]]
target = AND[&#39;y&#39;]

w = train(inputs, target, w, 0.25, 10)
</code></pre></div>

<p>Checking the performance:</p>
<div class="codehilite"><pre><span></span><code>g(np.c_[inputs, -np.ones((len(inputs), 1))], w)

array([0, 0, 0, 1])

Thus, it has learned the function perfectly. Now for OR:
</code></pre></div>

<div class="codehilite"><pre><span></span><code>OR = pd.DataFrame({&#39;x1&#39;: (0,0,1,1), &#39;x2&#39;: (0,1,0,1), &#39;y&#39;: (0,1,1,1)})
OR

    x1  x2  y
0   0   0   0
1   0   1   1
2   1   0   1
3   1   1   1
</code></pre></div>

<div class="codehilite"><pre><span></span><code>w = np.random.randn(3)*1e-4
</code></pre></div>

<div class="codehilite"><pre><span></span><code>inputs = OR[[&#39;x1&#39;,&#39;x2&#39;]]
target = OR[&#39;y&#39;]

w = train(inputs, target, w, 0.25, 20)
</code></pre></div>

<div class="codehilite"><pre><span></span><code>g(np.c_[inputs, -np.ones((len(inputs), 1))], w)
</code></pre></div>

<p>array([0, 1, 1, 1])
Also 100% correct.</p>
<p>Exercise: XOR
Now try running the model on the XOR function, where a one is returned for either x1 or x2 being true, but not both. What happens here?</p>
<p>Let's explore the problem graphically:</p>
<div class="codehilite"><pre><span></span><code>AND.plot(kind=&#39;scatter&#39;, x=&#39;x1&#39;, y=&#39;x2&#39;, c=&#39;y&#39;, s=50, colormap=&#39;winter&#39;)
plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), &#39;k--&#39;);
</code></pre></div>

<p><img alt="aiml" src="img/Picture28.png" /></p>
<div class="codehilite"><pre><span></span><code>XOR = pd.DataFrame({&#39;x1&#39;: (0,0,1,1), &#39;x2&#39;: (0,1,0,1), &#39;y&#39;: (0,1,1,0)})

XOR.plot(kind=&#39;scatter&#39;, x=&#39;x1&#39;, y=&#39;x2&#39;, c=&#39;y&#39;, s=50, colormap=&#39;winter&#39;);
</code></pre></div>

<p><img alt="aiml" src="img/Picture29.png" /></p>
<p><img alt="aiml" src="img/Picture30.png" /></p>
<h2 id="multi-layer-perceptron"><strong>Multi-layer Perceptron</strong><a class="headerlink" href="#multi-layer-perceptron" title="Permanent link">#</a></h2>
<p>The solution to fitting more complex (i.e. non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply <strong>add more weights.</strong></p>
<p>There are two ways to add complexity:</p>
<ol>
<li>Add backward connections, so that output neurons feed back to input nodes, resulting in a <strong>recurrent network</strong></li>
<li>Add neurons between the input nodes and the outputs, creating an additional ("hidden") layer to the network, resulting in a <strong>multi-layer perceptron</strong></li>
</ol>
<p>The latter approach is more common in applications of neural networks.</p>
<p><img alt="aiml" src="img/Picture31.png" /></p>
<p>How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.</p>
<p>Updating a multi-layer perceptron (MLP) is a matter of:</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">moving</span><span class="w"> </span><span class="kr">for</span><span class="n">ward</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span><span class="p">,</span><span class="w"> </span><span class="n">calculating</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="kr">input</span><span class="n">s</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">estimates</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">moving</span><span class="w"> </span><span class="n">backward</span><span class="w"> </span><span class="n">updating</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">resulting</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="kr">for</span><span class="n">ward</span><span class="w"> </span><span class="n">propagation</span><span class="mf">.</span>
</code></pre></div>

<p>In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.</p>
<h2 id="backpropagation"><strong>Backpropagation</strong><a class="headerlink" href="#backpropagation" title="Permanent link">#</a></h2>
<p>Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters. These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent. This may be the most common method for training neural networks. Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.</p>
<p><img alt="aiml" src="img/Picture32.png" /></p>
<h2 id="review-the-chain-rule"><strong>Review: The chain rule</strong><a class="headerlink" href="#review-the-chain-rule" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture33.png" /></p>
<h2 id="notation"><strong>Notation</strong><a class="headerlink" href="#notation" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture34.png" /></p>
<h2 id="backpropagation-in-general"><strong>Backpropagation in general</strong><a class="headerlink" href="#backpropagation-in-general" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture35.png" />
<img alt="aiml" src="img/Picture36.png" />
<img alt="aiml" src="img/Picture37.png" />
<img alt="aiml" src="img/Picture38.png" />
<img alt="aiml" src="img/Picture39.png" /></p>
<h2 id="backpropagation-in-practice"><strong>Backpropagation in practice</strong><a class="headerlink" href="#backpropagation-in-practice" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture40.png" />
<img alt="aiml" src="img/Picture41.png" /></p>
<p><img alt="aiml" src="img/Picture42.png" />
<img alt="aiml" src="img/Picture43.png" /></p>
<h2 id="toy-python-example"><strong>Toy Python example</strong><a class="headerlink" href="#toy-python-example" title="Permanent link">#</a></h2>
<p>Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations. The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Ensure python 3 forward compatibility</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SigmoidLayer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_output</span><span class="p">,</span> <span class="n">n_input</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="kp">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SigmoidNetwork</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        :parameters:</span>
<span class="sd">            - layer_sizes : list of int</span>
<span class="sd">                List of layer sizes of length L+1 (including the input dimensionality)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">SigmoidLayer</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="kp">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="kp">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forward pass - compute a^n for n in {0, ... L}</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">layer_outputs</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># Backward pass - compute \partial C/\partial z^m for m in {L, ..., 1}</span>
        <span class="n">cost_partials</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">layer_outputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])):</span>
            <span class="n">cost_partials</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">cost_partials</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">layer_output</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">layer_output</span><span class="p">))</span>
        <span class="n">cost_partials</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>

        <span class="c1"># Compute weight gradient step</span>
        <span class="n">W_updates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cost_partial</span><span class="p">,</span> <span class="n">layer_output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cost_partials</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">layer_outputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">W_updates</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">cost_partial</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">layer_output</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># and biases</span>
        <span class="n">b_updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">cost_partial</span><span class="o">.</span><span class="kp">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">cost_partial</span> <span class="ow">in</span> <span class="n">cost_partials</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>

        <span class="k">for</span> <span class="n">W_update</span><span class="p">,</span> <span class="n">b_update</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">W_updates</span><span class="p">,</span> <span class="n">b_updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">W_update</span><span class="o">*</span><span class="n">learning_rate</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="n">b_update</span><span class="o">*</span><span class="n">learning_rate</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="kp">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>nn = SigmoidNetwork([2, 2, 1])
X = np.array([[0, 1, 0, 1], 
              [0, 0, 1, 1]])
y = np.array([0, 1, 1, 0])
for n in range(int(1e3)):
    nn.train(X, y, learning_rate=1.)
print(&quot;Input\tOutput\tQuantized&quot;)
for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:
    print(&quot;{}\t{:.4f}\t{}&quot;.format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] &gt; .5)))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">logistic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="nl">beta</span><span class="p">:</span><span class="w"> </span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">h</span><span class="p">))</span>

<span class="nv">@interact</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">25</span><span class="p">))</span>
<span class="n">def</span><span class="w"> </span><span class="n">logistic_plot</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">hvals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hvals</span><span class="p">,</span><span class="w"> </span><span class="n">logistic</span><span class="p">(</span><span class="n">hvals</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">))</span>
</code></pre></div>

<p><img alt="aiml" src="img/Picture44.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="n">hyperbolic_tangent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="nl">h</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">h</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">h</span><span class="p">))</span>

<span class="nv">@interact</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">25</span><span class="p">))</span>
<span class="n">def</span><span class="w"> </span><span class="n">tanh_plot</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">hvals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hvals</span><span class="o">*</span><span class="n">theta</span>
<span class="w">    </span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hvals</span><span class="p">,</span><span class="w"> </span><span class="n">hyperbolic_tangent</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
</code></pre></div>

<h2 id="gradient-descent"><strong>Gradient Descent</strong><a class="headerlink" href="#gradient-descent" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture45.png" /></p>
<p><img alt="aiml" src="img/Picture46.png" /></p>
<p><img alt="aiml" src="img/Picture47.png" /></p>
<p><img alt="aiml" src="img/Picture48.png" /></p>
<p><img alt="aiml" src="img/Picture49.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Define the sigmoid activation function and its derivative</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Input dataset (X) and output dataset (y)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Initialize weights and biases randomly
input_layer_neurons = X.shape[1]
hidden_layer_neurons = 2
output_layer_neurons = 1
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Weight matrices
W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
W2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Bias vectors
b1 = np.random.uniform(size=(1, hidden_layer_neurons))
b2 = np.random.uniform(size=(1, output_layer_neurons))
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Learning rate
learning_rate = 0.5
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Training the neural network
for epoch in range(10000):
    # Forward propagation
    hidden_layer_input = np.dot(X, W1) + b1
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, W2) + b2
    predicted_output = sigmoid(output_layer_input)

    # Compute the error
    error = y - predicted_output

    # Backpropagation
    # Calculate the gradient for the output layer
    d_predicted_output = error <span class="gs">* sigmoid_derivative(predicted_output)</span>

<span class="gs">    # Calculate the error for the hidden layer</span>
<span class="gs">    hidden_layer_error = d_predicted_output.dot(W2.T)</span>
<span class="gs">    d_hidden_layer_output = hidden_layer_error *</span> sigmoid_derivative(hidden_layer_output)

    # Update the weights and biases
    W2 += hidden_layer_output.T.dot(d_predicted_output) <span class="gs">* learning_rate</span>
<span class="gs">    b2 += np.sum(d_predicted_output, axis=0, keepdims=True) *</span> learning_rate

    W1 += X.T.dot(d_hidden_layer_output) <span class="gs">* learning_rate</span>
<span class="gs">    b1 += np.sum(d_hidden_layer_output, axis=0, keepdims=True) *</span> learning_rate
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Display the final predicted output
print(&quot;Final predicted output:\n&quot;, predicted_output)
</code></pre></div>

<div class="codehilite"><pre><span></span><code>#<span class="w"> </span><span class="nv">Display</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">weights</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">biases</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;\nFinal weights for W1:\n&quot;</span>,<span class="w"> </span><span class="nv">W1</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;\nFinal weights for W2:\n&quot;</span>,<span class="w"> </span><span class="nv">W2</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;\nFinal biases for b1:\n&quot;</span>,<span class="w"> </span><span class="nv">b1</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;\nFinal biases for b2:\n&quot;</span>,<span class="w"> </span><span class="nv">b2</span><span class="ss">)</span>
</code></pre></div>

<h3 id="explanation">Explanation:<a class="headerlink" href="#explanation" title="Permanent link">#</a></h3>
<p><strong>Initialization</strong>:</p>
<ul>
<li>
<p>Input dataset <code>X</code> and output dataset <code>y</code>.</p>
</li>
<li>
<p>Weight matrices <code>W1</code> and <code>W2</code> and bias vectors <code>b1</code> and <code>b2</code> are initialized randomly.</p>
</li>
<li>
<p>The learning rate is set to 0.5.</p>
</li>
</ul>
<p><strong>Forward Propagation</strong>:</p>
<ul>
<li>
<p>Compute the input and output for the hidden layer.</p>
</li>
<li>
<p>Compute the input and output for the output layer (predicted output).</p>
</li>
</ul>
<p><strong>Error Calculation</strong>:</p>
<ul>
<li>Compute the error by subtracting the predicted output from the actual output.</li>
</ul>
<p><strong>Backpropagation</strong>:</p>
<ul>
<li>
<p>Compute the gradient of the error with respect to the predicted output.</p>
</li>
<li>
<p>Compute the error propagated back to the hidden layer.</p>
</li>
<li>
<p>Compute the gradient of the hidden layer output.</p>
</li>
</ul>
<p><strong>Weight and Bias Update</strong>:</p>
<ul>
<li>Update the weights and biases for both layers using the computed gradients and learning rate.</li>
</ul>
<p><strong>Training Loop</strong>:</p>
<ul>
<li>The above steps are repeated for a specified number of epochs (10,000 in this case).</li>
</ul>
<p>After training, the final predicted output, weights, and biases are printed. This simple example uses a neural network with one hidden layer to demonstrate the key concepts of backpropagation.</p>
<p><img alt="aiml" src="img/deep-learning-example.png" /></p>
<p>this activation function may take any of several forms, such as a logistic function.</p>
<h2 id="example-of-deep-learning"><span style="color:orange"><strong>Example of Deep Learning</strong></span><a class="headerlink" href="#example-of-deep-learning" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/deep-learning-example.png" /></p>
<ul>
<li>In the example given above, we provide the raw data of images to the first layer of the input layer. </li>
<li>After then, these input layer will determine the patterns of local contrast that means it will differentiate on the basis of colors, luminosity, etc.</li>
<li>Then the 1st hidden layer will determine the face feature, i.e., it will fixate on eyes, nose, and lips, etc. And then, it will fixate those face features on the correct face template. </li>
<li>So, in the 2nd hidden layer, it will actually determine the correct face here as it can be seen in the above image, after which it will be sent to the output layer. </li>
<li>Likewise, more hidden layers can be added to solve more complex problems, for example, if you want to find out a particular kind of face having large or light complexions. So, as and when the hidden layers increase, we are able to solve complex problems.</li>
</ul>
<h2 id="types-of-deep-learning-networks"><span style="color:red"><strong>Types of Deep Learning Networks</strong></span><a class="headerlink" href="#types-of-deep-learning-networks" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/types-of-deep-learning-networks.png" /></p>
<h2 id="1-feed-forward-neural-network"><span style="color:blue"><strong>1. Feed Forward Neural Network</strong></span><a class="headerlink" href="#1-feed-forward-neural-network" title="Permanent link">#</a></h2>
<p>A feed-forward neural network is none other than an Artificial Neural Network, which ensures that the nodes do not form a cycle. In this kind of neural network, all the perceptrons are organized within layers, such that the input layer takes the input, and the output layer generates the output. Since the hidden layers do not link with the outside world, it is named as hidden layers. Each of the perceptrons contained in one single layer is associated with each node in the subsequent layer. It can be concluded that all of the nodes are fully connected. It does not contain any visible or invisible connection between the nodes in the same layer. There are no back-loops in the feed-forward network. To minimize the prediction error, the backpropagation algorithm can be used to update the weight values.</p>
<h2 id="applications"><strong>Applications:</strong><a class="headerlink" href="#applications" title="Permanent link">#</a></h2>
<ul>
<li>
<h2 id="data-compression"><span style="color:green"><strong>Data Compression</strong></span><a class="headerlink" href="#data-compression" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="pattern-recognition"><span style="color:green"><strong>Pattern Recognition</strong></span><a class="headerlink" href="#pattern-recognition" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="computer-vision"><span style="color:green"><strong>Computer Vision</strong></span><a class="headerlink" href="#computer-vision" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="sonar-target-recognition"><span style="color:green"><strong>Sonar Target Recognition</strong></span><a class="headerlink" href="#sonar-target-recognition" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="speech-recognition"><span style="color:green"><strong>Speech Recognition</strong></span><a class="headerlink" href="#speech-recognition" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="handwritten-characters-recognition"><span style="color:green"><strong>Handwritten Characters Recognition</strong></span><a class="headerlink" href="#handwritten-characters-recognition" title="Permanent link">#</a></h2>
</li>
</ul>
<h2 id="2-recurrent-neural-network"><span style="color:blue"><strong>2. Recurrent Neural Network</strong></span><a class="headerlink" href="#2-recurrent-neural-network" title="Permanent link">#</a></h2>
<p>Recurrent neural networks are yet another variation of feed-forward networks. Here each of the neurons present in the hidden layers receives an input with a specific delay in time. The Recurrent neural network mainly accesses the preceding info of existing iterations. For example, to guess the succeeding word in any sentence, one must have knowledge about the words that were previously used. It not only processes the inputs but also shares the length as well as weights crossways time. It does not let the size of the model to increase with the increase in the input size. However, the only problem with this recurrent neural network is that it has slow computational speed as well as it does not contemplate any future input for the current state. It has a problem with reminiscing prior information.</p>
<h2 id="applications_1"><strong>Applications:</strong><a class="headerlink" href="#applications_1" title="Permanent link">#</a></h2>
<ul>
<li>
<h2 id="machine-translation"><span style="color:green"><strong>Machine Translation</strong></span><a class="headerlink" href="#machine-translation" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="robot-control"><span style="color:green"><strong>Robot Control</strong></span><a class="headerlink" href="#robot-control" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="time-series-prediction"><span style="color:green"><strong>Time Series Prediction</strong></span><a class="headerlink" href="#time-series-prediction" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="speech-recognition_1"><span style="color:green"><strong>Speech Recognition</strong></span><a class="headerlink" href="#speech-recognition_1" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="speech-synthesis"><span style="color:green"><strong>Speech Synthesis</strong></span><a class="headerlink" href="#speech-synthesis" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="time-series-anomaly-detection"><span style="color:green"><strong>Time Series Anomaly Detection</strong></span><a class="headerlink" href="#time-series-anomaly-detection" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="rhythm-learning"><span style="color:green"><strong>Rhythm Learning</strong></span><a class="headerlink" href="#rhythm-learning" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="music-composition"><span style="color:green"><strong>Music Composition</strong></span><a class="headerlink" href="#music-composition" title="Permanent link">#</a></h2>
</li>
</ul>
<h2 id="3-convolutional-neural-network"><span style="color:blue"><strong>3. Convolutional Neural Network</strong></span><a class="headerlink" href="#3-convolutional-neural-network" title="Permanent link">#</a></h2>
<p>Convolutional Neural Networks are a special kind of neural network mainly used for image classification, clustering of images and object recognition. DNNs enable unsupervised construction of hierarchical image representations. To achieve the best accuracy, deep convolutional neural networks are preferred more than any other neural network.</p>
<h2 id="applications_2"><strong>Applications:</strong><a class="headerlink" href="#applications_2" title="Permanent link">#</a></h2>
<ul>
<li>
<h2 id="identify-faces-street-signs-tumors"><span style="color:green"><strong>Identify Faces, Street Signs, Tumors.</strong></span><a class="headerlink" href="#identify-faces-street-signs-tumors" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="image-recognition"><span style="color:green"><strong>Image Recognition.</strong></span><a class="headerlink" href="#image-recognition" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="video-analysis"><span style="color:green"><strong>Video Analysis.</strong></span><a class="headerlink" href="#video-analysis" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="nlp"><span style="color:green"><strong>NLP.</strong></span><a class="headerlink" href="#nlp" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="anomaly-detection"><span style="color:green"><strong>Anomaly Detection.</strong></span><a class="headerlink" href="#anomaly-detection" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="drug-discovery"><span style="color:green"><strong>Drug Discovery.</strong></span><a class="headerlink" href="#drug-discovery" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="checkers-game"><span style="color:green"><strong>Checkers Game.</strong></span><a class="headerlink" href="#checkers-game" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="time-series-forecasting"><span style="color:green"><strong>Time Series Forecasting.</strong></span><a class="headerlink" href="#time-series-forecasting" title="Permanent link">#</a></h2>
</li>
</ul>
<h2 id="4-restricted-boltzmann-machine"><span style="color:blue"><strong>4. Restricted Boltzmann Machine</strong></span><a class="headerlink" href="#4-restricted-boltzmann-machine" title="Permanent link">#</a></h2>
<p>RBMs are yet another variant of Boltzmann Machines. Here the neurons present in the input layer and the hidden layer encompasses symmetric connections amid them. However, there is no internal association within the respective layer. But in contrast to RBM, Boltzmann machines do encompass internal connections inside the hidden layer. These restrictions in BMs helps the model to train efficiently.</p>
<h2 id="applications_3"><strong>Applications:</strong><a class="headerlink" href="#applications_3" title="Permanent link">#</a></h2>
<ul>
<li>
<h2 id="filtering"><span style="color:green"><strong>Filtering.</strong></span><a class="headerlink" href="#filtering" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="feature-learning"><span style="color:green"><strong>Feature Learning.</strong></span><a class="headerlink" href="#feature-learning" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="classification"><span style="color:green"><strong>Classification.</strong></span><a class="headerlink" href="#classification" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="risk-detection"><span style="color:green"><strong>Risk Detection.</strong></span><a class="headerlink" href="#risk-detection" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="business-and-economic-analysis"><span style="color:green"><strong>Business and Economic analysis.</strong></span><a class="headerlink" href="#business-and-economic-analysis" title="Permanent link">#</a></h2>
</li>
</ul>
<h2 id="5-autoencoders"><span style="color:blue"><strong>5. Autoencoders</strong></span><a class="headerlink" href="#5-autoencoders" title="Permanent link">#</a></h2>
<p>An autoencoder neural network is another kind of unsupervised machine learning algorithm. Here the number of hidden cells is merely small than that of the input cells. But the number of input cells is equivalent to the number of output cells. An autoencoder network is trained to display the output similar to the fed input to force AEs to find common patterns and generalize the data. The autoencoders are mainly used for the smaller representation of the input. It helps in the reconstruction of the original data from compressed data. This algorithm is comparatively simple as it only necessitates the output identical to the input.</p>
<ul>
<li><strong>Encoder:</strong> Convert input data in lower dimensions.</li>
<li><strong>Decoder:</strong> Reconstruct the compressed data.</li>
</ul>
<h2 id="applications_4"><strong>Applications:</strong><a class="headerlink" href="#applications_4" title="Permanent link">#</a></h2>
<ul>
<li>
<h2 id="classification_1"><span style="color:green"><strong>Classification.</strong></span><a class="headerlink" href="#classification_1" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="clustering"><span style="color:green"><strong>Clustering.</strong></span><a class="headerlink" href="#clustering" title="Permanent link">#</a></h2>
</li>
<li>
<h2 id="feature-compression"><span style="color:green"><strong>Feature Compression.</strong></span><a class="headerlink" href="#feature-compression" title="Permanent link">#</a></h2>
</li>
</ul>
<h2 id="deep-learning-applications"><strong>Deep learning applications</strong><a class="headerlink" href="#deep-learning-applications" title="Permanent link">#</a></h2>
<ul>
<li>
<p><strong>Self-Driving Cars</strong>
In self-driven cars, it is able to capture the images around it by processing a huge amount of data, and then it will decide which actions should be incorporated to take a left or right or should it stop. So, accordingly, it will decide what actions it should take, which will further reduce the accidents that happen every year.</p>
</li>
<li>
<p><strong>Voice Controlled Assistance</strong>
When we talk about voice control assistance, then Siri is the one thing that comes into our mind. So, you can tell Siri whatever you want it to do it for you, and it will search it for you and display it for you.</p>
</li>
<li>
<p><strong>Automatic Image Caption Generation</strong>
Whatever image that you upload, the algorithm will work in such a way that it will generate caption accordingly. If you say blue colored eye, it will display a blue-colored eye with a caption at the bottom of the image.</p>
</li>
<li>
<p><strong>Automatic Machine Translation</strong>
With the help of automatic machine translation, we are able to convert one language into another with the help of deep learning.</p>
</li>
</ul>
<p><span style="color:purple">Limitations</span></p>
<ul>
<li>It only learns through the observations.</li>
<li>It comprises of biases issues.</li>
</ul>
<p><span style="color:purple">Advantages</span></p>
<ul>
<li>It lessens the need for feature engineering.</li>
<li>It eradicates all those costs that are needless.</li>
<li>It easily identifies difficult defects.</li>
<li>It results in the best-in-class performance on problems.</li>
</ul>
<p><span style="color:purple">Disadvantages</span></p>
<ul>
<li>It requires an ample amount of data.</li>
<li>It is quite expensive to train.</li>
<li>It does not have strong theoretical groundwork.</li>
</ul>
<h2 id="introduction"><strong>Introduction</strong><a class="headerlink" href="#introduction" title="Permanent link">#</a></h2>
<ul>
<li>Brains <strong>biological network</strong> provides basis for connecting elements in a real-life scenario for information processing and insight generation. </li>
<li>A <strong>hierarchy of neurons connected through layers,</strong> where the output of one layer becomes the input for another layers, the information passes from one layer to another layer as weights.</li>
<li>The <strong>weights associated with each neuron</strong> contain insights so that recognition and reasoning becomes easier for the next level.</li>
<li><strong>Artificial neural network</strong> is a very popular and effective method that consists of layers associated with weights. </li>
<li>The <strong>association between different layers</strong> is governed by mathematical equation that passes information from one layer to the other. </li>
<li>A bunch of mathematical equations are at work inside one artificial neural network model. </li>
</ul>
<h2 id="neural-networks"><strong>Neural Networks</strong><a class="headerlink" href="#neural-networks" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture1.jpg" /></p>
<p><img alt="aiml" src="img/Picture2.png" /></p>
<h2 id="task"><strong>Task</strong><a class="headerlink" href="#task" title="Permanent link">#</a></h2>
<p><img alt="aiml" src="img/Picture3.png" /></p>
<h2 id="what-is-deep-learning-dl"><strong>What is Deep Learning (DL)?</strong><a class="headerlink" href="#what-is-deep-learning-dl" title="Permanent link">#</a></h2>
<p>A machine learning subfield of learning representations of data. Exceptional effective at learning patterns.
Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers.
If you provide the system tons of information, it begins to understand it and respond in useful ways.</p>
<p><img alt="aiml" src="img/Picture4.png" /></p>
<h2 id="why-is-dl-useful"><strong>Why is DL useful?</strong><a class="headerlink" href="#why-is-dl-useful" title="Permanent link">#</a></h2>
<ul>
<li>Manually designed features are often <strong>over-specified, incomplete</strong> and take a long <strong>time to design</strong> and validate</li>
<li>Learned Features are <strong>easy to adapt, fast</strong> to learn</li>
<li>Deep learning provides a very <strong>flexible,</strong> (almost?) <strong>universal,</strong> learnable framework for representing world, visual and linguistic information.</li>
<li>Can learn both unsupervised and supervised</li>
<li>Effective <strong>end-to-end</strong> joint system learning</li>
<li>Utilize large amounts of training data</li>
</ul>
<p><img alt="aiml" src="img/Picture5.png" /></p>
<div class="codehilite"><pre><span></span><code><span class="nv">In</span><span class="w"> </span><span class="o">~</span><span class="mi">2010</span><span class="w"> </span><span class="nv">DL</span><span class="w"> </span><span class="nv">started</span><span class="w"> </span><span class="nv">outperforming</span><span class="w"> </span><span class="nv">other</span><span class="w"> </span><span class="nv">ML</span><span class="w"> </span><span class="nv">techniques</span><span class="w"> </span>
<span class="nv">first</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">speech</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">vision</span>,<span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">NLP</span>
</code></pre></div>

<p><img alt="aiml" src="img/Picture6.png" /></p>
<p><img alt="aiml" src="img/Picture7.png" /></p>
<p><img alt="aiml" src="img/Picture8.png" /></p>
<p><img alt="aiml" src="img/Picture9.png" /></p>
<p><img alt="aiml" src="img/Picture10.png" /></p>
<h2 id="types-of-neural-networks"><strong>Types of Neural Networks</strong><a class="headerlink" href="#types-of-neural-networks" title="Permanent link">#</a></h2>
<ul>
<li><strong>Single hidden layer neural network:</strong> this is the simplest form of neural network as in this there is only one hidden layer.</li>
<li><strong>Multiple hidden layer neural networks:</strong> in this form more than one hidden layer will connect the input data with the output data. The complexity of calculation increases in this form as it requires more computational power to the system to process information</li>
<li><strong>Feed forward neural networks:</strong> in this form of neural network architecture, the information is passed one directionally from one layer to another layer; there is no iteration from the first level of learning.</li>
<li><strong>Back propagation neural networks:</strong> in this form of neural network there are two important steps, feed forward works in passing information from input to the hidden and from hidden to output layer and secondly it calculates error and propagate it back to the previous layers.</li>
</ul>
<p><img alt="aiml" src="img/Picture11.png" /></p>
<p><img alt="aiml" src="img/Picture12.png" /></p>
<p><img alt="aiml" src="img/Picture13.png" /></p>
<p><img alt="aiml" src="img/Picture14.png" /></p>
<p><img alt="aiml" src="img/Picture15.png" /></p>
<p><img alt="aiml" src="img/Picture16.png" /></p>
<p><img alt="aiml" src="img/Picture17.png" /></p>
<p><img alt="aiml" src="img/Picture18.png" /></p>
<p><img alt="aiml" src="img/Picture19.png" /></p>
<p><img alt="aiml" src="img/Picture20.png" /></p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
