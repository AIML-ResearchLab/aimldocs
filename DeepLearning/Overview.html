<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ganesh kinkar Giri" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Overview - AIML documents</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Overview";
        var mkdocs_page_input_path = "DeepLearning/Overview.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> AIML documents
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">AIML</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >Programing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Programing/python.html">PYTHON</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Statistic</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Descriptive Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Measures of Central Tendency</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean.html">Mean</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median.html">Median</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode.html">Mode</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Position (Relative Standing)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles.html">Percentiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles.html">Quartiles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles.html">Deciles</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score.html">Z-Score</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Shape of the Distribution</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness.html">Skewness</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis.html">Kurtosis</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Visualization Tools</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/Histogram.html">Histogram</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/BarChart.html">Bar Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/PieChart.html">Pie Chart</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot.html">Box Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot.html">Line Plot</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot.html">Dot Plot</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Measures of Dispersion (Variability)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range.html">Range</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance.html">Variance</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation.html">Standard Deviation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange.html">Interquartile Range(IQR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation.html">Cofficient of Variation</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Inferential Statistics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" >Population and Sample</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/Population.html">Population</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/Sample.html">Sample</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods.html">Sampling Methods</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Estimation</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/PointEstimation.html">Point Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/IntervalEstimation.html">Interval Estimation</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Estimation/MarginError.html">Margin of Error</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression and Correlation Analysis</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LinearRegression.html">Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/MultipleRegression.html">Multiple Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Regression-and-Correlation-Analysis/CorrelationCoefficients.html">Correlation Coefficients</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Hypothesis Testing</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/NullHypothesis.html">Null Hypothesis (H₀)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/AlternativeHypothesis.html">Alternative Hypothesis (H₁)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TestStatistic.html">Test Statistic</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/pvalue.html">p-value</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/SignificanceLevel.html">Significance Level (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIError.html">Type I Error (α)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/TypeIIError.html">Type II Error (β)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Hypothesis-Testing/PoweroftheTest.html">Power of the Test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/t-test.html">t-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/z-test.html">z-test</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/ANOVA.html">ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Parametric-Tests/F-test.html">F-test</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-Parametric Tests</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU.html">Mann-Whitney U</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis.html">Kruskal-Wallis</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon.html">Wilcoxon</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square.html">Chi-square</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Resampling Methods</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Resampling-Methods/Bootstrapping.html">Bootstrapping</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Resampling-Methods/Jackknife.html">Jackknife</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Analysis of Variance (ANOVA)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/One-way-ANOVA.html">One-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/Two-way-ANOVA.html">Two-way ANOVA</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/ANOVA/Post-hoc-Tests.html">Post-hoc Tests</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Probability Theory</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions.html">Probability Distributions</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/CentralLimitTheorem.html">Central Limit Theorem</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../Statistic/InferentialStatistics/Probability-Theory/BayesianInference.html">Bayesian Inference</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Time Series</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Trend.html">Trend</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Seasonality.html">Seasonality</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Cyclic.html">Cyclic</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Noise.html">Irregular/Noise</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Stationarity.html">Stationarity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Non-stationary.html">Non-stationary</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Autocorrelation.html">Autocorrelation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Lag.html">Lag</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/MovingAverages.html">Moving Averages</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Holt-Winters.html">Holt-Winters Method</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Additive.html">Additive</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Multiplicative.html">Multiplicative</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AR.html">AR (Auto Regression)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/ARIMA.html">ARIMA</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Arimax.html">Arimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Sarimax.html">Sarimax</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Smoothing.html">Smoothing</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AutomatedForecasting.html">Automated Forecasting</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/AutomatedTimeSeries.html">Automated Time Series</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../Statistic/TimeSeries/Multivariate.html">Uni, Bi and Multivariate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/metrics.html">Metrics Evaluation</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/timeseries.html">Time Series Old</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Statistic/statistic-details.html">Statistic Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data manipulation and analysis</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Data-manipulation-and-analysis/data-manipulation-analysis.html">PANDAS</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Data Processing</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/sql.html">Basic SQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/sql-datascience.html">Using SQL for Data Science</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/unstructured-data.html">Unstructured Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Data-processing/exploratory-data-analysis.html">Exploratory Data Analysis(EDA)</a>
                </li>
                <li class="toctree-l2"><a class="" href="../Data-processing/building-ml-models-on-text-data.md">Building ML Models on Text Data</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Databases</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/PostgreSQL.html">PostgreSQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/MySQL.html">MySQL</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Databases/MongoDB.html">MongoDB</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Machine Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../MachineLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Supervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Regression.html">Regression</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/Classification.html">Classification</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/CrossValidation.html">Cross Validation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/HyperparameterTuning.html">Hyperparameter Tuning</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/SupervisedLearning/TuningDecisionThreshold.html">Tuning decision threshold</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Regression Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/SimpleLinearRegression.html">Simple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/MultipleLinearRegression.html">Multiple Linear Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/PolynomialRegression.html">Polynomial Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/RidgeLassoRegression.html">Ridge & Lasso Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/SupportVectorRegression.html">Support Vector Regression (SVR)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/DecisionTreeRegression.html">Decision Tree Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/RegressionModels/RandomForestRegression.html">Random Forest Regression</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/LogisticRegression.html">Logistic Regression</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/SupportVectorMachines.html">Support Vector Machines</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/SinglelayerPerceptron.html">Single-layer Perceptron</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/LinearClassificationModels/StochasticGradientDescent.html">Stochastic Gradient Descent (SGD)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Non-linear Classification Models</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/DecisionTreeClassification.html">Decision Tree Classification</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KNearestNeighbours.html">K-Nearest Neighbours</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/NaiveBayes.html">Naive Bayes</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/RandomForests.html">Random Forests</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/AdaBoost.html">AdaBoost</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/BaggingClassifier.html">Bagging Classifier</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/Ensemblelearningclassifiers.html">Ensemble learning classifiers</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="../MachineLearning/SupervisedLearning/NonlinearClassificationModels/KernelSVM.html">Kernel SVM</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Unsupervised Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/overview.html">Overview</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/Clustering.html">Clustering</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/UnsupervisedLearning/Pca.html">Principal Component Analysis(PCA)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Reinforcement Learning</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../MachineLearning/ReinforcementLearning/ReinforcementLearning.html">Overview</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Linear Algebra</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/Overview.html">Overview</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Deep Learning</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="#">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="Vanishing.html">Vanishing and Exploding Gradients Problems</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Components of Neural Networks</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="Components/LayersNeuralNetworks.html">Layers in Neural Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/WeightsBiases.html">Weights and Biases</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/ForwardPropagation.html">Forward Propagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/ActivationFunctions.html">Activation Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/LossFunctions.html">Loss Functions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/Backpropagation.html">Backpropagation</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Components/LearningRate.html">Learning Rate</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Optimization Algorithm</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/GradientDescent.html">Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/SGD.html">Stochastic Gradient Descent (SGD)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/Adam.html">Adam (Adaptive Moment Estimation)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/BatchNormalization.html">Batch Normalization</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/Mini-batch-GD.html">Mini-batch Gradient Descent</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/Momentum-based-GO.html">Momentum-based Gradient Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/AdagradOptimizer.html">Adagrad Optimizer</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="OptimizationAlgorithm/RMSPropOptimizer.html">RMSProp Optimizer</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Models</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="Models/FNN.html">Feedforward Neural Network (FNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" >Recurrent Neural Network (RNN)</a>
    <ul>
                <li class="toctree-l4"><a class="reference internal" href="Models/RNN.html">Recurrent Neural Network (RNN)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="Models/LSTM.html">LSTM (Long Short-Term Memory)</a>
                </li>
                <li class="toctree-l4"><a class="reference internal" href="Models/GRU.html">GRU (Gated Recurrent Unit)</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/CNN.html">Convolutional Neural Network (CNN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/RBFN.html">Radial Basis Function Network (RBFN)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/ComputerVision.html">Computer Vision</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/GANs.html">Generative Adversarial Networks (GANs)</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/Transformer.html">Transformer Networks</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/Autoencoders.html">Autoencoders</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="Models/SOM.html">Self-Organizing Maps (SOM)</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Natural Language Processing(NLP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../NLP/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../NLP/nlpdetails.html">NLP Details</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Retrieval-Augmented Generation(RAG)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../RAG/rag.html">RAG</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >AI agents</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../AIagents/aiagents.html">AI agents</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Agentic AI</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/general.html">general</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/overview.html">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/crewai.html">crewai</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/LangGraph.html">LangGraph</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/AutoGen.html">AutoGen</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/aws.html">AWS</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../AgenticAI/azure.html">AZURE</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" >Agent Development Kit</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/adk.html">ADK</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/Agents.html">Agents</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/Tools.html">Tools</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../AgenticAI/GCP/a2a.html">Tools</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >MCPModel Context Protocol (MCP)</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../MCP/mcp.html">MCP</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Models Details information</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Models/Ollama.html">Ollama</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Note Book</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Notebook/allnotebook.html">All Notebook</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AIML documents</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">AIML</li>
          <li class="breadcrumb-item">Deep Learning</li>
      <li class="breadcrumb-item active">Overview</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 style="color:red;">✅ Deep Learning</h2>

<h3 style="color:blue;">📌 What is Deep Learning?</h3>

<p><img alt="alt text" src="images/DPL1.png" /></p>
<p><img alt="alt text" src="images/DPL2.png" /></p>
<p><img alt="alt text" src="images/DPL3.png" /></p>
<p><img alt="alt text" src="images/DPL4.png" /></p>
<p><img alt="alt text" src="images/DPL5.png" /></p>
<p><strong>Deep Learning</strong> is a subset of <strong>Machine Learning (ML)</strong> that uses algorithms called <strong>artificial neural networks</strong>, inspired by the structure and function of the human brain. Deep learning is particularly powerful when working with unstructured data like images, audio, text, or videos.</p>
<p>The <strong>"deep"</strong> in deep learning refers to the number of layers in these neural networks. A neural network is composed of layers of interconnected nodes <strong>(neurons)</strong>. A deep neural network has many <strong>hidden layers</strong> between the input and output layers, allowing it to learn and represent data at various levels of abstraction.</p>
<h3 style="color:blue;">📌 Key Concepts</h3>

<ol>
<li><strong>Neural Networks</strong></li>
</ol>
<p>A neural network is made up of layers of nodes (neurons):</p>
<ul>
<li>
<p><strong>Input layer</strong> (where data is fed)</p>
</li>
<li>
<p><strong>Hidden layers</strong> (where computation happens)</p>
</li>
<li>
<p><strong>Output layer</strong> (where the result is produced)</p>
</li>
<li>
<p><strong>Deep Neural Networks (DNN)</strong></p>
</li>
</ul>
<p>A "deep" network has <strong>multiple hidden layers</strong>. These allow it to learn <strong>complex patterns</strong>.</p>
<ol>
<li>
<p><strong>Common Deep Learning Architectures:</strong></p>
</li>
<li>
<p><strong>CNN (Convolutional Neural Networks)</strong> – for images</p>
</li>
<li>
<p><strong>RNN (Recurrent Neural Networks)</strong> – for sequences, e.g., text</p>
</li>
<li>
<p><strong>Transformers</strong> – modern architectures used in NLP (like ChatGPT)</p>
</li>
</ol>
<p><img alt="alt text" src="images/DL1.png" /></p>
<p><img alt="alt text" src="images/DL2.png" /></p>
<p><img alt="alt text" src="images/DL3.png" /></p>
<p><img alt="alt text" src="images/DL4.png" /></p>
<p><img alt="alt text" src="images/DL5.png" /></p>
<h3 style="color:blue;">📌 What is a Neural Network?</h3>

<p>Neural networks are machine learning models that mimic the complex functions of the human brain.
These models consist of interconnected nodes or neurons that process data, learn patterns and enable tasks such as pattern recognition and decision-making.</p>
<p><img alt="alt text" src="images/DL6.png" /></p>
<p><img alt="alt text" src="images/DL7.png" /></p>
<p><img alt="alt text" src="images/DL8.png" /></p>
<p><img alt="alt text" src="images/DL9.png" /></p>
<h3 style="color:blue;">📌 Understanding Neural Networks in Deep Learning</h3>

<p>Neural networks are capable of learning and identifying patterns directly from data without pre-defined rules. These networks are built from several key components:</p>
<ol>
<li>
<p><strong>Neurons:</strong> The basic units that receive inputs, each neuron is governed by a threshold and an activation function.</p>
</li>
<li>
<p><strong>Connections:</strong> Links between neurons that carry information, regulated by weights and biases.</p>
</li>
<li>
<p><strong>Weights and Biases:</strong> These parameters determine the strength and influence of connections.</p>
</li>
<li>
<p><strong>Propagation Functions:</strong> Mechanisms that help process and transfer data across layers of neurons.</p>
</li>
<li>
<p><strong>Learning Rule:</strong> The method that adjusts weights and biases over time to improve accuracy.</p>
</li>
</ol>
<h3 style="color:blue;">📌 Neural networks follows a structured, three-stage process:</h3>

<ol>
<li>
<p><strong>Input Computation:</strong> Data is fed into the network.</p>
</li>
<li>
<p><strong>Output Generation:</strong> Based on the current parameters, the network generates an output.</p>
</li>
<li>
<p><strong>Iterative Refinement:</strong> The network refines its output by adjusting weights and biases, gradually improving its performance on diverse tasks.</p>
</li>
</ol>
<h3 style="color:blue;">📌 In an adaptive learning environment:</h3>

<ul>
<li>
<p>The neural network is exposed to a simulated scenario or dataset.</p>
</li>
<li>
<p>Parameters such as weights and biases are updated in response to new data or conditions.</p>
</li>
<li>
<p>With each adjustment, the network’s response evolves allowing it to adapt effectively to different tasks or environments.</p>
</li>
</ul>
<p><img alt="alt text" src="images/DL10.png" /></p>
<h3 style="color:blue;">📌 Layers in Neural Network Architecture:</h3>

<p><img alt="alt text" src="images/DL11.png" /></p>
<h3 style="color:blue;">📌 What is Forward Propagation?</h3>

<p>When data is input into the network, it passes through the network in the forward direction, from the input layer through the hidden layers to the output layer. This process is known as forward propagation. Here’s what happens during this phase:</p>
<p><strong>1. Linear Transformation:</strong> Each neuron in a layer receives inputs which are multiplied by the weights associated with the connections. These products are summed together and a bias is added to the sum. This can be represented mathematically as:</p>
<p><img alt="alt text" src="images/DLL1.png" /></p>
<p>where</p>
<ul>
<li>
<p><strong>w</strong> represents the weights</p>
</li>
<li>
<p><strong>x</strong> represents the inputs</p>
</li>
<li>
<p><strong>b</strong> is the bias</p>
</li>
</ul>
<p><strong>2. Activation:</strong> The result of the linear transformation (denoted as z) is then passed through an activation function. The activation function is crucial because it introduces non-linearity into the system, enabling the network to learn more complex patterns. Popular activation functions include <code>ReLU</code>, <code>sigmoid</code> and <code>tanh</code>.</p>
<p><strong>Forward Propagation</strong> is the process of <strong>passing input data through the neural network layer-by-layer</strong> to get an output (or prediction).</p>
<ul>
<li>
<p>Computing <strong>weighted sums</strong></p>
</li>
<li>
<p>Applying <strong>activation functions</strong></p>
</li>
<li>
<p>Passing the output to the next layer, until reaching the final prediction</p>
</li>
</ul>
<p>It's like <strong>feeding data forward</strong> through the network.</p>
<h3 style="color:blue;">🛒 Real-Time Example: Predicting Purchase Decision in E-Commerce</h3>

<p>Imagine you're building a model to predict whether a customer will buy a product or not, based on:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Time on website</td>
<td>10 minutes</td>
</tr>
<tr>
<td>Pages visited</td>
<td>5</td>
</tr>
<tr>
<td>Previous purchases</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Feed this input into a small neural network to predict: <strong>Buy (1) or Not Buy (0)</strong>.</p>
<h3 style="color:blue;">🧠 Neural Network Structure</h3>

<p>Let’s say your network looks like this:</p>
<ul>
<li>
<p><strong>Input Layer:</strong> 3 neurons (for 3 input features)</p>
</li>
<li>
<p><strong>Hidden Layer:</strong> 2 neurons</p>
</li>
<li>
<p><strong>Output Layer:</strong> 1 neuron (Buy or Not)</p>
</li>
</ul>
<p>Input → [Hidden1, Hidden2] → Output</p>
<h3 style="color:blue;">➗ Step-by-Step Forward Propagation</h3>

<p><strong>🎯 Inputs</strong></p>
<div class="codehilite"><pre><span></span><code>x = [10, 5, 2]  # Time, Pages, Purchases
</code></pre></div>

<p><strong>🔗 Weights (Randomly initialized)</strong></p>
<p>Hidden Layer weights:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">w1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[[<span class="mi">0</span>.<span class="mi">2</span>,<span class="w"> </span><span class="mi">0</span>.<span class="mi">4</span>,<span class="w"> </span><span class="mi">0</span>.<span class="mi">1</span>],<span class="w">   </span>#<span class="w"> </span><span class="nv">Weights</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">Hidden1</span>
<span class="w">      </span>[<span class="mi">0</span>.<span class="mi">5</span>,<span class="w"> </span><span class="mi">0</span>.<span class="mi">3</span>,<span class="w"> </span><span class="mi">0</span>.<span class="mi">2</span>]]<span class="w">   </span>#<span class="w"> </span><span class="nv">Weights</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">Hidden2</span>
</code></pre></div>

<p>Output Layer weights:</p>
<div class="codehilite"><pre><span></span><code>w2 = [0.6, 0.9]  # Weights from Hidden1 and Hidden2 to Output
</code></pre></div>

<p><strong>📈 Step 1: Input → Hidden Layer</strong></p>
<p><strong>For Hidden1:</strong></p>
<div class="codehilite"><pre><span></span><code>z1 = 10*0.2 + 5*0.4 + 2*0.1 = 2 + 2 + 0.2 = 4.2
a1 = sigmoid(4.2) ≈ 0.985
</code></pre></div>

<p><strong>For Hidden2:</strong></p>
<div class="codehilite"><pre><span></span><code>z2 = 10*0.5 + 5*0.3 + 2*0.2 = 5 + 1.5 + 0.4 = 6.9
a2 = sigmoid(6.9) ≈ 0.999
</code></pre></div>

<p>Now, hidden layer outputs:</p>
<p>hidden_output = [0.985, 0.999]</p>
<p><strong>🧮 Step 2: Hidden → Output</strong></p>
<div class="codehilite"><pre><span></span><code>z3 = 0.985*0.6 + 0.999*0.9 = 0.591 + 0.899 = 1.49
a3 = sigmoid(1.49) ≈ 0.816
</code></pre></div>

<p><strong>✅ Final Prediction:</strong> 0.816</p>
<p>This means:</p>
<p><code>There's an 81.6% chance that the customer will buy the product.</code></p>
<p><strong>🧠 Summary</strong></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Layer</th>
<th>Formula</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Hidden1</td>
<td><code>z = x·w + b</code> → <code>sigmoid(z)</code></td>
<td><code>0.985</code></td>
</tr>
<tr>
<td>2</td>
<td>Hidden2</td>
<td>same as above</td>
<td><code>0.999</code></td>
</tr>
<tr>
<td>3</td>
<td>Output</td>
<td><code>z = hidden·w + b</code> → <code>sigmoid(z)</code></td>
<td><code>0.816</code></td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">📌 Why It Matters</h3>

<p>Forward propagation is how the neural network <strong>generates predictions</strong> before learning. Once predictions are made, we compare them to the actual result, and then <strong>backpropagation</strong> is used to update weights to improve future predictions.</p>
<p><strong>Simple Python example using NumPy:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Inputs</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Weights</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
               <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>

<span class="c1"># Forward pass</span>
<span class="n">hidden_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">hidden_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>

<span class="n">final_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">hidden_output</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">final_input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final output (purchase probability): </span><span class="si">{</span><span class="n">output</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Final output (purchase probability):</strong> 0.816</p>
<h3 style="color:blue;">📌 What is Backpropagation?</h3>
<p>After forward propagation, the network evaluates its performance using a loss function which measures the difference between the actual output and the predicted output. The goal of training is to minimize this loss. This is where backpropagation comes into play:</p>
<ol>
<li>
<p><strong>Loss Calculation:</strong> The network calculates the loss which provides a measure of error in the predictions. The loss function could vary; common choices are mean squared error for regression tasks or cross-entropy loss for classification.</p>
</li>
<li>
<p><strong>Gradient Calculation:</strong> The network computes the gradients of the loss function with respect to each weight and bias in the network. This involves applying the chain rule of calculus to find out how much each part of the output error can be attributed to each weight and bias.</p>
</li>
<li>
<p><strong>Weight Update:</strong> Once the gradients are calculated, the weights and biases are updated using an optimization algorithm like stochastic gradient descent (SGD). The weights are adjusted in the opposite direction of the gradient to minimize the loss. The size of the step taken in each update is determined by the learning rate.</p>
</li>
</ol>
<p>Backpropagation is the <strong>learning process</strong> in deep learning. After forward propagation (i.e., making a prediction), the model:</p>
<ol>
<li>
<p>Compares the <strong>predicted output</strong> to the <strong>actual value</strong> using a <strong>loss function</strong></p>
</li>
<li>
<p>Calculates <strong>how wrong</strong> the prediction was (the error)</p>
</li>
<li>
<p>Moves <strong>backward through the network</strong>, adjusting the <strong>weights</strong> so that future predictions improve</p>
</li>
</ol>
<p><strong>👉 Forward propagation = prediction</strong></p>
<p><strong>👉 Backpropagation = learning</strong></p>
<h3 style="color:blue;">🔁 Using the Same Example: E-Commerce Purchase Prediction</h3>

<p><strong>🧠 Setup Recap</strong></p>
<ul>
<li>
<p><strong>Input:</strong> [10, 5, 2]</p>
</li>
<li>
<p><strong>Predicted output:</strong> 0.816 (from forward propagation)</p>
</li>
<li>
<p><strong>Actual output (label):</strong> 1 (customer actually bought)</p>
</li>
<li>
<p><strong>Loss function:</strong> Binary Cross-Entropy</p>
</li>
</ul>
<h3 style="color:blue;">⚙️ Step-by-Step Backpropagation</h3>

<p>We use <strong>gradient descent</strong> to update the weights by calculating the <strong>gradient of the loss</strong> w.r.t. each weight.</p>
<p><strong>🔧 1. Compute Loss (Binary Cross Entropy)</strong></p>
<p><img alt="alt text" src="images/DL12.png" /></p>
<p>This is the <strong>error</strong> we want to minimize.</p>
<p><strong>🔄 2. Compute Gradients (Chain Rule)</strong></p>
<p>We use the <strong>chain rule</strong> of calculus to backpropagate the error.</p>
<p>Let’s focus on the output neuron and then the hidden layer.</p>
<p><strong>🧮 a. Output Layer</strong></p>
<p>We calculate how much the output neuron contributed to the error.</p>
<p>Let’s denote:</p>
<p><img alt="alt text" src="images/DL13.png" /></p>
<p><strong>🔁 b. Hidden Layer</strong></p>
<p>We now calculate how the hidden neurons contributed to the output error.</p>
<p><img alt="alt text" src="images/DL14.png" /></p>
<p><strong>🔁 3. Update Weights</strong></p>
<p><img alt="alt text" src="images/DL15.png" /></p>
<p><strong>🔄 This Process Repeats...</strong></p>
<p>In each <strong>epoch</strong> (training cycle), the network:</p>
<ul>
<li>
<p>Performs forward pass (predict)</p>
</li>
<li>
<p>Calculates loss (compare with actual)</p>
</li>
<li>
<p>Performs backward pass (update weights)</p>
</li>
</ul>
<p>Over many <strong>epochs</strong>, the network <strong>learns patterns</strong> and improves accuracy.</p>
<p><strong>🎓 Visualization</strong></p>
<p>Input → Hidden Layer → Output (forward)
       ← Gradients ←           (backward)</p>
<p><strong>✅ Summary</strong></p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>What Happens</th>
</tr>
</thead>
<tbody>
<tr>
<td>Forward Prop</td>
<td>Predict output</td>
</tr>
<tr>
<td>Compare</td>
<td>Calculate loss</td>
</tr>
<tr>
<td>Backward Prop</td>
<td>Compute gradients</td>
</tr>
<tr>
<td>Update</td>
<td>Adjust weights</td>
</tr>
</tbody>
</table>
<p><strong>Here’s a mini example in NumPy (gradient calculation):</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_deriv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Input and label</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Weights</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Forward</span>
<span class="n">hidden_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">hidden_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>
<span class="n">final_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">final_input</span><span class="p">)</span>

<span class="c1"># Loss and backprop</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">sigmoid_deriv</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">d_hidden</span> <span class="o">=</span> <span class="n">d_output</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_deriv</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">)</span>

<span class="c1"># Update weights</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">w2</span> <span class="o">+=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">d_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
<span class="n">w1</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Updated output: </span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Updated output: [[0.7875618]]
</code></pre></div>

<p><strong>📦 No Libraries Required (uses only numpy)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Code inside the notebook</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Input features: [Time on website, Pages visited, Previous purchases]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  <span class="c1"># shape (1,3)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>         <span class="c1"># Target: Buy (1)</span>

<span class="c1"># Initialize weights (3 inputs → 2 hidden, 2 hidden → 1 output)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="kp">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># weights from input → hidden</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># weights from hidden → output</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward propagation</span>
    <span class="n">hidden_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
    <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>

    <span class="n">final_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">final_input</span><span class="p">)</span>

    <span class="c1"># Backpropagation</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>
    <span class="n">d_output</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="n">error_hidden</span> <span class="o">=</span> <span class="n">d_output</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">d_hidden</span> <span class="o">=</span> <span class="n">error_hidden</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">)</span>

    <span class="c1"># Update weights</span>
    <span class="n">w2</span> <span class="o">+=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">d_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">w1</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> → Loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="kp">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">abs</span><span class="p">(</span><span class="n">error</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Output: </span><span class="si">{</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Epoch   0 → Loss: 0.3706, Output: 0.6294  
Epoch 100 → Loss: 0.1836, Output: 0.8164  
Epoch 200 → Loss: 0.1310, Output: 0.8690  
Epoch 300 → Loss: 0.1058, Output: 0.8942  
Epoch 400 → Loss: 0.0906, Output: 0.9094  
Epoch 500 → Loss: 0.0803, Output: 0.9197  
Epoch 600 → Loss: 0.0727, Output: 0.9273  
Epoch 700 → Loss: 0.0668, Output: 0.9332  
Epoch 800 → Loss: 0.0622, Output: 0.9378  
Epoch 900 → Loss: 0.0583, Output: 0.9417  
</code></pre></div>

<p><strong>Loss vs. Epoch plot</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Epochs and corresponding loss values</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3706</span><span class="p">,</span> <span class="mf">0.1836</span><span class="p">,</span> <span class="mf">0.1310</span><span class="p">,</span> <span class="mf">0.1058</span><span class="p">,</span> <span class="mf">0.0906</span><span class="p">,</span> <span class="mf">0.0803</span><span class="p">,</span> <span class="mf">0.0727</span><span class="p">,</span> <span class="mf">0.0668</span><span class="p">,</span> <span class="mf">0.0622</span><span class="p">,</span> <span class="mf">0.0583</span><span class="p">]</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss vs. Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="alt text" src="images/DL16.png" /></p>
<p>Here is the <strong>Loss vs. Epoch</strong> plot. You can see the loss steadily decreases over time, which shows the model is learning and improving its predictions.</p>
<h3 style="color:blue;">📌 Iteration</h3>

<p>This process of forward propagation, loss calculation, backpropagation and weight update is repeated for many iterations over the dataset. Over time, this iterative process reduces the loss and the network's predictions become more accurate.</p>
<p>Through these steps, neural networks can adapt their parameters to better approximate the relationships in the data, thereby improving their performance on tasks such as classification, regression or any other predictive modeling.</p>
<h3 style="color:blue;">📌 Example of Email Classification</h3>

<p>Let's consider a record of an email dataset:</p>
<p><img alt="alt text" src="images/DL17.png" /></p>
<p>To classify this email, we will create a feature vector based on the analysis of keywords such as "free" "win" and "offer"</p>
<p>The feature vector of the record can be presented as:</p>
<ul>
<li>
<p>"free": Present (1)</p>
</li>
<li>
<p>"win": Absent (0)</p>
</li>
<li>
<p>"offer": Present (1)</p>
</li>
</ul>
<h3 style="color:blue;">📌 How Neurons Process Data in a Neural Network</h3>

<p>In a neural network, input data is passed through multiple layers, including one or more hidden layers. Each neuron in these hidden layers performs several operations, transforming the input into a usable output.</p>
<ol>
<li>
<p><strong>Input Layer:</strong> The input layer contains 3 nodes that indicates the presence of each keyword.</p>
</li>
<li>
<p><strong>Hidden Layer:</strong> The input vector is passed through the hidden layer. Each neuron in the hidden layer performs two primary operations: a weighted sum followed by an activation function.</p>
</li>
</ol>
<p><strong>Weights:</strong></p>
<ul>
<li>
<p>Neuron H1: [0.5,−0.2,0.3]</p>
</li>
<li>
<p>Neuron H2: [0.4,0.1,−0.5]</p>
</li>
</ul>
<p><strong>Input Vector:</strong> [1,0,1]</p>
<p><strong>Weighted Sum Calculation</strong></p>
<ul>
<li>
<p><strong>For H1:</strong> (1×0.5)+(0×−0.2)+(1×0.3)=0.5+0+0.3=0.8</p>
</li>
<li>
<p><strong>For H2:</strong> (1×0.4)+(0×0.1)+(1×−0.5)=0.4+0−0.5=−0.1</p>
</li>
</ul>
<p><strong>Activation Function</strong></p>
<p>Here we will use <code>ReLu activation function</code>:</p>
<ul>
<li>
<p><strong>H1 Output:</strong> ReLU(0.8)= 0.8</p>
</li>
<li>
<p><strong>H2 Output:</strong> ReLu(-0.1) = 0</p>
</li>
</ul>
<p><strong>3. Output Layer</strong></p>
<p>The activated values from the hidden neurons are sent to the output neuron where they are again processed using a weighted sum and an activation function.</p>
<ul>
<li>
<p><strong>Output Weights:</strong> [0.7, 0.2]</p>
</li>
<li>
<p><strong>Input from Hidden Layer:</strong> [0.8, 0]</p>
</li>
<li>
<p><strong>Weighted Sum:</strong> (0.8×0.7)+(0×0.2)=0.56+0=0.56</p>
</li>
<li>
<p><strong>Activation (Sigmoid):</strong> <img alt="alt text" src="images/DL18.png" /></p>
</li>
</ul>
<p><strong>4. Final Classification</strong></p>
<ul>
<li>
<p>The output value of approximately <strong>0.636</strong> indicates the probability of the email being spam.</p>
</li>
<li>
<p>Since this value is greater than 0.5, the neural network classifies the email as spam (1).</p>
</li>
</ul>
<p><img alt="alt text" src="images/DL19.png" /></p>
<h3 style="color:blue;">📌 Learning of a Neural Network</h3>

<p><strong>1. Learning with Supervised Learning</strong></p>
<p>In supervised learning, a neural network learns from labeled input-output pairs provided by a teacher. The network generates outputs based on inputs and by comparing these outputs to the known desired outputs, an error signal is created. The network iteratively adjusts its parameters to minimize errors until it reaches an acceptable performance level.</p>
<p><strong>2. Learning with Unsupervised Learning</strong></p>
<p>Unsupervised learning involves data without labeled output variables. The primary goal is to understand the underlying structure of the input data (X). Unlike supervised learning, there is no instructor to guide the process. Instead, the focus is on modeling data patterns and relationships, with techniques like clustering and association commonly used.</p>
<p><strong>3. Learning with Reinforcement Learning</strong></p>
<p>Reinforcement learning enables a neural network to learn through interaction with its environment. The network receives feedback in the form of rewards or penalties, guiding it to find an optimal policy or strategy that maximizes cumulative rewards over time. This approach is widely used in applications like gaming and decision-making.</p>
<h3 style="color:blue;">📌 Types of Neural Networks</h3>

<h3 style="color:blue;">🧠 1. Feedforward Neural Network (FNN)</h3>

<ul>
<li>
<p><strong>Description:</strong> The simplest type; data flows in one direction (input → hidden → output).</p>
</li>
<li>
<p><strong>Use Case:</strong> Basic classification/regression tasks.</p>
</li>
<li>
<p><strong>Example:</strong> Predicting house prices, email spam detection.</p>
</li>
</ul>
<h3 style="color:blue;">🔁 2. Recurrent Neural Network (RNN)</h3>

<ul>
<li>
<p><strong>Description:</strong> Designed for <strong>sequential data</strong>. It has memory of previous inputs.</p>
</li>
<li>
<p><strong>Use Case:</strong> Time series, speech recognition, text generation.</p>
</li>
<li>
<p><strong>Example:</strong> Language modeling, stock price prediction.</p>
</li>
</ul>
<h3 style="color:blue;">🔄 Variants of RNN:</h3>

<ul>
<li>
<p><strong>LSTM (Long Short-Term Memory):</strong> Solves vanishing gradient problem; better for long sequences.</p>
</li>
<li>
<p><strong>GRU (Gated Recurrent Unit):</strong> A simpler alternative to LSTM.</p>
</li>
</ul>
<h3 style="color:blue;">🖼️ 3. Convolutional Neural Network (CNN)</h3>

<ul>
<li>
<p><strong>Description:</strong> Uses filters/kernels to detect spatial patterns in images.</p>
</li>
<li>
<p><strong>Use Case:</strong> Image classification, object detection, facial recognition.</p>
</li>
<li>
<p><strong>Example:</strong> Self-driving cars, medical imaging.</p>
</li>
</ul>
<h3 style="color:blue;">🧮 4. Radial Basis Function Network (RBFN)</h3>

<ul>
<li>
<p><strong>Description:</strong> Uses radial basis functions as activation functions; good for pattern recognition.</p>
</li>
<li>
<p><strong>Use Case:</strong> Function approximation, time-series prediction.</p>
</li>
<li>
<p><strong>Example:</strong> Signal classification.</p>
</li>
</ul>
<h3 style="color:blue;">🕸️ 5. Modular Neural Network (MNN)</h3>

<ul>
<li>
<p><strong>Description:</strong> Combines multiple networks (modules) that work independently and combine their outputs.</p>
</li>
<li>
<p><strong>Use Case:</strong> When tasks can be split across different models.</p>
</li>
<li>
<p><strong>Example:</strong> Multi-modal tasks (e.g., combining image + text inputs).</p>
</li>
</ul>
<h3 style="color:blue;">🌐 6. Generative Adversarial Networks (GANs)</h3>

<ul>
<li>
<p><strong>Description:</strong> Consists of two networks — Generator &amp; Discriminator — competing against each other.</p>
</li>
<li>
<p><strong>Use Case:</strong> Image generation, data augmentation, deepfake creation.</p>
</li>
<li>
<p><strong>Example:</strong> Creating realistic human faces, art generation.</p>
</li>
</ul>
<h3 style="color:blue;">🔤 7. Transformer Networks</h3>

<ul>
<li>
<p><strong>Description:</strong> Uses self-attention mechanism; excels at handling long-range dependencies.</p>
</li>
<li>
<p><strong>Use Case:</strong> NLP tasks (translation, summarization, Q&amp;A).</p>
</li>
<li>
<p><strong>Example:</strong> ChatGPT, BERT, GPT, T5</p>
</li>
</ul>
<h3 style="color:blue;">🤖 8. Autoencoders</h3>

<ul>
<li>
<p><strong>Description:</strong> Learns compressed representations of data (encoder) and reconstructs them (decoder).</p>
</li>
<li>
<p><strong>Use Case:</strong> Dimensionality reduction, denoising, anomaly detection.</p>
</li>
<li>
<p><strong>Example:</strong> Recommender systems, image compression.</p>
</li>
</ul>
<h3 style="color:blue;">🧱 9. Self-Organizing Maps (SOM)</h3>

<ul>
<li>
<p><strong>Description:</strong> Unsupervised network that reduces dimensions and clusters data.</p>
</li>
<li>
<p><strong>Use Case:</strong> Exploratory data analysis, visualization.</p>
</li>
<li>
<p><strong>Example:</strong> Customer segmentation.</p>
</li>
</ul>
<h3 style="color:blue;">📊 Summary Table</h3>

<table>
<thead>
<tr>
<th>Neural Network Type</th>
<th>Key Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feedforward Neural Network</td>
<td>General classification/regression</td>
</tr>
<tr>
<td>CNN</td>
<td>Image and video analysis</td>
</tr>
<tr>
<td>RNN / LSTM / GRU</td>
<td>Text, speech, sequential data</td>
</tr>
<tr>
<td>GAN</td>
<td>Image &amp; video generation</td>
</tr>
<tr>
<td>Autoencoder</td>
<td>Dimensionality reduction, anomaly detection</td>
</tr>
<tr>
<td>Transformer</td>
<td>Natural Language Processing (NLP)</td>
</tr>
<tr>
<td>RBFN</td>
<td>Function approximation</td>
</tr>
<tr>
<td>SOM</td>
<td>Clustering, dimensionality reduction</td>
</tr>
<tr>
<td>Modular NN</td>
<td>Complex multi-task systems</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">📌 How to choose the right neural network for your problem?</h3>

<p>Choosing the <strong>right neural network</strong> for your problem depends on <strong>three key factors</strong>:</p>
<ol>
<li>
<p><strong>✅ Nature of the data</strong></p>
</li>
<li>
<p><strong>✅ Type of problem</strong></p>
</li>
<li>
<p><strong>✅ Resources (compute, time, data volume)</strong></p>
</li>
</ol>
<h3 style="color:blue;">🔍 1. Understand Your Data Type</h3>

<table>
<thead>
<tr>
<th>Data Type</th>
<th>Description</th>
<th>Common Networks</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Images</strong></td>
<td>Photos, videos, medical scans</td>
<td>CNN, GAN</td>
</tr>
<tr>
<td><strong>Sequences</strong></td>
<td>Time-series, speech, stock data</td>
<td>RNN, LSTM, GRU, Transformer</td>
</tr>
<tr>
<td><strong>Text</strong></td>
<td>Sentences, documents</td>
<td>RNN, LSTM, Transformer</td>
</tr>
<tr>
<td><strong>Tabular</strong></td>
<td>Excel/CSV data (structured rows/columns)</td>
<td>Feedforward Neural Network</td>
</tr>
<tr>
<td><strong>Mixed Modal</strong></td>
<td>Combining text + image + numbers</td>
<td>Modular NN, Multimodal Transformers</td>
</tr>
<tr>
<td><strong>Unlabeled</strong></td>
<td>No ground truth (unsupervised)</td>
<td>Autoencoders, SOM, GAN</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">🔧 2. Match Problem Type to Model</h3>

<table>
<thead>
<tr>
<th>Problem Type</th>
<th>Recommended Networks</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Classification</strong></td>
<td>FNN, CNN, RNN, Transformers</td>
</tr>
<tr>
<td><strong>Regression</strong></td>
<td>FNN, RBFN, LSTM</td>
</tr>
<tr>
<td><strong>Object Detection</strong></td>
<td>CNN (YOLO, Faster R-CNN)</td>
</tr>
<tr>
<td><strong>Image Generation</strong></td>
<td>GANs</td>
</tr>
<tr>
<td><strong>Text Generation</strong></td>
<td>Transformers (GPT), LSTM</td>
</tr>
<tr>
<td><strong>Translation</strong></td>
<td>Transformer (like T5, BERT, MarianMT)</td>
</tr>
<tr>
<td><strong>Anomaly Detection</strong></td>
<td>Autoencoders, LSTM (for time series)</td>
</tr>
<tr>
<td><strong>Clustering/Segmentation</strong></td>
<td>SOM, CNN (for image segmentation), k-Means + Autoencoders</td>
</tr>
<tr>
<td><strong>Recommendation Systems</strong></td>
<td>Autoencoders, Transformers, Embedding models</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">🧠 3. Consider Model Complexity and Resources</h3>

<table>
<thead>
<tr>
<th>Factor</th>
<th>Light Models</th>
<th>Heavy Models</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Data Size</strong></td>
<td>Small → FNN, SVM</td>
<td>Large → CNN, Transformers</td>
</tr>
<tr>
<td><strong>Hardware</strong></td>
<td>CPU → FNN, Autoencoders</td>
<td>GPU → CNN, Transformers</td>
</tr>
<tr>
<td><strong>Real-time need</strong></td>
<td>Fast → FNN, LSTM</td>
<td>Slower → BERT, GPT</td>
</tr>
</tbody>
</table>
<h3 style="color:blue;">📊 Decision Flowchart (Simplified)</h3>

<div class="codehilite"><pre><span></span><code>→<span class="w"> </span><span class="k">Do</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">images</span>?
<span class="w">     </span>→<span class="w"> </span><span class="nv">Yes</span><span class="w"> </span>→<span class="w"> </span><span class="nv">CNN</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">GAN</span>
<span class="w">     </span>→<span class="w"> </span><span class="nv">No</span><span class="w"> </span>→
→<span class="w"> </span><span class="k">Do</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">sequential</span><span class="w"> </span><span class="nv">data</span>?
<span class="w">     </span>→<span class="w"> </span><span class="nv">Yes</span><span class="w"> </span>→<span class="w"> </span><span class="nv">RNN</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">LSTM</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">Transformer</span>
<span class="w">     </span>→<span class="w"> </span><span class="nv">No</span><span class="w"> </span>→
→<span class="w"> </span><span class="nv">Is</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">data</span><span class="w"> </span><span class="nv">tabular</span><span class="w"> </span><span class="ss">(</span><span class="nv">structured</span><span class="ss">)</span>?
<span class="w">     </span>→<span class="w"> </span><span class="nv">Yes</span><span class="w"> </span>→<span class="w"> </span><span class="nv">FNN</span><span class="w"> </span><span class="ss">(</span><span class="nv">MLP</span><span class="ss">)</span>
→<span class="w"> </span><span class="nv">Is</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">problem</span><span class="w"> </span><span class="nv">text</span><span class="o">-</span><span class="nv">based</span><span class="w"> </span><span class="ss">(</span><span class="nv">NLP</span><span class="ss">)</span>?
<span class="w">     </span>→<span class="w"> </span><span class="nv">Yes</span><span class="w"> </span>→<span class="w"> </span><span class="nv">Transformer</span><span class="w"> </span><span class="ss">(</span><span class="nv">e</span>.<span class="nv">g</span>.,<span class="w"> </span><span class="nv">BERT</span><span class="o">/</span><span class="nv">GPT</span><span class="ss">)</span>
→<span class="w"> </span><span class="nv">Is</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">data</span><span class="w"> </span><span class="nv">unlabeled</span>?
<span class="w">     </span>→<span class="w"> </span><span class="nv">Yes</span><span class="w"> </span>→<span class="w"> </span><span class="nv">Autoencoder</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">SOM</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">GAN</span>
</code></pre></div>

<h3 style="color:blue;">🎯 Example Use Cases</h3>

<table>
<thead>
<tr>
<th>Use Case</th>
<th>Best Neural Network</th>
</tr>
</thead>
<tbody>
<tr>
<td>Detecting spam emails</td>
<td>RNN, LSTM, Transformer</td>
</tr>
<tr>
<td>Diagnosing diseases from X-rays</td>
<td>CNN</td>
</tr>
<tr>
<td>Predicting stock prices</td>
<td>LSTM, GRU</td>
</tr>
<tr>
<td>Translating English to French</td>
<td>Transformer (T5, MarianMT)</td>
</tr>
<tr>
<td>Chatbot like ChatGPT</td>
<td>Transformer (GPT)</td>
</tr>
<tr>
<td>Recommending movies on Netflix</td>
<td>Autoencoder, Transformer</td>
</tr>
</tbody>
</table>
<p><strong>✅ Tips</strong></p>
<ul>
<li>
<p><strong>Start simple:</strong> Begin with FNN or Logistic Regression if you’re unsure.</p>
</li>
<li>
<p><strong>Use pre-trained models:</strong> Especially for NLP and vision (e.g., BERT, ResNet).</p>
</li>
<li>
<p><strong>Use AutoML:</strong> Tools like Google AutoML, H2O.ai, or AutoKeras can auto-select the best architecture.</p>
</li>
<li>
<p><strong>Don’t overcomplicate:</strong> Deep learning isn’t always better than traditional ML.</p>
</li>
</ul>
<h3 style="color:blue;">📌 A Neural Network Playground</h3>

<p><a href="https://playground.tensorflow.org/">A Neural Network Playground</a></p>
<p><a href="https://www.ccom.ucsd.edu/~cdeotte/programs/neuralnetwork.html">A Neural Network Playground</a></p>
<p><a href="https://huggingface.co/spaces/ameerazam08/neural-network-playground">A Neural Network Playground</a></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../LinearAlgebra/Overview.html" class="btn btn-neutral float-left" title="Overview"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="Vanishing.html" class="btn btn-neutral float-right" title="Vanishing and Exploding Gradients Problems">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../LinearAlgebra/Overview.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="Vanishing.html" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
