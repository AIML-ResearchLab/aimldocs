{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This the documet is provide details information about Artificial Intelligence and Machine Learning(AIML).</p>"},{"location":"#author","title":"Author","text":"<p>Ganesh Kinkar Giri</p>"},{"location":"A2A/a2a/","title":"Agent2Agent (A2A) Protocol","text":""},{"location":"A2A/a2a/#what-is-a2a-protocol","title":"What is A2A Protocol?","text":"<p>A2A Protocol stands for Agent-to-Agent Protocol. It is a standardized communication protocol that allows autonomous AI agents to talk to, coordinate with, and delegate tasks to other agents in a secure, observable, and structured way\u2014without human intervention.</p> <p>A2A = how agents collaborate with other agents</p>"},{"location":"A2A/a2a/#why-a2a-protocol-exists","title":"Why A2A Protocol exists","text":"<p>In agentic AI systems, a single agent is not enough for:</p> <ul> <li>Complex reasoning</li> <li>Long workflows</li> <li>Cross-domain actions</li> <li>Enterprise-grade automation</li> </ul> <p>So we build multiple specialized agents (Planner, Executor, Validator, Security, etc.).</p> <p>\u27a1\ufe0f A2A Protocol defines how these agents interact.</p>"},{"location":"A2A/a2a/#simple-analogy","title":"Simple analogy","text":"Human world Agentic AI world Email / Slack A2A messages Meeting agenda Task contract Role handoff Agent delegation Status update Agent state sync Approval request Human-in-the-loop gate"},{"location":"A2A/a2a/#core-responsibilities-of-a2a-protocol","title":"Core responsibilities of A2A Protocol","text":"<p>1\ufe0f\u20e3 Agent discovery</p> <p>Agents must know:</p> <ul> <li>Who exists</li> <li>What skills they have</li> <li>How to reach them</li> </ul> <pre><code>{\n  \"agent_id\": \"terraform_executor\",\n  \"capabilities\": [\"iac\", \"aws\", \"terraform_apply\"]\n}\n</code></pre> <p>2\ufe0f\u20e3 Task delegation</p> <p>One agent assigns work to another.</p> <pre><code>{\n  \"from_agent\": \"planner\",\n  \"to_agent\": \"executor\",\n  \"task\": \"Create VPC and EC2 using Terraform\",\n  \"constraints\": {\n    \"region\": \"ap-south-1\",\n    \"budget_limit\": 50\n  }\n}\n</code></pre> <p>3\ufe0f\u20e3 Structured messaging</p> <p>Messages are machine-readable, not free text.</p> <p>Typical message types:</p> <ul> <li><code>TASK_REQUEST</code></li> <li><code>TASK_RESPONSE</code></li> <li><code>STATUS_UPDATE</code></li> <li><code>ERROR</code></li> <li><code>APPROVAL_REQUIRED</code></li> </ul> <p>4\ufe0f\u20e3 State synchronization</p> <p>Agents share:</p> <ul> <li>Progress</li> <li>Intermediate results</li> <li>Failures</li> <li>Retries</li> </ul> <pre><code>{\n  \"agent\": \"executor\",\n  \"state\": \"IN_PROGRESS\",\n  \"step\": \"terraform plan\"\n}\n</code></pre> <p>5\ufe0f\u20e3 Trust, identity &amp; security</p> <p>Enterprise systems require:</p> <ul> <li>Agent identity</li> <li>Authentication</li> <li>Authorization</li> <li>Policy enforcement</li> </ul> <p>Example:</p> <pre><code>{\n  \"agent_id\": \"security_agent\",\n  \"permission\": \"approve_prod_changes\"\n}\n</code></pre>"},{"location":"A2A/a2a/#where-a2a-fits-in-agentic-architecture","title":"Where A2A fits in Agentic Architecture","text":"<pre><code>User\n \u2193\nOrchestrator / Planner\n \u2193 (A2A Protocol)\n+-------------------+\n| Agent Network     |\n|-------------------|\n| Planner Agent     |\n| Executor Agent    |\n| Validator Agent   |\n| Security Agent    |\n| Learning Agent    |\n+-------------------+\n</code></pre> <p>\u27a1\ufe0f A2A is the glue between agents</p>"},{"location":"A2A/a2a/#a2a-vs-tools-vs-mcp-important-distinction","title":"A2A vs Tools vs MCP (important distinction)","text":"Aspect Tools MCP A2A Purpose Call functions External capability access Agent collaboration Who calls Agent \u2192 Tool Agent \u2192 Server Agent \u2192 Agent Stateful \u274c \u26a0\ufe0f \u2705 Long workflows \u274c \u274c \u2705 Governance Minimal Medium High <p>\u2714 A2A is for multi-agent systems \u2714 Tools/MCP are for capabilities</p>"},{"location":"A2A/a2a/#detailed-comparison-table-important","title":"Detailed comparison table (important)","text":"Dimension Current Multi-Agent A2A-Based System Communication Function / graph edges Protocol messages Control Central orchestrator Distributed Agent autonomy Low High Agent identity Implicit Explicit Agent discovery Hard-coded Dynamic Delegation Static Dynamic &amp; negotiated Long-running tasks Weak Native Failure handling Orchestrator retries Agent-to-agent recovery Governance After-thought Built-in Human-in-loop Manual Protocol-level Cross-org agents Almost impossible Designed for it"},{"location":"A2A/a2a/#why-use-the-a2a-protocol","title":"Why use the A2A Protocol","text":"<p>Interoperability</p> <p>Connect agents built on different platforms (LangGraph, CrewAI, Semantic Kernel, custom solutions) to create powerful, composite AI systems.</p> <p>Complex Workflows</p> <p>Enable agents to delegate sub-tasks, exchange information, and coordinate actions to solve complex problems that a single agent cannot.</p> <p>Secure &amp; Opaque</p> <p>Agents interact without needing to share internal memory, tools, or proprietary logic, ensuring security and preserving intellectual property.</p>"},{"location":"A2A/a2a/#how-does-a2a-work-with-mcp","title":"How does A2A work with MCP?","text":"<p>A2A and Model Context Protocol (MCP) are complementary standards for building robust agentic applications:</p> <ul> <li> <p>Model Context Protocol (MCP): Provides agent-to-tool communication. It's a complementary standard that standardizes how an agent connects to its tools, APIs, and resources to get information.</p> </li> <li> <p>IBM ACP: Incorporated into the A2A Protocol</p> </li> <li> <p>Cisco agntcy: A framework that provides components to the Internet of Agents with discovery, group communication, identity and observability and leverages A2A and MCP for agent communication and tool calling.</p> </li> <li> <p>A2A: Provides agent-to-agent communication. As a universal, decentralized standard, A2A acts as the public internet that allows ai agents\u2014including those using MCP, or built with frameworks like agntcy\u2014to interoperate, collaborate, and share their findings.</p> </li> </ul>"},{"location":"A2A/a2a/#what-is-a2a","title":"What is A2A?","text":"<p>The A2A protocol is an open standard that enables seamless communication and collaboration between AI agents. It provides a common language for agents built using diverse frameworks and by different vendors, fostering interoperability and breaking down silos. Agents are autonomous problem-solvers that act independently within their environment. A2A allows agents from different developers, built on different frameworks, and owned by different organizations to unite and work together.</p>"},{"location":"A2A/a2a/#why-use-the-a2a-protocol_1","title":"Why Use the A2A Protocol","text":"<p>A2A addresses key challenges in AI agent collaboration. It provides a standardized approach for agents to interact. </p>"},{"location":"A2A/a2a/#problems-that-a2a-solves","title":"Problems that A2A Solves","text":"<p>Consider a user request for an AI assistant to plan an international trip. This task involves orchestrating multiple specialized agents, such as:</p> <ul> <li>A flight booking agent</li> <li>A hotel reservation agent</li> <li>An agent for local tour recommendations</li> <li>A currency conversion agent</li> </ul> <p>Without A2A, integrating these diverse agents presents several challenges:</p> <ul> <li> <p>Agent Exposure: Developers often wrap agents as tools to expose them to other agents, similar to how tools are exposed in a Multi-agent Control Platform (Model Context Protocol). However, this approach is inefficient because agents are designed to negotiate directly. Wrapping agents as tools limits their capabilities. A2A allows agents to be exposed as they are, without requiring this wrapping.</p> </li> <li> <p>Custom Integrations: Each interaction requires custom, point-to-point solutions, creating significant engineering overhead.</p> </li> <li> <p>Slow Innovation: Bespoke development for each new integration slows innovation.</p> </li> <li> <p>Scalability Issues: Systems become difficult to scale and maintain as the number of agents and interactions grows.</p> </li> <li> <p>Interoperability: This approach limits interoperability, preventing the organic formation of complex AI ecosystems.</p> </li> <li> <p>Security Gaps: Ad hoc communication often lacks consistent security measures.</p> </li> </ul> <p>The A2A protocol addresses these challenges by establishing interoperability for AI agents to interact reliably and securely.</p>"},{"location":"A2A/a2a/#a2a-example-scenario","title":"A2A Example Scenario","text":"<p>A User's Complex Request</p> <p>A user interacts with an AI assistant, giving it a complex prompt like \"Plan an international trip.\"</p> <p></p> <p>The Need for Collaboration</p> <p>The AI assistant receives the prompt and realizes it needs to call upon multiple specialized agents to fulfill the request. These agents include a Flight Booking Agent, a Hotel Reservation Agent, a Currency Conversion Agent, and a Local Tours Agent.</p> <p></p> <p>The Interoperability Challenge</p> <p>The core problem: The agents are unable to work together because each has its own bespoke development and deployment.</p> <p>The consequence of a lack of a standardized protocol is that these agents cannot collaborate with each other let alone discover what they can do. The individual agents (Flight, Hotel, Currency, and Tours) are isolated.</p> <p>The \"With A2A\" Solution</p> <p>The A2A Protocol provides standard methods and data structures for agents to communicate with one another, regardless of their underlying implementation, so the same agents can be used as an interconnected system, communicating seamlessly through the standardized protocol.</p> <p>The AI assistant, now acting as an orchestrator, receives the cohesive information from all the A2A-enabled agents. It then presents a single, complete travel plan as a seamless response to the user's initial prompt.</p> <p></p>"},{"location":"A2A/a2a/#core-benefits-of-a2a","title":"Core Benefits of A2A","text":"<p>Implementing the A2A protocol offers significant advantages across the AI ecosystem:</p> <ul> <li> <p>Secure collaboration: Without a standard, it's difficult to ensure secure communication between agents. A2A uses HTTPS for secure communication and maintains opaque operations, so agents can't see the inner workings of other agents during collaboration.</p> </li> <li> <p>Interoperability: A2A breaks down silos between different AI agent ecosystems, enabling agents from various vendors and frameworks to work together seamlessly.</p> </li> <li> <p>Agent autonomy: A2A allows agents to retain their individual capabilities and act as autonomous entities while collaborating with other agents.</p> </li> <li> <p>Reduced integration complexity: The protocol standardizes agent communication, enabling teams to focus on the unique value their agents provide.</p> </li> <li> <p>Support for LRO: The protocol supports long-running operations (LRO) and streaming with Server-Sent Events (SSE) and asynchronous execution.</p> </li> </ul>"},{"location":"A2A/a2a/#key-design-principles-of-a2a","title":"Key Design Principles of A2A","text":"<p>A2A development follows principles that prioritize broad adoption, enterprise-grade capabilities, and future-proofing.</p> <ul> <li> <p>Simplicity: A2A leverages existing standards like HTTP, JSON-RPC, and Server-Sent Events (SSE). This avoids reinventing core technologies and accelerates developer adoption.</p> </li> <li> <p>Enterprise Readiness: A2A addresses critical enterprise needs. It aligns with standard web practices for robust authentication, authorization, security, privacy, tracing, and monitoring.</p> </li> <li> <p>Asynchronous: A2A natively supports long-running tasks. It handles scenarios where agents or users might not remain continuously connected. It uses mechanisms like streaming and push notifications.</p> </li> <li> <p>Modality Independent: The protocol allows agents to communicate using a wide variety of content types. This enables rich and flexible interactions beyond plain text.</p> </li> <li> <p>Opaque Execution: Agents collaborate effectively without exposing their internal logic, memory, or proprietary tools. Interactions rely on declared capabilities and exchanged context. This preserves intellectual property and enhances security.</p> </li> </ul>"},{"location":"A2A/a2a/#understanding-the-agent-stack-a2a-mcp-agent-frameworks-and-models","title":"Understanding the Agent Stack: A2A, MCP, Agent Frameworks and Models","text":"<p>A2A is situated within a broader agent stack, which includes:</p> <ul> <li> <p>A2A: Standardizes communication among agents deployed in different organizations and developed using diverse frameworks.</p> </li> <li> <p>MCP: Connects models to data and external resources.</p> </li> <li> <p>Frameworks (like ADK): Provide toolkits for constructing agents.</p> </li> <li> <p>Models: Fundamental to an agent's reasoning, these can be any Large Language Model (LLM).</p> </li> </ul> <p></p>"},{"location":"A2A/a2a/#a2a-and-mcp","title":"A2A and MCP","text":"<p>In the broader ecosystem of AI communication, you might be familiar with protocols designed to facilitate interactions between agents, models, and tools. Notably, the Model Context Protocol (MCP) is an emerging standard focused on connecting Large Language Models (LLMs) with data and external resources.</p> <p>The Agent2Agent (A2A) protocol is designed to standardize communication between AI agents, particularly those deployed in external systems. A2A is positioned to complement MCP, addressing a distinct yet related aspect of agent interaction.</p> <ul> <li> <p>MCP's Focus: Reducing the complexity involved in connecting agents with tools and data. Tools are typically stateless and perform specific, predefined functions (e.g., a calculator, a database query).</p> </li> <li> <p>A2A's Focus: Enabling agents to collaborate within their native modalities, allowing them to communicate as agents (or as users) rather than being constrained to tool-like interactions. This enables complex, multi-turn interactions where agents reason, plan, and delegate tasks to other agents. For example, this facilitates multi-turn interactions, such as those involving negotiation or clarification when placing an order.</p> </li> </ul> <p></p>"},{"location":"A2A/a2a/#a2a-request-lifecycle","title":"A2A Request Lifecycle","text":"<p>The A2A request lifecycle is a sequence that details the four main steps a request follows: <code>agent discovery</code>, <code>authentication</code>, <code>sendMessage API</code>, and <code>sendMessageStream API</code>. The following diagram provides a deeper look into the operational flow, illustrating the interactions between the <code>client</code>, <code>A2A server</code>, and <code>auth server</code>.</p> <p></p>"},{"location":"A2A/a2a/#core-concepts-and-components-in-a2a","title":"Core Concepts and Components in A2A","text":"<p>A2A uses a set of core concepts that define how agents interact. Understand these core building blocks to develop or integrate with A2A-compliant systems.</p> <p></p>"},{"location":"A2A/a2a/#core-actors-in-a2a-interactions","title":"Core Actors in A2A Interactions","text":"<ul> <li> <p>User: The end user, which can be a human operator or an automated service. The user initiates a request or defines a goal that requires assistance from one or more AI agents.</p> </li> <li> <p>A2A Client (Client Agent): An application, service, or another AI agent that acts on behalf of the user. The client initiates communication using the A2A protocol.</p> </li> <li> <p>A2A Server (Remote Agent): An AI agent or an agentic system that exposes an HTTP endpoint implementing the A2A protocol. It receives requests from clients, processes tasks, and returns results or status updates. From the client's perspective, the remote agent operates as an opaque (black-box) system, meaning its internal workings, memory, or tools are not exposed.</p> </li> </ul>"},{"location":"A2A/a2a/#fundamental-communication-elements","title":"Fundamental Communication Elements","text":"Element Description Key Purpose Agent Card A JSON metadata document describing an agent's identity, capabilities, endpoint, skills, and authentication requirements. Enables clients to discover agents and understand how to interact with them securely and effectively. Task A stateful unit of work initiated by an agent, with a unique ID and a defined lifecycle. Facilitates tracking of long-running operations and enables multi-turn interactions and collaboration. Message A single turn of communication between a client and an agent, containing content and a role (e.g., \"user\" or \"agent\"). Conveys instructions, context, questions, answers, or status updates that are not necessarily formal artifacts. Part The fundamental content container (e.g., TextPart, FilePart, DataPart) used within Messages and Artifacts. Provides flexibility for agents to exchange various content types within messages and artifacts. Artifact A tangible output generated by an agent during a task (e.g., a document, image, or structured data). Delivers the concrete results of an agent's work, ensuring structured and retrievable outputs."},{"location":"A2A/a2a/#interaction-mechanisms","title":"Interaction Mechanisms","text":"<p>The A2A Protocol supports various interaction patterns to accommodate different needs for responsiveness and persistence. These mechanisms ensure that agents can exchange information efficiently and reliably, regardless of the task's complexity or duration:</p> <ul> <li> <p>Request/Response (Polling): Clients send a request and the server responds. For long-running tasks, the client periodically polls the server for updates.</p> </li> <li> <p>Streaming with Server-Sent Events (SSE): Clients initiate a stream to receive real-time, incremental results or status updates from the server over an open HTTP connection.</p> </li> <li> <p>Push Notifications: For very long-running tasks or disconnected scenarios, the server can actively send asynchronous notifications to a client-provided webhook when significant task updates occur.</p> </li> </ul>"},{"location":"A2A/a2a/#agent-cards","title":"Agent Cards","text":"<p>The Agent Card is a JSON document that serves as a digital business card for initial discovery and interaction setup. It provides essential metadata about an agent. Clients parse this information to determine if an agent is suitable for a given task, how to structure requests, and how to communicate securely. Key information includes identity, service endpoint (URL), A2A capabilities, authentication requirements, and a list of skills.</p>"},{"location":"A2A/a2a/#messages-and-parts","title":"Messages and Parts","text":"<p>A message represents a single turn of communication between a client and an agent. It includes a role (\"user\" or \"agent\") and a unique <code>messageId</code>. It contains one or more Part objects, which are granular containers for the actual content. This design allows A2A to be modality independent.</p> <ul> <li> <p>TextPart: Contains plain textual content.</p> </li> <li> <p>FilePart: Represents a file. It can be transmitted either inline (Base64 encoded) or through a URI. It includes metadata like \"name\" and \"mimeType\".</p> </li> <li> <p>DataPart: Carries structured JSON data. This is useful for forms, parameters, or any machine-readable information.</p> </li> </ul>"},{"location":"A2A/a2a/#artifacts","title":"Artifacts","text":"<p>An artifact represents a tangible output or a concrete result generated by a remote agent during task processing. Unlike general messages, artifacts are the actual deliverables. An artifact has a unique artifactId, a human-readable name, and consists of one or more part objects. Artifacts are closely tied to the task lifecycle and can be streamed incrementally to the client.</p>"},{"location":"A2A/a2a/#agent-response-task-or-message","title":"Agent Response: Task or Message","text":"<p>The agent response can be a new <code>Task</code> (when the agent needs to perform a long-running operation) or a <code>Message</code> (when the agent can respond immediately).</p>"},{"location":"A2A/a2a/#other-important-concepts","title":"Other Important Concepts","text":"Aspect Description Context (<code>contextId</code>) A server-generated identifier used to logically group multiple related Task objects, providing shared context across a series of interactions. Transport and Format A2A communication occurs over HTTP(S), using JSON-RPC 2.0 as the payload format for all requests and responses. Authentication &amp; Authorization A2A relies on standard web security practices. Authentication requirements are declared in the Agent Card, and credentials (e.g., OAuth tokens, API keys) are passed via HTTP headers, separate from A2A messages. Agent Discovery The process by which clients locate Agent Cards to identify available A2A servers and understand their capabilities. Extensions A2A allows agents to declare custom protocol extensions within their Agent Card, enabling extensibility beyond the core protocol."},{"location":"A2A/a2a/#agent-discovery-in-a2a","title":"Agent Discovery in A2A","text":"<p>To collaborate using the Agent2Agent (A2A) protocol, AI agents need to first find each other and understand their capabilities. A2A standardizes agent self-descriptions through the Agent Card. However, discovery methods for these Agent Cards vary by environment and requirements. The Agent Card defines what an agent offers. Various strategies exist for a client agent to discover these cards. The choice of strategy depends on the deployment environment and security requirements.</p>"},{"location":"A2A/a2a/#the-role-of-the-agent-card","title":"The Role of the Agent Card","text":"<p>The Agent Card is a JSON document that serves as a digital \"business card\" for an A2A Server (the remote agent). It is crucial for agent discovery and interaction. The key information included in an Agent Card is as follows:</p> <ul> <li> <p>Identity: Includes <code>name</code>, <code>description</code>, and <code>provider information</code>.</p> </li> <li> <p>Service Endpoint: Specifies the <code>url</code> for the A2A service.</p> </li> <li> <p>A2A Capabilities: Lists supported features such as <code>streaming</code> or <code>pushNotifications</code>.</p> </li> <li> <p>Authentication: Details the required schemes (e.g., <code>\"Bearer\"</code>, <code>\"OAuth2\"</code>).</p> </li> <li> <p>Skills: Describes the agent's tasks using <code>AgentSkill</code> objects, including <code>id</code>, <code>name</code>, <code>description</code>, <code>inputModes</code>, <code>outputModes</code>, and <code>examples</code>.</p> </li> </ul> <p>Client agents use the Agent Card to determine an agent's suitability, structure requests, and ensure secure communication.</p>"},{"location":"A2A/a2a/#discovery-strategies","title":"Discovery Strategies","text":"<p>The following sections detail common strategies used by client agents to discover remote Agent Cards:</p>"},{"location":"A2A/a2a/#1-well-known-uri","title":"1. Well-Known URI","text":"<p>This approach is recommended for public agents or agents intended for broad discovery within a specific domain.</p> <ul> <li>Mechanism: A2A Servers make their Agent Card discoverable by hosting it at a standardized, <code>well-known</code> URI on their domain. The standard path is <code>https://{agent-server-domain}/.well-known/agent-card.json</code>, following the principles of <code>RFC 8615</code>.</li> </ul> <p>Process:  1. A client agent knows or programmatically discovers the domain of a potential A2A Server (e.g., <code>smart-thermostat.example.com</code>).</p> <ol> <li> <p>The client performs an HTTP GET request to <code>https://smart-thermostat.example.com/.well-known/agent-card.json</code>.</p> </li> <li> <p>If the Agent Card exists and is accessible, the server returns it as a JSON response.</p> </li> </ol> <p>Advantages:</p> <ul> <li>Ease of implementation</li> <li>Adheres to standards</li> <li>Facilitates automated discovery</li> </ul> <p>Considerations:</p> <ul> <li>Best suited for open or domain-controlled discovery scenarios.</li> <li>Authentication is necessary at the endpoint serving the Agent Card if it contains sensitive details.</li> </ul>"},{"location":"A2A/a2a/#2-curated-registries-catalog-based-discovery","title":"2. Curated Registries (Catalog-Based Discovery)","text":"<p>This approach is employed in enterprise environments or public marketplaces, where Agent Cards are often managed by a central registry. The curated registry acts as a central repository, allowing clients to query and discover agents based on criteria like \"skills\" or \"tags\".</p> <ul> <li> <p>Mechanism: An intermediary service (the registry) maintains a collection of Agent Cards. Clients query this registry to find agents based on various criteria (e.g., skills offered, tags, provider name, capabilities).</p> </li> <li> <p>Process:</p> </li> <li> <p>A2A Servers publish their Agent Cards to the registry.</p> </li> <li>Client agents query the registry's API, and search by criteria such as \"specific skills\".</li> <li> <p>The registry returns matching Agent Cards or references.</p> </li> <li> <p>Advantages:</p> </li> <li> <p>Centralized management and governance.</p> </li> <li>Capability-based discovery (e.g., by skill).</li> <li>Support for access controls and trust frameworks.</li> <li> <p>Applicable in both private and public marketplaces.</p> </li> <li> <p>Considerations:</p> </li> <li> <p>Requires deployment and maintenance of a registry service.</p> </li> <li>The current A2A specification does not prescribe a standard API for curated registries.</li> </ul>"},{"location":"A2A/a2a/#3-direct-configuration-private-discovery","title":"3. Direct Configuration / Private Discovery","text":"<p>This approach is used for tightly coupled systems, private agents, or development purposes, where clients are directly configured with Agent Card information or URLs.</p> <ul> <li> <p>Mechanism: Client applications utilize hardcoded details, configuration files, environment variables, or proprietary APIs for discovery.</p> </li> <li> <p>Process: The process is specific to the application's deployment and configuration strategy.</p> </li> <li> <p>Advantages: This method is straightforward for establishing connections within known, static relationships.</p> </li> <li> <p>Considerations:</p> </li> <li> <p>Inflexible for dynamic discovery scenarios.</p> </li> <li>Changes to Agent Card information necessitate client reconfiguration.</li> <li>Proprietary API-based discovery also lacks standardization.</li> </ul>"},{"location":"A2A/a2a/#securing-agent-cards","title":"Securing Agent Cards","text":"<p>Agent Cards include sensitive information, such as:</p> <ul> <li>URLs for internal or restricted agents.</li> <li>Descriptions of sensitive skills.</li> </ul>"},{"location":"A2A/a2a/#protection-mechanisms","title":"Protection Mechanisms","text":"<p>To mitigate risks, the following protection mechanisms should be considered:</p> <ul> <li> <p>Authenticated Agent Cards: We recommend the use of authenticated extended agent cards for sensitive information or for serving a more detailed version of the card.</p> </li> <li> <p>Secure Endpoints: Implement access controls on the HTTP endpoint serving the Agent Card (e.g., <code>/.well-known/agent-card.json</code> or registry API). The methods include:</p> </li> <li> <p>Mutual TLS (mTLS)</p> </li> <li>Network restrictions (e.g., IP ranges)</li> <li> <p>HTTP Authentication (e.g., OAuth 2.0)</p> </li> <li> <p>Registry Selective Disclosure: Registries return different Agent Cards based on the client's identity and permissions.</p> </li> </ul> <p>Any Agent Card containing sensitive data must be protected with authentication and authorization mechanisms. The A2A specification strongly recommends the use of out-of-band dynamic credentials rather than embedding static secrets within the Agent Card.</p>"},{"location":"A2A/a2a/#enterprise-implementation-of-a2a","title":"Enterprise Implementation of A2A","text":"<p>The Agent2Agent (A2A) protocol is designed with enterprise requirements at its core.</p>"},{"location":"A2A/a2a/#1transport-level-security-tls","title":"1.Transport Level Security (TLS)","text":"<p>Ensuring the confidentiality and integrity of data in transit is fundamental for any enterprise application.</p> <ul> <li>HTTPS Mandate: All A2A communication in production environments must occur over HTTPS.</li> <li>Modern TLS Standards: Implementations should use modern TLS versions.</li> <li>Server Identity Verification: 2A clients should verify the A2A server's identity by validating its TLS certificate against trusted certificate authorities during the TLS handshake.</li> </ul>"},{"location":"A2A/a2a/#2authentication","title":"2.Authentication","text":"<p>A2A delegates authentication to standard web mechanisms. It primarily relies on HTTP headers and established standards like OAuth2 and OpenID Connect. Authentication requirements are advertised by the A2A server in its Agent Card.</p> <ul> <li>No Identity in Payload:A2A protocol payloads, such as JSON-RPC messages, don't carry user or client identity information directly. </li> <li>Agent Card Declaration:The A2A server's Agent Card describes the authentication schemes it supports in its security field and aligns with those defined in the OpenAPI Specification for authentication. </li> <li>Out-of-Band Credential Acquisition:The A2A Client obtains the necessary credentials, such as OAuth 2.0 tokens or API keys, through processes external to the A2A protocol itself. </li> <li>HTTP Header Transmission:Credentials must be transmitted in standard HTTP headers as per the requirements of the chosen authentication scheme. Examples include <code>Authorization: Bearer &lt;TOKEN&gt; or API-Key: &lt;KEY_VALUE&gt;</code>.</li> <li>Server-Side Validation: The A2A server must authenticate every incoming request using the credentials provided in the HTTP headers.</li> <li>In-Task Authentication (Secondary Credentials):If an agent needs additional credentials to access a different system or service during a task (for example, to use a specific tool on the user's behalf), the A2A server indicates to the client that more information is needed. The client is then responsible for obtaining these secondary credentials through a process outside of the A2A protocol itself (for example, an OAuth flow) and providing them back to the A2A server to continue the task.</li> </ul> <p>## 3.Authorization</p> <p>Once a client is authenticated, the A2A server is responsible for authorizing the request. Authorization logic is specific to the agent's implementation, the data it handles, and applicable enterprise policies.</p> <ul> <li> <p>Granular Control: Authorization should be applied based on the authenticated identity, which could represent an end user, a client application, or both.</p> </li> <li> <p>Skill-Based Authorization: Access can be controlled on a per-skill basis, as advertised in the Agent Card. For example, specific OAuth scopes should grant an authenticated client access to invoke certain skills but not others.</p> </li> <li> <p>Data and Action-Level Authorization: Agents that interact with backend systems, databases, or tools must enforce appropriate authorization before performing sensitive actions or accessing sensitive data through those underlying resources. The agent acts as a gatekeeper.</p> </li> <li> <p>Principle of Least Privilege: Agents must grant only the necessary permissions required for a client or user to perform their intended operations through the A2A interface.</p> </li> </ul> <p>## 4.Data Privacy and Confidentiality</p> <p>Protecting sensitive data exchanged between agents is paramount, requiring strict adherence to privacy regulations and best practices.</p> <ul> <li> <p>Sensitivity Awareness: Implementers must be acutely aware of the sensitivity of data exchanged in Message and Artifact parts of A2A interactions. </p> </li> <li> <p>Compliance: Ensure compliance with relevant data privacy regulations such as GDPR, CCPA, and HIPAA, based on the domain and data involved.</p> </li> <li> <p>Data Minimization: Avoid including or requesting unnecessarily sensitive information in A2A exchanges.</p> </li> <li> <p>Secure Handling: Protect data both in transit, using TLS as mandated, and at rest if persisted by agents, according to enterprise data security policies and regulatory requirements.</p> </li> </ul>"},{"location":"A2A/a2a/#5tracing-observability-and-monitoring","title":"5.Tracing, Observability, and Monitoring","text":"<p>A2A's reliance on HTTP allows for straightforward integration with standard enterprise tracing, logging, and monitoring tools, providing critical visibility into inter-agent workflows.</p> <ul> <li> <p>Distributed Tracing: A2A Clients and Servers should participate in distributed tracing systems. For example, use OpenTelemetry to propagate trace context, including trace IDs and span IDs, through standard HTTP headers, such as W3C Trace Context headers. This enables end-to-end visibility for debugging and performance analysis.</p> </li> <li> <p>Comprehensive Logging: Log details on both client and server, including taskId, sessionId, correlation IDs, and trace context for troubleshooting and auditing.</p> </li> <li> <p>Metrics: A2A servers should expose key operational metrics, such as request rates, error rates, task processing latency, and resource utilization, to enable performance monitoring, alerting, and capacity planning. </p> </li> <li> <p>Auditing: Audit significant events, such as task creation, critical state changes, and agent actions, especially when involving sensitive data or high-impact operations.</p> </li> </ul>"},{"location":"A2A/a2a/#6api-management-and-governance","title":"6.API Management and Governance","text":"<p>For A2A servers exposed externally, across organizational boundaries, or even within large enterprises, integration with API Management solutions is highly recommended, as this provides:</p> <ul> <li> <p>Centralized Policy Enforcement: Consistent application of security policies such as authentication and authorization, rate limiting, and quotas.</p> </li> <li> <p>Traffic Management: Load balancing, routing, and mediation.</p> </li> <li> <p>Analytics and Reporting: Insights into agent usage, performance, and trends.</p> </li> <li> <p>Developer Portals: Facilitate discovery of A2A-enabled agents, provide documentation such as Agent Cards, and streamline onboarding for client developers.</p> </li> </ul>"},{"location":"A2A/a2a/#life-of-a-task","title":"Life of a Task","text":"<p>In the Agent2Agent (A2A) Protocol, interactions can range from simple, stateless exchanges to complex, long-running processes. When an agent receives a message from a client, it can respond in one of two fundamental ways:</p> <ul> <li> <p>Respond with a Stateless <code>Message</code>: This type of response is typically used for immediate, self-contained interactions that conclude without requiring further state management.</p> </li> <li> <p>Initiate a Stateful <code>Task</code>: If the response is a Task, the agent will process it through a defined lifecycle, communicating progress and requiring input as needed, until it reaches an interrupted state (e.g., input-required, auth-required) or a terminal state (e.g., completed, canceled, rejected, failed).</p> </li> </ul>"},{"location":"A2A/a2a/#group-related-interactions","title":"Group Related Interactions","text":"<p>A <code>contextId</code> is a crucial identifier that logically groups multiple <code>Task</code> objects and independent <code>Message</code> objects, providing continuity across a series of interactions.</p> <ul> <li> <p>When a client sends a message for the first time, the agent responds with a new <code>contextId</code>. If a task is initiated, it will also have a <code>taskId</code>.</p> </li> <li> <p>Clients can send subsequent messages and include the same <code>contextId</code> to indicate that they are continuing their previous interaction within the same context.</p> </li> <li> <p>Clients optionally attach the <code>taskId</code> to a subsequent message to indicate that it continues that specific task.</p> </li> </ul> <p>The <code>contextId</code> enables collaboration towards a common goal or a shared contextual session across multiple, potentially concurrent tasks. Internally, an A2A agent (especially one using an LLM) uses the <code>contextId</code> to manage its internal conversational state or its LLM context.</p>"},{"location":"A2A/a2a/#agent-response-message-or-task","title":"Agent Response: Message or Task","text":"<p>The choice between responding with a Message or a Task depends on the nature of the interaction and the agent's capabilities:</p> <ul> <li> <p>Messages for Trivial Interactions: Message objects are suitable for transactional interactions that don't require long-running processing or complex state management. An agent might use messages to negotiate the acceptance or scope of a task before committing to a Task object.</p> </li> <li> <p>Tasks for Stateful Interactions: Once an agent maps the intent of an incoming message to a supported capability that requires substantial, trackable work over an extended period, the agent responds with a <code>Task</code> object.</p> </li> </ul> <p>Conceptually, agents operate at different levels of complexity:</p> <ul> <li> <p>Message-only Agents: Always respond with <code>Message</code> objects. They typically don't manage complex state or long-running executions, and use <code>contextId</code> to tie messages together. These agents might directly wrap LLM invocations and simple tools.</p> </li> <li> <p>Task-generating Agents: Always respond with <code>Task</code> objects, even for responses, which are then modeled as completed tasks. Once a task is created, the agent will only return <code>Task</code> objects in response to messages sent, and once a task is complete, no more messages can be sent. This approach avoids deciding between Task versus <code>Message</code>, but creates completed task objects for even simple interactions.</p> </li> <li> <p>Hybrid Agents: Generate both <code>Message</code> and <code>Task</code> objects. These agents use messages to negotiate agent capability and the scope of work for a task, then send a <code>Task</code> object to track execution and manage states like <code>input-required</code> or error handling. Once a task is created, the agent will only return Task objects in response to messages sent, and once a task is complete, no more messages can be sent. A hybrid agent uses messages to negotiate the scope of a task, and then generate a task to track its execution.</p> </li> </ul>"},{"location":"A2A/a2a/#task-refinements","title":"Task Refinements","text":"<p>Clients often need to send new requests based on task results or refine the outputs of previous tasks. This is modeled by starting another interaction using the same <code>contextId</code> as the original task. Clients further hint the agent by providing references to the original task using <code>referenceTaskIds</code> in the <code>Message</code> object. The agent then responds with either a new <code>Task</code> or a <code>Message</code>.</p>"},{"location":"A2A/a2a/#task-immutability","title":"Task Immutability","text":"<p>Once a task reaches a terminal state (completed, canceled, rejected, or failed), it cannot restart. Any subsequent interaction related to that task, such as a refinement, must initiate a new task within the same <code>contextId</code>. This principle offers several benefits:</p> <ul> <li> <p>Task Immutability. Clients reliably reference tasks and their associated state, artifacts, and messages, providing a clean mapping of inputs to outputs. This is valuable for orchestration and traceability.</p> </li> <li> <p>Clear Unit of Work. Every new request, refinement, or follow-up becomes a distinct task. This simplifies bookkeeping, allows for granular tracking of an agent's work, and enables tracing each artifact to a specific unit of work.</p> </li> <li> <p>Easier Implementation. This removes ambiguity for agent developers regarding whether to create a new task or restart an existing one.</p> </li> </ul>"},{"location":"A2A/a2a/#parallel-follow-ups","title":"Parallel Follow-ups","text":"<p>A2A supports parallel work by enabling agents to create distinct, parallel tasks for each follow-up message sent within the same contextId. This allows clients to track individual tasks and create new dependent tasks as soon as a prerequisite task is complete.</p> <p>For example:</p> <ul> <li>Task 1: Book a flight to Helsinki.</li> <li>Task 2: Based on Task 1, book a hotel.</li> <li>Task 3: Based on Task 1, book a snowmobile activity.</li> <li>Task 4: Based on Task 2, add a spa reservation to the hotel booking.</li> </ul>"},{"location":"A2A/a2a/#referencing-previous-artifacts","title":"Referencing Previous Artifacts","text":"<p>The serving agent infers the relevant artifact from a referenced task or from the <code>contextId</code>. As the domain expert, the serving agent is best suited to resolve ambiguity or identify missing information. If there is ambiguity, the agent asks the client for clarification by returning an <code>input-required</code> state. The client then specifies the artifact in its response, optionally populating artifact references (<code>artifactId</code>, <code>taskId</code>) in <code>Part</code> metadata.</p>"},{"location":"A2A/a2a/#tracking-artifact-mutation","title":"Tracking Artifact Mutation","text":"<p>Follow-up or refinement tasks often lead to the creation of new artifacts based on older ones. Tracking these mutations is important to ensure that only the most recent version of an artifact is used in subsequent interactions. This could be conceptualized as a version history, where each new artifact is linked to its predecessor.</p> <p>However, the client is in the best position to manage this artifact linkage. The client determines what constitutes an acceptable result and has the ability to accept or reject new versions. Therefore, the serving agent shouldn't be responsible for tracking artifact mutations, and this linkage is not part of the A2A protocol specification. Clients should maintain this version history on their end and present the latest acceptable version to the user.</p> <p>To facilitate client-side tracking, serving agents should use a consistent artifact-name when generating a refined version of an existing artifact.</p> <p>When initiating follow-up or refinement tasks, the client should explicitly reference the specific artifact they intend to refine, ideally the \"latest\" version from their perspective. If the artifact reference is not provided, the serving agent can:</p> <ul> <li>Attempt to infer the intended artifact based on the current contextId.</li> <li>If there is ambiguity or insufficient context, the agent should respond with an input-required task state to request clarification from the client.</li> </ul>"},{"location":"A2A/a2a/#example-follow-up-scenario","title":"Example Follow-up Scenario","text":"<p>The following example illustrates a typical task flow with a follow-up:</p> <ol> <li>Client sends a message to the agent:</li> </ol> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"req-001\",\n  \"method\": \"message.send\",\n  \"params\": {\n    \"message\": {\n      \"role\": \"user\",\n      \"parts\": [\n        {\n          \"text\": \"Generate an image of a sailboat on the ocean.\"\n        }\n      ],\n      \"messageId\": \"msg-user-001\"\n    }\n  }\n}\n</code></pre> <ol> <li>Agent responds with a boat image (completed task):</li> </ol> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"req-001\",\n  \"result\": {\n    \"id\": \"task-boat-gen-123\",\n    \"contextId\": \"ctx-conversation-abc\",\n    \"status\": {\n      \"state\": \"completed\"\n    },\n    \"artifacts\": [\n      {\n        \"artifactId\": \"artifact-boat-v1-xyz\",\n        \"name\": \"sailboat_image.png\",\n        \"description\": \"A generated image of a sailboat on the ocean.\",\n        \"parts\": [\n          {\n            \"file\": {\n              \"name\": \"sailboat_image.png\",\n              \"mediaType\": \"image/png\",\n              \"fileWithBytes\": \"base64_encoded_png_data_of_a_sailboat\"\n            }\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <ol> <li>Client asks to color the boat red. This refinement request refers to the previous <code>taskId</code> and uses the same <code>contextId</code>.</li> </ol> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"req-002\",\n  \"method\": \"message.send\",\n  \"params\": {\n    \"message\": {\n      \"role\": \"user\",\n      \"messageId\": \"msg-user-002\",\n      \"contextId\": \"ctx-conversation-abc\",\n      \"referenceTaskIds\": [\n        \"task-boat-gen-123\"\n      ],\n      \"parts\": [\n        {\n          \"text\": \"Please modify the sailboat to be red.\"\n        }\n      ]\n    }\n  }\n}\n</code></pre> <ol> <li>Agent responds with a new image artifact (new task, same context, same artifact name): The agent creates a new task within the same <code>contextId</code>. The new boat image artifact retains the same name but has a new <code>artifactId</code>.</li> </ol> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"req-002\",\n  \"result\": {\n    \"id\": \"task-boat-color-456\",\n    \"contextId\": \"ctx-conversation-abc\",\n    \"status\": {\n      \"state\": \"completed\"\n    },\n    \"artifacts\": [\n      {\n        \"artifactId\": \"artifact-boat-v2-red-pqr\",\n        \"name\": \"sailboat_image.png\",\n        \"description\": \"A generated image of a red sailboat on the ocean.\",\n        \"parts\": [\n          {\n            \"file\": {\n              \"name\": \"sailboat_image.png\",\n              \"mediaType\": \"image/png\",\n              \"fileWithBytes\": \"base64_encoded_png_data_of_a_RED_sailboat\"\n            }\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"A2A/a2a/#extensions-in-a2a","title":"Extensions in A2A","text":"<p>The Agent2Agent (A2A) protocol provides a strong foundation for inter-agent communication. However, specific domains or advanced use cases often require additional structure, custom data, or new interaction patterns beyond the generic methods. Extensions are A2A's powerful mechanism for layering new capabilities onto the base protocol.</p> <p>Extensions allow for extending the A2A protocol with new data, requirements, RPC methods, and state machines. Agents declare their support for specific extensions in their Agent Card, and clients can then opt-in to the behavior offered by an extension as part of requests they make to the agent. Extensions are identified by a URI and defined by their own specification. Anyone is able to define, publish, and implement an extension.</p> <p>The flexibility of extensions allows for customizing A2A without fragmenting the core standard, fostering innovation and domain-specific optimizations.</p>"},{"location":"A2A/a2a/#scope-of-extensions","title":"Scope of Extensions","text":"<p>The exact set of possible ways to use extensions is intentionally broad, facilitating the ability to expand A2A beyond known use cases. However, some foreseeable applications include:</p> <ul> <li> <p>Data-only Extensions: Exposing new, structured information in the Agent Card that doesn't impact the request-response flow. For example, an extension could add structured data about an agent's GDPR compliance.</p> </li> <li> <p>Profile Extensions: Overlaying additional structure and state change requirements on the core request-response messages. This type effectively acts as a profile on the core A2A protocol, narrowing the space of allowed values (for example, requiring all messages to use <code>DataParts</code> adhering to a specific schema). This can also include augmenting existing states in the task state machine by using metadata. For example, an extension could define a 'generating-image' sub-state when <code>TaskStatus.state</code> is 'working' and <code>TaskStatus.message.metadata[\"generating-image\"]</code> is true.</p> </li> <li> <p>Method Extensions (Extended Skills): Adding entirely new RPC methods beyond the core set defined by the protocol. An Extended Skill refers to a capability or function an agent gains or exposes specifically through the implementation of an extension that defines new RPC methods. For example, a <code>task-history</code> extension might add a <code>tasks/search</code> RPC method to retrieve a list of previous tasks, effectively providing the agent with a new, extended skill.</p> </li> <li> <p>State Machine Extensions: Adding new states or transitions to the task state machine.</p> </li> </ul>"},{"location":"A2A/a2a/#list-of-example-extensions","title":"List of Example Extensions","text":"Extension Name Description Secure Passport Extension Adds a trusted, contextual layer for immediate personalization and reduced overhead (v1). Hello World / Timestamp Extension A simple extension demonstrating how to augment base A2A types by adding timestamps to the metadata field of Message and Artifact objects (v1). Traceability Extension Provides traceability capabilities, including a Python implementation and basic usage examples, enabling end-to-end request tracking (v1). Agent Gateway Protocol (AGP) Extension A core protocol layer or routing extension that introduces Autonomous Squads (ASq) and routes Intent payloads based on declared Capabilities, enhancing system scalability (v1)."},{"location":"A2A/a2a/#limitations","title":"Limitations","text":"<p>There are some changes to the protocol that extensions don't allow, primarily to prevent breaking core type validations:</p> <ul> <li> <p>Changing the Definition of Core Data Structures: For example, adding new fields or removing required fields to protocol-defined data structures). Extensions should place custom attributes in the metadata map present on core data structures.</p> </li> <li> <p>Adding New Values to Enum Types: Extensions should use existing enum values and annotate additional semantic meaning in the metadata field.</p> </li> </ul>"},{"location":"A2A/a2a/#extension-declaration","title":"Extension Declaration","text":"<p>Agents declare their support for extensions in their Agent Card by including <code>AgentExtension</code> objects within their <code>AgentCapabilities</code> object.</p> <p>The following is an example of an Agent Card with an extension:</p> <pre><code>{\n  \"name\": \"Magic 8-ball\",\n  \"description\": \"An agent that can tell your future... maybe.\",\n  \"version\": \"0.1.0\",\n  \"url\": \"https://example.com/agents/eightball\",\n  \"capabilities\": {\n    \"streaming\": true,\n    \"extensions\": [\n      {\n        \"uri\": \"https://example.com/ext/konami-code/v1\",\n        \"description\": \"Provide cheat codes to unlock new fortunes\",\n        \"required\": false,\n        \"params\": {\n          \"hints\": [\n            \"When your sims need extra cash fast\",\n            \"You might deny it, but we've seen the evidence of those cows.\"\n          ]\n        }\n      }\n    ]\n  },\n  \"defaultInputModes\": [\"text/plain\"],\n  \"defaultOutputModes\": [\"text/plain\"],\n  \"skills\": [\n    {\n      \"id\": \"fortune\",\n      \"name\": \"Fortune teller\",\n      \"description\": \"Seek advice from the mystical magic 8-ball\",\n      \"tags\": [\"mystical\", \"untrustworthy\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"A2A/a2a/#required-extensions","title":"Required Extensions","text":"<p>While extensions generally offer optional functionality, some agents may have stricter requirements. When an Agent Card declares an extension as required: true, it signals to clients that some aspect of the extension impacts how requests are structured or processed, and that the client must abide by it. Agents shouldn't mark data-only extensions as required. If a client does not request activation of a required extension, or fails to follow its protocol, the agent should reject the incoming request with an appropriate error.</p>"},{"location":"A2A/a2a/#extension-specification","title":"Extension Specification","text":"<p>The detailed behavior and structure of an extension are defined by its specification. While the exact format is not mandated, it should contain at at least:</p> <ul> <li>The specific URI(s) that identify the extension.</li> <li>The schema and meaning of objects specified in the <code>params</code> field of the <code>AgentExtension</code> object.</li> <li>Schemas of any additional data structures communicated between client and agent.</li> <li>Details of new request-response flows, additional endpoints, or any other logic required to implement the extension.</li> </ul>"},{"location":"A2A/a2a/#extension-dependencies","title":"Extension Dependencies","text":"<p>Extensions might depend on other extensions. This can be a required dependency (where the extension cannot function without the dependent) or an optional one (where additional functionality is enabled if another extension is present). Extension specifications should document these dependencies. It is the client's responsibility to activate an extension and all its required dependencies as listed in the extension's specification.</p>"},{"location":"A2A/a2a/#extension-activation","title":"Extension Activation","text":"<p>Extensions default to being inactive, providing a baseline experience for extension-unaware clients. Clients and agents perform negotiation to determine which extensions are active for a specific request.</p> <ol> <li> <p>Client Request: A client requests extension activation by including the A2A-Extensions header in the HTTP request to the agent. The value is a comma-separated list of extension URIs the client intends to activate.</p> </li> <li> <p>Agent Processing: Agents are responsible for identifying supported extensions in the request and performing the activation. Any requested extensions not supported by the agent can be ignored.</p> </li> <li> <p>Response: Once the agent has identified all activated extensions, the response SHOULD include the A2A-Extensions header, listing all extensions that were successfully activated for that request.</p> </li> </ol> <p></p> <p>Example request showing extension activation:</p> <pre><code>POST /agents/eightball HTTP/1.1\nHost: example.com\nContent-Type: application/json\nA2A-Extensions: https://example.com/ext/konami-code/v1\nContent-Length: 519\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"message/send\",\n  \"id\": \"1\",\n  \"params\": {\n    \"message\": {\n      \"kind\": \"message\",\n      \"messageId\": \"1\",\n      \"role\": \"user\",\n      \"parts\": [{\"kind\": \"text\", \"text\": \"Oh magic 8-ball, will it rain today?\"}]\n    },\n    \"metadata\": {\n      \"https://example.com/ext/konami-code/v1/code\": \"motherlode\"\n    }\n  }\n}\n</code></pre> <p>Corresponding response echoing activated extensions:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\nA2A-Extensions: https://example.com/ext/konami-code/v1\nContent-Length: 338\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"1\",\n  \"result\": {\n    \"kind\": \"message\",\n    \"messageId\": \"2\",\n    \"role\": \"agent\",\n    \"parts\": [{\"kind\": \"text\", \"text\": \"That's a bingo!\"}]\n  }\n}\n</code></pre>"},{"location":"A2A/a2a/#implementation-considerations","title":"Implementation Considerations","text":"<p>While the A2A protocol defines the functionality of extensions, this section provides guidance on their implementation\u2014best practices for authoring, versioning, and distributing extension implementations.</p> <ul> <li> <p>Versioning: Extension specifications evolve. It is crucial to have a clear versioning strategy to ensure that clients and agents can negotiate compatible implementations.</p> </li> <li> <p>Recommendation: Use the extension's URI as the primary version identifier, ideally including a version number (for example, https://example.com/ext/my-extension/v1).</p> </li> <li>Breaking Changes: A new URI MUST be used when introducing a breaking change to an extension's logic, data structures, or required parameters.</li> <li> <p>Handling Mismatches: If a client requests a version not supported by the agent, the agent SHOULD ignore the activation request for that extension; it MUST NOT fall back to a different version.</p> </li> <li> <p>Discoverability and Publication:</p> </li> <li> <p>Specification Hosting: The extension specification document should be hosted at the extension's URI.</p> </li> <li>Permanent Identifiers: Authors are encouraged to use a permanent identifier service, such as w3id.org, for their extension URIs to prevent broken links.</li> <li> <p>Community Registry (Future): The A2A community might establish a central registry for discovering and browsing available extensions in the future.</p> </li> <li> <p>Packaging and Reusability (A2A SDKs and Libraries): To promote adoption, extension logic should be packaged into reusable libraries that can be integrated into existing A2A client and server applications.</p> </li> <li> <p>An extension implementation should be distributed as a standard package for its language ecosystem (for example, a PyPI package for Python, an npm package for TypeScript/JavaScript).</p> </li> <li>The objective is to provide a streamlined integration experience for developers. A well-designed extension package should allow a developer to add it to their server with minimal code, for example:</li> </ul> <pre><code>import logging\nimport os\n\nimport click\n\nfrom a2a.server.apps import A2AStarletteApplication\nfrom a2a.server.request_handlers import DefaultRequestHandler\nfrom a2a.server.tasks import InMemoryTaskStore\nfrom a2a.types import AgentCapabilities, AgentCard, AgentSkill\nfrom agent import ReimbursementAgent\nfrom agent_executor import ReimbursementAgentExecutor\nfrom dotenv import load_dotenv\nfrom timestamp_ext import TimestampExtension\n\n\nload_dotenv()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass MissingAPIKeyError(Exception):\n    \"\"\"Exception for missing API key.\"\"\"\n\n\n@click.command()\n@click.option('--host', default='localhost')\n@click.option('--port', default=10002)\ndef main(host, port):\n    try:\n        # Check for API key only if Vertex AI is not configured\n        if not os.getenv('GOOGLE_GENAI_USE_VERTEXAI') == 'TRUE':\n            if not os.getenv('GEMINI_API_KEY'):\n                raise MissingAPIKeyError(\n                    'GEMINI_API_KEY environment variable not set and GOOGLE_GENAI_USE_VERTEXAI is not TRUE.'\n                )\n\n        hello_ext = TimestampExtension()\n        capabilities = AgentCapabilities(\n            streaming=True,\n            extensions=[\n                hello_ext.agent_extension(),\n            ],\n        )\n        skill = AgentSkill(\n            id='process_reimbursement',\n            name='Process Reimbursement Tool',\n            description='Helps with the reimbursement process for users given the amount and purpose of the reimbursement.',\n            tags=['reimbursement'],\n            examples=[\n                'Can you reimburse me $20 for my lunch with the clients?'\n            ],\n        )\n        agent_card = AgentCard(\n            name='Reimbursement Agent',\n            description='This agent handles the reimbursement process for the employees given the amount and purpose of the reimbursement.',\n            url=f'http://{host}:{port}/',\n            version='1.0.0',\n            default_input_modes=ReimbursementAgent.SUPPORTED_CONTENT_TYPES,\n            default_output_modes=ReimbursementAgent.SUPPORTED_CONTENT_TYPES,\n            capabilities=capabilities,\n            skills=[skill],\n        )\n        agent_executor = ReimbursementAgentExecutor()\n        # Use the decorator version of the extension for highest ease of use.\n        agent_executor = hello_ext.wrap_executor(agent_executor)\n        request_handler = DefaultRequestHandler(\n            agent_executor=agent_executor,\n            task_store=InMemoryTaskStore(),\n        )\n        server = A2AStarletteApplication(\n            agent_card=agent_card, http_handler=request_handler\n        )\n        import uvicorn\n\n        uvicorn.run(server.build(), host=host, port=port)\n    except MissingAPIKeyError as e:\n        logger.error(f'Error: {e}')\n        exit(1)\n    except Exception as e:\n        logger.error(f'An error occurred during server startup: {e}')\n        exit(1)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>This example showcases how A2A SDKs or libraries such as a2a.server in Python facilitate the implementation of A2A agents and extensions.</p> <ul> <li> <p>Security: Extensions modify the core behavior of the A2A protocol, and therefore introduce new security considerations:</p> </li> <li> <p>Input Validation: Any new data fields, parameters, or methods introduced by an extension MUST be rigorously validated. Treat all extension-related data from an external party as untrusted input.</p> </li> <li>Scope of Required Extensions: Be mindful when marking an extension as required: true in an Agent Card. This creates a hard dependency for all clients and should only be used for extensions fundamental to the agent's core function and security (for example, a message signing extension).</li> <li>Authentication and Authorization: If an extension adds new methods, the implementation MUST ensure these methods are subject to the same authentication and authorization checks as the core A2A methods. An extension MUST NOT provide a way to bypass the agent's primary security controls.</li> </ul>"},{"location":"A2A/a2a/#streaming-and-asynchronous-operations-for-long-running-tasks","title":"Streaming and Asynchronous Operations for Long-Running Tasks","text":"<p>The Agent2Agent (A2A) protocol is explicitly designed to handle tasks that might not complete immediately. Many AI-driven operations are often long-running, involve multiple steps, produce incremental results, or require human intervention. A2A provides mechanisms for managing such asynchronous interactions, ensuring that clients receive updates effectively, whether they remain continuously connected or operate in a more disconnected fashion.</p>"},{"location":"A2A/a2a/#streaming-with-server-sent-events-sse","title":"Streaming with Server-Sent Events (SSE)","text":"<p>For tasks that produce incremental results (like generating a long document or streaming media) or provide ongoing status updates, A2A supports real-time communication using Server-Sent Events (SSE). This approach is ideal when the client is able to maintain an active HTTP connection with the A2A Server.</p> <p>The following key features detail how SSE streaming is implemented and managed within the A2A protocol:</p> <ul> <li>Server Capability: The A2A Server must indicate its support for streaming by setting <code>capabilities.streaming: true</code> in its Agent Card.</li> <li> <p>Initiating a Stream: The client uses the message/stream RPC method to send an initial message (for example, a prompt or command) and simultaneously subscribe to updates for that task.</p> </li> <li> <p>Server Response and Connection: If the subscription is successful, the server responds with an HTTP 200 OK status and a Content-Type: text/event-stream. This HTTP connection remains open for the server to push events to the client.</p> </li> <li> <p>Event Structure and Types: The server sends events over this stream. Each event's data field contains a JSON-RPC 2.0 Response object, typically a SendStreamingMessageResponse. The result field of the SendStreamingMessageResponse contains:</p> <ul> <li>Task: Represents the current state of the work.</li> <li>TaskStatusUpdateEvent: Communicates changes in the task's lifecycle state (for example, from working to input-required or completed). It also provides intermediate messages from the agent.</li> <li>TaskArtifactUpdateEvent: Delivers new or updated Artifacts generated by the task. This is used to stream large files or data structures in chunks, with fields like append and lastChunk to help reassemble.</li> </ul> </li> <li> <p>Stream Termination: The server signals the end of updates for a cycle by setting final: true in a TaskStatusUpdateEvent. This typically occurs when the task reaches a terminal state. After this, the server usually closes the SSE connection.</p> </li> <li> <p>Resubscription: If a client's SSE connection breaks prematurely while a task is still active, the client is able to attempt to reconnect to the stream using the tasks/resubscribe RPC method.</p> </li> </ul>"},{"location":"A2A/a2a/#when-to-use-streaming","title":"When to Use Streaming","text":"<p>Streaming with SSE is best suited for:</p> <ul> <li>Real-time progress monitoring of long-running tasks.</li> <li>Receiving large results (artifacts) incrementally.</li> <li>Interactive, conversational exchanges where immediate feedback or partial responses are beneficial.</li> <li>Applications requiring low-latency updates from the agent.</li> </ul>"},{"location":"A2A/a2a/#protocol-specification-references","title":"Protocol Specification References","text":"<p>Refer to the Protocol Specification for detailed structures:</p> <ul> <li>message/stream</li> <li>tasks/subscribe</li> </ul>"},{"location":"A2A/a2a/#push-notifications-for-disconnected-scenarios","title":"Push Notifications for Disconnected Scenarios","text":"<p>For very long-running tasks (for example, lasting minutes, hours, or even days) or when clients are unable to or prefer not to maintain persistent connections (like mobile clients or serverless functions), A2A supports asynchronous updates using push notifications. This allows the A2A Server to actively notify a client-provided webhook when a significant task update occurs.</p> <p>The following key features detail how push notifications are implemented and managed within the A2A protocol:</p> <ul> <li>Server Capability: The A2A Server must indicate its support for this feature by setting capabilities.pushNotifications: true in its Agent Card.</li> <li> <p>Configuration: The client provides a PushNotificationConfig to the server. This configuration is supplied:</p> <ul> <li>Within the initial message/send or message/stream request, or</li> <li>Separately, using the tasks/pushNotificationConfig/set RPC method for an existing task. The PushNotificationConfig includes a url (the HTTPS webhook URL), an optional token (for client-side validation), and optional authentication details (for the A2A Server to authenticate to the webhook).</li> </ul> </li> <li> <p>Notification Trigger: The A2A Server decides when to send a push notification, typically when a task reaches a significant state change (for example, terminal state, input-required, or auth-required). </p> </li> <li>Notification Payload: The A2A protocol defines the HTTP body payload as a StreamResponse object, matching the format used in streaming operations. The payload contains one of: task, message, statusUpdate, or artifactUpdate. See Push Notification Payload for detailed structure.</li> <li>Client Action: Upon receiving a push notification (and successfully verifying its authenticity), the client typically uses the tasks/get RPC method with the taskId from the notification to retrieve the complete, updated Task object, including any new artifacts.</li> </ul> <p>## When to Use Push Notifications</p> <p>Push notifications are ideal for:</p> <ul> <li>Very long-running tasks that can take minutes, hours, or days to complete.</li> <li>Clients that cannot or prefer not to maintain persistent connections, such as mobile applications or serverless functions.</li> <li>Scenarios where clients only need to be notified of significant state changes rather than continuous updates.</li> </ul>"},{"location":"A2A/a2a/#push-notifications-protocol-specification","title":"Push Notifications \u2013 Protocol Specification","text":""},{"location":"A2A/a2a/#protocol-specification-references_1","title":"Protocol Specification References","text":"<p>Refer to the following protocol specifications for detailed structures:</p> <ul> <li><code>tasks/pushNotificationConfig/set</code></li> <li><code>tasks/get</code></li> </ul>"},{"location":"A2A/a2a/#client-side-push-notification-service","title":"Client-Side Push Notification Service","text":"<p>The URL specified in <code>PushNotificationConfig.url</code> points to a client-side Push Notification Service. This service is responsible for receiving HTTP <code>POST</code> notifications from the A2A Server.</p>"},{"location":"A2A/a2a/#responsibilities","title":"Responsibilities","text":"<ul> <li>Authenticate incoming notifications</li> <li>Validate notification relevance</li> <li>Relay notifications or their content to the appropriate client application logic or system</li> </ul>"},{"location":"A2A/a2a/#security-considerations-for-push-notifications","title":"Security Considerations for Push Notifications","text":"<p>Push notifications are asynchronous and server-initiated, making security a critical requirement. Both the A2A Server and the Client Webhook Receiver share responsibility for secure operation.</p>"},{"location":"A2A/a2a/#a2a-server-security","title":"A2A Server Security","text":"<p>(When sending notifications to client webhook)</p>"},{"location":"A2A/a2a/#webhook-url-validation","title":"Webhook URL Validation","text":"<p>A2A Servers MUST NOT blindly trust and send <code>POST</code> requests to arbitrary URLs provided by clients.</p>"},{"location":"A2A/a2a/#risks","title":"Risks","text":"<ul> <li>Server-Side Request Forgery (SSRF)</li> <li>Distributed Denial of Service (DDoS) amplification</li> </ul>"},{"location":"A2A/a2a/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Allowlisting trusted domains</li> <li>Ownership verification (e.g., challenge\u2013response mechanisms)</li> <li>Network controls (e.g., egress firewalls)</li> </ul>"},{"location":"A2A/a2a/#authenticating-to-the-clients-webhook","title":"Authenticating to the Client's Webhook","text":"<p>The A2A Server MUST authenticate itself to the client's webhook URL according to the authentication schemes defined in <code>PushNotificationConfig.authentication</code>.</p>"},{"location":"A2A/a2a/#common-authentication-schemes","title":"Common Authentication Schemes","text":"<ul> <li>Bearer Tokens (OAuth 2.0)</li> <li>API Keys</li> <li>HMAC Signatures</li> <li>Mutual TLS (mTLS)</li> </ul>"},{"location":"A2A/a2a/#client-webhook-receiver-security","title":"Client Webhook Receiver Security","text":"<p>(When receiving notifications from A2A Server)</p>"},{"location":"A2A/a2a/#authenticating-the-a2a-server","title":"Authenticating the A2A Server","text":"<p>The webhook endpoint MUST rigorously verify that incoming requests originate from a legitimate A2A Server.</p>"},{"location":"A2A/a2a/#verification-methods","title":"Verification Methods","text":"<ul> <li>Validate JWT signatures using trusted public keys</li> <li>Verify HMAC signatures</li> <li>Validate API keys</li> <li>Validate <code>PushNotificationConfig.token</code> (if provided)</li> </ul>"},{"location":"A2A/a2a/#preventing-replay-attacks","title":"Preventing Replay Attacks","text":""},{"location":"A2A/a2a/#timestamps","title":"Timestamps","text":"<ul> <li>Notifications SHOULD include a timestamp</li> <li>Webhooks SHOULD reject notifications that are older than an acceptable threshold</li> </ul>"},{"location":"A2A/a2a/#nonces-unique-identifiers","title":"Nonces / Unique Identifiers","text":"<ul> <li>Use single-use identifiers (e.g., JWT <code>jti</code> claim or event IDs)</li> <li>Prevent duplicate or replayed notifications</li> </ul>"},{"location":"A2A/a2a/#secure-key-management-and-rotation","title":"Secure Key Management and Rotation","text":"<ul> <li>Implement secure key storage and management</li> <li>Rotate cryptographic keys regularly</li> <li>Use JWKS (JSON Web Key Set) for efficient asymmetric key rotation and validation</li> </ul>"},{"location":"A2A/a2a/#example-asymmetric-key-flow-jwt-jwks","title":"Example Asymmetric Key Flow (JWT + JWKS)","text":""},{"location":"A2A/a2a/#client-configuration","title":"Client Configuration","text":"<ul> <li>Client sets <code>PushNotificationConfig</code> with:</li> <li><code>authentication.schemes: [\"Bearer\"]</code></li> <li>Expected JWT <code>issuer</code> and/or <code>audience</code></li> </ul>"},{"location":"A2A/a2a/#a2a-server-sending-notification","title":"A2A Server \u2013 Sending Notification","text":"<ol> <li>Generates a JWT signed with its private key</li> <li>Includes standard claims:</li> <li><code>iss</code> (issuer)</li> <li><code>aud</code> (audience)</li> <li><code>iat</code> (issued at)</li> <li><code>exp</code> (expiration)</li> <li><code>jti</code> (JWT ID)</li> <li><code>taskId</code></li> <li>JWT header includes:</li> <li>Signing algorithm</li> <li>Key ID (<code>kid</code>)</li> <li>Publishes public keys via a JWKS endpoint</li> <li>Sends HTTP <code>POST</code> with JWT in the <code>Authorization</code> header</li> </ol>"},{"location":"A2A/a2a/#client-webhook-receiving-notification","title":"Client Webhook \u2013 Receiving Notification","text":"<ol> <li>Extracts JWT from the <code>Authorization</code> header</li> <li>Reads <code>kid</code> from JWT header</li> <li>Fetches the corresponding public key from the A2A Server\u2019s JWKS endpoint (Caching keys is recommended)</li> <li>Verifies the JWT signature</li> <li>Validates claims:</li> <li><code>iss</code>, <code>aud</code></li> <li><code>iat</code>, <code>exp</code></li> <li><code>jti</code></li> <li>Validates <code>PushNotificationConfig.token</code> (if present)</li> </ol>"},{"location":"A2A/a2a/#a2a-and-mcp-detailed-comparison","title":"A2A and MCP: Detailed Comparison","text":""},{"location":"A2A/a2a/#a2a-vs-mcp-complementary-protocols-for-agentic-ai-systems","title":"A2A vs MCP: Complementary Protocols for Agentic AI Systems","text":"<p>In AI agent development, two key protocol types emerge to facilitate interoperability:</p> <ul> <li>One connects agents to tools and resources</li> <li>The other enables agent-to-agent collaboration</li> </ul> <p>The Agent2Agent (A2A) Protocol and the Model Context Protocol (MCP) address these distinct but highly complementary needs.</p>"},{"location":"A2A/a2a/#model-context-protocol-mcp","title":"Model Context Protocol (MCP)","text":"<p>The Model Context Protocol (MCP) defines how an AI agent interacts with and utilizes individual tools and resources, such as databases, APIs, or predefined functions.</p>"},{"location":"A2A/a2a/#capabilities","title":"Capabilities","text":"<ul> <li>Standardizes how AI models and agents connect to tools, APIs, and external resources</li> <li>Defines structured descriptions of tool capabilities (similar to LLM function calling)</li> <li>Passes inputs to tools and receives structured outputs</li> <li>Supports common use cases such as:</li> <li>LLM calling an external API</li> <li>Agent querying a database</li> <li>Agent invoking predefined functions</li> </ul>"},{"location":"A2A/a2a/#agent2agent-protocol-a2a","title":"Agent2Agent Protocol (A2A)","text":"<p>The Agent2Agent (A2A) Protocol enables independent agents to collaborate with one another to achieve shared goals.</p>"},{"location":"A2A/a2a/#capabilities_1","title":"Capabilities","text":"<ul> <li>Standardizes peer-to-peer communication between autonomous AI agents</li> <li>Enables discovery, negotiation, and coordination between agents</li> <li>Supports shared task management, conversational context, and complex data exchange</li> <li>Supports use cases such as:</li> <li>A customer service agent delegating work to a billing agent</li> <li>A travel agent coordinating with flight, hotel, and activity agents</li> </ul>"},{"location":"A2A/a2a/#why-different-protocols","title":"Why Different Protocols?","text":"<p>Both MCP and A2A are essential for building advanced agentic systems, but they operate in different interaction domains.</p>"},{"location":"A2A/a2a/#tools-and-resources-mcp-domain","title":"Tools and Resources (MCP Domain)","text":"<p>Characteristics - Well-defined, structured inputs and outputs - Often stateless - Perform discrete, specific functions</p> <p>Examples - Calculator - Database query API - Weather lookup service</p> <p>Purpose - Agents use tools to gather information and perform well-scoped actions</p>"},{"location":"A2A/a2a/#agents-a2a-domain","title":"Agents (A2A Domain)","text":"<p>Characteristics - Autonomous, reasoning-driven systems - Maintain state across interactions - Use multiple tools - Engage in multi-turn dialogue - Adapt to evolving or novel tasks</p> <p>Purpose - Agents collaborate with other agents to solve complex, end-to-end problems</p>"},{"location":"A2A/a2a/#a2a-mcp-complementary-protocols-for-agentic-systems","title":"A2A \u2764\ufe0f MCP: Complementary Protocols for Agentic Systems","text":"<p>In a typical agentic application:</p> <ul> <li>A2A is used for communication between agents</li> <li>MCP is used internally by each agent to interact with its tools and resources</li> </ul>"},{"location":"A2A/a2a/#conceptual-flow","title":"Conceptual Flow","text":"<p>Agents collaborate via A2A, while each agent independently uses MCP to access its tools.</p>"},{"location":"A2A/a2a/#example-scenario-auto-repair-shop","title":"Example Scenario: Auto Repair Shop","text":"<p>Consider an auto repair shop staffed by autonomous AI agent \"mechanics\".</p>"},{"location":"A2A/a2a/#customer-interaction-user-to-agent-using-a2a","title":"Customer Interaction (User-to-Agent using A2A)","text":"<p>A customer communicates with the Shop Manager agent.</p> <p>Example \"My car is making a rattling noise.\"</p> <p>This interaction uses A2A.</p>"},{"location":"A2A/a2a/#multi-turn-diagnostics-agent-to-agent-using-a2a","title":"Multi-turn Diagnostics (Agent-to-Agent using A2A)","text":"<p>The Shop Manager engages in a diagnostic conversation.</p> <p>Examples - \"Can you send a video of the noise?\" - \"How long has the fluid been leaking?\"</p> <p>This multi-turn, contextual interaction uses A2A.</p>"},{"location":"A2A/a2a/#internal-tool-usage-agent-to-tool-using-mcp","title":"Internal Tool Usage (Agent-to-Tool using MCP)","text":"<p>The assigned Mechanic agent uses MCP to interact with specialized tools.</p> <p>Examples <pre><code>scan_vehicle_for_error_codes(vehicle_id=\"XYZ123\")\nget_repair_procedure(error_code=\"P0300\", vehicle_make=\"Toyota\", vehicle_model=\"Camry\")\nraise_platform(height_meters=2)\n</code></pre></p> <p>These are structured, tool-based interactions via MCP.</p>"},{"location":"A2A/a2a/#supplier-interaction-agent-to-agent-using-a2a","title":"Supplier Interaction (Agent-to-Agent using A2A)","text":"<p>The Mechanic agent contacts a Parts Supplier agent.</p>"},{"location":"A2A/a2a/#example","title":"example","text":"<pre><code>\"Do you have part #12345 in stock for a Toyota Camry 2018?\"\n</code></pre> <p>Example</p> <pre><code>\"Do you have part #12345 in stock for a Toyota Camry 2018?\"\n</code></pre>"},{"location":"AIML/BinomialDistribution/","title":"\ud83d\udce6 What is Binomial Distribution?","text":"<p>A Binomial Distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, each with the same probability of success.</p> <p>It describes the outcome of binary scenarios, e.g. toss of a coin, it will either be head or tails.</p> <p>It has three parameters:</p> <p>n - number of trials.</p> <p>p - probability of occurence of each trial (e.g. for toss of a coin 0.5 each).</p> <p>q or 1\u2212p - Probability of failure on a single trial</p> <p>X - Random variable: number of successes in n trials</p> <p>P(X = k) - Probability of getting exactly k successes</p>"},{"location":"AIML/BinomialDistribution/#binomial-formula","title":"\ud83e\uddee Binomial Formula:","text":"<p>n  - (k\u200b) is the combination (n choose k)</p> <pre><code>k\n</code></pre> <ul> <li>p    : probability of k successes<pre><code>n-k\n</code></pre> <ul> <li>(1 - p)    : probability of (n\u2212k) failures</li> </ul> </li> </ul>"},{"location":"AIML/BinomialDistribution/#real-time-example-email-campaign-success","title":"\u2705 Real-Time Example: Email Campaign Success","text":"<p>Scenario: You\u2019re a marketing manager sending emails to 1000 customers. Based on historical data, you know that 20% (p=0.2) of people open your email.</p> <p>Let\u2019s calculate and understand: - What is the probability that exactly 220 people open the email? - What\u2019s the expected number of opens? - What\u2019s the standard deviation?</p>"},{"location":"AIML/BinomialDistribution/#parameters","title":"\ud83e\uddee Parameters:","text":"<ul> <li>n = 1000 (emails sent)</li> <li>p = 0.2 (open rate)</li> <li>k = 220 (interested in exactly 220 opens)</li> </ul>"},{"location":"AIML/BinomialDistribution/#real-time-insights-you-can-get","title":"\ud83d\udcca Real-Time Insights You Can Get:","text":"<pre><code>Question                                                    Answer Type\nHow many people will likely open the email?                 Expected value = n \u00d7 p = 200\nHow spread out will the results be?                         Standard deviation = \u221a(np(1\u2212p)) \u2248 12.65\nWhat\u2019s the chance of getting 220 opens?                     Use binomial PMF\nWhat if I want at least 250 opens?                          Use cumulative probability\n</code></pre>"},{"location":"AIML/BinomialDistribution/#use-in-business","title":"\ud83d\udd27 Use in Business:","text":"<ul> <li>\ud83d\udcc8 A/B Testing: Measure how many users click CTA button A vs. button B</li> <li>\ud83d\udc8c Email Campaigns: Predict open/click rates</li> <li>\ud83c\udfaf Conversion Analysis: Predict success of lead conversions in sales funnels</li> <li>\ud83e\uddea Quality Control: How many defective items in a batch of products</li> </ul> <p>Here\u2019s the Binomial Distribution plot for your email campaign:</p> <ul> <li> <p>\u2705 Mean (Expected Opens) = 200 (red line)</p> </li> <li> <p>\ud83d\udccd Highlighted <code>k = 220</code> (green line): you can see the probability of getting exactly 220 opens</p> </li> <li> <p>\u26a0\ufe0f <code>k = 250</code> (orange line): very low probability \u2014 it's at the edge of the distribution</p> </li> </ul>"},{"location":"AIML/BinomialDistribution/#interpretation","title":"\ud83d\udd0d Interpretation:","text":"<ul> <li> <p>Most of your email open counts will hover around 200, with some fluctuation.</p> </li> <li> <p>Getting 250+ opens is rare.</p> </li> <li> <p>You can use this to forecast campaign performance, set realistic KPIs, or run A/B tests confidently.</p> </li> </ul> <p>Want to calculate exact probability values like:</p> <ul> <li>\ud83d\udcca P(X = 220) (exactly 220 opens)</li> <li>\ud83d\udcc8 P(X \u2265 250) (at least 250 opens)</li> </ul>"},{"location":"AIML/BinomialDistribution/#difference-between-normal-and-binomial-distribution","title":"Difference Between Normal and Binomial Distribution","text":""},{"location":"AIML/BinomialDistribution/#real-world-examples","title":"\ud83d\udccc Real-World Examples","text":""},{"location":"AIML/BinomialDistribution/#binomial","title":"\ud83d\udd39 Binomial:","text":"<ul> <li>Tossing a coin 10 times and counting heads.</li> <li>Counting how many customers clicked an ad (click or no-click).</li> <li>Number of defective items in a batch of 50.</li> </ul>"},{"location":"AIML/BinomialDistribution/#normal","title":"\ud83d\udd39 Normal:","text":"<ul> <li>Height of 10,000 people.</li> <li>Temperature over a year in a city.</li> <li>Blood pressure measurements.</li> </ul> <p>Here's the visual comparison between the Binomial Distribution (blue bars) and its Normal approximation (red dashed curve):</p> <ul> <li>Blue bars: Exact probabilities from the Binomial Distribution (n = 100, p = 0.5)</li> <li> <p>Red dashed line: Normal Distribution approximation using the same mean (50) and standard deviation</p> </li> <li> <p>As n increases, the Binomial Distribution starts looking more like a Normal Distribution</p> </li> <li> <p>This is why we often use the Normal Approximation when working with large n, especially for quick calculations</p> </li> </ul> <p></p> <p>Here's the visual for the skewed case:</p>"},{"location":"AIML/BinomialDistribution/#binomial-n30-p02-vs-normal-approximation","title":"\ud83d\udcca Binomial (n=30, p=0.2) vs \ud83d\udd14 Normal Approximation","text":""},{"location":"AIML/BinomialDistribution/#key-takeaways","title":"\ud83d\udd0d Key Takeaways:","text":"<ul> <li> <p>The Binomial Distribution is right-skewed because the probability of success p = 0.2 is low.</p> </li> <li> <p>The Normal Approximation (red dashed line) doesn't match the binomial well \u2014 especially in the tails.</p> </li> <li> <p>This shows that Normal Approximation is not reliable when:</p> <ul> <li>n is small</li> <li>p is far from 0.5</li> </ul> </li> </ul>"},{"location":"AIML/BinomialDistribution/#binomial-distribution-with-p08-and-n30","title":"Binomial Distribution with p=0.8 and n=30","text":"<p>The Binomial Distribution with parameters n = 30 and p = 0.8 s indeed left-skewed because the probability of success is high.causing most outcomes to cluster toward the upper end (near np = 24 ).</p> <p></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm\n\nn, p = 30, 0.8\nx = np.arange(0, n+1)\nmu = n * p\nsigma = np.sqrt(n * p * (1 - p))\n\n# Binomial PMF\nbinomial_pmf = binom.pmf(x, n, p)\n\n# Normal PDF (for approximation)\nx_cont = np.linspace(0, n, 1000)\nnormal_pdf = norm.pdf(x_cont, mu, sigma)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.bar(x, binomial_pmf, color='blue', alpha=0.6, label='Binomial PMF')\nplt.plot(x_cont, normal_pdf, 'r-', lw=2, label='Normal Approximation')\nplt.title(f'Binomial vs. Normal Approximation (n={n}, p={p})')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Output Interpretation: - The plot will show a left-skewed Binomial distribution with a peak near 24. - The Normal curve will roughly match the center but deviate in the left tail (X &lt; 20)</p> <p>Exact vs. Approximate Probabilities:</p> <p>For example, to compute P(X\u226420):</p> <ul> <li> <p>Exact (Binomial): <pre><code>from scipy.stats import binom\nprint(binom.cdf(20, n=30, p=0.8))  # Output: ~0.061 (6.1%)\n</code></pre></p> </li> <li> <p>Normal Approximation (with continuity correction): <pre><code>from scipy.stats import norm\nprint(norm.cdf(20.5, loc=mu, scale=sigma))  # Output: ~0.085 (8.5%)\n</code></pre></p> </li> </ul> <p>The Normal approximation overestimates the tail probability due to skewness.</p>"},{"location":"AIML/ExponentialDistribution/","title":"\u26a1 What is Exponential Distribution?","text":"<p>The Exponential Distribution models the time between events in a Poisson process \u2014 where events occur continuously and independently at a constant average rate.</p>"},{"location":"AIML/ExponentialDistribution/#probability-density-function-pdf","title":"\ud83e\uddee Probability Density Function (PDF):","text":""},{"location":"AIML/ExponentialDistribution/#key-properties","title":"\ud83d\udce6 Key Properties","text":"<pre><code>| **Property**           | **Value**                    |\n|------------------------|------------------------------|\n| Domain                 | \\( x \\in [0, \\infty) \\)       |\n| Mean                   | \\( \\frac{1}{\\lambda} \\)       |\n| Memoryless Property    | \u2705 Yes                        |\n| Skewness               | Right-skewed                 |\n| Related to             | Poisson Distribution         |\n</code></pre>"},{"location":"AIML/ExponentialDistribution/#real-time-use-cases","title":"\ud83e\udde0 Real-Time Use Cases","text":"<ol> <li>\ud83d\udd52 Server Downtime / Time Between Failures Example: Time between system crashes or hardware failures.</li> </ol> <p>If server crashes occur at a constant average rate, exponential distribution models the waiting time until the next crash.</p> <ol> <li>\ud83d\udcde Call Center / Customer Support Time between incoming calls.</li> </ol> <p>If calls arrive independently and at a constant average rate, exponential models the time until the next call.</p> <ol> <li>\ud83e\uddea Medical / Survival Analysis Time until a patient responds to a treatment.</li> </ol> <p>Time until death or relapse in survival analysis.</p> <ol> <li>\ud83d\udca1 Queueing Systems Time between customers arriving at a queue (like supermarket, ATM, etc.)</li> </ol> <p>Example</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate exponential data\ndata = np.random.exponential(scale=1.0, size=1000)\n\nplt.hist(data, bins=50, density=True, color='skyblue', edgecolor='black')\nplt.title(\"Exponential Distribution (\u03bb = 1.0)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Probability Density\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/FeatureEngineering/","title":"What is Feature Engineering?","text":"<p>Feature engineering, in data science, refers to manipulation \u2014 addition, deletion, combination, mutation \u2014 of your data set to improve machine learning model training, leading to better performance and greater accuracy. </p> <p>Effective feature engineering is based on sound knowledge of the business problem and the available data sources.</p>"},{"location":"AIML/FeatureEngineering/#feature-engineering-in-ml-lifecycle-diagram","title":"Feature engineering in ML lifecycle diagram","text":"<p>Feature engineering involves transforming raw data into a format that enhances the performance of machine learning models. The key steps in feature engineering include:</p> <ul> <li> <p>Data Exploration and Understanding: Explore and understand the dataset, including the types of features and their distributions. Understanding the shape of the data is key.</p> </li> <li> <p>Handling Missing Data: Address missing values through imputation or removal of instances or features with missing data. There are many algorithmic approaches to handling missing data.</p> </li> <li> <p>Variable Encoding: Convert categorical variables into a numerical format suitable for machine learning algorithms using methods.</p> </li> <li> <p>Feature Scaling: Standardize or normalize numerical features to ensure they are on a similar scale, improving model performance.</p> </li> <li> <p>Feature Creation: Generate new features by combining existing ones to capture relationships between variables.</p> </li> <li> <p>Handling Outliers: Identify and address outliers in the data through techniques like trimming or transforming the data.</p> </li> <li> <p>Normalization: Normalize features to bring them to a common scale, important for algorithms sensitive to feature magnitudes.</p> </li> <li> <p>Binning or Discretization: Convert continuous features into discrete bins to capture specific patterns in certain ranges.</p> </li> <li> <p>Text Data Processing: If dealing with text data, perform tasks such as tokenization, stemming, and removing stop words.</p> </li> <li> <p>Time Series Features: Extract relevant timebased features such as lag features or rolling statistics for time series data.</p> </li> <li> <p>Vector Features: Vector features are commonly used for training in machine learning. In machine learning, data is represented in the form of features, and these features are often organized into vectors. A vector is a mathematical object that has both magnitude and direction and can be represented as an array of numbers.</p> </li> <li> <p>Feature Selection: Identify and select the most relevant features to improve model interpretability and efficiency using techniques like univariate feature selection or recursive feature elimination.</p> </li> <li> <p>Feature Extraction: Feature extraction aims to reduce data complexity (often known as \u201cdata dimensionality\u201d) while retaining as much relevant information as possible. This helps to improve the performance and efficiency of machine learning algorithms and simplify the analysis process. Feature extraction may involve the creation of new features (\u201cfeature engineering\u201d) and data manipulation to separate and simplify the use of meaningful features from irrelevant ones. Create new features or reduce dimensionality using techniques such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-DSNE).</p> </li> <li> <p>Cross-validation: selecting features prior to cross-validation can introduce significant bias. Evaluate the impact of feature engineering on model performance using cross-validation techniques.</p> </li> </ul>"},{"location":"AIML/FeatureEngineering/#common-feature-types","title":"Common feature types:","text":"<ul> <li> <p>Numerical: Values with numeric types (int, float, etc.). Examples: age, salary, height.</p> </li> <li> <p>Categorical Features: Features that can take one of a limited number of values. Examples: gender (male, female, non-binary), color (red, blue, green).</p> </li> <li> <p>Ordinal Features: Categorical features that have a clear ordering. Examples: T-shirt size (S, M, L, XL).</p> </li> <li> <p>Binary Features: A special case of categorical features with only two categories. Examples: is_smoker (yes, no), has_subscription (true, false).</p> </li> <li> <p>Text Features: Features that contain textual data. Textual data typically requires special preprocessing steps (like tokenization) to transform it into a format suitable for machine learning models.</p> </li> </ul>"},{"location":"AIML/FeatureEngineering/#feature-normalization","title":"Feature normalization","text":"<p>Since data features can be measured on different scales, it's often necessary to standardize or normalize them, especially when using algorithms that are sensitive to the magnitude and scale of variables (like gradient descent-based algorithms, k-means clustering, or support vector machines).</p> <p>Normalization standardizes the range of independent variables or features of the data. This process can make certain algorithms converge faster and lead to better model performance, especially for algorithms sensitive to the scale of input features.</p> <p>Feature normalization helps in the following ways:</p> <ul> <li> <p>Scale Sensitivity: Features on larger scales can disproportionately influence the outcome.</p> </li> <li> <p>Better Performance: Normalization can lead to better performance in many machine learning models by ensuring that each feature contributes approximately proportionate to the final decision. This is especially meaningful for optimization algorithms, as they can achieve convergence more quickly with normalized features.</p> </li> </ul> <p>Some features, however, may need to have a larger influence on the outcome. In addition, normalization may result in some loss of useful information. Therefore, be judicious when applying normalization during the feature extraction process.</p>"},{"location":"AIML/LogisticDistribution/","title":"\ud83d\udcd0 What is the Logistic Distribution?","text":"<p>The Logistic Distribution is a continuous probability distribution used for modeling growth and for classification problems.</p> <p>Its shape is similar to the normal distribution \u2014 symmetric, bell-shaped \u2014 but with heavier tails.</p> <p></p>"},{"location":"AIML/LogisticDistribution/#real-world-use-cases","title":"\u2705 Real-World Use Cases","text":"<ol> <li>Logistic Regression (Binary Classification)<ul> <li>\ud83c\udfaf Models the probability that an output belongs to class 1 (vs class 0).</li> <li>Uses the sigmoid function (i.e., logistic CDF) to squash any real-valued input to a value between 0 and 1.</li> </ul> </li> </ol> <p>Example: Predicting whether an email is spam or not spam based on features like subject, sender, keywords, etc.</p> <ol> <li> <p>Neural Networks</p> <ul> <li>\ud83d\udd01 Activation function like sigmoid uses the logistic distribution shape.</li> <li>\ud83e\udde0 Used to map any value into a bounded range [0, 1].</li> </ul> </li> <li> <p>Growth Models</p> <ul> <li>\ud83d\udcc8 Population or disease spread modeling (e.g., COVID-19 curves).</li> <li>Logistic distribution is used when the rate of growth is proportional to both current value and remaining capacity.</li> </ul> </li> <li> <p>Marketing &amp; Adoption Rates</p> <ul> <li>\ud83d\udcca Used in diffusion of innovation \u2014 how quickly people adopt new tech (like smartphones, electric cars).</li> <li>Shows slow start \u2192 rapid growth \u2192 saturation.</li> </ul> </li> </ol> <p>Example</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import logistic\n\nx = np.linspace(-10, 10, 1000)\npdf = logistic.pdf(x, loc=0, scale=1)\ncdf = logistic.cdf(x, loc=0, scale=1)\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(x, pdf, label='PDF', color='green')\nplt.title(\"Logistic Distribution - PDF\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(x, cdf, label='CDF (Sigmoid)', color='blue')\nplt.title(\"Logistic Distribution - CDF\")\nplt.grid(True)\nplt.legend()\n\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/LogisticDistribution/#logistic-vs-normal-distribution","title":"\ud83d\udd04 Logistic vs Normal Distribution","text":"<pre><code>Feature                     Normal                                  Logistic\nPDF shape                   Bell-shaped                             Bell-shaped\nTails                       Light                                   Heavier\nCDF                         Error function                          Sigmoid function\nML use case                 Less in classification                  Logistic regression, sigmoid\n</code></pre>"},{"location":"AIML/LogisticDistribution/#difference-between-logistic-and-normal-distribution","title":"Difference Between Logistic and Normal Distribution","text":"Feature Normal Distribution Logistic Distribution Shape Bell-shaped, symmetric Bell-shaped, symmetric Tails Lighter tails Heavier tails Peak Sharper peak Flatter peak PDF (Probability Function) Involves exponential and \u03c0 Involves exponential only CDF Error function Sigmoid function Common in Natural phenomena, regression, hypothesis testing Classification, logistic regression, neural nets Use in ML Assumes continuous output Used when output is a probability (0\u20131) Analytical Simplicity Harder to compute CDF Easier and faster (sigmoid)"},{"location":"AIML/LogisticDistribution/#real-world-use-in-aiml","title":"\ud83e\udde0 Real-World Use in AI/ML","text":"Scenario Best Fit Distribution Why? Predicting exam scores, height Normal Real-world values cluster around a mean Classifying emails (spam/not) Logistic Probabilistic output between 0 and 1 Modeling neural network activations Logistic Sigmoid is derived from logistic CDF Forecasting disease spread Logistic Used in logistic growth curves"},{"location":"AIML/MultinomialDistribution/","title":"\ud83c\udfaf What is a Multinomial Distribution?","text":"<p>The Multinomial Distribution is a generalization of the Binomial Distribution. While the Binomial Distribution deals with binary outcomes (e.g., success/failure), the Multinomial Distribution handles scenarios with more than two possible outcomes.</p>"},{"location":"AIML/MultinomialDistribution/#definition","title":"\ud83e\uddee Definition","text":"<p>The multinomial distribution gives the probability of counts for each possible outcome when you perform a fixed number of independent experiments, each with multiple outcomes.</p> <p>Parameters: - n: Number of trials (e.g., total votes, total tosses)</p> <ul> <li> <p>k: Number of possible outcomes per trial (e.g., categories)</p> </li> <li> <p>p\u2081, p\u2082, ..., p\u2096: Probabilities of each outcome (must sum to 1)</p> </li> </ul> <p></p> <p>\u2705 Real-Life Example</p> <p>\ud83d\uddf3\ufe0f Election Voting Let\u2019s say there are 3 political parties: A, B, and C.     - 100 people vote.     - Probability of voting:             - Party A: 0.4             - Party B: 0.35             - Party C: 0.25</p> <p>You want to find the probability that:     - 40 votes for A     - 35 votes for B     - 25 votes for C</p>"},{"location":"AIML/MultinomialDistribution/#use-case-in-aiml","title":"\ud83e\udde0 Use Case in AI/ML","text":""},{"location":"AIML/MultinomialDistribution/#text-classification-nlp","title":"\ud83c\udff7\ufe0f Text Classification (NLP)","text":"<p>Multinomial distribution is the foundation of the Multinomial Naive Bayes algorithm, which is widely used in NLP tasks such as:</p> <ul> <li>Spam Detection</li> <li>Sentiment Analysis</li> <li>Topic Classification</li> </ul> <p>Each word in a document is considered as a trial, and the probability of each word belonging to a particular class (like spam or not spam) is calculated using the multinomial model.</p>"},{"location":"AIML/NormalDistribution/","title":"Normal (Gaussian) Distribution","text":""},{"location":"AIML/NormalDistribution/#normal-distribution","title":"Normal Distribution","text":"<p>The Normal Distribution is one of the most important distributions.</p> <p>It is also called the Gaussian Distribution after the German mathematician Carl Friedrich Gauss.</p> <p>The Normal Distribution is a bell-shaped curve that shows how values are distributed:</p> <p>Use the random.normal() method to get a Normal Data Distribution.</p> <ul> <li>Most values are around the mean</li> <li>Fewer values are at the extremes</li> </ul> <p>It's the most commonly used distribution in statistics and machine learning.</p>"},{"location":"AIML/NormalDistribution/#real-world-examples","title":"\ud83d\udcca Real-World Examples","text":"<ul> <li>Heights of people</li> <li>Test scores</li> <li>Blood pressure readings</li> <li>Measurement errors</li> </ul> <p>These all often follow a normal distribution.</p>"},{"location":"AIML/NormalDistribution/#mathematical-definition","title":"\ud83d\udd22 Mathematical Definition","text":"<p>The probability density function (PDF) of a normal distribution:</p> <p></p> <p>Where: - \u03bc = mean (center of the distribution) - \u03c3 = standard deviation (spread or width of the bell) - e = Euler\u2019s number (\u2248 2.718)</p>"},{"location":"AIML/NormalDistribution/#key-properties","title":"\ud83d\udccc Key Properties","text":"<pre><code>Property                        Meaning\nSymmetric                       Centered at the mean\nBell-shaped                     Smooth curve, peak at mean\nMean = Median = Mode            All are the same in a perfect normal dist\nDefined by two params           Mean (\u03bc), Std. Dev. (\u03c3)\nArea under curve = 1            Total probability is 100%\n</code></pre>"},{"location":"AIML/NormalDistribution/#empirical-rule-68-95-997","title":"\ud83d\udccf Empirical Rule (68-95-99.7)","text":"<p>The Empirical Rule tells us how data is spread around the mean (center) when the data is normally distributed.</p> <p>Here\u2019s what it means:</p>"},{"location":"AIML/NormalDistribution/#68-of-data-lies-within-1-standard-deviation","title":"\u2705 68% of data lies within \u00b11 standard deviation (\u03c3)","text":"<ul> <li>Range: from -1 to +1</li> <li>Example: If test scores are normally distributed with mean = 70 and std dev = 10.<ul> <li>then 68% of students scored between 60 and 80</li> </ul> </li> </ul>"},{"location":"AIML/NormalDistribution/#95-of-data-lies-within-2-standard-deviations","title":"\u2705 95% of data lies within \u00b12 standard deviations (\u03c3)","text":"<ul> <li>Range: from -2 to +2</li> <li>So almost all data is within this range</li> </ul>"},{"location":"AIML/NormalDistribution/#997-of-data-lies-within-3-standard-deviations","title":"\u2705 99.7% of data lies within \u00b13 standard deviations (\u03c3)","text":"<ul> <li>Range: from -3 to +3</li> <li>Nearly all the data lives here</li> </ul>"},{"location":"AIML/NormalDistribution/#visualization-idea","title":"\ud83d\udcc8 Visualization Idea","text":"<pre><code>     -3\u03c3      -2\u03c3      -1\u03c3      0       +1\u03c3     +2\u03c3     +3\u03c3\n      |--------|--------|-------|--------|-------|--------|\n      |   0.15%|  2.35% | 13.5% | 34%    |13.5%  | 2.35%  | 0.15% |\n</code></pre> <p>Add it all up:</p> <ul> <li>34% + 34% = 68% within \u00b11\u03c3</li> <li>13.5% + 34% + 34% + 13.5% = 95% within \u00b12\u03c3</li> <li>Almost everything = 99.7% within \u00b13\u03c3</li> </ul> <p>The Bell Curve is Symmetrical So, if 68% of the data lies within \u00b11\u03c3, that means:</p> <ul> <li>34% is on the left side of the mean (between -1\u03c3 and 0)</li> <li>34% is on the right side (between 0 and +1\u03c3)</li> </ul>"},{"location":"AIML/NormalDistribution/#full-breakdown-of-standard-normal-distribution","title":"\ud83d\udcca Full Breakdown of Standard Normal Distribution","text":"<pre><code>Range       % of Total Data                 Notes\n\u03bc \u00b1 1\u03c3      68%                             From -1\u03c3 to +1\u03c3 (34% left, 34% right)\n\u03bc \u00b1 2\u03c3      95%                             From -2\u03c3 to +2\u03c3 \u2192 includes 68% + more\n\u03bc \u00b1 3\u03c3      99.7%                           Almost all data (everything within -3 to +3\u03c3)\n</code></pre>"},{"location":"AIML/NormalDistribution/#but-what-about-whats-outside-those-ranges","title":"\ud83e\uddee But what about what's outside those ranges?","text":"<p>Here\u2019s the exact breakdown of the tails:</p> <pre><code>     &lt; -3\u03c3       -2\u03c3 to -3\u03c3    -1\u03c3 to -2\u03c3    -1\u03c3 to 0    0 to +1\u03c3   +1\u03c3 to +2\u03c3   +2\u03c3 to +3\u03c3    &gt; +3\u03c3\n     0.15%        2.35%         13.5%         34%         34%        13.5%        2.35%         0.15%\n</code></pre> <ul> <li>2.35% of the data lies between -2\u03c3 and -3\u03c3, and another 2.35% between +2\u03c3 and +3\u03c3</li> <li>0.15% lies beyond -3\u03c3 and another 0.15% beyond +3\u03c3</li> </ul>"},{"location":"AIML/NormalDistribution/#quick-visual","title":"\ud83e\udde0 Quick Visual","text":"<pre><code>       |&lt;--0.15--|&lt;--2.35--|&lt;--13.5--|&lt;--34--|--34--&gt;|--13.5--&gt;|--2.35--&gt;|--0.15--&gt;|\n       -3\u03c3       -2\u03c3       -1\u03c3       0       +1\u03c3      +2\u03c3       +3\u03c3\n</code></pre> <ul> <li>The total area under the curve is 100%</li> <li>99.7% is within \u00b13\u03c3</li> <li>The remaining 0.3% (0.15% on each end) is extreme outlier data</li> </ul> <p>Note: These values are extremely rare \u2014 and in machine learning or statistics, they may be considered anomalies or noise.</p> <p></p> <p>Here\u2019s the visual breakdown of the normal distribution with each region clearly marked:</p> <ul> <li>The center green areas (\u00b11\u03c3) represent 68%</li> <li>The yellow areas between \u00b11\u03c3 to \u00b12\u03c3 add up to 27% (13.5% each side)</li> <li>The orange areas between \u00b12\u03c3 to \u00b13\u03c3 contribute 4.7% (2.35% each side)</li> <li>The red tails beyond \u00b13\u03c3 are the extreme 0.3% (0.15% on each end)</li> </ul> <p>It has three parameters:</p>"},{"location":"AIML/NormalDistribution/#example-students-test-scores","title":"\ud83c\udf93 Example: Students' Test Scores","text":"<p>Imagine a standardized math test is given to 10,000 students.</p> <p>The scores are: - Normally distributed - Mean (\u03bc) = 70 - Standard Deviation (\u03c3) = 10</p>"},{"location":"AIML/NormalDistribution/#what-this-means","title":"\ud83e\udde0 What this means:","text":"<ul> <li>Most students score around 70</li> <li>Some score higher, some lower, in a symmetric bell shape</li> </ul>"},{"location":"AIML/NormalDistribution/#lets-apply-the-empirical-rule","title":"\ud83d\udcca Let\u2019s apply the Empirical Rule:","text":"<pre><code>Score Range         Std Dev Range           % of Students               Count out of 10,000\n60 to 80            \u03bc \u00b1 1\u03c3                  68%                         6,800 students\n50 to 90            \u03bc \u00b1 2\u03c3                  95%                         9,500 students\n40 to 100           \u03bc \u00b1 3\u03c3                  99.7%                       9,970 students\n&lt; 40 or &gt; 100       Outside \u00b13\u03c3             0.3%                        ~30 students\n</code></pre> <pre><code>Score Range         Between Which \u03c3             % of Students           Real Count (out of 10,000)\n&lt; 40                Less than -3\u03c3               0.15%                   15 students\n40\u201350               -3\u03c3 to -2\u03c3                  2.35%                   235 students\n50\u201360               -2\u03c3 to -1\u03c3                  13.5%                   1,350 students\n60\u201370               -1\u03c3 to 0\u03c3                   34%                     3,400 students\n70\u201380               0\u03c3 to +1\u03c3                   34%                     3,400 students\n80\u201390               +1\u03c3 to +2\u03c3                  13.5%                   1,350 students\n90\u2013100              +2\u03c3 to +3\u03c3                  2.35%                   235 students\n&gt; 100               More than +3\u03c3               0.15%                   15 students\n</code></pre> <ul> <li> <p>loc - (Mean) where the peak of the bell exists.</p> </li> <li> <p>scale - (Standard Deviation) how flat the graph distribution should be.</p> </li> <li> <p>size - The shape of the returned array.</p> </li> </ul> <p>Example Generate a random normal distribution of size 2x3:</p> <pre><code>from numpy import random\n\nx = random.normal(size=(2, 3))\n\nprint(x)\n\nOutput:\n[[ 1.08425956  0.21924346 -0.87622924]\n [-1.84470937 -0.02399501 -1.62717006]]\n</code></pre> <p>Example Generate a random normal distribution of size 2x3 with mean at 1 and standard deviation of 2:</p> <pre><code>from numpy import random\n\nx = random.normal(loc=1, scale=2, size=(2, 3))\n\nprint(x)\n\nOutput:\n[[-1.1917958   1.32752796  1.04626068]\n [-1.74596895  1.31380769  1.01775866]]\n</code></pre>"},{"location":"AIML/NormalDistribution/#visualization-of-normal-distribution","title":"Visualization of Normal Distribution","text":"<p>Example</p> <p><pre><code>from numpy import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.displot(random.normal(size=1000), kind=\"kde\")\n\nplt.show()\n</code></pre> Output: </p> <p>Note: The curve of a Normal Distribution is also known as the Bell Curve because of the bell-shaped curve.</p>"},{"location":"AIML/NumPy/","title":"What is NumPy?","text":"<p>NumPy, short for Numerical Python, is an open-source Python library. It supports multi-dimensional arrays (matrices) and provides a wide range of mathematical functions for array operations. It is used in scientific computing, and in areas like data analysis, machine learning, etc.</p>"},{"location":"AIML/NumPy/#why-to-use-numpy","title":"Why to Use NumPy?","text":"<p>In Python we have lists that serve the purpose of arrays, but they are slow to process. NumPy aims to provide an array object that is up to 50x faster than traditional Python lists. The array object in NumPy is called ndarray, it provides a lot of supporting functions that make working with ndarray very easy. Arrays are very frequently used in data science, where speed and resources are very important.</p> <ul> <li>NumPy provides various math functions for calculations like addition, algebra, and data analysis.</li> <li>NumPy provides various objects representing arrays and multi-dimensional arrays which can be used to handle large data such as images, sounds, etc.</li> <li>NumPy also works with other libraries like SciPy (for scientific computing), Pandas (for data analysis), and scikit-learn (for machine learning).</li> <li>NumPy is fast and reliable, which makes it a great choice for numerical computing in Python.</li> </ul>"},{"location":"AIML/NumPy/#why-is-numpy-faster-than-lists","title":"Why is NumPy Faster Than Lists?","text":"<p>NumPy arrays are stored at one continuous place in memory unlike lists, so processes can access and manipulate them very efficiently. This behavior is called locality of reference in computer science. This is the main reason why NumPy is faster than lists. Also it is optimized to work with latest CPU architectures.</p>"},{"location":"AIML/NumPy/#which-language-is-numpy-written-in","title":"Which Language is NumPy written in?","text":"<p>NumPy is a Python library and is written partially in Python, but most of the parts that require fast computation are written in C or C++.</p>"},{"location":"AIML/NumPy/#numpy-applications","title":"NumPy Applications","text":"<p>The following are some common application areas where NumPy is extensively used:</p> <ul> <li>Data Analysis: In Data analysis, while handling data, we can create data (in the form of array objects), filter the data, and perform various operations such as mean, finding the standard deviations, etc.</li> <li>Machine Learning &amp; AI: Popular machine learning tools like TensorFlow and PyTorch use NumPy to manage input data, handle model parameters, and process the output values.</li> <li>Array Manipulation: NumPy allows you to create, resize, slice, index, stack, split, and combine arrays.</li> <li>Finance &amp; Economics: NumPy is used for financial analysis, including portfolio optimization, risk assessment, time series analysis, and statistical modelling.</li> <li>Image &amp; Signal Processing: NumPy helps process and analyze images and signals for various applications.</li> <li>Data Visualization: NumPy independently does not create visualizations, but it works with libraries like Matplotlib and Seaborn to generate charts and graphs from numerical data.</li> </ul>"},{"location":"AIML/NumPy/#example-1","title":"Example: 1","text":"<p>Checking NumPy Version</p> <pre><code>import numpy as np\n\nprint(np.__version__)\n</code></pre>"},{"location":"AIML/NumPy/#example-2","title":"Example: 2","text":"<p>Create a NumPy array:</p> <pre><code>import numpy as np\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n</code></pre> <p>Output: <pre><code>[1 2 3 4 5]\n\nprint(type(arr)) =&gt; &lt;class 'numpy.ndarray'&gt;\n</code></pre></p>"},{"location":"AIML/NumPy/#dimensions-in-arrays","title":"Dimensions in Arrays","text":"<p>A dimension in arrays is one level of array depth (nested arrays).</p> <p>nested array: are arrays that have arrays as their elements.</p> <p>Scalar: A scalar is a single value \u2014 just one number, string, or boolean.It has no dimensions \u2014 it's just a standalone value. <pre><code>x = 5          # scalar (integer)\ny = 3.14       # scalar (float)\nz = \"hello\"    # scalar (string)\n</code></pre></p> <p>Array:An array is a collection of values \u2014 it can hold many numbers or elements, and it has one or more dimensions. <pre><code>import numpy as np\n\na = np.array([1, 2, 3])         # 1D array (vector)\nb = np.array([[1, 2], [3, 4]])  # 2D array (matrix)\n</code></pre></p> <p>An array can be: - 1D (like a list) - 2D (like a table or matrix) - nD (higher-dimensional)</p>"},{"location":"AIML/NumPy/#computation","title":"Computation","text":"<p>Computation means carrying out calculations \u2014 it\u2019s the process of solving problems using mathematical operations (addition, multiplication, etc.), often with a computer.</p>"},{"location":"AIML/NumPy/#matrix-mathematical-structure","title":"Matrix (Mathematical Structure)","text":"<p>A matrix is a grid of numbers arranged in rows and columns. Example:</p> <p><pre><code>A = | 1  2 |\n    | 3  4 |\n</code></pre> - This is a 2x2 matrix (2 rows, 2 columns).</p> <pre><code>import numpy as np\n\nA = np.array([[1, 2],\n              [3, 4]])\n</code></pre>"},{"location":"AIML/NumPy/#types-of-matrix-computations","title":"Types of matrix computations:","text":"<ul> <li>Addition: A + B</li> <li>Scalar multiplication: 3 * A</li> <li>Matrix multiplication: np.dot(A, B) or A @ B</li> <li>Transpose: A.T</li> <li>Inverse: np.linalg.inv(A) (if invertible)</li> </ul> <p>In AI/ML, matrices are everywhere:     - Images are matrices of pixels     - Neural networks use weight matrices     - Data tables are often treated as matrices</p>"},{"location":"AIML/NumPy/#0-d-arrays","title":"0-D Arrays:","text":"<p>0-D arrays, or Scalars, are the elements in an array. Each value in an array is a 0-D array.</p>"},{"location":"AIML/NumPy/#example-3","title":"Example: 3","text":"<p>Create a 0-D array with value 42</p> <pre><code>import numpy as np\n\narr = np.array(42)\n\nprint(arr)\n</code></pre> <p>Output: <pre><code>42\n</code></pre></p>"},{"location":"AIML/NumPy/#1-d-arrays","title":"1-D Arrays:","text":"<p>An array that has 0-D arrays as its elements is called uni-dimensional or 1-D array.</p> <p>These are the most common and basic arrays.</p>"},{"location":"AIML/NumPy/#example-4","title":"Example: 4","text":"<p>Create a 1-D array containing the values 1,2,3,4,5</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\n\nprint(arr)\n</code></pre> <p>Output: <pre><code>[1 2 3 4 5]\n</code></pre></p>"},{"location":"AIML/NumPy/#2-d-arrays","title":"2-D Arrays","text":"<p>An array that has 1-D arrays as its elements is called a 2-D array. These are often used to represent matrix or 2nd order tensors.</p> <p>NumPy has a whole sub module dedicated towards matrix operations called numpy.mat</p>"},{"location":"AIML/NumPy/#example-5","title":"Example: 5","text":"<p>Create a 2-D array containing two arrays with the values 1,2,3 and 4,5,6</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr)\n</code></pre> <p>Output: <pre><code>[[1 2 3]\n [4 5 6]]\n</code></pre></p>"},{"location":"AIML/NumPy/#3-d-arrays","title":"3-D arrays","text":"<p>An array that has 2-D arrays (matrices) as its elements is called 3-D array. These are often used to represent a 3rd order tensor.</p>"},{"location":"AIML/NumPy/#example-6","title":"Example: 6","text":"<p>Create a 3-D array with two 2-D arrays, both containing two arrays with the values 1,2,3 and 4,5,6</p> <pre><code>import numpy as np\n\narr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n\nprint(arr)\n</code></pre> <p>Output: <pre><code>[[[1 2 3]\n  [4 5 6]]\n\n [[1 2 3]\n  [4 5 6]]]\n</code></pre></p>"},{"location":"AIML/NumPy/#check-number-of-dimensions","title":"Check Number of Dimensions?","text":"<p>NumPy Arrays provides the ndim attribute that returns an integer that tells us how many dimensions the array have.</p>"},{"location":"AIML/NumPy/#example-7","title":"Example: 7","text":"<p>Check how many dimensions the arrays have:</p> <pre><code>import numpy as np\n\na = np.array(42)\nb = np.array([1, 2, 3, 4, 5])\nc = np.array([[1, 2, 3], [4, 5, 6]])\nd = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n\nprint(a.ndim)\nprint(b.ndim)\nprint(c.ndim)\nprint(d.ndim)\n</code></pre> <p>Output: <pre><code>0\n1\n2\n3\n</code></pre></p>"},{"location":"AIML/NumPy/#higher-dimensional-arrays","title":"Higher Dimensional Arrays","text":"<p>An array can have any number of dimensions.</p> <p>When the array is created, you can define the number of dimensions by using the ndmin argument.</p>"},{"location":"AIML/NumPy/#example-8","title":"Example: 8","text":"<p>Create an array with 5 dimensions and verify that it has 5 dimensions:</p> <ul> <li>NumPy support maximum array of dimensions  is = 64   MAXDIMS (=64)</li> </ul> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4], ndmin=5)\n\nprint(arr)\nprint('number of dimensions :', arr.ndim)\n</code></pre> <p>Output: <pre><code>[[[[[1 2 3 4]]]]]\nnumber of dimensions : 5\n</code></pre></p> <ul> <li>5th dim: In this array the innermost dimension (5th dim) has 4 elements.</li> <li>4th dim: the 4th dim has 1 element that is the vector.</li> <li>3rd dim: the 3rd dim has 1 element that is the matrix with the vector</li> <li>2nd dim: the 2nd dim has 1 element that is 3D array.</li> <li>1st dim: 1st dim has 1 element that is a 4D array.</li> </ul>"},{"location":"AIML/NumPy/#key-concept-n-d-arrays-dont-automatically-treat-contents-as-matrices","title":"Key Concept: N-D Arrays Don't Automatically Treat Contents as Matrices","text":"<p>In NumPy:     - A matrix is just a 2D array (with shape (rows, cols)).     - A vector is a 1D array (shape (n,)).     - A scalar has shape () (0D).</p> <ul> <li>Everything beyond 2D is a tensor \u2014 and all dimensions are just levels of nesting.</li> <li>NumPy only treats an array as a matrix if it\u2019s 2D, like: [[1, 2], [3, 4]].</li> <li>Does NumPy Consider a 3D Array a Matrix?<ul> <li>NumPy does not consider a 3D array a matrix.</li> <li>Why?<ul> <li>In linear algebra, a matrix is strictly a 2D structure: it has rows and columns.</li> </ul> </li> </ul> </li> </ul>"},{"location":"AIML/NumPy/#numpy-follows-this-convention","title":"NumPy follows this convention.","text":"<pre><code>Shape               Interpretation\n(3,)                1D array (vector)\n(3, 4)              2D array (matrix)\n(2, 3, 4)           3D array (tensor)\n(1, 1, 1, 1, 4)     5D tensor\n</code></pre> <p>A 3D array in NumPy is: A stack of matrices (or a cube of numbers).</p> <p>Example:</p> <pre><code>import numpy as np\n\narr = np.array([\n  [[1, 2], [3, 4]],\n  [[5, 6], [7, 8]]\n])\n</code></pre> <p>Shape:</p> <p><pre><code>(2, 2, 2)\n</code></pre> Interpretation:     - 2 matrices     - Each matrix is 2x2</p>"},{"location":"AIML/NumPy/#matrix-vs-tensor-in-numpy","title":"Matrix vs Tensor in NumPy","text":"<pre><code>Concept         Description                                     NumPy term\nScalar          0D single value                                 np.array(5)\nVector          1D array                                        np.array([1,2])\nMatrix          2D array (rows \u00d7 cols)                          np.array([[1,2], [3,4]])\nTensor          3D+ array (e.g., 3D, 4D...)                     np.array([[[...]]])\n</code></pre>"},{"location":"AIML/NumPy/#numpy-array-indexing","title":"NumPy Array Indexing","text":"<p>Array indexing is the same as accessing an array element. You can access an array element by referring to its index number. The indexes in NumPy arrays start with 0, meaning that the first element has index 0, and the second has index 1 etc.</p> <p>What is Indexing? Indexing is how you access elements inside a NumPy array using their position (like in a list). NumPy supports powerful indexing for:</p> <ul> <li>1D, 2D, 3D+ arrays</li> <li>Slicing</li> <li>Boolean conditions</li> <li>Fancy indexing</li> </ul>"},{"location":"AIML/NumPy/#1d-array-indexing","title":"\u2705 1D Array Indexing","text":"<p><pre><code>import numpy as np\na = np.array([10, 20, 30, 40, 50])\n</code></pre> Access:</p> <pre><code>print(a[0])\n10\n\nprint(a[-1])\n50  (last element)\n</code></pre>"},{"location":"AIML/NumPy/#2d-array-indexing-matrix","title":"\u2705 2D Array Indexing (Matrix)","text":"<pre><code>b = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n</code></pre> <pre><code>1st row:\n\nb[0, 0]  # (1st row, 1st column)\nb[0, 1]   # (1st row, 2nd column), like wise.\n\n2nd row:\n\nb[1, 0]   # (2nd row, 1st column)\nb[1, 2]  # (2nd row, 3rd column)\n\n3rd row:\n\nb[2, 0]   # (3rd row, 1st column)\nb[2, 1]   # (3rd row, 2nd column)\n\nwhole 3rd row:\n\nb[2]     # \u2192 [7, 8, 9] (whole 3rd row)\n\nwhole 2nd column:\n\nb[:, 1]  # \u2192 [2, 5, 8] (whole 2nd column)\n</code></pre>"},{"location":"AIML/NumPy/#3d-array-indexing-tensor","title":"\u2705 3D Array Indexing (Tensor)","text":"<pre><code>c = np.array([\n  [[1, 2], [3, 4]],\n  [[5, 6], [7, 8]]\n])\n</code></pre> <p>Shape: (2, 2, 2)</p> <p>Access:</p> <pre><code>c[0, 0, 0] -&gt; 1\n\nc[0, 0, 1] -&gt; 2\n\nc[1, 1, 0] -&gt; 7\n</code></pre>"},{"location":"AIML/NumPy/#numpy-array-slicing","title":"NumPy Array Slicing","text":"<p>Slicing arrays</p> <p>Slicing in python means taking elements from one given index to another given index. We pass slice instead of index like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1</p>"},{"location":"AIML/NumPy/#slicing","title":"\u2702\ufe0f Slicing","text":"<pre><code>a = np.array([10, 20, 30, 40, 50])\na[1:4]     # \u2192 [20 30 40]\na[:3]      # \u2192 [10 20 30]\na[::2]     # \u2192 [10 30 50] (every 2nd element)\n</code></pre> <p>2D slicing:</p> <pre><code>b = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\nb[0:2, 1:]  # \u2192 [[2, 3], [5, 6]]\n\n- 0:2 \u2192 select rows 0 and 1 (i.e., the first two rows)\n- 1: \u2192 select columns starting from index 1 (i.e., the second and third columns) \n</code></pre>"},{"location":"AIML/NumPy/#boolean-indexing","title":"\ud83e\udde0 Boolean Indexing","text":"<pre><code>a = np.array([1, 2, 3, 4, 5])\na[a &gt; 3]  # \u2192 [4 5]\n</code></pre>"},{"location":"AIML/NumPy/#negative-slicing","title":"Negative Slicing","text":"<p>Use the minus operator to refer to an index from the end:</p> <p>Example Slice from the index 3 from the end to index 1 from the end:</p> <pre><code>arr = np.array([1, 2, 3, 4, 5, 6, 7])\nprint(arr[-3:-1])\n\nOutput: [5 6]\n</code></pre>"},{"location":"AIML/NumPy/#step","title":"STEP","text":"<p>Use the step value to determine the step of the slicing:</p> <p>Example Return every other element from index 1 to index 5:</p> <pre><code>arr = np.array([1, 2, 3, 4, 5, 6, 7])\n\nOutput: [2 4]\n</code></pre> <p>Example Return every other element from the entire array:</p> <pre><code>arr = np.array([1, 2, 3, 4, 5, 6, 7])\nprint(arr[::2])\n\nOutput: [1 3 5 7]\n</code></pre>"},{"location":"AIML/NumPy/#slicing-2-d-arrays","title":"Slicing 2-D Arrays","text":"<p>Example From the second element, slice elements from index 1 to index 4 (not included):</p> <pre><code>arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\nprint(arr[1, 1:4])\n\nOutput: [7 8 9]\n</code></pre> <p>Example From both elements, return index 2:</p> <pre><code>arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\nprint(arr[0:2, 2])\n\nOutput: [3 8]\n</code></pre> <p>Example From both elements, slice index 1 to index 4 (not included), this will return a 2-D array:</p> <pre><code>arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\nprint(arr[0:2, 1:4])\n\nOutput: [[2 3 4]\n        [7 8 9]]\n</code></pre>"},{"location":"AIML/NumPy/#numpy-data-types","title":"NumPy Data Types","text":""},{"location":"AIML/NumPy/#data-types-in-python","title":"Data Types in Python","text":"<p>By default Python have these data types:</p> <ul> <li>strings - used to represent text data, the text is given under quote marks. e.g. \"ABCD\"</li> <li>integer - used to represent integer numbers. e.g. -1, -2, -3</li> <li>float - used to represent real numbers. e.g. 1.2, 42.42</li> <li>boolean - used to represent True or False.</li> <li>complex - used to represent complex numbers. e.g. 1.0 + 2.0j, 1.5 + 2.5j</li> </ul>"},{"location":"AIML/NumPy/#data-types-in-numpy","title":"Data Types in NumPy","text":"<p>NumPy has some extra data types, and refer to data types with one character, like i for integers, u for unsigned integers etc.</p> <p>Below is a list of all data types in NumPy and the characters used to represent them.</p> <ul> <li>i - integer</li> <li>b - boolean</li> <li>u - unsigned integer</li> <li>f - float</li> <li>c - complex float</li> <li>m - timedelta</li> <li>M - datetime</li> <li>O - object</li> <li>S - string</li> <li>U - unicode string</li> <li>V - fixed chunk of memory for other type ( void )</li> </ul>"},{"location":"AIML/NumPy/#checking-the-data-type-of-an-array","title":"Checking the Data Type of an Array","text":"<p>The NumPy array object has a property called dtype that returns the data type of the array:</p> <p>Example:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4])\n\nprint(arr.dtype)\n</code></pre>"},{"location":"AIML/NumPy/#converting-data-type-on-existing-arrays","title":"Converting Data Type on Existing Arrays","text":"<p>The best way to change the data type of an existing array, is to make a copy of the array with the astype() method.</p> <p>The astype() function creates a copy of the array, and allows you to specify the data type as a parameter.</p> <p>The data type can be specified using a string, like 'f' for float, 'i' for integer etc. or you can use the data type directly like float for float and int for integer.</p> <p>Example Change data type from float to integer by using 'i' as parameter value:</p> <pre><code>import numpy as np\n\narr = np.array([1.1, 2.1, 3.1])\n\nnewarr = arr.astype('i')\n\nprint(newarr)\nprint(newarr.dtype)\n</code></pre>"},{"location":"AIML/NumPy/#numpy-array-copy-vs-view","title":"NumPy Array Copy vs View","text":""},{"location":"AIML/NumPy/#the-difference-between-copy-and-view","title":"The Difference Between Copy and View","text":"<p>The main difference between a copy and a view of an array is that the copy is a new array, and the view is just a view of the original array.</p> <p>The copy owns the data and any changes made to the copy will not affect original array, and any changes made to the original array will not affect the copy.</p> <p>The view does not own the data and any changes made to the view will affect the original array, and any changes made to the original array will affect the view.</p>"},{"location":"AIML/NumPy/#copy","title":"COPY:","text":"<p>ExampleGet Make a copy, change the original array, and display both arrays:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nx = arr.copy()\narr[0] = 42\n\nprint(arr)\nprint(x)\n\n\nOutput:\n[42  2  3  4  5]\n[1 2 3 4 5]\n</code></pre>"},{"location":"AIML/NumPy/#view","title":"VIEW:","text":"<p>Example Make a view, change the original array, and display both arrays:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nx = arr.view()\narr[0] = 42\n\nprint(arr)\nprint(x)\n\n\nOutput:\n[42  2  3  4  5]\n[42  2  3  4  5]\n</code></pre> <p>Make a view, change the view, and display both arrays:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nx = arr.view()\nx[0] = 31\n\nprint(arr)\nprint(x)\n\nOutput:\n[31  2  3  4  5]\n[31  2  3  4  5]\n</code></pre>"},{"location":"AIML/NumPy/#check-if-array-owns-its-data","title":"Check if Array Owns its Data","text":"<p>As mentioned above, copies owns the data, and views does not own the data, but how can we check this?</p> <p>Every NumPy array has the attribute base that returns None if the array owns the data. Otherwise, the base  attribute refers to the original object.</p> <p>Example</p> <pre><code>Print the value of the base attribute to check if an array owns it's data or not:\n\nimport numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\n\nx = arr.copy()\ny = arr.view()\n\nprint(x.base)\nprint(y.base)\n\nOutput:\nNone\n[1 2 3 4 5]\n</code></pre>"},{"location":"AIML/NumPy/#numpy-array-shape","title":"NumPy Array Shape","text":"<p>Shape of an Array</p> <p>The shape of an array is the number of elements in each dimension.</p>"},{"location":"AIML/NumPy/#get-the-shape-of-an-array","title":"Get the Shape of an Array","text":"<p>NumPy arrays have an attribute called shape that returns a tuple with each index having the number of corresponding elements.</p> <p>Example Print the shape of a 2-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3, 4],\n                [5, 6, 7, 8]]\n                )\n\nprint(arr.shape)\n\nOutput:\n(2, 4)\n\n\nThe shape tells us:\n\n2 \u2192 there are 2 rows \u2192 the first dimension (axis 0)\n\n4 \u2192 each row has 4 elements \u2192 the second dimension (axis 1)\n\n[\n  [1, 2, 3, 4],  \u2190 1st row\n  [5, 6, 7, 8]   \u2190 2nd row\n]\n\n\n    Columns \u2192\n    0   1   2   3\nR  +---------------\no  | 1   2   3   4\nw  | 5   6   7   8\ns\n\u2193\n</code></pre> <p>Example Create an array with 5 dimensions using ndmin using a vector with values 1,2,3,4 and verify that last dimension has value 4:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4], ndmin=5)\n\nprint(arr)\nprint('shape of array :', arr.shape)\n\nOutput:\n[[[[[1 2 3 4]]]]]\nshape of array : (1, 1, 1, 1, 4)\n</code></pre>"},{"location":"AIML/NumPy/#numpy-array-reshaping","title":"NumPy Array Reshaping","text":"<p>Reshaping arrays</p> <p>Reshaping means changing the shape of an array.</p> <p>The shape of an array is the number of elements in each dimension.</p> <p>By reshaping we can add or remove dimensions or change number of elements in each dimension.</p>"},{"location":"AIML/NumPy/#reshape-from-1-d-to-2-d","title":"Reshape From 1-D to 2-D","text":"<p>Example Convert the following 1-D array with 12 elements into a 2-D array.</p> <p>The outermost dimension will have 4 arrays, each with 3 elements:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n\nnewarr = arr.reshape(4, 3)\n\nprint(newarr)\n\nOutput:\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n</code></pre>"},{"location":"AIML/NumPy/#reshape-from-1-d-to-3-d","title":"Reshape From 1-D to 3-D","text":"<p>Example Convert the following 1-D array with 12 elements into a 3-D array.</p> <p>The outermost dimension will have 2 arrays that contains 3 arrays, each with 2 elements:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n\nnewarr = arr.reshape(2, 3, 2)\n\nprint(newarr)\n\nOutput:\n[[[ 1  2]\n  [ 3  4]\n  [ 5  6]]\n\n [[ 7  8]\n  [ 9 10]\n  [11 12]]]\n</code></pre>"},{"location":"AIML/NumPy/#can-we-reshape-into-any-shape","title":"Can We Reshape Into any Shape?","text":"<p>Yes, as long as the elements required for reshaping are equal in both shapes.</p> <p>We can reshape an 8 elements 1D array into 4 elements in 2 rows 2D array but we cannot reshape it into a 3 elements 3 rows 2D array as that would require 3x3 = 9 elements.</p> <p>Example Try converting 1D array with 8 elements to a 2D array with 3 elements in each dimension (will raise an error):</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nnewarr = arr.reshape(3, 3)\n\nprint(newarr)\n\nOutput:\nValueError: cannot reshape array of size 8 into shape (3,3)\n</code></pre>"},{"location":"AIML/NumPy/#returns-copy-or-view","title":"Returns Copy or View?","text":"<p>Example Check if the returned array is a copy or a view:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nprint(arr.reshape(2, 4).base)\n\n\nOutput:\n[1 2 3 4 5 6 7 8]  -&gt; The example above returns the original array, so it is a view.\n</code></pre>"},{"location":"AIML/NumPy/#unknown-dimension","title":"Unknown Dimension","text":"<p>You are allowed to have one \"unknown\" dimension.</p> <p>Meaning that you do not have to specify an exact number for one of the dimensions in the reshape method.</p> <p>Pass -1 as the value, and NumPy will calculate this number for you.</p> <p>Example Convert 1D array with 8 elements to 3D array with 2x2 elements:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nnewarr = arr.reshape(2, 2, -1)\n\nprint(newarr)\n\nOutput:\n[[[1 2]\n  [3 4]]\n\n [[5 6]\n  [7 8]]]\n</code></pre>"},{"location":"AIML/NumPy/#flattening-the-arrays","title":"Flattening the arrays","text":"<p>Flattening array means converting a multidimensional array into a 1D array.</p> <p>We can use reshape(-1) to do this.</p> <p>Example</p> <p>Convert the 2D array into a 1D array:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\nnewarr = arr.reshape(-1)\n\nprint(newarr)\n\nOutput:\n[1 2 3 4 5 6]\n</code></pre> <p>Note: There are a lot of functions for changing the shapes of arrays in numpy flatten, ravel and also for rearranging the elements rot90, flip, fliplr, flipud etc. These fall under Intermediate to Advanced section of numpy.</p>"},{"location":"AIML/NumPy/#numpy-array-iterating","title":"NumPy Array Iterating","text":"<p>Iterating Arrays</p> <p>Iterating means going through elements one by one.</p> <p>As we deal with multi-dimensional arrays in numpy, we can do this using basic for loop of python.</p> <p>If we iterate on a 1-D array it will go through each element one by one.</p> <p>Example Iterate on the elements of the following 1-D array:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3])\n\nfor x in arr:\n  print(x)\n\nOutput:\n1\n2\n3\n</code></pre> <p>Iterating 2-D Arrays</p> <p>In a 2-D array it will go through all the rows.</p> <p>Example Iterate on the elements of the following 2-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\nfor x in arr:\n  print(x)\n\nOutput:\n[1 2 3]\n[4 5 6]\n</code></pre> <p>If we iterate on a n-D array it will go through n-1th dimension one by one.</p> <p>To return the actual values, the scalars, we have to iterate the arrays in each dimension.</p> <p>Example Iterate on each scalar element of the 2-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\n\nfor x in arr:\n  for y in x:\n    print(y)\n\nOutput:\n1\n2\n3\n4\n5\n6\n</code></pre>"},{"location":"AIML/NumPy/#iterating-3-d-arrays","title":"Iterating 3-D Arrays","text":"<p>In a 3-D array it will go through all the 2-D arrays.</p> <p>Example Iterate on the elements of the following 3-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\nfor x in arr:\n  print(x)\n\nOutput:\n[[1 2 3]\n [4 5 6]]\n[[ 7  8  9]\n [10 11 12]]\n</code></pre> <p>To return the actual values, the scalars, we have to iterate the arrays in each dimension.</p> <p>Example Iterate down to the scalars:</p> <pre><code>import numpy as np\n\narr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\nfor x in arr:\n  for y in x:\n    for z in y:\n      print(z)\n\nOutput:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n</code></pre>"},{"location":"AIML/NumPy/#iterating-arrays-using-nditer","title":"Iterating Arrays Using nditer()","text":"<p>The function nditer() is a helping function that can be used from very basic to very advanced iterations. It solves some basic issues which we face in iteration, lets go through it with examples.</p> <p>Iterating on Each Scalar Element</p> <p>In basic for loops, iterating through each scalar of an array we need to use n for loops which can be difficult to write for arrays with very high dimensionality.</p> <p>Example Iterate through the following 3-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\nfor x in np.nditer(arr):\n  print(x)\n\nOutput:\n1\n2\n3\n4\n5\n6\n7\n8\n</code></pre>"},{"location":"AIML/NumPy/#iterating-array-with-different-data-types","title":"Iterating Array With Different Data Types","text":"<p>We can use op_dtypes argument and pass it the expected datatype to change the datatype of elements while iterating.</p> <p>NumPy does not change the data type of the element in-place (where the element is in array) so it needs some other space to perform this action, that extra space is called buffer, and in order to enable it in nditer() we pass flags=['buffered'].</p> <p>Example Iterate through the array as a string:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3])\n\nfor x in np.nditer(arr, flags=['buffered'], op_dtypes=['S']):\n  print(x)\n\nOutput:\nnp.bytes_(b'1')\nnp.bytes_(b'2')\nnp.bytes_(b'3')\n</code></pre>"},{"location":"AIML/NumPy/#iterating-with-different-step-size","title":"Iterating With Different Step Size","text":"<p>We can use filtering and followed by iteration.</p> <p>Example Iterate through every scalar element of the 2D array skipping 1 element:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n\nfor x in np.nditer(arr[:, ::2]):\n  print(x)\n\nOutput:\n1\n3\n5\n7\n</code></pre>"},{"location":"AIML/NumPy/#enumerated-iteration-using-ndenumerate","title":"Enumerated Iteration Using ndenumerate()","text":"<p>Enumeration means mentioning sequence number of somethings one by one.</p> <p>Sometimes we require corresponding index of the element while iterating, the ndenumerate() method can be used for those usecases.</p> <p>Example Enumerate on following 1D arrays elements:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3])\n\nfor idx, x in np.ndenumerate(arr):\n  print(idx, x)\n\n\nOutput:\n(0,) 1\n(1,) 2\n(2,) 3\n</code></pre> <p>Example Enumerate on following 2D array's elements:</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n\nfor idx, x in np.ndenumerate(arr):\n  print(idx, x)\n\nOutput:\n(0, 0) 1\n(0, 1) 2\n(0, 2) 3\n(0, 3) 4\n(1, 0) 5\n(1, 1) 6\n(1, 2) 7\n(1, 3) 8\n</code></pre>"},{"location":"AIML/NumPy/#numpy-joining-array","title":"NumPy Joining Array","text":""},{"location":"AIML/NumPy/#joining-numpy-arrays","title":"Joining NumPy Arrays","text":"<p>Joining means putting contents of two or more arrays in a single array.</p> <p>In SQL we join tables based on a key, whereas in NumPy we join arrays by axes.</p> <p>We pass a sequence of arrays that we want to join to the concatenate() function, along with the axis. If axis is not explicitly passed, it is taken as 0.</p> <p>ExampleGet your own Python Server Join two arrays</p> <pre><code>import numpy as np\n\narr1 = np.array([1, 2, 3])\n\narr2 = np.array([4, 5, 6])\n\narr = np.concatenate((arr1, arr2))\n\nprint(arr)\n\nOutput:\n[1 2 3 4 5 6]\n</code></pre> <p>Example Join two 2-D arrays along rows (axis=1):</p> <pre><code>import numpy as np\n\narr1 = np.array([[1, 2], [3, 4]])\n\narr2 = np.array([[5, 6], [7, 8]])\n\narr = np.concatenate((arr1, arr2), axis=1)\n\nprint(arr)\n\nOutput:\n[[1 2 5 6]\n [3 4 7 8]]\n</code></pre>"},{"location":"AIML/NumPy/#joining-arrays-using-stack-functions","title":"Joining Arrays Using Stack Functions","text":"<p>Stacking is same as concatenation, the only difference is that stacking is done along a new axis.</p> <p>We can concatenate two 1-D arrays along the second axis which would result in putting them one over the other, ie. stacking.</p> <p>We pass a sequence of arrays that we want to join to the stack() method along with the axis. If axis is not explicitly passed it is taken as 0.</p> <p>Example</p> <pre><code>import numpy as np\n\narr1 = np.array([1, 2, 3])\n\narr2 = np.array([4, 5, 6])\n\narr = np.stack((arr1, arr2), axis=1)\n\nprint(arr)\n\n\nOutput:\n[[1 4]\n [2 5]\n [3 6]]\n</code></pre>"},{"location":"AIML/NumPy/#stacking-along-rows","title":"Stacking Along Rows","text":"<p>NumPy provides a helper function: hstack() to stack along rows.</p> <p>Example</p> <pre><code>import numpy as np\n\narr1 = np.array([1, 2, 3])\n\narr2 = np.array([4, 5, 6])\n\narr = np.hstack((arr1, arr2))\n\nprint(arr)\n\nOutput:\n[1 2 3 4 5 6]\n</code></pre>"},{"location":"AIML/NumPy/#stacking-along-columns","title":"Stacking Along Columns","text":"<p>NumPy provides a helper function: vstack()  to stack along columns.</p> <p>Example</p> <pre><code>import numpy as np\n\narr1 = np.array([1, 2, 3])\n\narr2 = np.array([4, 5, 6])\n\narr = np.vstack((arr1, arr2))\n\nprint(arr)\n\nOutput:\n[[1 2 3]\n [4 5 6]]\n</code></pre>"},{"location":"AIML/NumPy/#stacking-along-height-depth","title":"Stacking Along Height (depth)","text":"<p>NumPy provides a helper function: dstack() to stack along height, which is the same as depth.</p> <p>Example</p> <pre><code>import numpy as np\n\narr1 = np.array([1, 2, 3])\n\narr2 = np.array([4, 5, 6])\n\narr = np.dstack((arr1, arr2))\n\nprint(arr)\n\n\nOutput:\n[[[1 4]\n  [2 5]\n  [3 6]]]\n</code></pre>"},{"location":"AIML/NumPy/#numpy-splitting-array","title":"NumPy Splitting Array","text":""},{"location":"AIML/NumPy/#splitting-numpy-arrays","title":"Splitting NumPy Arrays","text":"<p>Splitting is reverse operation of Joining.</p> <p>Joining merges multiple arrays into one and Splitting breaks one array into multiple.</p> <p>We use array_split() for splitting arrays, we pass it the array we want to split and the number of splits.</p> <p>Example Split the array in 3 parts:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6])\n\nnewarr = np.array_split(arr, 3)\n\nprint(newarr)\n\n\nOutput:\n[array([1, 2]), array([3, 4]), array([5, 6])]\n</code></pre> <p>If the array has less elements than required, it will adjust from the end accordingly.</p> <p>Example Split the array in 4 parts:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6])\n\nnewarr = np.array_split(arr, 4)\n\nprint(newarr)\n\n\nOutput:\n[array([1, 2]), array([3, 4]), array([5]), array([6])]\n</code></pre> <p>Note: We also have the method split() available but it will not adjust the elements when elements are less in source array for splitting like in example above, array_split() worked properly but split() would fail.</p>"},{"location":"AIML/NumPy/#split-into-arrays","title":"Split Into Arrays","text":"<p>The return value of the array_split() method is an array containing each of the split as an array.</p> <p>If you split an array into 3 arrays, you can access them from the result just like any array element:</p> <p>Example Access the splitted arrays:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6])\n\nnewarr = np.array_split(arr, 3)\n\nprint(newarr[0])\nprint(newarr[1])\nprint(newarr[2])\n\nOutput:\n[1 2]\n[3 4]\n[5 6]\n</code></pre>"},{"location":"AIML/NumPy/#splitting-2-d-arrays","title":"Splitting 2-D Arrays","text":"<p>Use the same syntax when splitting 2-D arrays.</p> <p>Use the array_split() method, pass in the array you want to split and the number of splits you want to do.</p> <p>Example Split the 2-D array into three 2-D arrays.</p> <p><pre><code>import numpy as np\n\narr = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n\nnewarr = np.array_split(arr, 3)\n\nprint(newarr)\n\nOutput:\n[array([[1, 2],\n       [3, 4]]), array([[5, 6],\n       [7, 8]]), array([[ 9, 10],\n       [11, 12]])]\n</code></pre> The example above returns three 2-D arrays.</p> <p>Example Split the 2-D array into three 2-D arrays.</p> <pre><code>[array([[1, 2, 3],\n       [4, 5, 6]]), array([[ 7,  8,  9],\n       [10, 11, 12]]), array([[13, 14, 15],\n       [16, 17, 18]])]\n\n\nOutput:\n[array([[1, 2, 3],\n       [4, 5, 6]]), array([[ 7,  8,  9],\n       [10, 11, 12]]), array([[13, 14, 15],\n       [16, 17, 18]])]\n</code></pre> <p>The example above returns three 2-D arrays.</p> <p>In addition, you can specify which axis you want to do the split around.</p> <p>The example below also returns three 2-D arrays, but they are split along the row (axis=1).</p> <p>Example Split the 2-D array into three 2-D arrays along rows.</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]])\n\nnewarr = np.array_split(arr, 3, axis=1)\n\nprint(newarr)\n\nOutput:\n\n\n[array([[ 1],\n       [ 4],\n       [ 7],\n       [10],\n       [13],\n       [16]]), array([[ 2],\n       [ 5],\n       [ 8],\n       [11],\n       [14],\n       [17]]), array([[ 3],\n       [ 6],\n       [ 9],\n       [12],\n       [15],\n       [18]])]\n</code></pre> <p>An alternate solution is using hsplit() opposite of hstack()</p> <p>Example Use the hsplit() method to split the 2-D array into three 2-D arrays along rows.</p> <pre><code>import numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18]])\n\nnewarr = np.hsplit(arr, 3)\n\nprint(newarr)\n\n\nOutput:\n[array([[ 1],\n       [ 4],\n       [ 7],\n       [10],\n       [13],\n       [16]]), array([[ 2],\n       [ 5],\n       [ 8],\n       [11],\n       [14],\n       [17]]), array([[ 3],\n       [ 6],\n       [ 9],\n       [12],\n       [15],\n       [18]])]\n</code></pre> <p>Note: Similar alternates to vstack() and dstack() are available as vsplit() and dsplit().</p>"},{"location":"AIML/NumPy/#numpy-searching-arrays","title":"NumPy Searching Arrays","text":""},{"location":"AIML/NumPy/#searching-arrays","title":"Searching Arrays","text":"<p>You can search an array for a certain value, and return the indexes that get a match.</p> <p>To search an array, use the where() method.</p> <p>Example Find the indexes where the value is 4:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 4, 4])\n\nx = np.where(arr == 4)\n\nprint(x)\n\nOutput:\n(array([3, 5, 6]),)  -&gt; Which means that the value 4 is present at index 3, 5, and 6.\n</code></pre> <p>Example Find the indexes where the values are even:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nx = np.where(arr%2 == 0)\n\nprint(x)\n\nOutput:\n(array([1, 3, 5, 7]),)\n</code></pre> <p>Example Find the indexes where the values are odd:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nx = np.where(arr%2 == 1)\n\nprint(x)\n\nOutput:\n(array([0, 2, 4, 6]),)\n</code></pre>"},{"location":"AIML/NumPy/#search-sorted","title":"Search Sorted","text":"<p>There is a method called searchsorted() which performs a binary search in the array, and returns the index where the specified value would be inserted to maintain the search order.</p> <p>The searchsorted() method is assumed to be used on sorted arrays.</p> <p>Example Find the indexes where the value 7 should be inserted:</p> <pre><code>import numpy as np\n\narr = np.array([6, 7, 8, 9])\n\nx = np.searchsorted(arr, 7)\n\nprint(x)\n\n\nOutput:\n1        -&gt; The number 7 should be inserted on index 1 to remain the sort order.\n</code></pre> <p>The method starts the search from the left and returns the first index where the number 7 is no longer larger than the next value.</p>"},{"location":"AIML/NumPy/#search-from-the-right-side","title":"Search From the Right Side","text":"<p>By default the left most index is returned, but we can give side='right' to return the right most index instead.</p> <p>Example Find the indexes where the value 7 should be inserted, starting from the right:</p> <pre><code>import numpy as np\n\narr = np.array([6, 7, 8, 9])\n\nx = np.searchsorted(arr, 7, side='right')\n\nprint(x)\n\nOutput:\n2        -&gt; The number 7 should be inserted on index 2 to remain the sort order.\n</code></pre> <p>The method starts the search from the right and returns the first index where the number 7 is no longer less than the next value.</p>"},{"location":"AIML/NumPy/#multiple-values","title":"Multiple Values","text":"<p>To search for more than one value, use an array with the specified values.</p> <p>Example Find the indexes where the values 2, 4, and 6 should be inserted:</p> <pre><code>import numpy as np\n\narr = np.array([1, 3, 5, 7])\n\nx = np.searchsorted(arr, [2, 4, 6])\n\nprint(x)\n\n\nOutput:\n[1 2 3]         \n\narr = [1, 3, 5, 7]\n        \u2191  \u2191  \u2191  \u2191\nIndex:  0  1  2  3\n</code></pre> <p>The return value is an array: [1 2 3] containing the three indexes where 2, 4, 6 would be inserted in the original array to maintain the order.</p>"},{"location":"AIML/NumPy/#numpy-sorting-arrays","title":"NumPy Sorting Arrays","text":""},{"location":"AIML/NumPy/#sorting-arrays","title":"Sorting Arrays","text":"<p>Sorting means putting elements in an ordered sequence.</p> <p>Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending.</p> <p>The NumPy ndarray object has a function called sort(), that will sort a specified array.</p> <p>Example Sort the array:</p> <pre><code>import numpy as np\n\narr = np.array([3, 2, 0, 1])\n\nprint(np.sort(arr))\n\nOutput:\n[0 1 2 3]\n</code></pre> <p>Note: This method returns a copy of the array, leaving the original array unchanged.</p> <p>Example Sort the array alphabetically:</p> <pre><code>import numpy as np\n\narr = np.array(['banana', 'cherry', 'apple'])\n\nprint(np.sort(arr))\n\n\nOutput:\n\n['apple' 'banana' 'cherry']\n</code></pre> <p>Example Sort a boolean array:</p> <pre><code>import numpy as np\n\narr = np.array([True, False, True])\n\nprint(np.sort(arr))\n\n\nOutput:\n[False  True  True]\n</code></pre>"},{"location":"AIML/NumPy/#sorting-a-2-d-array","title":"Sorting a 2-D Array","text":"<p>If you use the sort() method on a 2-D array, both arrays will be sorted:</p> <p>Example Sort a 2-D array:</p> <pre><code>import numpy as np\n\narr = np.array([[3, 2, 4], [5, 0, 1]])\n\nprint(np.sort(arr))\n\nOutput:\n[[2 3 4]\n [0 1 5]]\n</code></pre>"},{"location":"AIML/NumPy/#numpy-filter-array","title":"NumPy Filter Array","text":""},{"location":"AIML/NumPy/#filtering-arrays","title":"Filtering Arrays","text":"<p>Getting some elements out of an existing array and creating a new array out of them is called filtering.</p> <p>In NumPy, you filter an array using a boolean index list.</p> <p>A boolean index list is a list of booleans corresponding to indexes in the array.</p> <p>If the value at an index is True that element is contained in the filtered array, if the value at that index is False that element is excluded from the filtered array.</p> <p>Example Create an array from the elements on index 0 and 2:</p> <pre><code>import numpy as np\n\narr = np.array([41, 42, 43, 44])\n\nx = [True, False, True, False]\n\nnewarr = arr[x]\n\nprint(newarr)\n\nOutput:\n[41 43]\n\n\nThe example above will return [41, 43], why?\n\nBecause the new array contains only the values where the filter array had the value True, in this case, index 0 and 2.\n</code></pre>"},{"location":"AIML/NumPy/#creating-the-filter-array","title":"Creating the Filter Array","text":"<p>In the example above we hard-coded the True and False values, but the common use is to create a filter array based on conditions.</p> <p>Example Create a filter array that will return only values higher than 42:</p> <pre><code>import numpy as np\n\narr = np.array([41, 42, 43, 44])\n\n# Create an empty list\nfilter_arr = []\n\n# go through each element in arr\nfor element in arr:\n  # if the element is higher than 42, set the value to True, otherwise False:\n  if element &gt; 42:\n    filter_arr.append(True)\n  else:\n    filter_arr.append(False)\n\nnewarr = arr[filter_arr]\n\nprint(filter_arr)\nprint(newarr)\n\nOutput:\n[False, False, True, True]\n[43 44]\n</code></pre> <p>Example Create a filter array that will return only even elements from the original array:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7])\n\n# Create an empty list\nfilter_arr = []\n\n# go through each element in arr\nfor element in arr:\n  # if the element is completely divisble by 2, set the value to True, otherwise False\n  if element % 2 == 0:\n    filter_arr.append(True)\n  else:\n    filter_arr.append(False)\n\nnewarr = arr[filter_arr]\n\nprint(filter_arr)\nprint(newarr)\n\nOutput:\n[False, True, False, True, False, True, False]\n[2 4 6]\n</code></pre>"},{"location":"AIML/NumPy/#creating-filter-directly-from-array","title":"Creating Filter Directly From Array","text":"<p>The above example is quite a common task in NumPy and NumPy provides a nice way to tackle it.</p> <p>We can directly substitute the array instead of the iterable variable in our condition and it will work just as we expect it to.</p> <p>Example Create a filter array that will return only values higher than 42:</p> <pre><code>import numpy as np\n\narr = np.array([41, 42, 43, 44])\n\nfilter_arr = arr &gt; 42\n\nnewarr = arr[filter_arr]\n\nprint(filter_arr)\nprint(newarr)\n\n\nOutput:\n[False False  True  True]\n[43 44]\n</code></pre> <p>Example Create a filter array that will return only even elements from the original array:</p> <pre><code>import numpy as np\n\narr = np.array([1, 2, 3, 4, 5, 6, 7])\n\nfilter_arr = arr % 2 == 0\n\nnewarr = arr[filter_arr]\n\nprint(filter_arr)\nprint(newarr)\n\nOutput:\n[False  True False  True False  True False]\n[2 4 6]\n</code></pre>"},{"location":"AIML/NumPy/#1-array-creation","title":"\ud83e\udde0 1. Array Creation","text":"<p>Used for creating datasets, weight matrices, etc.</p> <pre><code>np.array()         # Convert list to array\nnp.zeros(), np.ones()  # Initialize weights\nnp.eye()           # Identity matrix\nnp.arange(), np.linspace()  # Range of values\n</code></pre>"},{"location":"AIML/NumPy/#2-array-operations-math","title":"\u2699\ufe0f 2. Array Operations &amp; Math","text":"<p>Used for vectorized operations (fast!)</p> <pre><code>+ - * /            # Element-wise ops\nnp.dot(), np.matmul()  # Matrix multiplication\nnp.sum(), np.mean(), np.std(), np.var()\nnp.max(), np.min(), np.argmax(), np.argmin()\nnp.exp(), np.log(), np.sqrt()\nnp.clip()          # Limit values (e.g. activation limits)\n</code></pre>"},{"location":"AIML/NumPy/#3-reshaping-indexing","title":"\ud83d\udcd0 3. Reshaping &amp; Indexing","text":"<p>Used to prepare data for ML models (like reshaping images, slicing time series)</p> <pre><code>np.reshape(), np.ravel(), np.flatten()\nnp.transpose(), np.swapaxes()\nnp.concatenate(), np.stack(), np.split()\nnp.where(), np.argwhere()\nnp.unique()\n</code></pre>"},{"location":"AIML/NumPy/#4-random-numbers-for-initializing-weights-data-splitting-etc","title":"\ud83d\udcca 4. Random Numbers (for initializing weights, data splitting, etc.)","text":"<pre><code>np.random.rand(), np.random.randn()   # Uniform &amp; normal dist\nnp.random.randint()\nnp.random.shuffle(), np.random.permutation()\nnp.random.seed()   # Set seed for reproducibility\n</code></pre>"},{"location":"AIML/NumPy/#5-logical-boolean-indexing","title":"\ud83d\udcc9 5. Logical &amp; Boolean Indexing","text":"<p>Used for masking, filtering, conditional operations.</p> <pre><code>a[a &gt; 0]                  # Filter positives\nnp.any(), np.all()\nnp.isfinite(), np.isnan() # Data cleaning\n</code></pre>"},{"location":"AIML/NumPy/#6-linear-algebra-used-in-neural-networks-pca-etc","title":"\ud83e\uddea 6. Linear Algebra (used in neural networks, PCA, etc.)","text":"<pre><code>np.linalg.inv()      # Inverse\nnp.linalg.pinv()     # Pseudo-inverse\nnp.linalg.norm()     # Vector norms\nnp.linalg.eig(), np.linalg.svd()\n</code></pre>"},{"location":"AIML/NumPy/#common-mlai-tasks-using-numpy","title":"\ud83e\udde0 Common ML/AI Tasks Using NumPy:","text":"<ul> <li> <p>Feature scaling / normalization: np.mean(), np.std()</p> </li> <li> <p>Distance calculations: np.linalg.norm()</p> </li> <li> <p>Vectorized loss functions (MSE, Cross Entropy, etc.)</p> </li> <li> <p>Gradient descent implementation</p> </li> <li> <p>Custom ML algorithms (k-NN, k-means, PCA)</p> </li> </ul>"},{"location":"AIML/PoissonDistribution/","title":"\ud83d\udd22 What is Poisson Distribution?","text":"<p>The Poisson distribution models the number of times an event occurs in a fixed interval of time or space, given that:</p> <ul> <li>Events happen independently.</li> <li>The average rate (\u03bb) of events is constant.</li> <li>Two events can't occur at exactly the same instant.</li> </ul> <p></p>"},{"location":"AIML/PoissonDistribution/#real-time-use-cases-of-poisson-distribution","title":"\u2705 Real-Time Use Cases of Poisson Distribution","text":""},{"location":"AIML/PoissonDistribution/#1-website-traffic-modeling","title":"1. Website Traffic Modeling","text":"<ul> <li>\ud83d\udcca Use: Estimate number of users visiting a site per minute</li> <li>\ud83c\udfaf Goal: Predict spikes and scale infrastructure (load balancer, autoscaling)</li> <li>\ud83e\udde0 ML Use: Traffic anomaly detection, feature for time-series models</li> </ul>"},{"location":"AIML/PoissonDistribution/#2-customer-support-tickets","title":"2. Customer Support Tickets","text":"<ul> <li>\ud83d\udcde Use: Number of support calls/emails per hour</li> <li>\ud83d\udee0 Why: Helps in agent workload prediction and staff planning</li> <li>\ud83d\udcc8 ML Use: Input feature for forecasting demand using XGBoost/Prophet</li> </ul>"},{"location":"AIML/PoissonDistribution/#3-bank-fraud-detection","title":"3. Bank Fraud Detection","text":"<ul> <li>\ud83d\udcb3 Use: Track number of transactions per account in an hour/day</li> <li>\u26a0\ufe0f Logic: If a user makes 100 transactions in a short period (way above \u03bb), flag it!</li> <li>\ud83e\udd16 ML Use: Anomaly detection, input to fraud scoring models</li> </ul>"},{"location":"AIML/PoissonDistribution/#4-industrial-iot-sensor-events","title":"4. Industrial IoT Sensor Events","text":"<ul> <li>\ud83c\udfed Use: Number of machine faults per shift</li> <li>\ud83d\udd27 Goal: Predict future maintenance needs (predictive maintenance)</li> <li>\ud83e\udde0 ML Use: Poisson regression for count prediction models</li> </ul>"},{"location":"AIML/PoissonDistribution/#5-call-center-telecom-networks","title":"5. Call Center / Telecom Networks","text":"<ul> <li>\u260e\ufe0f Use: Incoming calls per second on telecom networks</li> <li>\ud83e\udde0 ML Use: Train models for resource allocation or latency prediction</li> </ul>"},{"location":"AIML/PoissonDistribution/#visualization-of-poisson-distribution","title":"Visualization of Poisson Distribution","text":"<p>Example</p> <pre><code>from numpy import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.displot(random.poisson(lam=2, size=1000))\n\nplt.show()\n</code></pre> <p></p> <p>Example</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n\u03bb = 4  # avg 4 calls per minute\nx = np.arange(0, 15)\npmf = poisson.pmf(x, \u03bb)\n\nplt.bar(x, pmf, color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"\ud83d\udcde Poisson Distribution: Calls per Minute (\u03bb = 4)\")\nplt.xlabel(\"Number of Calls\")\nplt.ylabel(\"Probability\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <pre><code>Feature                                 Description\nDistribution Type                       Discrete (integer values)\nTypical Use Cases                       Count-based: arrivals, failures, events\nKey Parameter                           \u03bb (average rate of occurrence)\nIn ML                                   Anomaly detection, Poisson regression, forecasting\n</code></pre>"},{"location":"AIML/Seaborn/","title":"An introduction to seaborn","text":"<p>Seaborn is a library for making statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures.</p> <p>Seaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.</p> <p>Here\u2019s an example of what seaborn can do:</p> <pre><code> Import seaborn\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n</code></pre> <p></p>"},{"location":"AIML/Seaborn/#visualize-distributions-with-seaborn","title":"Visualize Distributions With Seaborn","text":"<p>Seaborn is a library that uses Matplotlib underneath to plot graphs. It will be used to visualize random distributions.</p>"},{"location":"AIML/Seaborn/#displots","title":"Displots","text":"<p>Displot stands for distribution plot, it takes as input an array and plots a curve corresponding to the distribution of points in the array.</p> <p>Import Matplotlib Import the pyplot object of the Matplotlib module in your code using the following statement:</p> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <p>Import Seaborn Import the Seaborn module in your code using the following statement:</p> <pre><code>import seaborn as sns\n</code></pre>"},{"location":"AIML/Seaborn/#plotting-a-displot","title":"Plotting a Displot","text":"<p>ExampleGet</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.displot([0, 1, 2, 3, 4, 5])\n\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/Seaborn/#plotting-a-displot-without-the-histogram","title":"Plotting a Displot Without the Histogram","text":"<p>Example</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.displot([0, 1, 2, 3, 4, 5], kind=\"kde\")\n\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/Statistics/","title":"Statistics","text":"<p>For any Machine Learning project, understanding the underlying statistics and using the right plots is crucial during EDA (Exploratory Data Analysis), feature selection, model evaluation, and interpretability.</p>"},{"location":"AIML/Statistics/#1-descriptive-statistics","title":"\ud83d\udd0d 1. Descriptive Statistics","text":"<p>Used to summarize and understand the data.</p> Task Statistics Central Tendency Mean, Median, Mode Spread Standard Deviation, Variance, IQR Shape Skewness, Kurtosis Outliers Z-score, IQR method Data Distribution Count, Frequency tables"},{"location":"AIML/Statistics/#2-eda-visualization-plots","title":"\ud83d\udcca 2. EDA &amp; Visualization Plots","text":"Purpose Plot Type Use Case Univariate analysis Histogram, KDE Plot, Boxplot Distribution of a single feature Bivariate analysis Scatter plot, Line plot, Heatmap Feature relationships Outlier detection Boxplot, Violin plot Detect extreme values Skewness check Histogram, QQ plot Check Normality assumption Class imbalance Bar plot, Pie chart Classification target imbalance Correlation Heatmap, Pairplot Check multicollinearity Missing values Heatmap, Bar plot Identify null patterns Time-series Line plot, Rolling Mean Temporal patterns"},{"location":"AIML/Statistics/#3-distribution-specific-plots-for-statistical-analysis","title":"\ud83d\udcc8 3. Distribution-Specific Plots (for Statistical Analysis)","text":"Distribution When Used Plot Normal Continuous features, model assumption Histogram + KDE, QQ Plot Binomial Binary outcomes Bar plot Poisson Event count in time/window PMF, Histogram Exponential Time until event Histogram, PDF Uniform Simulations Flat histogram Logistic Classification (0-1 outcome) Sigmoid curve Multinomial Categorical features Bar plot (for categories)"},{"location":"AIML/Statistics/#4-model-evaluation-plots","title":"\ud83d\udcca 4. Model Evaluation Plots","text":"Task Plot Use Case Classification Confusion Matrix, ROC Curve, PR Curve Evaluate classification performance Regression Residual plot, QQ plot, Predicted vs Actual Evaluate regression fit Model Selection Learning Curve, Validation Curve Diagnose overfitting/underfitting Feature Importance Bar plot, SHAP, Permutation Interpret model Clustering Elbow Plot, Silhouette Plot Choose number of clusters"},{"location":"AIML/Statistics/#5-advanced-ml-specific-visualization","title":"\ud83e\udde0 5. Advanced &amp; ML-specific Visualization","text":"Technique Visual Usage PCA / t-SNE / UMAP 2D/3D scatter plots Visualize high-dimensional data SHAP / LIME Force plots, Beeswarm plots Explain predictions Decision Trees Tree plot Interpret model rules Time-Series Forecasting Trend/Seasonality Decomposition Understand components"},{"location":"AIML/Statistics/#bonus-tools","title":"\ud83d\udee0\ufe0f Bonus Tools","text":"<ul> <li> <p>Seaborn: For statistical visualizations</p> </li> <li> <p>Matplotlib: Core plotting</p> </li> <li> <p>Plotly: Interactive plots</p> </li> <li> <p>Pandas Profiling / Sweetviz / D-Tale: Automated EDA tools</p> </li> </ul>"},{"location":"AIML/UniformDistribution/","title":"\ud83d\udcd0 What is a Uniform Distribution?","text":"<p>A Uniform Distribution is a probability distribution where all outcomes are equally likely within a certain interval.</p> <p>Used to describe probability where every event has equal chances of occuring.</p>"},{"location":"AIML/UniformDistribution/#types","title":"\u2705 Types:","text":"<ol> <li> <p>Discrete Uniform Distribution</p> <ul> <li>Limited number of distinct, equally likely outcomes.</li> <li>\ud83c\udfb2 Example: Rolling a fair die \u2192 1, 2, 3, 4, 5, 6.</li> </ul> </li> <li> <p>Continuous Uniform Distribution</p> <ul> <li>Infinite possible values within a continuous range.</li> <li>\ud83d\udccf Example: Random float from 0 to 1.</li> </ul> </li> </ol>"},{"location":"AIML/UniformDistribution/#formula-continuous","title":"\ud83d\udcca Formula (Continuous):","text":"<p>Where:  - a = lower bound  - b = upper bound  - Every value between a and b is equally likely</p> <p># \u2705 Real-World Use Cases in AI/ML  1. Weight Initialization in Neural Networks     - \u2699\ufe0f Frameworks like TensorFlow &amp; PyTorch often use uniform distribution to initialize weights.     - \ud83c\udfaf Goal: Avoid starting too high or too low \u2192 speeds up convergence.</p> <p>Example:</p> <pre><code>np.random.uniform(low=-0.05, high=0.05, size=(3, 3))\n</code></pre> <ol> <li> <p>Random Sampling / Bootstrapping</p> </li> <li> <p>\ud83d\udcca Uniform distribution is used to randomly select samples for:</p> <ul> <li>Cross-validation</li> <li>Bagging algorithms (like Random Forests)</li> <li>Data augmentation</li> </ul> </li> <li> <p>Hyperparameter Search</p> <ul> <li>\ud83c\udfaf Used in random search for tuning hyperparameters.</li> <li>For example, selecting learning rates uniformly between 0.001 and 0.1.</li> </ul> </li> </ol> <p>Example</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\na, b = 0, 10\ndata = np.random.uniform(a, b, 10000)\n\nplt.hist(data, bins=50, color='mediumseagreen', edgecolor='black', density=True)\nplt.title(f\"\ud83d\udcca Uniform Distribution (a={a}, b={b})\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Probability Density\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/aiml-overview/","title":"Aiml overview","text":""},{"location":"AIML/aiml-overview/#what-is-machine-learning","title":"What is Machine Learning","text":"<p>In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role of Machine Learning.</p> <p>Machine learning (ML) is a type of Artificial Intelligence (AI) that allows computers to learn and make decisions without being explicitly programmed. It involves feeding data into algorithms that can then identify patterns and make predictions on new data. Machine learning is used in a wide variety of applications, including image and speech recognition, natural language processing, and recommender systems.</p> <p></p> <p></p>"},{"location":"AIML/aiml-overview/#why-we-need-machine-learning","title":"Why we need Machine Learning?","text":"<p>Machine learning is able to learn, train from data and solve/predict complex solutions which cannot be done with traditional programming. It enables us with better decision making and solve complex business problems in optimized time. Machine learning has applications in various fields, like Healthcare, finance, educations, sports and more.</p> <ol> <li> <p>Solving Complex Business Problems: It is too complex to tackle problems like Image recognition, Natural language processing, disease diagnose etc. with Traditional programming. Machine learning can handle such problems by learning from examples or making predictions, rather than following some rigid rules.</p> </li> <li> <p>Handling Large Volumes of Data: Expansion of Internet and users is producing massive amount of data. Machine Learning can process these data effectively and analyze, predict useful insights from them.</p> <ul> <li>For example, ML can analyze millions of everyday transactions to detect any fraud activity in real time.</li> <li>Social platforms like Facebook, Instagram use ML to analyze billions of post, like and share to predict next recommendation in your feed.</li> </ul> </li> <li> <p>Automate Repetitive Tasks: With Machine Learning, we can automate time-consuming and repetitive tasks, with better accuracy.</p> <ul> <li>GMail uses ML to filter out Spam emails and ensure your Index stay clean and spam free. Using traditional programming or handling these manually will only make the system error-prone.</li> <li>Customer Support chatbots can use ML to solve frequent occurring problems like Checking order status, Password reset etc.</li> <li>Big organizations can use ML to process large amount of data (like Invoices etc) to extract historical and current key insights.</li> </ul> </li> <li> <p>Personalized User Experience: All social-media, OTT and E-commerce platforms uses Machine learning to recommend better feed based on user preference or interest.</p> <ul> <li>Netflix recommends movies and TV shows based on what you\u2019ve watched</li> <li>E-commerce platforms suggesting products you are likely to buy.</li> </ul> </li> <li> <p>Self Improvement in Performance: ML models are able to improve themselves based on more data, like user-behavior and feedback. For example,</p> <ul> <li>Voice Assistants (Siri, Alexa, Google Assistant) \u2013 Voice assistants continuously improve as they process millions of voice inputs. They adapt to user preferences, understand regional accents better, and handle ambiguous queries more effectively.</li> <li>Search Engines (Google, Bing) \u2013 Search engines analyze user behavior to refine their ranking algorithms.</li> <li>Self-driving Cars \u2013 Self-driving cars use data from millions of miles driven (both in simulations and real-world scenarios) to enhance their decision-making.</li> </ul> </li> </ol>"},{"location":"AIML/aiml-overview/#introduction-to-machine-learning","title":"Introduction to Machine Learning","text":"<ul> <li>A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. </li> <li>Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data.</li> <li>For the purpose of developing predictive models, machine learning brings together statistics and computer science. </li> </ul>"},{"location":"AIML/aiml-overview/#classification-of-machine-learning","title":"Classification of Machine Learning","text":""},{"location":"AIML/aiml-overview/#types-of-machine-learning","title":"Types of Machine Learning","text":"<p>Machine learning can be broadly categorized into three types:</p> <ol> <li> <p>Supervised learning: Trains models on labeled data to predict or classify new, unseen data.</p> </li> <li> <p>Unsupervised learning: Finds patterns or groups in unlabeled data, like clustering or dimensionality reduction.</p> </li> <li> <p>Reinforcement learning: Learns through trial and error to maximize rewards, ideal for decision-making tasks.</p> </li> </ol> <p></p>"},{"location":"AIML/aiml-overview/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<p>Machine learning is fundamentally built upon data, which serves as the foundation for training and testing models. Data consists of inputs (features) and outputs (labels). A model learns patterns during training and is tested on unseen data to evaluate its performance and generalization. In order to make predictions, there are essential steps through which data passes in order to produce a machine learning model that can make predictions.</p> <ol> <li>ML workflow</li> <li>Data Cleaning</li> <li>Feature Scaling</li> <li>Data Preprocessing in Python</li> </ol>"},{"location":"AIML/AgenticAI/crewai/","title":"Introduction","text":"<p>Build AI agent teams that work together to tackle complex tasks</p>"},{"location":"AIML/AgenticAI/crewai/#what-is-crewai","title":"What is CrewAI?","text":"<p>CrewAI is a cutting-edge framework for orchestrating autonomous AI agents.</p> <p>CrewAI enables you to create AI teams where each agent has specific roles, tools, and goals, working together to accomplish complex tasks.</p> <p>Think of it as assembling your dream team - each member (agent) brings unique skills and expertise, collaborating seamlessly to achieve your objectives.</p>"},{"location":"AIML/AgenticAI/crewai/#how-crewai-works","title":"How CrewAI Works","text":"<pre><code>Just like a company has departments (Sales, Engineering, Marketing) working together under leadership to achieve business goals, CrewAI helps you create an organization of AI agents with specialized roles collaborating to accomplish complex tasks.\n</code></pre> Component Description Key Features Crew The top-level organization \u2022 Manages AI agent teams  \u2022 Oversees workflows  \u2022 Ensures collaboration  \u2022 Delivers outcomes AI Agents Specialized team members \u2022 Have specific roles (researcher, writer)  \u2022 Use designated tools  \u2022 Can delegate tasks  \u2022 Make autonomous decisions Process Workflow management system \u2022 Defines collaboration patterns  \u2022 Controls task assignments  \u2022 Manages interactions  \u2022 Ensures efficient execution Tasks Individual assignments \u2022 Have clear objectives  \u2022 Use specific tools  \u2022 Feed into larger process  \u2022 Produce actionable results"},{"location":"AIML/AgenticAI/crewai/#how-it-all-works-together","title":"How It All Works Together","text":"<ol> <li>The Crew organizes the overall operation</li> <li>AI Agents work on their specialized tasks</li> <li>The Process ensures smooth collaboration</li> <li>Tasks get completed to achieve the goal</li> </ol>"},{"location":"AIML/AgenticAI/crewai/#key-features","title":"Key Features","text":"<ol> <li>Role-Based Agents: Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers</li> <li>Flexible Tools: Equip agents with custom tools and APIs to interact with external services and data sources</li> <li>Intelligent Collaboration: Agents work together, sharing insights and coordinating tasks to achieve complex objectives</li> <li>Task Management: Define sequential or parallel workflows, with agents automatically handling task dependencies</li> </ol>"},{"location":"AIML/AgenticAI/crewai/#why-choose-crewai","title":"Why Choose CrewAI?","text":"<ul> <li>Autonomous Operation: Agents make intelligent decisions based on their roles and available tools</li> <li>Natural Interaction: Agents communicate and collaborate like human team members</li> <li>Extensible Design: Easy to add new tools, roles, and capabilities</li> <li>Production Ready: Built for reliability and scalability in real-world applications</li> </ul>"},{"location":"AIML/AgenticAI/crewai/#crewai-examples","title":"CrewAI Examples","text":"<p>A collection of examples that show how to use CrewAI framework to automate workflows.</p> <ol> <li>recruitment</li> </ol>"},{"location":"AIML/AgenticAI/crewai/#ai-crew-for-recruitment","title":"AI Crew for Recruitment","text":""},{"location":"AIML/AgenticAI/crewai/#introduction_1","title":"Introduction","text":"<p>This project demonstrates the use of the CrewAI framework to automate the recruitment process. CrewAI orchestrates autonomous AI agents, enabling them to collaborate and execute complex tasks efficiently.</p>"},{"location":"AIML/AgenticAI/crewai/#crewai-framework","title":"CrewAI Framework","text":"<p>CrewAI is designed to facilitate the collaboration of role-playing AI agents. In this example, these agents work together to streamline the recruitment process, ensuring the best fit between candidates and job roles.</p>"},{"location":"AIML/AgenticAI/crewai/#running-the-script","title":"Running the Script","text":"<pre><code>It uses GPT-4o by default so you should have access to that to run it.\n</code></pre> <p>Disclaimer: This will use gpt-4o unless you change it to use a different model, and by doing so it may incur different costs.</p> <ul> <li>Configure Environment: Copy .env.example and set up the environment variables for OpenAI and other tools as needed.</li> </ul> <p>.env</p> <pre><code>OPENAI_API_KEY=Your openai key\nSERPER_API_KEY=Your serper key\nLINKEDIN_COOKIE=Your linkedin cookie\n</code></pre> <ul> <li>Install Dependencies: Run poetry lock &amp;&amp; poetry install</li> <li>Customize: Modify src/recruitment/main.py to add custom inputs for your agents and tasks.</li> <li>Customize Further: Check src/recruitment/config/agents.yaml to update your agents and src/recruitment/config/tasks.yaml to update your tasks.</li> <li>Custom Tools: You can find custom tools at recruitment/src/recruitment/tools/</li> <li>Execute the Script: Run poetry run recruitment and input your project details.</li> </ul>"},{"location":"AIML/AgenticAI/crewai/#details-explanation","title":"Details &amp; Explanation","text":"<ul> <li>Running the Script: Execute poetry run recruitment. The script will leverage the CrewAI framework to automate recruitment tasks and generate a detailed report.</li> <li>Running Training: Execute poetry run train n where n is the number of training iterations.</li> <li>Key Components:<ul> <li><code>src/recruitment/main.py:</code> Main script file.</li> <li><code>src/recruitment/crew.py:</code> Main crew file where agents and tasks come together, and the main logic is executed.</li> <li><code>src/recruitment/config/agents.yaml:</code> Configuration file for defining agents.</li> <li><code>src/recruitment/config/tasks.yaml:</code> Configuration file for defining tasks.</li> <li><code>src/recruitment/tools:</code> Contains tool classes used by the agents.</li> </ul> </li> </ul> <p>config/agents.yaml:</p> <pre><code>researcher:\n  role: &gt;\n    Job Candidate Researcher\n  goal: &gt;\n    Find potential candidates for the job\n  backstory: &gt;\n    You are adept at finding the right candidates by exploring various online\n    resources. Your skill in identifying suitable candidates ensures the best\n    match for job positions.\n\nmatcher:\n  role: &gt;\n    Candidate Matcher and Scorer\n  goal: &gt;\n    Match the candidates to the best jobs and score them\n  backstory: &gt;\n    You have a knack for matching the right candidates to the right job positions\n    using advanced algorithms and scoring techniques. Your scores help\n    prioritize the best candidates for outreach.\n\n\ncommunicator:\n  role: &gt;\n    Candidate Outreach Strategist\n  goal: &gt;\n    Develop outreach strategies for the selected candidates\n  backstory: &gt;\n    You are skilled at creating effective outreach strategies and templates to\n    engage candidates. Your communication tactics ensure high response rates\n    from potential candidates.\n\nreporter:\n  role: &gt;\n    Candidate Reporting Specialist\n  goal: &gt;\n    Report the best candidates to the recruiters\n  backstory: &gt;\n    You are proficient at compiling and presenting detailed reports for recruiters.\n    Your reports provide clear insights into the best candidates to pursue.\n</code></pre> <p>config/tasks.yaml</p> <pre><code>research_candidates_task:\n  description: &gt;\n    Conduct thorough research to find potential candidates for the specified job.\n    Utilize various online resources and databases to gather a comprehensive list of potential candidates.\n    Ensure that the candidates meet the job requirements provided.\n\n    Job Requirements:\n    {job_requirements}\n  expected_output: &gt;\n    A list of 10 potential candidates with their contact information and brief profiles highlighting their suitability.\n\nmatch_and_score_candidates_task:\n  description: &gt;\n    Evaluate and match the candidates to the best job positions based on their qualifications and suitability.\n    Score each candidate to reflect their alignment with the job requirements, ensuring a fair and transparent assessment process.\n    Don't try to scrape people's linkedin, since you don't have access to it.\n\n    Job Requirements:\n    {job_requirements}\n  expected_output: &gt;\n    A ranked list of candidates with detailed scores and justifications for each job position.\n\noutreach_strategy_task:\n  description: &gt;\n    Develop a comprehensive strategy to reach out to the selected candidates.\n    Create effective outreach methods and templates that can engage the candidates and encourage them to consider the job opportunity.\n\n    Job Requirements:\n    {job_requirements}\n  expected_output: &gt;\n    A detailed list of outreach methods and templates ready for implementation, including communication strategies and engagement tactics.\n\nreport_candidates_task:\n  description: &gt;\n    Compile a comprehensive report for recruiters on the best candidates to put forward.\n    Summarize the findings from the previous tasks and provide clear recommendations based on the job requirements.\n  expected_output: &gt;\n    A detailed report with the best candidates to pursue, no need to include the job requirements formatted as markdown without '```', including profiles, scores, and outreach strategies.\n</code></pre> <p>tools/client.py</p> <pre><code>import os\nimport urllib\nfrom selenium.webdriver.common.by import By\n\nfrom .driver import Driver\n\nclass Client:\n  def __init__(self):\n    url = 'https://linkedin.com/'\n    cookie = {\n      \"name\": \"li_at\",\n      \"value\": os.environ[\"LINKEDIN_COOKIE\"],\n      \"domain\": \".linkedin.com\"\n    }\n\n    self.driver = Driver(url, cookie)\n\n  def find_people(self, skills):\n    skills = skills.split(\",\")\n    search = \" \".join(skills)\n    encoded_string = urllib.parse.quote(search.lower())\n    url = f\"https://www.linkedin.com/search/results/people/?keywords={encoded_string}\"\n    self.driver.navigate(url)\n\n    people = self.driver.get_elements(\"ul li div div.linked-area\")\n\n    results = []\n    for person in people:\n      try:\n        result = {}\n        result[\"name\"] = person.find_element(By.CSS_SELECTOR, \"span.entity-result__title-line\").text\n        result[\"position\"] = person.find_element(By.CSS_SELECTOR, \"div.entity-result__primary-subtitle\").text\n        result[\"location\"] = person.find_element(By.CSS_SELECTOR, \"div.entity-result__secondary-subtitle\").text\n        result[\"profile_link\"] = person.find_element(By.CSS_SELECTOR, \"a.app-aware-link\").get_attribute(\"href\")\n      except Exception as e:\n        print(e)\n        continue\n      results.append(result)\n    return results\n\n  def close(self):\n    self.driver.close()\n</code></pre> <p>tools/driver.py</p> <pre><code>import time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.firefox.options import Options\n\nclass Driver:\n    def __init__(self, url, cookie=None):\n        self.driver = self._create_driver(url, cookie)\n\n    def navigate(self, url, wait=3):\n        self.driver.get(url)\n        time.sleep(wait)\n\n    def scroll_to_bottom(self, wait=3):\n        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(wait)\n        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(wait)\n\n    def get_element(self, selector):\n        return self.driver.find_element(By.CSS_SELECTOR, selector)\n\n    def get_elements(self, selector):\n        return self.driver.find_elements(By.CSS_SELECTOR, selector)\n\n    def fill_text_field(self, selector, text):\n        element = self.get_element(selector)\n        element.clear()\n        element.send_keys(text)\n\n    def click_button(self, selector):\n        element = self.get_element(selector)\n        element.click()\n\n    def _create_driver(self, url, cookie):\n        options = Options()\n        # options.add_argument(\"--headless\")\n        driver = webdriver.Firefox(options=options)\n        driver.get(url)\n        if cookie:\n            driver.add_cookie(cookie)\n        return driver\n\n    def close(self):\n        self.driver.close()\n</code></pre> <p>tools/linkedin.py</p> <pre><code>from crewai_tools import BaseTool\n\nfrom .client import Client as LinkedinClient\n\n\nclass LinkedInTool(BaseTool):\n    name: str = \"Retrieve LinkedIn profiles\"\n    description: str = (\n        \"Retrieve LinkedIn profiles given a list of skills. Comma separated\"\n    )\n\n    def _run(self, skills: str) -&gt; str:\n        linkedin_client = LinkedinClient()\n        people = linkedin_client.find_people(skills)\n        people = self._format_publications_to_text(people)\n        linkedin_client.close()\n        return people\n\n    def _format_publications_to_text(self, people):\n        result = [\"\\n\".join([\n            \"Person Profile\",\n            \"-------------\",\n            p['name'],\n            p['position'],\n            p['location'],\n            p[\"profile_link\"],\n        ]) for p in people]\n        result = \"\\n\\n\".join(result)\n\n        return result\n</code></pre> <p>crew.py</p> <pre><code>from crewai import Agent, Crew, Process, Task\nfrom crewai.project import CrewBase, agent, crew, task\nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool\nfrom recruitment.tools.linkedin import LinkedInTool\n\n@CrewBase\nclass RecruitmentCrew():\n    \"\"\"Recruitment crew\"\"\"\n    agents_config = 'config/agents.yaml'\n    tasks_config = 'config/tasks.yaml'\n\n    @agent\n    def researcher(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['researcher'],\n                        tools=[SerperDevTool(), ScrapeWebsiteTool(), LinkedInTool()],\n            allow_delegation=False,\n                        verbose=True\n        )\n\n    @agent\n    def matcher(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['matcher'],\n            tools=[SerperDevTool(), ScrapeWebsiteTool()],\n            allow_delegation=False,\n                        verbose=True\n        )\n\n    @agent\n    def communicator(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['communicator'],\n            tools=[SerperDevTool(), ScrapeWebsiteTool()],\n            allow_delegation=False,\n                        verbose=True\n        )\n\n    @agent\n    def reporter(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['reporter'],\n            allow_delegation=False,\n                        verbose=True\n        )\n\n    @task\n    def research_candidates_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['research_candidates_task'],\n            agent=self.researcher()\n        )\n\n    @task\n    def match_and_score_candidates_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['match_and_score_candidates_task'],\n            agent=self.matcher()\n        )\n\n    @task\n    def outreach_strategy_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['outreach_strategy_task'],\n            agent=self.communicator()\n        )\n\n    @task\n    def report_candidates_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['report_candidates_task'],\n            agent=self.reporter(),\n            context=[self.research_candidates_task(), self.match_and_score_candidates_task(), self.outreach_strategy_task()],\n        )\n\n    @crew\n    def crew(self) -&gt; Crew:\n        \"\"\"Creates the Recruitment crew\"\"\"\n        return Crew(\n            agents=self.agents,\n            tasks=self.tasks,\n            process=Process.sequential,\n            verbose=2,\n        )\n</code></pre> <p>main.py</p> <pre><code>#!/usr/bin/env python\nimport sys\nfrom recruitment.crew import RecruitmentCrew\n\n\ndef run():\n    # Replace with your inputs, it will automatically interpolate any tasks and agents information\n    inputs = {\n        'job_requirements': \"\"\"\n        job_requirement:\n  title: &gt;\n    Ruby on Rails and React Engineer\n  description: &gt;\n    We are seeking a skilled Ruby on Rails and React engineer to join our team.\n    The ideal candidate will have experience in both backend and frontend development,\n    with a passion for building high-quality web applications.\n\n  responsibilities: &gt;\n    - Develop and maintain web applications using Ruby on Rails and React.\n    - Collaborate with teams to define and implement new features.\n    - Write clean, maintainable, and efficient code.\n    - Ensure application performance and responsiveness.\n    - Identify and resolve bottlenecks and bugs.\n\n  requirements: &gt;\n    - Proven experience with Ruby on Rails and React.\n    - Strong understanding of object-oriented programming.\n    - Proficiency with JavaScript, HTML, CSS, and React.\n    - Experience with SQL or NoSQL databases.\n    - Familiarity with code versioning tools, such as Git.\n\n  preferred_qualifications: &gt;\n    - Experience with cloud services (AWS, Google Cloud, or Azure).\n    - Familiarity with Docker and Kubernetes.\n    - Knowledge of GraphQL.\n    - Bachelor's degree in Computer Science or a related field.\n\n  perks_and_benefits: &gt;\n    - Competitive salary and bonuses.\n    - Health, dental, and vision insurance.\n    - Flexible working hours and remote work options.\n    - Professional development opportunities.\n        \"\"\"\n    }\n    RecruitmentCrew().crew().kickoff(inputs=inputs)\n\ndef train():\n    \"\"\"\n    Train the crew for a given number of iterations.\n    \"\"\"\n    inputs = {\n        'job_requirements': \"\"\"\n        job_requirement:\n  title: &gt;\n    Ruby on Rails and React Engineer\n  description: &gt;\n    We are seeking a skilled Ruby on Rails and React engineer to join our team.\n    The ideal candidate will have experience in both backend and frontend development,\n    with a passion for building high-quality web applications.\n\n  responsibilities: &gt;\n    - Develop and maintain web applications using Ruby on Rails and React.\n    - Collaborate with teams to define and implement new features.\n    - Write clean, maintainable, and efficient code.\n    - Ensure application performance and responsiveness.\n    - Identify and resolve bottlenecks and bugs.\n\n  requirements: &gt;\n    - Proven experience with Ruby on Rails and React.\n    - Strong understanding of object-oriented programming.\n    - Proficiency with JavaScript, HTML, CSS, and React.\n    - Experience with SQL or NoSQL databases.\n    - Familiarity with code versioning tools, such as Git.\n\n  preferred_qualifications: &gt;\n    - Experience with cloud services (AWS, Google Cloud, or Azure).\n    - Familiarity with Docker and Kubernetes.\n    - Knowledge of GraphQL.\n    - Bachelor's degree in Computer Science or a related field.\n\n  perks_and_benefits: &gt;\n    - Competitive salary and bonuses.\n    - Health, dental, and vision insurance.\n    - Flexible working hours and remote work options.\n    - Professional development opportunities.\n        \"\"\"\n    }\n    try:\n        RecruitmentCrew().crew().train(n_iterations=int(sys.argv[1]), inputs=inputs)\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while training the crew: {e}\")\n</code></pre> <ol> <li>Create your crew:</li> </ol> <p>Create a new crew project by running the following command in your terminal. This will create a new directory called <code>recruitment</code> with the basic structure for your crew.</p> <pre><code>pip install crewai\n</code></pre> <pre><code>(agent-ai-venv) ganeshkinkargiri.@M7QJY5-A67EFC4A Agentic-AI % crewai create crew recruitment\nCreating folder recruitment...\nCache expired or not found. Fetching provider data from the web...\nDownloading  [####################################]  349185/16798\nSelect a provider to set up:\n1. openai\n2. anthropic\n3. gemini\n4. nvidia_nim\n5. groq\n6. ollama\n7. watson\n8. bedrock\n9. azure\n10. cerebras\n11. sambanova\n12. other\nq. Quit\nEnter the number of your choice or 'q' to quit: 6     \nSelect a model to use for Ollama:\n1. ollama/llama3.1\n2. ollama/mixtral\nq. Quit\nEnter the number of your choice or 'q' to quit: 1\nAPI keys and model saved to .env file\nSelected model: ollama/llama3.1\n  - Created recruitment/.gitignore\n  - Created recruitment/pyproject.toml\n  - Created recruitment/README.md\n  - Created recruitment/knowledge/user_preference.txt\n  - Created recruitment/src/recruitment/__init__.py\n  - Created recruitment/src/recruitment/main.py\n  - Created recruitment/src/recruitment/crew.py\n  - Created recruitment/src/recruitment/tools/custom_tool.py\n  - Created recruitment/src/recruitment/tools/__init__.py\n  - Created recruitment/src/recruitment/config/agents.yaml\n  - Created recruitment/src/recruitment/config/tasks.yaml\nCrew recruitment created successfully!\n(agent-ai-venv) ganeshkinkargiri.@M7QJY5-A67EFC4A Agentic-AI % \n</code></pre> <ol> <li>Navigate to your new crew project:</li> </ol> <pre><code>cd recruitment\n</code></pre> <ul> <li>Modify your <code>agents.yaml</code> file</li> <li>Modify your <code>tasks.yaml</code> file</li> <li> <p>Modify your <code>crew.py</code> file</p> </li> <li> <p>Run your crew:</p> </li> </ul> <pre><code>/Users/ganeshkinkargiri./.local/bin/poetry install \n\n/Users/ganeshkinkargiri./.local/bin/poetry run recruitment . \n\nstreamlit run recruitment/main.py\n</code></pre>"},{"location":"AIML/AgenticAI/crewai/#tools","title":"Tools","text":"<ol> <li> <p>SerperDevTool: ```https://serper.dev/ The World's Fastest &amp; Cheapest Google Search API</p> </li> <li> <p>ScrapeWebsiteTool:  The ScrapeWebsiteTool is designed to extract and read the content of a specified website. A tool designed to extract and read the content of a specified website. It is capable of handling various types of web pages by making HTTP requests and parsing the received HTML content. This tool can be particularly useful for web scraping tasks, data collection, or extracting specific information from websites.</p> </li> <li> <p>AI Mind Tool: The AIMindTool is designed to query data sources in natural language. The AIMindTool is a wrapper around AI-Minds provided by MindsDB. It allows you to query data sources in natural language by simply configuring their connection parameters. This tool is useful when you need answers to questions from your data stored in various data sources including PostgreSQL, MySQL, MariaDB, ClickHouse, Snowflake, and Google BigQuery.</p> </li> </ol> <p>Minds are AI systems that work similarly to large language models (LLMs) but go beyond by answering any question from any data.</p> <p>Example: <pre><code>from crewai_tools import AIMindTool\n\n# Initialize the AIMindTool\naimind_tool = AIMindTool(\n    datasources=[\n        {\n            \"description\": \"house sales data\",\n            \"engine\": \"postgres\",\n            \"connection_data\": {\n                \"user\": \"demo_user\",\n                \"password\": \"demo_password\",\n                \"host\": \"samples.mindsdb.com\",\n                \"port\": 5432,\n                \"database\": \"demo\",\n                \"schema\": \"demo_data\"\n            },\n            \"tables\": [\"house_sales\"]\n        }\n    ]\n)\n\n# Run a natural language query\nresult = aimind_tool.run(\"How many 3 bedroom houses were sold in 2008?\")\nprint(result)\n</code></pre></p> <ol> <li>Brave Search: The BraveSearchTool is designed to search the internet using the Brave Search API.</li> </ol> <p>This tool is designed to perform web searches using the Brave Search API. It allows you to search the internet with a specified query and retrieve relevant results. The tool supports customizable result counts and country-specific searches.</p> <p>Example: <pre><code>from crewai_tools import BraveSearchTool\n\n# Initialize the tool for internet searching capabilities\ntool = BraveSearchTool()\n\n# Execute a search\nresults = tool.run(search_query=\"CrewAI agent framework\")\nprint(results)\n</code></pre></p> <ol> <li>Browserbase Web Loader: Browserbase is a developer platform to reliably run, manage, and monitor headless browsers.</li> </ol> <p>Example: <pre><code>from crewai_tools import BrowserbaseLoadTool\n\n# Initialize the tool with the Browserbase API key and Project ID\ntool = BrowserbaseLoadTool()\n</code></pre></p> <ol> <li>Code Docs RAG Search: The CodeDocsSearchTool is a powerful RAG (Retrieval-Augmented Generation) tool designed for semantic searches within code documentation.</li> </ol> <p>The CodeDocsSearchTool is a powerful RAG (Retrieval-Augmented Generation) tool designed for semantic searches within code documentation. It enables users to efficiently find specific information or topics within code documentation. By providing a docs_url during initialization, the tool narrows down the search to that particular documentation site. Alternatively, without a specific docs_url, it searches across a wide array of code documentation known or discovered throughout its execution, making it versatile for various documentation search needs.</p> <p>Example: <pre><code>from crewai_tools import CodeDocsSearchTool\n\n# To search any code documentation content \n# if the URL is known or discovered during its execution:\ntool = CodeDocsSearchTool()\n\n# OR\n\n# To specifically focus your search on a given documentation site \n# by providing its URL:\ntool = CodeDocsSearchTool(docs_url='https://docs.example.com/reference')\n</code></pre></p> <p>Custom model and embeddings: <pre><code>tool = CodeDocsSearchTool(\n    config=dict(\n        llm=dict(\n            provider=\"ollama\", # or google, openai, anthropic, llama2, ...\n            config=dict(\n                model=\"llama2\",\n                # temperature=0.5,\n                # top_p=1,\n                # stream=true,\n            ),\n        ),\n        embedder=dict(\n            provider=\"google\", # or openai, ollama, ...\n            config=dict(\n                model=\"models/embedding-001\",\n                task_type=\"retrieval_document\",\n                # title=\"Embeddings\",\n            ),\n        ),\n    )\n)\n</code></pre></p> <ol> <li>Code Interpreter: The CodeInterpreterTool is a powerful tool designed for executing Python 3 code within a secure, isolated environment.</li> </ol> <p>The CodeInterpreterTool enables CrewAI agents to execute Python 3 code that they generate autonomously. The code is run in a secure, isolated Docker container, ensuring safety regardless of the content. This functionality is particularly valuable as it allows agents to create code, execute it, obtain the results, and utilize that information to inform subsequent decisions and actions.</p> <p>Example: <pre><code>from crewai import Agent, Task, Crew, Process\nfrom crewai_tools import CodeInterpreterTool\n\n# Initialize the tool\ncode_interpreter = CodeInterpreterTool()\n\n# Define an agent that uses the tool\nprogrammer_agent = Agent(\n    role=\"Python Programmer\",\n    goal=\"Write and execute Python code to solve problems\",\n    backstory=\"An expert Python programmer who can write efficient code to solve complex problems.\",\n    tools=[code_interpreter],\n    verbose=True,\n)\n\n# Example task to generate and execute code\ncoding_task = Task(\n    description=\"Write a Python function to calculate the Fibonacci sequence up to the 10th number and print the result.\",\n    expected_output=\"The Fibonacci sequence up to the 10th number.\",\n    agent=programmer_agent,\n)\n\n# Create and run the crew\ncrew = Crew(\n    agents=[programmer_agent],\n    tasks=[coding_task],\n    verbose=True,\n    process=Process.sequential,\n)\nresult = crew.kickoff()\n</code></pre></p> <p>You can also enable code execution directly when creating an agent:</p> <pre><code>from crewai import Agent\n\n# Create an agent with code execution enabled\nprogrammer_agent = Agent(\n    role=\"Python Programmer\",\n    goal=\"Write and execute Python code to solve problems\",\n    backstory=\"An expert Python programmer who can write efficient code to solve complex problems.\",\n    allow_code_execution=True,  # This automatically adds the CodeInterpreterTool\n    verbose=True,\n)\n</code></pre> <p>Agent Integration Example: Here\u2019s a more detailed example of how to integrate the CodeInterpreterTool with a CrewAI agent:</p> <pre><code>from crewai import Agent, Task, Crew\nfrom crewai_tools import CodeInterpreterTool\n\n# Initialize the tool\ncode_interpreter = CodeInterpreterTool()\n\n# Define an agent that uses the tool\ndata_analyst = Agent(\n    role=\"Data Analyst\",\n    goal=\"Analyze data using Python code\",\n    backstory=\"\"\"You are an expert data analyst who specializes in using Python \n    to analyze and visualize data. You can write efficient code to process \n    large datasets and extract meaningful insights.\"\"\",\n    tools=[code_interpreter],\n    verbose=True,\n)\n\n# Create a task for the agent\nanalysis_task = Task(\n    description=\"\"\"\n    Write Python code to:\n    1. Generate a random dataset of 100 points with x and y coordinates\n    2. Calculate the correlation coefficient between x and y\n    3. Create a scatter plot of the data\n    4. Print the correlation coefficient and save the plot as 'scatter.png'\n\n    Make sure to handle any necessary imports and print the results.\n    \"\"\",\n    expected_output=\"The correlation coefficient and confirmation that the scatter plot has been saved.\",\n    agent=data_analyst,\n)\n\n# Run the task\ncrew = Crew(\n    agents=[data_analyst],\n    tasks=[analysis_task],\n    verbose=True,\n    process=Process.sequential,\n)\nresult = crew.kickoff()\n</code></pre> <ol> <li> <p>Composio Tool: Composio provides 250+ production-ready tools for AI agents with flexible authentication management. Composio is an integration platform that allows you to connect your AI agents to 250+ tools. Key features include:</p> </li> <li> <p>Enterprise-Grade Authentication: Built-in support for OAuth, API Keys, JWT with automatic token refresh</p> </li> <li>Full Observability: Detailed tool usage logs, execution timestamps, and more</li> </ol> <p>Example:</p> <pre><code>from composio_crewai import ComposioToolSet, App, Action\nfrom crewai import Agent, Task, Crew\n\ntoolset = ComposioToolSet()\n</code></pre> <p>Connect your GitHub account</p> <pre><code>request = toolset.initiate_connection(app=App.GITHUB)\nprint(f\"Open this URL to authenticate: {request.redirectUrl}\")\n</code></pre> <p>Get Tools</p> <ul> <li>Retrieving all the tools from an app (not recommended for production):</li> </ul> <pre><code>tools = toolset.get_tools(apps=[App.GITHUB])\n</code></pre> <ul> <li>Filtering tools based on tags:</li> </ul> <pre><code>tag = \"users\"\n\nfiltered_action_enums = toolset.find_actions_by_tags(\n    App.GITHUB,\n    tags=[tag], \n)\n\ntools = toolset.get_tools(actions=filtered_action_enums)\n</code></pre> <ul> <li>Filtering tools based on use case:</li> </ul> <pre><code>use_case = \"Star a repository on GitHub\"\n\nfiltered_action_enums = toolset.find_actions_by_use_case(\n    App.GITHUB, use_case=use_case, advanced=False\n)\n\ntools = toolset.get_tools(actions=filtered_action_enums)\n</code></pre> <p>Using specific tools:</p> <p>In this demo, we will use the GITHUB_STAR_A_REPOSITORY_FOR_THE_AUTHENTICATED_USER action from the GitHub app.</p> <pre><code>tools = toolset.get_tools(\n    actions=[Action.GITHUB_STAR_A_REPOSITORY_FOR_THE_AUTHENTICATED_USER]\n)\n</code></pre> <p>Define agent:</p> <pre><code>crewai_agent = Agent(\n    role=\"GitHub Agent\",\n    goal=\"You take action on GitHub using GitHub APIs\",\n    backstory=\"You are AI agent that is responsible for taking actions on GitHub on behalf of users using GitHub APIs\",\n    verbose=True,\n    tools=tools,\n    llm= # pass an llm\n)\n</code></pre> <p>Execute task</p> <pre><code>task = Task(\n    description=\"Star a repo composiohq/composio on GitHub\",\n    agent=crewai_agent,\n    expected_output=\"Status of the operation\",\n)\n\ncrew = Crew(agents=[crewai_agent], tasks=[task])\n\ncrew.kickoff()\n</code></pre> <ol> <li>CSV RAG Search: The CSVSearchTool is a powerful RAG (Retrieval-Augmented Generation) tool designed for semantic searches within a CSV file\u2019s content.</li> </ol> <p>\u200b This tool is used to perform a RAG (Retrieval-Augmented Generation) search within a CSV file\u2019s content. It allows users to semantically search for queries in the content of a specified CSV file. This feature is particularly useful for extracting information from large CSV datasets where traditional search methods might be inefficient. All tools with \u201cSearch\u201d in their name, including CSVSearchTool, are RAG tools designed for searching different sources of data.</p> <p>Example: <pre><code>from crewai_tools import CSVSearchTool\n\n# Initialize the tool with a specific CSV file. \n# This setup allows the agent to only search the given CSV file.\ntool = CSVSearchTool(csv='path/to/your/csvfile.csv')\n\n# OR\n\n# Initialize the tool without a specific CSV file. \n# Agent will need to provide the CSV path at runtime.\ntool = CSVSearchTool()\n</code></pre></p> <p>Custom model and embeddings By default, the tool uses OpenAI for both embeddings and summarization. To customize the model, you can use a config dictionary as follows:</p> <pre><code>tool = CSVSearchTool(\n    config=dict(\n        llm=dict(\n            provider=\"ollama\", # or google, openai, anthropic, llama2, ...\n            config=dict(\n                model=\"llama2\",\n                # temperature=0.5,\n                # top_p=1,\n                # stream=true,\n            ),\n        ),\n        embedder=dict(\n            provider=\"google\", # or openai, ollama, ...\n            config=dict(\n                model=\"models/embedding-001\",\n                task_type=\"retrieval_document\",\n                # title=\"Embeddings\",\n            ),\n        ),\n    )\n)\n</code></pre> <ol> <li>DALL-E Tool:</li> </ol> <p>The DallETool is a powerful tool designed for generating images from textual descriptions.</p> <p>This tool is used to give the Agent the ability to generate images using the DALL-E model. It is a transformer-based model that generates images from textual descriptions. This tool allows the Agent to generate images based on the text input provided by the user.</p> <p>Example <pre><code>from crewai_tools import DallETool\n\nAgent(\n    ...\n    tools=[DallETool()],\n)\n</code></pre></p> <p>If needed you can also tweak the parameters of the DALL-E model by passing them as arguments to the DallETool class. For example:</p> <pre><code>from crewai_tools import DallETool\n\ndalle_tool = DallETool(model=\"dall-e-3\",\n                       size=\"1024x1024\",\n                       quality=\"standard\",\n                       n=1)\n\nAgent(\n    ...\n    tools=[dalle_tool]\n)\n</code></pre> <ol> <li>Directory RAG Search:</li> </ol> <p>The DirectorySearchTool is a powerful RAG (Retrieval-Augmented Generation) tool designed for semantic searches within a directory\u2019s content.</p> <p>The DirectorySearchTool enables semantic search within the content of specified directories, leveraging the Retrieval-Augmented Generation (RAG) methodology for efficient navigation through files. Designed for flexibility, it allows users to dynamically specify search directories at runtime or set a fixed directory during initial setup.</p> <p>\u200b Initialization and Usage Import the DirectorySearchTool from the crewai_tools package to start. You can initialize the tool without specifying a directory, enabling the setting of the search directory at runtime. Alternatively, the tool can be initialized with a predefined directory.</p> <pre><code>from crewai_tools import DirectorySearchTool\n\n# For dynamic directory specification at runtime\ntool = DirectorySearchTool()\n\n# For fixed directory searches\ntool = DirectorySearchTool(directory='/path/to/directory')\n</code></pre> <p>Custom Model and Embeddings</p> <p>The DirectorySearchTool uses OpenAI for embeddings and summarization by default. Customization options for these settings include changing the model provider and configuration, enhancing flexibility for advanced users.</p> <pre><code>tool = DirectorySearchTool(\n    config=dict(\n        llm=dict(\n            provider=\"ollama\", # Options include ollama, google, anthropic, llama2, and more\n            config=dict(\n                model=\"llama2\",\n                # Additional configurations here\n            ),\n        ),\n        embedder=dict(\n            provider=\"google\", # or openai, ollama, ...\n            config=dict(\n                model=\"models/embedding-001\",\n                task_type=\"retrieval_document\",\n                # title=\"Embeddings\",\n            ),\n        ),\n    )\n)\n</code></pre> <ol> <li>Directory Read: The DirectoryReadTool is a powerful utility designed to provide a comprehensive listing of directory contents.</li> </ol> <p>The DirectoryReadTool is a powerful utility designed to provide a comprehensive listing of directory contents. It can recursively navigate through the specified directory, offering users a detailed enumeration of all files, including those within subdirectories. This tool is crucial for tasks that require a thorough inventory of directory structures or for validating the organization of files within directories.</p> <ol> <li>DOCX RAG Search:</li> </ol> <p>The DOCXSearchTool is a RAG tool designed for semantic searching within DOCX documents.</p> <p>The DOCXSearchTool is a RAG tool designed for semantic searching within DOCX documents. It enables users to effectively search and extract relevant information from DOCX files using query-based searches. This tool is invaluable for data analysis, information management, and research tasks, streamlining the process of finding specific information within large document collections.</p> <ol> <li>EXA Search Web Loader:</li> </ol> <p>The EXASearchTool is designed to perform a semantic search for a specified query from a text\u2019s content across the internet.</p> <p>The EXASearchTool is designed to perform a semantic search for a specified query from a text\u2019s content across the internet. It utilizes the exa.ai API to fetch and display the most relevant search results based on the query provided by the user.</p> <ol> <li>File Read:</li> </ol> <p>The FileReadTool is designed to read files from the local file system.</p> <p>The FileReadTool conceptually represents a suite of functionalities within the crewai_tools package aimed at facilitating file reading and content retrieval. This suite includes tools for processing batch text files, reading runtime configuration files, and importing data for analytics. It supports a variety of text-based file formats such as .txt, .csv, .json, and more. Depending on the file type, the suite offers specialized functionality, such as converting JSON content into a Python dictionary for ease of use.</p> <ol> <li>File Write:</li> </ol> <p>The FileWriterTool is designed to write content to files.</p> <p>The FileWriterTool is a component of the crewai_tools package, designed to simplify the process of writing content to files with cross-platform compatibility (Windows, Linux, macOS). It is particularly useful in scenarios such as generating reports, saving logs, creating configuration files, and more. This tool handles path differences across operating systems, supports UTF-8 encoding, and automatically creates directories if they don\u2019t exist, making it easier to organize your output reliably across different platforms.</p> <ol> <li>Firecrawl Crawl Website:</li> </ol> <p>The FirecrawlCrawlWebsiteTool is designed to crawl and convert websites into clean markdown or structured data.</p> <ol> <li>Firecrawl Scrape Website:</li> </ol> <p>The FirecrawlScrapeWebsiteTool is designed to scrape websites and convert them into clean markdown or structured data.</p> <p>Firecrawl is a platform for crawling and convert any website into clean markdown or structured data.</p> <ol> <li>Firecrawl Search:</li> </ol> <p>The FirecrawlSearchTool is designed to search websites and convert them into clean markdown or structured data.</p> <p>Firecrawl is a platform for crawling and convert any website into clean markdown or structured data.</p> <ol> <li>Github Search:</li> </ol> <p>The GithubSearchTool is designed to search websites and convert them into clean markdown or structured data.</p> <p>The GithubSearchTool is a Retrieval-Augmented Generation (RAG) tool specifically designed for conducting semantic searches within GitHub repositories. Utilizing advanced semantic search capabilities, it sifts through code, pull requests, issues, and repositories, making it an essential tool for developers, researchers, or anyone in need of precise information from GitHub.</p> <ol> <li>Hyperbrowser Load Tool:</li> </ol> <p>The HyperbrowserLoadTool enables web scraping and crawling using Hyperbrowser.</p> <ol> <li>Linkup Search Tool:</li> </ol> <p>The LinkupSearchTool enables querying the Linkup API for contextual information.</p> <p>The LinkupSearchTool provides the ability to query the Linkup API for contextual information and retrieve structured results. This tool is ideal for enriching workflows with up-to-date and reliable information from Linkup, allowing agents to access relevant data during their tasks.</p> <ol> <li>LlamaIndex Tool:</li> </ol> <p>The LlamaIndexTool is a wrapper for LlamaIndex tools and query engines.</p> <p>The LlamaIndexTool is designed to be a general wrapper around LlamaIndex tools and query engines, enabling you to leverage LlamaIndex resources in terms of RAG/agentic pipelines as tools to plug into CrewAI agents. This tool allows you to seamlessly integrate LlamaIndex\u2019s powerful data processing and retrieval capabilities into your CrewAI workflows.</p> <ol> <li>Google Serper Search:</li> </ol> <p>The SerperDevTool is designed to search the internet and return the most relevant results.</p> <p>This tool is designed to perform a semantic search for a specified query from a text\u2019s content across the internet. It utilizes the serper.dev API to fetch and display the most relevant search results based on the query provided by the user.</p> <ol> <li>S3 Reader Tool:</li> </ol> <p>The S3ReaderTool enables CrewAI agents to read files from Amazon S3 buckets.</p> <p>The S3ReaderTool is designed to read files from Amazon S3 buckets. This tool allows CrewAI agents to access and retrieve content stored in S3, making it ideal for workflows that require reading data, configuration files, or any other content stored in AWS S3 storage.</p> <ol> <li>S3 Writer Tool:</li> </ol> <p>The S3WriterTool enables CrewAI agents to write content to files in Amazon S3 buckets.</p> <p>The S3WriterTool is designed to write content to files in Amazon S3 buckets. This tool allows CrewAI agents to create or update files in S3, making it ideal for workflows that require storing data, saving configuration files, or persisting any other content to AWS S3 storage.</p> <ol> <li>Scrapegraph Scrape Tool:</li> </ol> <p>The ScrapegraphScrapeTool leverages Scrapegraph AI\u2019s SmartScraper API to intelligently extract content from websites.</p> <p>The ScrapegraphScrapeTool is designed to leverage Scrapegraph AI\u2019s SmartScraper API to intelligently extract content from websites. This tool provides advanced web scraping capabilities with AI-powered content extraction, making it ideal for targeted data collection and content analysis tasks. Unlike traditional web scrapers, it can understand the context and structure of web pages to extract the most relevant information based on natural language prompts.</p> <ol> <li>Scrape Element From Website Tool:</li> </ol> <p>The ScrapeElementFromWebsiteTool enables CrewAI agents to extract specific elements from websites using CSS selectors.</p> <p>The ScrapeElementFromWebsiteTool is designed to extract specific elements from websites using CSS selectors. This tool allows CrewAI agents to scrape targeted content from web pages, making it useful for data extraction tasks where only specific parts of a webpage are needed.</p> <ol> <li>JSON RAG Search:</li> </ol> <p>The JSONSearchTool is designed to search JSON files and return the most relevant results.</p> <p>The JSONSearchTool is designed to facilitate efficient and precise searches within JSON file contents. It utilizes a RAG (Retrieve and Generate) search mechanism, allowing users to specify a JSON path for targeted searches within a particular JSON file. This capability significantly improves the accuracy and relevance of search results.</p> <ol> <li>MDX RAG Search:</li> </ol> <p>The MDXSearchTool is designed to search MDX files and return the most relevant results.</p> <p>The MDX Search Tool is a component of the crewai_tools package aimed at facilitating advanced markdown language extraction. It enables users to effectively search and extract relevant information from MD files using query-based searches. This tool is invaluable for data analysis, information management, and research tasks, streamlining the process of finding specific information within large document collections.</p> <ol> <li>MySQL RAG Search:</li> </ol> <p>The MySQLSearchTool is designed to search MySQL databases and return the most relevant results.</p> <p>This tool is designed to facilitate semantic searches within MySQL database tables. Leveraging the RAG (Retrieve and Generate) technology, the MySQLSearchTool provides users with an efficient means of querying database table content, specifically tailored for MySQL databases. It simplifies the process of finding relevant data through semantic search queries, making it an invaluable resource for users needing to perform advanced queries on extensive datasets within a MySQL database.</p> <ol> <li>MultiOn Tool:</li> </ol> <p>The MultiOnTool empowers CrewAI agents with the capability to navigate and interact with the web through natural language instructions.</p> <p>The MultiOnTool is designed to wrap MultiOn\u2019s web browsing capabilities, enabling CrewAI agents to control web browsers using natural language instructions. This tool facilitates seamless web browsing, making it an essential asset for projects requiring dynamic web data interaction and automation of web-based tasks.</p> <ol> <li>NL2SQL Tool:</li> </ol> <p>The NL2SQLTool is designed to convert natural language to SQL queries.</p> <p>This tool is used to convert natural language to SQL queries. When passsed to the agent it will generate queries and then use them to interact with the database.</p> <p>This enables multiple workflows like having an Agent to access the database fetch information based on the goal and then use the information to generate a response, report or any other output. Along with that proivdes the ability for the Agent to update the database based on its goal.</p> <p>Attention: Make sure that the Agent has access to a Read-Replica or that is okay for the Agent to run insert/update queries on the database.</p> <ol> <li>Patronus Evaluation Tools:</li> </ol> <p>The Patronus evaluation tools enable CrewAI agents to evaluate and score model inputs and outputs using the Patronus AI platform.</p> <p>The Patronus evaluation tools are designed to enable CrewAI agents to evaluate and score model inputs and outputs using the Patronus AI platform. These tools provide different levels of control over the evaluation process, from allowing agents to select the most appropriate evaluator and criteria to using predefined criteria or custom local evaluators.</p> <p>There are three main Patronus evaluation tools:</p> <ul> <li>PatronusEvalTool: Allows agents to select the most appropriate evaluator and criteria for the evaluation task.</li> <li>PatronusPredefinedCriteriaEvalTool: Uses predefined evaluator and criteria specified by the user.</li> <li> <p>PatronusLocalEvaluatorTool: Uses custom function evaluators defined by the user. \u200b</p> </li> <li> <p>PDF RAG Search:</p> </li> </ul> <p>The PDFSearchTool is designed to search PDF files and return the most relevant results.</p> <p>The PDFSearchTool is a RAG tool designed for semantic searches within PDF content. It allows for inputting a search query and a PDF document, leveraging advanced search techniques to find relevant content efficiently. This capability makes it especially useful for extracting specific information from large PDF files quickly.</p> <ol> <li>PG RAG Search:</li> </ol> <p>The PGSearchTool is designed to search PostgreSQL databases and return the most relevant results.</p> <p>The PGSearchTool is envisioned as a powerful tool for facilitating semantic searches within PostgreSQL database tables. By leveraging advanced Retrieve and Generate (RAG) technology, it aims to provide an efficient means for querying database table content, specifically tailored for PostgreSQL databases. The tool\u2019s goal is to simplify the process of finding relevant data through semantic search queries, offering a valuable resource for users needing to conduct advanced queries on extensive datasets within a PostgreSQL environment.</p> <ol> <li>Qdrant Vector Search Tool:</li> </ol> <p>Semantic search capabilities for CrewAI agents using Qdrant vector database</p> <p>The Qdrant Vector Search Tool enables semantic search capabilities in your CrewAI agents by leveraging Qdrant, a vector similarity search engine. This tool allows your agents to search through documents stored in a Qdrant collection using semantic similarity.</p> <ol> <li>RAG Tool:</li> </ol> <p>The RagTool is a dynamic knowledge base tool for answering questions using Retrieval-Augmented Generation.</p> <p>The RagTool is designed to answer questions by leveraging the power of Retrieval-Augmented Generation (RAG) through EmbedChain. It provides a dynamic knowledge base that can be queried to retrieve relevant information from various data sources. This tool is particularly useful for applications that require access to a vast array of information and need to provide contextually relevant answers.</p> <ol> <li>Scrape Website:</li> </ol> <p>The ScrapeWebsiteTool is designed to extract and read the content of a specified website.</p> <p>A tool designed to extract and read the content of a specified website. It is capable of handling various types of web pages by making HTTP requests and parsing the received HTML content. This tool can be particularly useful for web scraping tasks, data collection, or extracting specific information from websites.</p> <ol> <li>Scrapfly Scrape Website Tool:</li> </ol> <p>The ScrapflyScrapeWebsiteTool leverages Scrapfly\u2019s web scraping API to extract content from websites in various formats.</p> <p>The ScrapflyScrapeWebsiteTool is designed to leverage Scrapfly\u2019s web scraping API to extract content from websites. This tool provides advanced web scraping capabilities with headless browser support, proxies, and anti-bot bypass features. It allows for extracting web page data in various formats, including raw HTML, markdown, and plain text, making it ideal for a wide range of web scraping tasks.</p> <ol> <li>Selenium Scraper:</li> </ol> <p>The SeleniumScrapingTool is designed to extract and read the content of a specified website using Selenium.</p> <p>The SeleniumScrapingTool is crafted for high-efficiency web scraping tasks. It allows for precise extraction of content from web pages by using CSS selectors to target specific elements. Its design caters to a wide range of scraping needs, offering flexibility to work with any provided website URL.</p> <ol> <li>Snowflake Search Tool:</li> </ol> <p>The SnowflakeSearchTool enables CrewAI agents to execute SQL queries and perform semantic search on Snowflake data warehouses.</p> <p>The SnowflakeSearchTool is designed to connect to Snowflake data warehouses and execute SQL queries with advanced features like connection pooling, retry logic, and asynchronous execution. This tool allows CrewAI agents to interact with Snowflake databases, making it ideal for data analysis, reporting, and business intelligence tasks that require access to enterprise data stored in Snowflake.</p> <ol> <li>Spider Scraper:</li> </ol> <p>The SpiderTool is designed to extract and read the content of a specified website using Spider.</p> <p>Spider is the fastest open source scraper and crawler that returns LLM-ready data. It converts any website into pure HTML, markdown, metadata or text while enabling you to crawl with custom actions using AI.</p> <ol> <li>TXT RAG Search:</li> </ol> <p>The TXTSearchTool is designed to perform a RAG (Retrieval-Augmented Generation) search within the content of a text file.</p> <p>This tool is used to perform a RAG (Retrieval-Augmented Generation) search within the content of a text file. It allows for semantic searching of a query within a specified text file\u2019s content, making it an invaluable resource for quickly extracting information or finding specific sections of text based on the query provided.</p> <ol> <li>Vision Tool:</li> </ol> <p>The VisionTool is designed to extract text from images.</p> <p>This tool is used to extract text from images. When passed to the agent it will extract the text from the image and then use it to generate a response, report or any other output. The URL or the PATH of the image should be passed to the Agent.</p> <ol> <li>Weaviate Vector Search:</li> </ol> <p>The WeaviateVectorSearchTool is designed to search a Weaviate vector database for semantically similar documents.</p> <p>The WeaviateVectorSearchTool is specifically crafted for conducting semantic searches within documents stored in a Weaviate vector database. This tool allows you to find semantically similar documents to a given query, leveraging the power of vector embeddings for more accurate and contextually relevant search results.</p> <ol> <li>Website RAG Search:</li> </ol> <p>The WebsiteSearchTool is designed to perform a RAG (Retrieval-Augmented Generation) search within the content of a website.</p> <p>The WebsiteSearchTool is designed as a concept for conducting semantic searches within the content of websites. It aims to leverage advanced machine learning models like Retrieval-Augmented Generation (RAG) to navigate and extract information from specified URLs efficiently. This tool intends to offer flexibility, allowing users to perform searches across any website or focus on specific websites of interest. Please note, the current implementation details of the WebsiteSearchTool are under development, and its functionalities as described may not yet be accessible.</p> <p>The XMLSearchTool is a cutting-edge RAG tool engineered for conducting semantic searches within XML files. Ideal for users needing to parse and extract information from XML content efficiently, this tool supports inputting a search query and an optional XML file path. By specifying an XML path, users can target their search more precisely to the content of that file, thereby obtaining more relevant search outcomes.</p> <ol> <li>YouTube Channel RAG Search: The YoutubeVideoSearchTool is designed to perform a RAG (Retrieval-Augmented Generation) search within the content of a Youtube video.</li> </ol> <p>This tool is part of the crewai_tools package and is designed to perform semantic searches within Youtube video content, utilizing Retrieval-Augmented Generation (RAG) techniques. It is one of several \u201cSearch\u201d tools in the package that leverage RAG for different sources. The YoutubeVideoSearchTool allows for flexibility in searches; users can search across any Youtube video content without specifying a video URL, or they can target their search to a specific Youtube video by providing its URL.</p>"},{"location":"AIML/AgenticAI/crewai/#create-custom-tools","title":"Create Custom Tools","text":"<p>Comprehensive guide on crafting, using, and managing custom tools within the CrewAI framework, including new functionalities and error handling.</p> <p>Subclassing BaseTool</p> <p>To create a personalized tool, inherit from BaseTool and define the necessary attributes, including the args_schema for input validation, and the _run method.</p>"},{"location":"AIML/AgenticAI/langgraph/","title":"Langgraph","text":""},{"location":"AIML/AgenticAI/langgraph/#langgraph","title":"LangGraph","text":"<p>LangGraph is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration \u2014 offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.</p>"},{"location":"AIML/AgenticAI/langgraph/#install-the-library","title":"Install the Library","text":"<pre><code>pip install -U langgraph\npip install -U langchain-anthropic\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#simple-example-below-of-how-to-create-a-react-agent","title":"Simple example below of how to create a ReAct agent.","text":"<pre><code># This code depends on pip install langchain[anthropic]\nfrom langgraph.prebuilt import create_react_agent\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve API tokens from .env\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\nagent = create_react_agent(\"anthropic:claude-3-7-sonnet-latest\", tools=[search])\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#why-use-langgraph","title":"Why use LangGraph?","text":"<p>LangGraph is useful for building robust, modular, and scalable AI agents.It extends LangChain with graph-based execution, making it ideal for multi-agent workflows, streaming, and fine-grained control.</p> <p>Developers choose LangGraph for:</p> <ol> <li> <p>Reliability and controllability: </p> <ul> <li>Ensures structured execution of agent tasks.</li> <li>Supports moderation checks, human approvals, and context persistence for long-running workflows.</li> </ul> </li> <li> <p>Low-level and extensible: </p> <ul> <li>Provides full control over agent behavior using custom nodes and state management.</li> <li>Ideal for multi-agent collaboration, where each agent has a defined role.</li> </ul> </li> <li> <p>First-Class Streaming Support:</p> <ul> <li>Supports token-by-token streaming, making it great for real-time insights into agent decisions.</li> <li>Allows intermediate step streaming, improving observability and debugging.</li> </ul> </li> </ol>"},{"location":"AIML/AgenticAI/langgraph/#where-is-langgraph-useful","title":"Where is LangGraph Useful?","text":"<ul> <li>Multi-Agent Systems: When you need multiple specialized agents working together.</li> <li>Long-Running Workflows: If your agents need context persistence over time.</li> <li>Interactive Applications: When streaming responses improve user experience.</li> </ul> <p>LangGraph can be useful for designing complex AI pipelines where different agents handle different tasks while maintaining control and visibility. </p> <p>LangGraph is already trusted in production by major companies for AI-powered automation, making it a solid choice for building scalable, reliable, and controllable AI agents.</p>"},{"location":"AIML/AgenticAI/langgraph/#real-world-use-cases-of-langgraph","title":"Real-World Use Cases of LangGraph","text":"<ul> <li>Klarna \u2192 Customer Support Bot<ul> <li>Handles 85 million active users.</li> <li>Manages customer inquiries with multi-step workflows and automation.</li> </ul> </li> <li>Elastic \u2192 Security AI Assistant<ul> <li>Helps with threat detection and security analysis.</li> <li>Uses multi-agent collaboration for investigating security alerts.</li> </ul> </li> <li>Uber \u2192 Automated Unit Test Generation<ul> <li>Generates and refines unit tests for developers.</li> <li>Uses LangGraph for agent-based coding assistants.</li> </ul> </li> <li>Replit \u2192 AI-Powered Code Generation<ul> <li>Assists developers in writing, debugging, and optimizing code.</li> <li>Uses LangGraph\u2019s streaming and multi-agent capabilities.</li> </ul> </li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#langgraphs-ecosystem-integrations","title":"LangGraph\u2019s Ecosystem &amp; Integrations","text":"<p>LangGraph works standalone but integrates seamlessly with LangChain tools, making it easier to build, evaluate, and deploy AI agents.</p>"},{"location":"AIML/AgenticAI/langgraph/#key-integrations-for-better-llm-application-development","title":"Key Integrations for Better LLM Application Development","text":"<ul> <li> <p>LangSmith (Agent Evaluation &amp; Debugging)</p> <ul> <li>Debugs poor-performing LLM runs and optimizes workflows.</li> <li>Evaluates agent trajectories to improve decision-making.</li> <li>Provides observability in production.</li> </ul> </li> <li> <p>LangGraph Platform (Scaling &amp; Deployment)</p> <ul> <li>Deploys long-running, stateful AI agents at scale.</li> <li>Allows agent discovery, reuse, and configuration across teams.</li> <li>Features LangGraph Studio for visual prototyping and fast iteration.</li> </ul> </li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#to-integrate-langgraph-langsmith-into-your-ai-projects","title":"To integrate LangGraph + LangSmith into your AI projects","text":"<p>Set Up LangGraph with LangSmith for Debugging &amp; Observability - Install Dependencies     - First, install LangGraph, LangSmith, and LangChain:</p> <pre><code>        pip install langgraph langsmith langchain\n</code></pre> <ul> <li> <p>Set Up LangSmith API Key     Sign up for LangSmith at smith.langchain.com and get your API key. <pre><code>        https://smith.langchain.com/\n        LANGCHAIN_API_KEY=\"your_actual_api_key\"\n</code></pre> Then, set it in your environment:</p> </li> <li> <p>Enable Debugging for Agents</p> </li> </ul> <pre><code>        # This code depends on pip install langchain[anthropic]\n        from langgraph.prebuilt import create_react_agent\n        import os\n        from dotenv import load_dotenv\n\n        from langchain_openai import ChatOpenAI\n        from langsmith import traceable\n\n\n        # Load environment variables\n        load_dotenv()\n\n        LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n\n        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n        os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n\n        # Define the function\n        @traceable\n        def search(query: str):\n            \"\"\"Call to surf the web.\"\"\"\n            if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n                return \"It's 60 degrees and foggy.\"\n            return \"It's 90 degrees and sunny.\"\n\n        # Use OpenAI's GPT model\n        llm = ChatOpenAI(model=\"gpt-4-turbo\")\n\n        # Create the agent\n        agent = create_react_agent(llm, tools=[search])\n\n        # Invoke the agent\n        response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]})\n\n        print(response)\n</code></pre> <ul> <li>Now, all agent runs will be logged in LangSmith for debugging.</li> </ul> <p>Visualizing &amp; Debugging Agent Trajectories in LangSmith</p> <p>Once the agent is running, go to LangSmith UI and check:</p> <ul> <li>Logs of each agent action (inputs, outputs, reasoning).</li> <li>Failure points in decision-making.</li> <li>Performance metrics to optimize.</li> </ul> <p></p> <p></p> <p>Scaling with LangGraph Platform (Long-Running Agents &amp; Deployment)</p> <ul> <li>To make your AI stateful and scalable, use LangGraph Platform:</li> </ul> <pre><code>pip install langgraph[platform]\n</code></pre> <ul> <li>Deploy long-running agents with stateful memory.</li> <li>Use LangGraph Studio for drag-and-drop workflow design.</li> <li>Share &amp; configure agents across teams.</li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#graph-api-basics","title":"Graph API Basics","text":""},{"location":"AIML/AgenticAI/langgraph/#how-to-update-graph-state-from-nodes","title":"How to update graph state from nodes","text":"<p>Define state State in LangGraph can be a TypedDict, Pydantic model, or dataclass. </p> <p>State: The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.</p> <p>Schema: The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation.</p>"},{"location":"AIML/AgenticAI/langgraph/#how-to-use-pydantic-model-as-graph-state","title":"How to use Pydantic model as graph state","text":"<p>First we need to install the packages required</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#input-validation","title":"Input Validation","text":"<pre><code>from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\nfrom pydantic import BaseModel\n\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\n\ndef node(state: OverallState):\n    return {\"a\": \"goodbye\"}\n\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(node)  # node_1 is the first node\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\ngraph = builder.compile()\n\n# Test the graph with a valid input\ngraph.invoke({\"a\": \"hello\"})\n</code></pre> <p>Output:</p> <pre><code>{'a': 'goodbye'}\n</code></pre> <p>Invoke the graph with an invalid input</p> <pre><code>try:\n    graph.invoke({\"a\": 123})  # Should be a string\nexcept Exception as e:\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\n    print(e)\n</code></pre> <pre><code>An exception was raised because `a` is an integer rather than a string.\n1 validation error for OverallState\na\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#multiple-nodes","title":"Multiple Nodes","text":"<p>Run-time validation will also work in a multi-node graph. In the example below bad_node updates a to an integer.</p> <p>Because run-time validation occurs on inputs, the validation error will occur when ok_node is called (not when bad_node returns an update to the state which is inconsistent with the schema).</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\nfrom pydantic import BaseModel\n\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\n\ndef bad_node(state: OverallState):\n    return {\n        \"a\": 123  # Invalid\n    }\n\n\ndef ok_node(state: OverallState):\n    return {\"a\": \"goodbye\"}\n\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(bad_node)\nbuilder.add_node(ok_node)\nbuilder.add_edge(START, \"bad_node\")\nbuilder.add_edge(\"bad_node\", \"ok_node\")\nbuilder.add_edge(\"ok_node\", END)\ngraph = builder.compile()\n\n# Test the graph with a valid input\ntry:\n    graph.invoke({\"a\": \"hello\"})\nexcept Exception as e:\n    print(\"An exception was raised because bad_node sets `a` to an integer.\")\n    print(e)\n</code></pre> <p>Output:</p> <pre><code>An exception was raised because bad_node sets `a` to an integer.\n1 validation error for OverallState\na\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#advanced-pydantic-model-usage","title":"Advanced Pydantic Model Usage","text":"<p>This section covers more advanced topics when using Pydantic models with LangGraph.</p> <p>Serialization Behavior</p> <p>When using Pydantic models as state schemas, it's important to understand how serialization works, especially when: - Passing Pydantic objects as inputs - Receiving outputs from the graph - Working with nested Pydantic models</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\n\n\nclass NestedModel(BaseModel):\n    value: str\n\n\nclass ComplexState(BaseModel):\n    text: str\n    count: int\n    nested: NestedModel\n\n\ndef process_node(state: ComplexState):\n    # Node receives a validated Pydantic object\n    print(f\"Input state type: {type(state)}\")\n    print(f\"Nested type: {type(state.nested)}\")\n\n    # Return a dictionary update\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\n\n\n# Build the graph\nbuilder = StateGraph(ComplexState)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", END)\ngraph = builder.compile()\n\n# Create a Pydantic instance for input\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\nprint(f\"Input object type: {type(input_state)}\")\n\n# Invoke graph with a Pydantic instance\nresult = graph.invoke(input_state)\nprint(f\"Output type: {type(result)}\")\nprint(f\"Output content: {result}\")\n\n# Convert back to Pydantic model if needed\noutput_model = ComplexState(**result)\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\n</code></pre> <p>Runtime Type Coercion Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\n\n\nclass CoercionExample(BaseModel):\n    # Pydantic will coerce string numbers to integers\n    number: int\n    # Pydantic will parse string booleans to bool\n    flag: bool\n\n\ndef inspect_node(state: CoercionExample):\n    print(f\"number: {state.number} (type: {type(state.number)})\")\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\n    return {}\n\n\nbuilder = StateGraph(CoercionExample)\nbuilder.add_node(\"inspect\", inspect_node)\nbuilder.add_edge(START, \"inspect\")\nbuilder.add_edge(\"inspect\", END)\ngraph = builder.compile()\n\n# Demonstrate coercion with string inputs that will be converted\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\n\n# This would fail with a validation error\ntry:\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\nexcept Exception as e:\n    print(f\"\\nExpected validation error: {e}\")\n</code></pre> <p>Working with Message Models When working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire:</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\nfrom typing import List\n\n\nclass ChatState(BaseModel):\n    messages: List[AnyMessage]\n    context: str\n\n\ndef add_message(state: ChatState):\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\n\n\nbuilder = StateGraph(ChatState)\nbuilder.add_node(\"add_message\", add_message)\nbuilder.add_edge(START, \"add_message\")\nbuilder.add_edge(\"add_message\", END)\ngraph = builder.compile()\n\n# Create input with a message\ninitial_state = ChatState(\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\n)\n\nresult = graph.invoke(initial_state)\nprint(f\"Output: {result}\")\n\n# Convert back to Pydantic model to see message types\noutput_model = ChatState(**result)\nfor i, msg in enumerate(output_model.messages):\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#graphs","title":"Graphs","text":"<p>At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:</p> <ol> <li> <p>State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.</p> </li> <li> <p>Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.</p> </li> <li> <p>Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.</p> </li> </ol> <p>By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State.</p> <p>To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code.</p> <p>In short: nodes do the work. edges tell what to do next.</p> <p>LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s).</p> <p>These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues.</p> <p>A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.</p> <p>StateGraph: The StateGraph class is the main graph class to use. This is parameterized by a user defined State object.</p> <p>Compiling your graph: To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?</p> <p>Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:</p> <pre><code>graph = graph_builder.compile(...)\n</code></pre> <p>Note: You MUST compile your graph before you can use it.</p> <p>Multiple schemas Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:</p> <ul> <li>Internal nodes can pass information that is not required in the graph's input / output.</li> <li>We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.</li> </ul> <p>It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.</p> <p>Let's look at an example:</p> <pre><code>class InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -&gt; OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -&gt; PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -&gt; OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input=InputState,output=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n{'graph_output': 'My name is Lance'}\n</code></pre> <p>There are two subtle and important points to note here:</p> <ol> <li> <p>We pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.</p> </li> <li> <p>We initialize the graph with StateGraph(OverallState,input=InputState,output=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.</p> </li> </ol>"},{"location":"AIML/AgenticAI/langgraph/#reducers","title":"Reducers","text":"<p>Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:</p> <p>Default Reducer</p> <p>These two examples show how to use the default reducer:</p> <p>Example A:</p> <pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n</code></pre> <p>In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let's then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}</p> <p>Example B:</p> <pre><code>from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n</code></pre> <p>In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let's then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.</p>"},{"location":"AIML/AgenticAI/langgraph/#working-with-messages-in-graph-state","title":"Working with Messages in Graph State","text":"<p>Why use messages?</p> <p>Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.</p> <p>Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.</p> <p>However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.</p> <p>Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:</p> <pre><code># this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n</code></pre> <p>Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as it's reducer function.</p> <pre><code>from langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n</code></pre> <p>MessagesState</p> <p>Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:</p> <pre><code>from langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#nodes","title":"Nodes","text":"<p>In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a \"config\", containing optional configurable parameters (such as a thread_id).</p> <p>Similar to NetworkX, you add these nodes to a graph using the add_node method:</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(dict)\n\n\ndef my_node(state: dict, config: RunnableConfig):\n    print(\"In node: \", config[\"configurable\"][\"user_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\n# The second argument is optional\ndef my_other_node(state: dict):\n    return state\n\n\nbuilder.add_node(\"my_node\", my_node)\nbuilder.add_node(\"other_node\", my_other_node)\n...\n</code></pre> <p>Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.</p> <p>If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.</p> <pre><code>builder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#start-node","title":"START Node","text":"<p>The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.</p> <pre><code>from langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#end-node","title":"END Node","text":"<p>The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.</p> <pre><code>from langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#edges","title":"Edges","text":"<p>Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:</p> <ul> <li>Normal Edges: Go directly from one node to the next.</li> <li>Conditional Edges: Call a function to determine which node(s) to go to next.</li> <li>Entry Point: Which node to call first when user input arrives.</li> <li>Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.</li> </ul> <p>A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.</p> <p>Normal Edges: If you always want to go from node A to node B, you can use the add_edge method directly.</p> <pre><code>graph.add_edge(\"node_a\", \"node_b\")\n</code></pre> <p>Conditional Edges: If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:</p> <pre><code>graph.add_conditional_edges(\"node_a\", routing_function)\n</code></pre> <p>Similar to nodes, the routing_function accepts the current state of the graph and returns a value.</p> <p>By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.</p> <p>You can optionally provide a dictionary that maps the routing_function's output to the name of the next node.</p> <pre><code>graph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n</code></pre> <p>Note: Use Command instead of conditional edges if you want to combine state updates and routing in a single function.</p> <p>Command It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n</code></pre> <p>With Command you can also achieve dynamic control flow behavior (identical to conditional edges):</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n</code></pre> <p>Important When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.</p>"},{"location":"AIML/AgenticAI/langgraph/#when-should-i-use-command-instead-of-conditional-edges","title":"When should I use Command instead of conditional edges?","text":"<p>Use Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent.</p> <p>Use conditional edges to route between nodes conditionally without updating the state.</p>"},{"location":"AIML/AgenticAI/langgraph/#navigating-to-a-node-in-a-parent-graph","title":"Navigating to a node in a parent graph","text":"<p>If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n</code></pre> <p>Note: Setting graph to Command.PARENT will navigate to the closest parent graph.</p> <p>State updates with Command.PARENT When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph state schemas, you must define a reducer for the key you're updating in the parent graph state. See this example.</p>"},{"location":"AIML/AgenticAI/langgraph/#using-inside-tools","title":"Using inside tools","text":"<p>A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:</p> <pre><code>@tool\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\n    return Command(\n        update={\n            # update the state keys\n            \"user_info\": user_info,\n            # update the message history\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n        }\n    )\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.</p>"},{"location":"AIML/AgenticAI/langgraph/#persistence","title":"Persistence","text":"<p>LangGraph provides built-in persistence for your agent's state using checkpointers. Checkpointers save snapshots of the graph state at every superstep, allowing resumption at any time. This enables features like human-in-the-loop interactions, memory management, and fault-tolerance. You can even directly manipulate a graph's state after its execution using the appropriate get and update methods. For more details, see the persistence conceptual guide.</p>"},{"location":"AIML/AgenticAI/langgraph/#threads","title":"Threads","text":"<p>Threads in LangGraph represent individual sessions or conversations between your graph and a user. When using checkpointing, turns in a single conversation (and even steps within a single graph execution) are organized by a unique thread ID.</p>"},{"location":"AIML/AgenticAI/langgraph/#storage","title":"Storage","text":"<p>LangGraph provides built-in document storage through the BaseStore interface. Unlike checkpointers, which save state by thread ID, stores use custom namespaces for organizing data. This enables cross-thread persistence, allowing agents to maintain long-term memories, learn from past interactions, and accumulate knowledge over time. Common use cases include storing user profiles, building knowledge bases, and managing global preferences across all threads.</p>"},{"location":"AIML/AgenticAI/langgraph/#graph-migrations","title":"Graph Migrations","text":"<p>LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.</p> <ul> <li>For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)</li> <li>For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.</li> <li>For modifying state, we have full backwards and forwards compatibility for adding and removing keys</li> <li>State keys that are renamed lose their saved state in existing threads</li> <li>State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.</li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#configuration","title":"Configuration","text":"<p>When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single \"cognitive architecture\" (the graph) but have multiple different instance of it.</p> <p>You can optionally specify a config_schema when creating a graph.</p> <pre><code>class ConfigSchema(TypedDict):\n    llm: str\n\ngraph = StateGraph(State, config_schema=ConfigSchema)\n</code></pre> <p>You can then pass this configuration into the graph using the configurable config field.</p> <pre><code>config = {\"configurable\": {\"llm\": \"anthropic\"}}\n\ngraph.invoke(inputs, config=config)\n</code></pre> <p>You can then access and use this configuration inside a node:</p> <pre><code>def node_a(state, config):\n    llm_type = config.get(\"configurable\", {}).get(\"llm\", \"openai\")\n    llm = get_llm(llm_type)\n    ...\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#recursion-limit","title":"Recursion Limit","text":"<p>The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to .invoke/.stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:</p> <pre><code>graph.invoke(inputs, config={\"recursion_limit\": 5, \"configurable\":{\"llm\": \"anthropic\"}})\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#interrupt","title":"interrupt","text":"<p>Use the interrupt function to pause the graph at specific points to collect user input. The interrupt function surfaces interrupt information to the client, allowing the developer to collect user input, validate the graph state, or make decisions before resuming execution.</p> <pre><code>from langgraph.types import interrupt\n\ndef human_approval_node(state: State):\n    ...\n    answer = interrupt(\n        # This value will be sent to the client.\n        # It can be any JSON serializable value.\n        {\"question\": \"is it ok to continue?\"},\n    )\n    ...\n</code></pre> <p>Resuming the graph is done by passing a Command object to the graph with the resume key set to the value returned by the interrupt function.</p>"},{"location":"AIML/AgenticAI/langgraph/#breakpoints","title":"Breakpoints","text":"<p>Breakpoints pause graph execution at specific points and enable stepping through execution step by step. Breakpoints are powered by LangGraph's persistence layer, which saves the state after each graph step. Breakpoints can also be used to enable human-in-the-loop workflows, though we recommend using the interrupt function for this purpose.</p> <p>Read more about breakpoints in the Breakpoints conceptual guide.</p>"},{"location":"AIML/AgenticAI/langgraph/#subgraphs","title":"Subgraphs","text":"<p>A subgraph is a graph that is used as a node in another graph. This is nothing more than the age-old concept of encapsulation, applied to LangGraph. Some reasons for using subgraphs are:</p> <ul> <li>building multi-agent systems</li> <li>when you want to reuse a set of nodes in multiple graphs, which maybe share some state, you can define them once in a subgraph and then use them in multiple parent graphs</li> <li>when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph</li> </ul> <p>There are two ways to add subgraphs to a parent graph:</p> <ul> <li>add a node with the compiled subgraph: this is useful when the parent graph and the subgraph share state keys and you don't need to transform state on the way in or out</li> </ul> <pre><code>builder.add_node(\"subgraph\", subgraph_builder.compile())\n</code></pre> <ul> <li>add a node with a function that invokes the subgraph: this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph</li> </ul> <pre><code>subgraph = subgraph_builder.compile()\n\ndef call_subgraph(state: State):\n    return subgraph.invoke({\"subgraph_key\": state[\"parent_key\"]})\n\nbuilder.add_node(\"subgraph\", call_subgraph)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#as-a-compiled-graph","title":"As a compiled graph","text":"<p>The simplest way to create subgraph nodes is by using a compiled subgraph directly. When doing so, it is important that the parent graph and the subgraph state schemas share at least one key which they can use to communicate. If your graph and subgraph do not share any keys, you should write a function invoking the subgraph instead.</p> <p>Note: If you pass extra keys to the subgraph node (i.e., in addition to the shared keys), they will be ignored by the subgraph node. Similarly, if you return extra keys from the subgraph, they will be ignored by the parent graph.</p> <pre><code>from langgraph.graph import StateGraph\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\n# Define subgraph\ndef subgraph_node(state: SubgraphState):\n    # note that this subgraph node can communicate with the parent graph via the shared \"foo\" key\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node)\n...\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"subgraph\", subgraph)\n...\ngraph = builder.compile()\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#as-a-function","title":"As a function","text":"<p>You might want to define a subgraph with a completely different schema. In this case, you can create a node function that invokes the subgraph. This function will need to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.</p> <pre><code>class State(TypedDict):\n    foo: str\n\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\n# Define subgraph\ndef subgraph_node(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + \"baz\"}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node)\n...\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\ndef node(state: State):\n    # transform the state to the subgraph state\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n    # transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n\nbuilder = StateGraph(State)\n# note that we are using `node` function instead of a compiled subgraph\nbuilder.add_node(node)\n...\ngraph = builder.compile()\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#visualization","title":"Visualization","text":"<p>It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. visualize</p>"},{"location":"AIML/AgenticAI/langgraph/#streaming","title":"Streaming","text":"<p>LangGraph is built with first class support for streaming, including streaming updates from graph nodes during the execution, streaming tokens from LLM calls and more. See this conceptual guide for more information. streaming</p>"},{"location":"AIML/AgenticAI/langgraph/#how-to-create-branches-for-parallel-node-execution","title":"How to create branches for parallel node execution\u00b6","text":"<p>Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.</p>"},{"location":"AIML/AgenticAI/langgraph/#how-to-run-graph-nodes-in-parallel","title":"How to run graph nodes in parallel","text":"<p>In this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See this guide for more detail on updating state with reducers.</p> <pre><code>import operator\nfrom typing import Annotated, Any\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p>With the reducer, you can see that the values added in each node are accumulated.</p> <pre><code>graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n</code></pre> <pre><code>Adding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"D\" to ['A', 'B', 'C']\n</code></pre> <p>Note: In the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.</p> <p>Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.</p>"},{"location":"AIML/AgenticAI/langgraph/#parallel-node-fan-out-and-fan-in-with-extra-steps","title":"Parallel node fan-out and fan-in with extra steps","text":"<p>The above example showed how to fan-out and fan-in when each path was only one step. But what if one path had more than one step? Let's add a node b_2 in the \"b\" branch:</p> <pre><code>def b_2(state: State):\n    print(f'Adding \"B_2\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B_2\"]}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(b_2)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b_2\")\nbuilder.add_edge([\"b_2\", \"c\"], \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <pre><code>graph.invoke({\"aggregate\": []})\n</code></pre> <pre><code>Adding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"B_2\" to ['A', 'B', 'C']\nAdding \"D\" to ['A', 'B', 'C', 'B_2']\n</code></pre> <pre><code>{'aggregate': ['A', 'B', 'C', 'B_2', 'D']}\n</code></pre> <p>Note: In the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. What happens in the next step?</p> <p>We use add_edge([\"b_2\", \"c\"], \"d\") here to force node \"d\" to only run when both nodes \"b_2\" and \"c\" have finished execution. If we added two separate edges, node \"d\" would run twice: after node b2 finishes and once again after node c (in whichever order those nodes finish).</p>"},{"location":"AIML/AgenticAI/langgraph/#conditional-branching","title":"Conditional Branching","text":"<p>If your fan-out is not deterministic, you can use add_conditional_edges directly.</p> <pre><code>import operator\nfrom typing import Annotated, Sequence\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n    # Add a key to the state. We will set this key to determine\n    # how we branch.\n    which: str\n\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\n\ndef e(state: State):\n    print(f'Adding \"E\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"E\"]}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_node(e)\nbuilder.add_edge(START, \"a\")\n\n\ndef route_bc_or_cd(state: State) -&gt; Sequence[str]:\n    if state[\"which\"] == \"cd\":\n        return [\"c\", \"d\"]\n    return [\"b\", \"c\"]\n\n\nintermediates = [\"b\", \"c\", \"d\"]\nbuilder.add_conditional_edges(\n    \"a\",\n    route_bc_or_cd,\n    intermediates,\n)\nfor node in intermediates:\n    builder.add_edge(node, \"e\")\n\nbuilder.add_edge(\"e\", END)\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <pre><code>graph.invoke({\"aggregate\": [], \"which\": \"bc\"})\n</code></pre> <pre><code>graph.invoke({\"aggregate\": [], \"which\": \"cd\"})\n</code></pre> <pre><code>Adding \"A\" to []\nAdding \"C\" to ['A']\nAdding \"D\" to ['A']\nAdding \"E\" to ['A', 'C', 'D']\n</code></pre> <pre><code>{'aggregate': ['A', 'C', 'D', 'E'], 'which': 'cd'}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-create-map-reduce-branches-for-parallel-execution","title":"How to create map-reduce branches for parallel execution","text":"<p>Map-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks.</p> <p>Consider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise.</p> <p>(1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object).</p> <p>LangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph's state, allowing for flexible and dynamic workflow management.</p> <p></p>"},{"location":"AIML/AgenticAI/langgraph/#setup","title":"Setup","text":"<p>First, let's install the required packages and set our API keys</p> <pre><code>%%capture --no-stderr\n%pip install -U langchain-anthropic langgraph\n</code></pre> <pre><code>import os\nimport getpass\n\n\ndef _set_env(name: str):\n    if not os.getenv(name):\n        os.environ[name] = getpass.getpass(f\"{name}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-the-graph","title":"Define the graph","text":"<pre><code>import operator\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langchain_anthropic import ChatAnthropic\n\nfrom langgraph.types import Send\nfrom langgraph.graph import END, StateGraph, START\n\nfrom pydantic import BaseModel, Field\n\n# Model and prompts\n# Define model and prompts we will use\nsubjects_prompt = \"\"\"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\"\"\njoke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\nbest_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one.\n\n{jokes}\"\"\"\n\n\nclass Subjects(BaseModel):\n    subjects: list[str]\n\n\nclass Joke(BaseModel):\n    joke: str\n\n\nclass BestJoke(BaseModel):\n    id: int = Field(description=\"Index of the best joke, starting with 0\", ge=0)\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n\n# Graph components: define the components that will make up the graph\n\n\n# This will be the overall state of the main graph.\n# It will contain a topic (which we expect the user to provide)\n# and then will generate a list of subjects, and then a joke for\n# each subject\nclass OverallState(TypedDict):\n    topic: str\n    subjects: list\n    # Notice here we use the operator.add\n    # This is because we want combine all the jokes we generate\n    # from individual nodes back into one list - this is essentially\n    # the \"reduce\" part\n    jokes: Annotated[list, operator.add]\n    best_selected_joke: str\n\n\n# This will be the state of the node that we will \"map\" all\n# subjects to in order to generate a joke\nclass JokeState(TypedDict):\n    subject: str\n\n\n# This is the function we will use to generate the subjects of the jokes\ndef generate_topics(state: OverallState):\n    prompt = subjects_prompt.format(topic=state[\"topic\"])\n    response = model.with_structured_output(Subjects).invoke(prompt)\n    return {\"subjects\": response.subjects}\n\n\n# Here we generate a joke, given a subject\ndef generate_joke(state: JokeState):\n    prompt = joke_prompt.format(subject=state[\"subject\"])\n    response = model.with_structured_output(Joke).invoke(prompt)\n    return {\"jokes\": [response.joke]}\n\n\n# Here we define the logic to map out over the generated subjects\n# We will use this as an edge in the graph\ndef continue_to_jokes(state: OverallState):\n    # We will return a list of `Send` objects\n    # Each `Send` object consists of the name of a node in the graph\n    # as well as the state to send to that node\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n\n\n# Here we will judge the best joke\ndef best_joke(state: OverallState):\n    jokes = \"\\n\\n\".join(state[\"jokes\"])\n    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n    response = model.with_structured_output(BestJoke).invoke(prompt)\n    return {\"best_selected_joke\": state[\"jokes\"][response.id]}\n\n\n# Construct the graph: here we put everything together to construct our graph\ngraph = StateGraph(OverallState)\ngraph.add_node(\"generate_topics\", generate_topics)\ngraph.add_node(\"generate_joke\", generate_joke)\ngraph.add_node(\"best_joke\", best_joke)\ngraph.add_edge(START, \"generate_topics\")\ngraph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\ngraph.add_edge(\"generate_joke\", \"best_joke\")\ngraph.add_edge(\"best_joke\", END)\napp = graph.compile()\n</code></pre> <pre><code>from IPython.display import Image\n\nImage(app.get_graph().draw_mermaid_png())\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#use-the-graph","title":"Use the graph","text":"<pre><code># Call the graph: here we call it to generate a list of jokes\nfor s in app.stream({\"topic\": \"animals\"}):\n    print(s)\n</code></pre> <pre><code>{'generate_topics': {'subjects': ['Lions', 'Elephants', 'Penguins', 'Dolphins']}}\n{'generate_joke': {'jokes': [\"Why don't elephants use computers? They're afraid of the mouse!\"]}}\n{'generate_joke': {'jokes': [\"Why don't dolphins use smartphones? Because they're afraid of phishing!\"]}}\n{'generate_joke': {'jokes': [\"Why don't you see penguins in Britain? Because they're afraid of Wales!\"]}}\n{'generate_joke': {'jokes': [\"Why don't lions like fast food? Because they can't catch it!\"]}}\n{'best_joke': {'best_selected_joke': \"Why don't dolphins use smartphones? Because they're afraid of phishing!\"}}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-create-and-control-loops","title":"How to create and control loops","text":"<p>When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.</p> <p>You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.</p> <p>Let's consider a simple graph with a loop to better understand how these mechanisms work.</p>"},{"location":"AIML/AgenticAI/langgraph/#summary","title":"Summary","text":"<p>When creating a loop, you can include a conditional edge that specifies a termination condition:</p> <pre><code>builder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\ndef route(state: State) -&gt; Literal[\"b\", END]:\n    if termination_condition(state):\n        return END\n    else:\n        return \"a\"\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n</code></pre> <p>To control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:</p> <pre><code>from langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke(inputs, {\"recursion_limit\": 3})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-the-graph_1","title":"Define the graph","text":"<p>Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.</p> <pre><code>import operator\nfrom typing import Annotated, Literal\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n\n# Define edges\ndef route(state: State) -&gt; Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) &lt; 7:\n        return \"b\"\n    else:\n        return END\n\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p>This architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.</p> <p>In our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.</p> <p>Invoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.</p> <pre><code>graph.invoke({\"aggregate\": []})\n</code></pre> <pre><code>Node A sees []\nNode B sees ['A']\nNode A sees ['A', 'B']\nNode B sees ['A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B']\nNode B sees ['A', 'B', 'A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B', 'A', 'B']\n</code></pre> <pre><code>{'aggregate': ['A', 'B', 'A', 'B', 'A', 'B', 'A']}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#impose-a-recursion-limit","title":"Impose a recursion limit","text":"<p>In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:</p> <pre><code>from langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n</code></pre> <pre><code>Node A sees []\nNode B sees ['A']\nNode A sees ['A', 'B']\nNode B sees ['A', 'B', 'A']\nRecursion Error\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#loops-with-branches","title":"Loops with branches","text":"<p>To better understand how the recursion limit works, let's consider a more complex example. Below we implement a loop, but one step fans out into two nodes:</p> <pre><code>import operator\nfrom typing import Annotated, Literal\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n\ndef c(state: State):\n    print(f'Node C sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\n\ndef d(state: State):\n    print(f'Node D sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\n\n\n# Define edges\ndef route(state: State) -&gt; Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) &lt; 7:\n        return \"b\"\n    else:\n        return END\n\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge([\"c\", \"d\"], \"a\")\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p>This graph looks complex, but can be conceptualized as loop of supersteps:</p> <ol> <li>Node A</li> <li>Node B</li> <li>Nodes C and D</li> <li>Node A</li> <li>... We have a loop of four supersteps, where nodes C and D are executed concurrently.</li> </ol> <p>Invoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:</p> <pre><code>result = graph.invoke({\"aggregate\": []})\n</code></pre> <pre><code>Node A sees []\nNode B sees ['A']\nNode D sees ['A', 'B']\nNode C sees ['A', 'B']\nNode A sees ['A', 'B', 'C', 'D']\nNode B sees ['A', 'B', 'C', 'D', 'A']\nNode D sees ['A', 'B', 'C', 'D', 'A', 'B']\nNode C sees ['A', 'B', 'C', 'D', 'A', 'B']\nNode A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']\n</code></pre> <p>However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:</p> <pre><code>from langgraph.errors import GraphRecursionError\n\ntry:\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n</code></pre> <pre><code>Node A sees []\nNode B sees ['A']\nNode C sees ['A', 'B']\nNode D sees ['A', 'B']\nNode A sees ['A', 'B', 'C', 'D']\nRecursion Error\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-visualize-your-graph","title":"How to visualize your graph","text":""},{"location":"AIML/AgenticAI/langgraph/#set-up-graph","title":"Set up Graph","text":"<p>You can visualize any arbitrary Graph, including StateGraph. Let's have some fun by drawing fractals :).</p> <pre><code>import random\nfrom typing import Annotated, Literal\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nclass MyNode:\n    def __init__(self, name: str):\n        self.name = name\n\n    def __call__(self, state: State):\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\n\n\ndef route(state) -&gt; Literal[\"entry_node\", \"__end__\"]:\n    if len(state[\"messages\"]) &gt; 10:\n        return \"__end__\"\n    return \"entry_node\"\n\n\ndef add_fractal_nodes(builder, current_node, level, max_level):\n    if level &gt; max_level:\n        return\n\n    # Number of nodes to create at this level\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\n    for i in range(num_nodes):\n        nm = [\"A\", \"B\", \"C\"][i]\n        node_name = f\"node_{current_node}_{nm}\"\n        builder.add_node(node_name, MyNode(node_name))\n        builder.add_edge(current_node, node_name)\n\n        # Recursively add more nodes\n        r = random.random()\n        if r &gt; 0.2 and level + 1 &lt; max_level:\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\n        elif r &gt; 0.05:\n            builder.add_conditional_edges(node_name, route, node_name)\n        else:\n            # End\n            builder.add_edge(node_name, \"__end__\")\n\n\ndef build_fractal_graph(max_level: int):\n    builder = StateGraph(State)\n    entry_point = \"entry_node\"\n    builder.add_node(entry_point, MyNode(entry_point))\n    builder.add_edge(START, entry_point)\n\n    add_fractal_nodes(builder, entry_point, 1, max_level)\n\n    # Optional: set a finish point if required\n    builder.add_edge(entry_point, END)  # or any specific node\n\n    return builder.compile()\n\n\napp = build_fractal_graph(3)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#mermaid","title":"Mermaid","text":"<p>We can also convert a graph class into Mermaid syntax.</p> <pre><code>print(app.get_graph().draw_mermaid())\n</code></pre> <pre><code>%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n    __start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n    entry_node(entry_node)\n    node_entry_node_A(node_entry_node_A)\n    node_entry_node_B(node_entry_node_B)\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\n    __end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n    __start__ --&gt; entry_node;\n    entry_node --&gt; __end__;\n    entry_node --&gt; node_entry_node_A;\n    entry_node --&gt; node_entry_node_B;\n    node_entry_node_B --&gt; node_node_entry_node_B_A;\n    node_entry_node_B --&gt; node_node_entry_node_B_B;\n    node_entry_node_B --&gt; node_node_entry_node_B_C;\n    node_entry_node_A -.-&gt; entry_node;\n    node_entry_node_A -.-&gt; __end__;\n    node_node_entry_node_B_A -.-&gt; entry_node;\n    node_node_entry_node_B_A -.-&gt; __end__;\n    node_node_entry_node_B_B -.-&gt; entry_node;\n    node_node_entry_node_B_B -.-&gt; __end__;\n    node_node_entry_node_B_C -.-&gt; entry_node;\n    node_node_entry_node_B_C -.-&gt; __end__;\n    classDef default fill:#f2f0ff,line-height:1.2\n    classDef first fill-opacity:0\n    classDef last fill:#bfb6fc\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#png","title":"PNG","text":"<p>If preferred, we could render the Graph into a .png. Here we could use three options:</p> <ul> <li>Using Mermaid.ink API (does not require additional packages)</li> <li>Using Mermaid + Pyppeteer (requires pip install pyppeteer)</li> <li>Using graphviz (which requires pip install graphviz)</li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#using-mermaidink","title":"Using Mermaid.Ink","text":"<p>By default, draw_mermaid_png() uses Mermaid.Ink's API to generate the diagram.</p> <pre><code>from IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            draw_method=MermaidDrawMethod.API,\n        )\n    )\n)\n</code></pre> <p></p>"},{"location":"AIML/AgenticAI/langgraph/#using-mermaid-pyppeteer","title":"Using Mermaid + Pyppeteer","text":"<pre><code>%%capture --no-stderr\n%pip install --quiet pyppeteer\n%pip install --quiet nest_asyncio\n</code></pre> <pre><code>import nest_asyncio\n\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            curve_style=CurveStyle.LINEAR,\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n            wrap_label_n_words=9,\n            output_file_path=None,\n            draw_method=MermaidDrawMethod.PYPPETEER,\n            background_color=\"white\",\n            padding=10,\n        )\n    )\n)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#using-graphviz","title":"Using Graphviz","text":"<pre><code>%%capture --no-stderr\n%pip install pygraphviz\n</code></pre> <pre><code>try:\n    display(Image(app.get_graph().draw_png()))\nexcept ImportError:\n    print(\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\n    )\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#fine-grained-control","title":"Fine-grained Control","text":""},{"location":"AIML/AgenticAI/langgraph/#how-to-combine-control-flow-and-state-updates-with-command","title":"How to combine control flow and state updates with Command","text":"<p>It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n</code></pre> <p>If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-add-node-retry-policies","title":"How to add node retry policies","text":"<p>There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc.</p> <pre><code>%%capture --no-stderr\n%pip install -U langgraph langchain_anthropic langchain_community\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n</code></pre> <p>In order to configure the retry policy, you have to pass the retry parameter to the add_node. The retry parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters:</p> <pre><code>from langgraph.pregel import RetryPolicy\n\nRetryPolicy()\n</code></pre> <pre><code>RetryPolicy(initial_interval=0.5, backoff_factor=2.0, max_interval=128.0, max_attempts=3, jitter=True, retry_on=&lt;function default_retry_on at 0x78b964b89940&gt;)\n</code></pre> <p>By default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:</p> <ul> <li>ValueError</li> <li>TypeError</li> <li>ArithmeticError</li> <li>ImportError</li> <li>LookupError</li> <li>NameError</li> <li>SyntaxError</li> <li>RuntimeError</li> <li>ReferenceError</li> <li>StopIteration</li> <li>StopAsyncIteration</li> <li>OSError</li> </ul> <p>In addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.</p>"},{"location":"AIML/AgenticAI/langgraph/#passing-a-retry-policy-to-a-node","title":"Passing a retry policy to a node","text":"<p>Lastly, we can pass RetryPolicy objects when we call the add_node function. In the example below we pass two different retry policies to each of our nodes:</p> <pre><code>import operator\nimport sqlite3\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph import END, StateGraph, START\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_core.messages import AIMessage\n\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\n\nmodel = ChatAnthropic(model_name=\"claude-2.1\")\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n\n\ndef query_database(state):\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\n    return {\"messages\": [AIMessage(content=query_result)]}\n\n\ndef call_model(state):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n\n# Define a new graph\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\n    \"query_database\",\n    query_database,\n    retry=RetryPolicy(retry_on=sqlite3.OperationalError),\n)\nbuilder.add_node(\"model\", call_model, retry=RetryPolicy(max_attempts=5))\nbuilder.add_edge(START, \"model\")\nbuilder.add_edge(\"model\", \"query_database\")\nbuilder.add_edge(\"query_database\", END)\n\ngraph = builder.compile()\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-return-state-before-hitting-recursion-limit","title":"How to return state before hitting recursion limit","text":"<p>Setting the graph recursion limit can help you control how long your graph will stay running, but if the recursion limit is hit your graph returns an error - which may not be ideal for all use cases. Instead you may wish to return the value of the state just before the recursion limit is hit. This how-to will show you how to do this.</p>"},{"location":"AIML/AgenticAI/langgraph/#without-returning-state","title":"Without returning state","text":"<p>We are going to define a dummy graph in this example that will always hit the recursion limit. First, we will implement it without returning the state and show that it hits the recursion limit. This graph is based on the ReAct architecture, but instead of actually making decisions and taking actions it just loops forever.</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph import START, END\n\n\nclass State(TypedDict):\n    value: str\n    action_result: str\n\n\ndef router(state: State):\n    if state[\"value\"] == \"end\":\n        return END\n    else:\n        return \"action\"\n\n\ndef decision_node(state):\n    return {\"value\": \"keep going!\"}\n\n\ndef action_node(state: State):\n    # Do your action here ...\n    return {\"action_result\": \"what a great result!\"}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"decision\", decision_node)\nworkflow.add_node(\"action\", action_node)\nworkflow.add_edge(START, \"decision\")\nworkflow.add_conditional_edges(\"decision\", router, [\"action\", END])\nworkflow.add_edge(\"action\", \"decision\")\napp = workflow.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n</code></pre> <p>Let's verify that our graph will always hit the recursion limit:</p> <pre><code>from langgraph.errors import GraphRecursionError\n\ntry:\n    app.invoke({\"value\": \"hi!\"})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n</code></pre> <pre><code>Recursion Error\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#with-returning-state","title":"With returning state","text":"<p>To avoid hitting the recursion limit, we can introduce a new key to our state called remaining_steps. It will keep track of number of steps until reaching the recursion limit. We can then check the value of remaining_steps to determine whether we should terminate the graph execution and return the state to the user without causing the RecursionError.</p> <p>To do so, we will use a special RemainingSteps annotation. Under the hood, it creates a special ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.</p> <p>Since our action node is going to always induce at least 2 extra steps to our graph (since the action node ALWAYS calls the decision node afterwards), we will use this channel to check if we are within 2 steps of the limit.</p> <p>Now, when we run our graph we should receive no errors and instead get the last value of the state before the recursion limit was hit.</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom typing import Annotated\n\nfrom langgraph.managed.is_last_step import RemainingSteps\n\n\nclass State(TypedDict):\n    value: str\n    action_result: str\n    remaining_steps: RemainingSteps\n\n\ndef router(state: State):\n    # Force the agent to end\n    if state[\"remaining_steps\"] &lt;= 2:\n        return END\n    if state[\"value\"] == \"end\":\n        return END\n    else:\n        return \"action\"\n\n\ndef decision_node(state):\n    return {\"value\": \"keep going!\"}\n\n\ndef action_node(state: State):\n    # Do your action here ...\n    return {\"action_result\": \"what a great result!\"}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"decision\", decision_node)\nworkflow.add_node(\"action\", action_node)\nworkflow.add_edge(START, \"decision\")\nworkflow.add_conditional_edges(\"decision\", router, [\"action\", END])\nworkflow.add_edge(\"action\", \"decision\")\napp = workflow.compile()\n</code></pre> <pre><code>app.invoke({\"value\": \"hi!\"})\n</code></pre> <pre><code>{'value': 'keep going!', 'action_result': 'what a great result!'}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#persistence_1","title":"Persistence","text":"<p>LangGraph Persistence makes it easy to persist state across graph runs (per-thread persistence) and across threads (cross-thread persistence). These how-to guides show how to add persistence to your graph.</p>"},{"location":"AIML/AgenticAI/langgraph/#how-to-add-thread-level-persistence-to-your-graph","title":"How to add thread-level persistence to your graph","text":"<p>Many AI applications need memory to share context across multiple interactions. In LangGraph, this kind of memory can be added to any StateGraph using thread-level persistence .</p> <p>When creating any LangGraph graph, you can set it up to persist its state by adding a checkpointer when compiling the graph:</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\ncheckpointer = MemorySaver()\ngraph.compile(checkpointer=checkpointer)\n</code></pre> <p>Note: If you need memory that is shared across multiple conversations or users (cross-thread persistence), check out this how-to guide.</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_anthropic\n</code></pre> <p>Next, we need to set API key for Anthropic (the LLM we will use).</p> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-graph","title":"Define graph","text":"<p>We will be using a single-node graph that calls a chat model.</p> <p>Let's first define the model we'll be using:</p> <pre><code>from langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n</code></pre> <p>Now we can define our StateGraph and add our model-calling node:</p> <pre><code>from typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, MessagesState, START\n\n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_edge(START, \"call_model\")\ngraph = builder.compile()\n</code></pre> <p>If we try to use this graph, the context of the conversation will not be persisted across interactions:</p> <pre><code>input_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHello Bob! It's nice to meet you. How are you doing today? Is there anything I can help you with or would you like to chat about something in particular?\n================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI apologize, but I don't have access to your personal information, including your name. I'm an AI language model designed to provide general information and answer questions to the best of my ability based on my training data. I don't have any information about individual users or their personal details. If you'd like to share your name, you're welcome to do so, but I won't be able to recall it in future conversations.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#add-persistence","title":"Add persistence","text":"<p>To add in persistence, we need to pass in a Checkpointer when compiling the graph.</p> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\ngraph = builder.compile(checkpointer=memory)\n# If you're using LangGraph Cloud or LangGraph Studio, you don't need to pass the checkpointer when compiling the graph, since it's done automatically.\n</code></pre> <p>We can now interact with the agent and see that it remembers previous messages!</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <p>You can always resume previous threads:</p> <pre><code>input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYour name is Bob, as you introduced yourself at the beginning of our conversation.\n</code></pre> <p>If we want to start a new conversation, we can pass in a different thread_id. Poof! All the memories are gone!</p> <pre><code>input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in graph.stream(\n    {\"messages\": [input_message]},\n    {\"configurable\": {\"thread_id\": \"2\"}},\n    stream_mode=\"values\",\n):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's is my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI apologize, but I don't have access to your personal information, including your name. As an AI language model, I don't have any information about individual users unless it's provided within the conversation. If you'd like to share your name, you're welcome to do so, but otherwise, I won't be able to know or guess it.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-add-thread-level-persistence-to-a-subgraph","title":"How to add thread-level persistence to a subgraph","text":"<pre><code>%%capture --no-stderr\n%pip install -U langgraph\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-the-graph-with-persistence","title":"Define the graph with persistence","text":"<p>To add persistence to a graph with subgraphs, all you need to do is pass a checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <p>Note: You shouldn't provide a checkpointer when compiling a subgraph. Instead, you must define a single checkpointer that you pass to parent_graph.compile(), and LangGraph will automatically propagate the checkpointer to the child subgraphs. If you pass the checkpointer to the subgraph.compile(), it will simply be ignored. This also applies when you add a node function that invokes the subgraph.</p> <p>Let's define a simple graph with a single subgraph node to show how to do this.</p> <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict\n\n\n# subgraph\n\n\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# parent graph\n\n\nclass State(TypedDict):\n    foo: str\n\n\ndef node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\n# note that we're adding the compiled subgraph as a node to the parent graph\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\n</code></pre> <p>We can now compile the graph with an in-memory checkpointer (MemorySaver).</p> <pre><code>checkpointer = MemorySaver()\n# You must only pass checkpointer when compiling the parent graph.\n# LangGraph will automatically propagate the checkpointer to the child subgraphs.\ngraph = builder.compile(checkpointer=checkpointer)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#verify-persistence-works","title":"Verify persistence works","text":"<p>Let's now run the graph and inspect the persisted state for both the parent graph and the subgraph to verify that persistence works. We should expect to see the final execution results for both the parent and subgraph in state.values.</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\n</code></pre> <pre><code>for _, chunk in graph.stream({\"foo\": \"foo\"}, config, subgraphs=True):\n    print(chunk)\n</code></pre> <pre><code>{'node_1': {'foo': 'hi! foo'}}\n{'subgraph_node_1': {'bar': 'bar'}}\n{'subgraph_node_2': {'foo': 'hi! foobar'}}\n{'node_2': {'foo': 'hi! foobar'}}\n</code></pre> <p>We can now view the parent graph state by calling graph.get_state() with the same config that we used to invoke the graph.</p> <pre><code>graph.get_state(config).values\n</code></pre> <pre><code>{'foo': 'hi! foobar'}\n</code></pre> <p>To view the subgraph state, we need to do two things:</p> <ol> <li>Find the most recent config value for the subgraph</li> <li>Use graph.get_state() to retrieve that value for the most recent subgraph config.</li> </ol> <p>To find the correct config, we can examine the state history from the parent graph and find the state snapshot before we return results from node_2 (the node with subgraph):</p> <pre><code>state_with_subgraph = [\n    s for s in graph.get_state_history(config) if s.next == (\"node_2\",)\n][0]\n</code></pre> <p>The state snapshot will include the list of tasks to be executed next. When using subgraphs, the tasks will contain the config that we can use to retrieve the subgraph state:</p> <pre><code>subgraph_config = state_with_subgraph.tasks[0].state\nsubgraph_config\n</code></pre> <pre><code>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': 'node_2:6ef111a6-f290-7376-0dfc-a4152307bc5b'}}\n</code></pre> <pre><code>graph.get_state(subgraph_config).values\n</code></pre> <pre><code>{'foo': 'hi! foobar', 'bar': 'bar'}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-add-cross-thread-persistence-to-your-graph","title":"How to add cross-thread persistence to your graph","text":"<p>In the previous guide you learned how to persist graph state across multiple interactions on a single thread. LangGraph also allows you to persist data across multiple threads. For instance, you can store information about users (their names or preferences) in a shared memory and reuse them in the new conversational threads.</p> <p>In this guide, we will show how to construct and use a graph that has a shared memory implemented using the Store interface.</p> <pre><code>%%capture --no-stderr\n%pip install -U langchain_openai langgraph\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n_set_env(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-store","title":"Define store","text":"<p>In this example we will create a graph that will be able to retrieve information about a user's preferences. We will do so by defining an InMemoryStore - an object that can store data in memory and query that data. We will then pass the store object when compiling the graph. This allows each node in the graph to access the store: when you define node functions, you can define store keyword argument, and LangGraph will automatically pass the store object you compiled the graph with.</p> <p>When storing objects using the Store interface you define two things:</p> <ul> <li>the namespace for the object, a tuple (similar to directories)</li> <li>the object key (similar to filenames)</li> </ul> <p>In our example, we'll be using (\"memories\", ) as namespace and random UUID as key for each new memory. <p>Importantly, to determine the user, we will be passing user_id via the config keyword argument of the node function.</p> <p>Let's first define an InMemoryStore already populated with some memories about the users.</p> <pre><code>from langgraph.store.memory import InMemoryStore\nfrom langchain_openai import OpenAIEmbeddings\n\nin_memory_store = InMemoryStore(\n    index={\n        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n        \"dims\": 1536,\n    }\n)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#create-graph","title":"Create graph","text":"<pre><code>import uuid\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.store.base import BaseStore\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n\n\n# NOTE: we're passing the Store param to the node --\n# this is the Store we compile the graph with\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    user_id = config[\"configurable\"][\"user_id\"]\n    namespace = (\"memories\", user_id)\n    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # Store new memories if the user asks the model to remember\n    last_message = state[\"messages\"][-1]\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n    response = model.invoke(\n        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n    )\n    return {\"messages\": response}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_edge(START, \"call_model\")\n\n# NOTE: we're passing the store object here when compiling the graph\ngraph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)\n# If you're using LangGraph Cloud or LangGraph Studio, you don't need to pass the store or checkpointer when compiling the graph, since it's done automatically.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#run-the-graph","title":"Run the graph!","text":"<p>Now let's specify a user ID in the config and tell the model our name:</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nHi! Remember: my name is Bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHello Bob! It's nice to meet you. I'll remember that your name is Bob. How can I assist you today?\n</code></pre> <pre><code>config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat is my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYour name is Bob.\n</code></pre> <p>We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:</p> <pre><code>for memory in in_memory_store.search((\"memories\", \"1\")):\n    print(memory.value)\n</code></pre> <pre><code>{'data': 'User name is Bob'}\n</code></pre> <p>Let's now run the graph for another user to verify that the memories about the first user are self contained:</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat is my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI apologize, but I don't have any information about your name. As an AI assistant, I don't have access to personal information about users unless it has been specifically shared in our conversation. If you'd like, you can tell me your name and I'll be happy to use it in our discussion.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-use-postgres-checkpointer-for-persistence","title":"How to use Postgres checkpointer for persistence","text":"<p>When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions.</p> <p>This how-to guide shows how to use Postgres as the backend for persisting checkpoint state using the langgraph-checkpoint-postgres library.</p> <p>For demonstration purposes we add persistence to the pre-built create react agent.</p> <p>In general, you can add a checkpointer to any custom graph that you build like this:</p> <pre><code>from langgraph.graph import StateGraph\n\nbuilder = StateGraph(....)\n# ... define the graph\ncheckpointer = # postgres checkpointer (see examples below)\ngraph = builder.compile(checkpointer=checkpointer)\n...\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#setup_1","title":"Setup","text":"<p>You will need access to a postgres instance.  Next, let's install the required packages and set our API keys</p> <pre><code>%%capture --no-stderr\n%pip install -U psycopg psycopg-pool langgraph langgraph-checkpoint-postgres\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-model-and-tools-for-the-graph","title":"Define model and tools for the graph","text":"<pre><code>from typing import Literal\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\nmodel = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#use-sync-connection","title":"Use sync connection","text":"<p>This sets up a synchronous connection to the database.</p> <p>Synchronous connections execute operations in a blocking manner, meaning each operation waits for completion before moving to the next one. The DB_URI is the database connection URI, with the protocol used for connecting to a PostgreSQL database, authentication, and host where database is running. The connection_kwargs dictionary defines additional parameters for the database connection.</p> <pre><code>DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n</code></pre> <pre><code>connection_kwargs = {\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#with-a-connection-pool","title":"With a connection pool","text":"<p>This manages a pool of reusable database connections: - Advantages: Efficient resource utilization, improved performance for frequent connections - Best for: Applications with many short-lived database operations</p> <pre><code>from psycopg_pool import ConnectionPool\n\nwith ConnectionPool(\n    # Example configuration\n    conninfo=DB_URI,\n    max_size=20,\n    kwargs=connection_kwargs,\n) as pool:\n    checkpointer = PostgresSaver(pool)\n\n    # NOTE: you need to call .setup() the first time you're using your checkpointer\n    checkpointer.setup()\n\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n    res = graph.invoke({\"messages\": [(\"human\", \"what's the weather in sf\")]}, config)\n    checkpoint = checkpointer.get(config)\n</code></pre> <pre><code>res\n</code></pre> <pre><code>{'messages': [HumanMessage(content=\"what's the weather in sf\", id='735b7deb-b0fe-4ad5-8920-2a3c69bbe9f7'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lJHMDYgfgRdiEAGfFsEhqqKV', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c56b3e04-08a9-4a59-b3f5-ee52d0ef0656-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_lJHMDYgfgRdiEAGfFsEhqqKV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}),\n  ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='0644bf7b-4d1b-4ebe-afa1-d2169ccce582', tool_call_id='call_lJHMDYgfgRdiEAGfFsEhqqKV'),\n  AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed9b8d0-9b50-4b87-b3a2-9860f51e9fd1-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}\n  ```\n\n  ```\n  checkpoint\n  ```\n\n  ```\n  {'v': 1,\n 'id': '1ef559b7-3b19-6ce8-8003-18d0f60634be',\n 'ts': '2024-08-08T15:32:42.108605+00:00',\n 'current_tasks': {},\n 'pending_sends': [],\n 'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8',\n   'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'},\n  'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'},\n  '__input__': {},\n  '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}},\n 'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af',\n  'tools': '00000000000000000000000000000005.',\n  'messages': '00000000000000000000000000000005.b9adc75836c78af94af1d6811340dd13',\n  '__start__': '00000000000000000000000000000002.',\n  'start:agent': '00000000000000000000000000000003.',\n  'branch:agent:should_continue:tools': '00000000000000000000000000000004.'},\n 'channel_values': {'agent': 'agent',\n  'messages': [HumanMessage(content=\"what's the weather in sf\", id='735b7deb-b0fe-4ad5-8920-2a3c69bbe9f7'),\n   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lJHMDYgfgRdiEAGfFsEhqqKV', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c56b3e04-08a9-4a59-b3f5-ee52d0ef0656-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_lJHMDYgfgRdiEAGfFsEhqqKV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}),\n   ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='0644bf7b-4d1b-4ebe-afa1-d2169ccce582', tool_call_id='call_lJHMDYgfgRdiEAGfFsEhqqKV'),\n   AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed9b8d0-9b50-4b87-b3a2-9860f51e9fd1-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}\n   ```\n\n   ## With a connection\n   This creates a single, dedicated connection to the database: - Advantages: Simple to use, suitable for longer transactions - Best for: Applications with fewer, longer-lived database operations\n\n   ```\n   from psycopg import Connection\n\n\nwith Connection.connect(DB_URI, **connection_kwargs) as conn:\n    checkpointer = PostgresSaver(conn)\n    # NOTE: you need to call .setup() the first time you're using your checkpointer\n    # checkpointer.setup()\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"2\"}}\n    res = graph.invoke({\"messages\": [(\"human\", \"what's the weather in sf\")]}, config)\n\n    checkpoint_tuple = checkpointer.get_tuple(config)\n</code></pre> <pre><code>checkpoint_tuple\n</code></pre> <pre><code>CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4650-6bfc-8003-1c5488f19318'}}, checkpoint={'v': 1, 'id': '1ef559b7-4650-6bfc-8003-1c5488f19318', 'ts': '2024-08-08T15:32:43.284551+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}}, 'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'tools': '00000000000000000000000000000005.', 'messages': '00000000000000000000000000000005.af9f229d2c4e14f4866eb37f72ec39f6', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in sf\", id='7a14f96c-2d88-454f-9520-0e0287a4abbb'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NcL4dBTYu4kSPGMKdxztdpjN', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-39adbf2c-36ef-40f6-9cad-8e1f8167fc19-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_NcL4dBTYu4kSPGMKdxztdpjN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='c9f82354-3225-40a8-bf54-81f3e199043b', tool_call_id='call_NcL4dBTYu4kSPGMKdxztdpjN'), AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-83888be3-d681-42ca-ad67-e2f5ee8550de-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}, metadata={'step': 3, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 94, 'prompt_tokens': 84, 'completion_tokens': 10}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_48196bc67a'}, id='run-83888be3-d681-42ca-ad67-e2f5ee8550de-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4087-681a-8002-88a5738f76f1'}}, pending_writes=[])\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#with-a-connection-string","title":"With a connection string","text":"<p>This creates a connection based on a connection string: - Advantages: Simplicity, encapsulates connection details - Best for: Quick setup or when connection details are provided as a string</p> <pre><code>with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"3\"}}\n    res = graph.invoke({\"messages\": [(\"human\", \"what's the weather in sf\")]}, config)\n\n    checkpoint_tuples = list(checkpointer.list(config))\n</code></pre> <pre><code>checkpoint_tuples\n</code></pre> <p><pre><code>[CheckpointTuple(config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-5024-6476-8003-cf0a750e6b37'}}, checkpoint={'v': 1, 'id': '1ef559b7-5024-6476-8003-cf0a750e6b37', 'ts': '2024-08-08T15:32:44.314900+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}}, 'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'tools': '00000000000000000000000000000005.', 'messages': '00000000000000000000000000000005.3f8b8d9923575b911e17157008ab75ac', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in sf\", id='5bf79d15-6332-4bf5-89bd-ee192b31ed84'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2958adc7-f6a4-415d-ade1-5ee77e0b9276-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='cac4f90a-dc3e-4bfa-940f-1c630289a583', tool_call_id='call_9y3q1BiwW7zGh2gk2faInTRk'), AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-97d3fb7a-3d2e-4090-84f4-dafdfe44553f-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}, metadata={'step': 3, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 94, 'prompt_tokens': 84, 'completion_tokens': 10}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_48196bc67a'}, id='run-97d3fb7a-3d2e-4090-84f4-dafdfe44553f-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}}, parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4b3d-6430-8002-b5c99d2eb4db'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4b3d-6430-8002-b5c99d2eb4db'}}, checkpoint={'v': 1, 'id': '1ef559b7-4b3d-6430-8002-b5c99d2eb4db', 'ts': '2024-08-08T15:32:43.800857+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}}, 'channel_versions': {'agent': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'messages': '00000000000000000000000000000004.1195f50946feaedb0bae1fdbfadc806b', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'tools': 'tools', 'messages': [HumanMessage(content=\"what's the weather in sf\", id='5bf79d15-6332-4bf5-89bd-ee192b31ed84'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2958adc7-f6a4-415d-ade1-5ee77e0b9276-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='cac4f90a-dc3e-4bfa-940f-1c630289a583', tool_call_id='call_9y3q1BiwW7zGh2gk2faInTRk')]}}, metadata={'step': 2, 'source': 'loop', 'writes': {'tools': {'messages': [ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='cac4f90a-dc3e-4bfa-940f-1c630289a583', tool_call_id='call_9y3q1BiwW7zGh2gk2faInTRk')]}}}, parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4b30-6078-8001-eaf8c9bd8844'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-4b30-6078-8001-eaf8c9bd8844'}}, checkpoint={'v': 1, 'id': '1ef559b7-4b30-6078-8001-eaf8c9bd8844', 'ts': '2024-08-08T15:32:43.795440+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}}, 'channel_versions': {'agent': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af', 'messages': '00000000000000000000000000000003.bab5fb3a70876f600f5f2fd46945ce5f', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in sf\", id='5bf79d15-6332-4bf5-89bd-ee192b31ed84'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2958adc7-f6a4-415d-ade1-5ee77e0b9276-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71})], 'branch:agent:should_continue:tools': 'agent'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': '{\"city\":\"sf\"}'}}]}, response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 71, 'prompt_tokens': 57, 'completion_tokens': 14}, 'finish_reason': 'tool_calls', 'system_fingerprint': 'fp_507c9469a1'}, id='run-2958adc7-f6a4-415d-ade1-5ee77e0b9276-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_9y3q1BiwW7zGh2gk2faInTRk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71})]}}}, parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-46d7-6116-8000-8976b7c89a2f'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-46d7-6116-8000-8976b7c89a2f'}}, checkpoint={'v': 1, 'id': '1ef559b7-46d7-6116-8000-8976b7c89a2f', 'ts': '2024-08-08T15:32:43.339573+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}}, 'channel_versions': {'messages': '00000000000000000000000000000002.ba0c90d32863686481f7fe5eab9ecdf0', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in sf\", id='5bf79d15-6332-4bf5-89bd-ee192b31ed84')], 'start:agent': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None}, parent_config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-46ce-6c64-bfff-ef7fe2663573'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '3', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-46ce-6c64-bfff-ef7fe2663573'}}, checkpoint={'v': 1, 'id': '1ef559b7-46ce-6c64-bfff-ef7fe2663573', 'ts': '2024-08-08T15:32:43.336188+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}, 'channel_values': {'__start__': {'messages': [['human', \"what's the weather in sf\"]]}}}, metadata={'step': -1, 'source': 'input', 'writes': {'messages': [['human', \"what's the weather in sf\"]]}}, parent_config=None, pending_writes=None)]\n ```\n\n ## Use async connection\n This sets up an asynchronous connection to the database.\n\nAsync connections allow non-blocking database operations. This means other parts of your application can continue running while waiting for database operations to complete. It's particularly useful in high-concurrency scenarios or when dealing with I/O-bound operations.\n\n\n## With a connection pool\n</code></pre> from psycopg_pool import AsyncConnectionPool</p> <p>async with AsyncConnectionPool(     # Example configuration     conninfo=DB_URI,     max_size=20,     kwargs=connection_kwargs, ) as pool:     checkpointer = AsyncPostgresSaver(pool)</p> <pre><code># NOTE: you need to call .setup() the first time you're using your checkpointer\nawait checkpointer.setup()\n\ngraph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\nconfig = {\"configurable\": {\"thread_id\": \"4\"}}\nres = await graph.ainvoke(\n    {\"messages\": [(\"human\", \"what's the weather in nyc\")]}, config\n)\n\ncheckpoint = await checkpointer.aget(config)\n</code></pre> <p><code></code> checkpoint <code></code> {'v': 1,  'id': '1ef559b7-5cc9-6460-8003-8655824c0944',  'ts': '2024-08-08T15:32:45.640793+00:00',  'current_tasks': {},  'pending_sends': [],  'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8',    'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'},   'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'},   'input': {},   'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}},  'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af',   'tools': '00000000000000000000000000000005.',   'messages': '00000000000000000000000000000005.d869fc7231619df0db74feed624efe41',   'start': '00000000000000000000000000000002.',   'start:agent': '00000000000000000000000000000003.',   'branch:agent:should_continue:tools': '00000000000000000000000000000004.'},  'channel_values': {'agent': 'agent',   'messages': [HumanMessage(content=\"what's the weather in nyc\", id='d883b8a0-99de-486d-91a2-bcfa7f25dc05'),    AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_H6TAYfyd6AnaCrkQGs6Q2fVp', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-6f542f84-ad73-444c-8ef7-b5ea75a2e09b-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_H6TAYfyd6AnaCrkQGs6Q2fVp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}),    ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='c0e52254-77a4-4ea9-a2b7-61dd2d65ec68', tool_call_id='call_H6TAYfyd6AnaCrkQGs6Q2fVp'),    AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-977140d4-7582-40c3-b2b6-31b542c430a3-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}    ```</p>"},{"location":"AIML/AgenticAI/langgraph/#with-a-connection","title":"With a connection","text":"<pre><code>   from psycopg import AsyncConnection\n\nasync with await AsyncConnection.connect(DB_URI, **connection_kwargs) as conn:\n    checkpointer = AsyncPostgresSaver(conn)\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"5\"}}\n    res = await graph.ainvoke(\n        {\"messages\": [(\"human\", \"what's the weather in nyc\")]}, config\n    )\n    checkpoint_tuple = await checkpointer.aget_tuple(config)\n</code></pre> <pre><code>checkpoint_tuple\n</code></pre> <pre><code>CheckpointTuple(config={'configurable': {'thread_id': '5', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-65b4-60ca-8003-1ef4b620559a'}}, checkpoint={'v': 1, 'id': '1ef559b7-65b4-60ca-8003-1ef4b620559a', 'ts': '2024-08-08T15:32:46.575814+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'tools': '00000000000000000000000000000005.', 'messages': '00000000000000000000000000000005.1557a6006d58f736d5cb2dd5c5f10111', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in nyc\", id='935e7732-b288-49bd-9ec2-1f7610cc38cb'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_94KtjtPmsiaj7T8yXvL7Ef31', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-790c929a-7982-49e7-af67-2cbe4a86373b-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_94KtjtPmsiaj7T8yXvL7Ef31', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='b2dc1073-abc4-4492-8982-434a7e32e445', tool_call_id='call_94KtjtPmsiaj7T8yXvL7Ef31'), AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-7e8a7f16-d8e1-457a-89f3-192102396449-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}, metadata={'step': 3, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 97, 'prompt_tokens': 88, 'completion_tokens': 9}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_48196bc67a'}, id='run-7e8a7f16-d8e1-457a-89f3-192102396449-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}}, parent_config={'configurable': {'thread_id': '5', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-62ae-6128-8002-c04af82bcd41'}}, pending_writes=[])\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#with-a-connection-string_1","title":"With a connection string","text":"<pre><code>async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"6\"}}\n    res = await graph.ainvoke(\n        {\"messages\": [(\"human\", \"what's the weather in nyc\")]}, config\n    )\n    checkpoint_tuples = [c async for c in checkpointer.alist(config)]\n</code></pre> <pre><code>checkpoint_tuples\n</code></pre> <p><pre><code>[CheckpointTuple(config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-723c-67de-8003-63bd4eab35af'}}, checkpoint={'v': 1, 'id': '1ef559b7-723c-67de-8003-63bd4eab35af', 'ts': '2024-08-08T15:32:47.890003+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'channel_versions': {'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'tools': '00000000000000000000000000000005.', 'messages': '00000000000000000000000000000005.b6fe2a26011590cfe8fd6a39151a9e92', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in nyc\", id='977ddb90-9991-44cb-9f73-361c6dd21396'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-47b10c48-4db3-46d8-b4fa-e021818e01c5-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='798c520f-4f9a-4f6d-a389-da721eb4d4ce', tool_call_id='call_QIFCuh4zfP9owpjToycJiZf7'), AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a34e05d-8bcf-41ad-adc3-715919fde64c-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}, metadata={'step': 3, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 97, 'prompt_tokens': 88, 'completion_tokens': 9}, 'finish_reason': 'stop', 'system_fingerprint': 'fp_48196bc67a'}, id='run-4a34e05d-8bcf-41ad-adc3-715919fde64c-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}}, parent_config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6bf5-63c6-8002-ed990dbbc96e'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6bf5-63c6-8002-ed990dbbc96e'}}, checkpoint={'v': 1, 'id': '1ef559b7-6bf5-63c6-8002-ed990dbbc96e', 'ts': '2024-08-08T15:32:47.231667+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'channel_versions': {'agent': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8', 'messages': '00000000000000000000000000000004.c9074f2a41f05486b5efb86353dc75c0', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.'}, 'channel_values': {'tools': 'tools', 'messages': [HumanMessage(content=\"what's the weather in nyc\", id='977ddb90-9991-44cb-9f73-361c6dd21396'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-47b10c48-4db3-46d8-b4fa-e021818e01c5-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='798c520f-4f9a-4f6d-a389-da721eb4d4ce', tool_call_id='call_QIFCuh4zfP9owpjToycJiZf7')]}}, metadata={'step': 2, 'source': 'loop', 'writes': {'tools': {'messages': [ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='798c520f-4f9a-4f6d-a389-da721eb4d4ce', tool_call_id='call_QIFCuh4zfP9owpjToycJiZf7')]}}}, parent_config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6be0-6926-8001-1a8ce73baf9e'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6be0-6926-8001-1a8ce73baf9e'}}, checkpoint={'v': 1, 'id': '1ef559b7-6be0-6926-8001-1a8ce73baf9e', 'ts': '2024-08-08T15:32:47.223198+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'channel_versions': {'agent': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af', 'messages': '00000000000000000000000000000003.097b5407d709b297591f1ef5d50c8368', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000003.', 'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, 'channel_values': {'agent': 'agent', 'messages': [HumanMessage(content=\"what's the weather in nyc\", id='977ddb90-9991-44cb-9f73-361c6dd21396'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-47b10c48-4db3-46d8-b4fa-e021818e01c5-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73})], 'branch:agent:should_continue:tools': 'agent'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'type': 'function', 'function': {'name': 'get_weather', 'arguments': '{\"city\":\"nyc\"}'}}]}, response_metadata={'logprobs': None, 'model_name': 'gpt-4o-mini-2024-07-18', 'token_usage': {'total_tokens': 73, 'prompt_tokens': 58, 'completion_tokens': 15}, 'finish_reason': 'tool_calls', 'system_fingerprint': 'fp_48196bc67a'}, id='run-47b10c48-4db3-46d8-b4fa-e021818e01c5-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_QIFCuh4zfP9owpjToycJiZf7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73})]}}}, parent_config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-663d-60b4-8000-10a8922bffbf'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-663d-60b4-8000-10a8922bffbf'}}, checkpoint={'v': 1, 'id': '1ef559b7-663d-60b4-8000-10a8922bffbf', 'ts': '2024-08-08T15:32:46.631935+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'channel_versions': {'messages': '00000000000000000000000000000002.2a79db8da664e437bdb25ea804457ca7', '__start__': '00000000000000000000000000000002.', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='977ddb90-9991-44cb-9f73-361c6dd21396')], 'start:agent': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None}, parent_config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6637-6d4e-bfff-6cecf690c3cb'}}, pending_writes=None),\n CheckpointTuple(config={'configurable': {'thread_id': '6', 'checkpoint_ns': '', 'checkpoint_id': '1ef559b7-6637-6d4e-bfff-6cecf690c3cb'}}, checkpoint={'v': 1, 'id': '1ef559b7-6637-6d4e-bfff-6cecf690c3cb', 'ts': '2024-08-08T15:32:46.629806+00:00', 'current_tasks': {}, 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'channel_values': {'__start__': {'messages': [['human', \"what's the weather in nyc\"]]}}}, metadata={'step': -1, 'source': 'input', 'writes': {'messages': [['human', \"what's the weather in nyc\"]]}}, parent_config=None, pending_writes=None)]\n ```\n\n # How to use MongoDB checkpointer for persistence\n\n When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions.\n\nThis reference implementation shows how to use MongoDB as the backend for persisting checkpoint state using the langgraph-checkpoint-mongodb library.\n\nFor demonstration purposes we add persistence to a prebuilt ReAct agent.\n\nIn general, you can add a checkpointer to any custom graph that you build like this:\n</code></pre> from langgraph.graph import StateGraph</p> <p>builder = StateGraph(...)</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-graph_2","title":"... define the graph","text":"<p>checkpointer = # mongodb checkpointer (see examples below) graph = builder.compile(checkpointer=checkpointer) ... <pre><code>## Setup\nTo use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don't already have one.\n\nNext, let's install the required packages and set our API keys\n</code></pre> %%capture --no-stderr %pip install -U pymongo langgraph langgraph-checkpoint-mongodb <pre><code>\n</code></pre> import getpass import os</p> <p>def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")</p> <p>_set_env(\"OPENAI_API_KEY\") <pre><code>\n</code></pre> OPENAI_API_KEY:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 <pre><code>## Define model and tools for the graph\n</code></pre> from typing import Literal</p> <p>from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent</p> <p>@tool def get_weather(city: Literal[\"nyc\", \"sf\"]):     \"\"\"Use this to get weather information.\"\"\"     if city == \"nyc\":         return \"It might be cloudy in nyc\"     elif city == \"sf\":         return \"It's always sunny in sf\"     else:         raise AssertionError(\"Unknown city\")</p> <p>tools = [get_weather] model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) <pre><code>## MongoDB checkpointer usage\n## With a connection string\n\nThis creates a connection to MongoDB directly using the connection string of your cluster. This is ideal for use in scripts, one-off operations and short-lived applications.\n</code></pre> from langgraph.checkpoint.mongodb import MongoDBSaver</p> <p>MONGODB_URI = \"localhost:27017\"  # replace this with your connection string</p> <p>with MongoDBSaver.from_conn_string(MONGODB_URI) as checkpointer:     graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)     config = {\"configurable\": {\"thread_id\": \"1\"}}     response = graph.invoke(         {\"messages\": [(\"human\", \"what's the weather in sf\")]}, config     ) <pre><code>\n</code></pre> response <pre><code>\n</code></pre> {'messages': [HumanMessage(content=\"what's the weather in sf\", additional_kwargs={}, response_metadata={}, id='729afd6a-fdc0-4192-a255-1dac065c79b2'),   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YqaO8oU3BhGmIz9VHTxqGyyN', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_39a40c96a0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b45c0c12-c68e-4392-92dd-5d325d0a9f60-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_YqaO8oU3BhGmIz9VHTxqGyyN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),   ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='0c72eb29-490b-44df-898f-8454c314eac1', tool_call_id='call_YqaO8oU3BhGmIz9VHTxqGyyN'),   AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_818c284075', 'finish_reason': 'stop', 'logprobs': None}, id='run-33f54c91-0ba9-48b7-9b25-5a972bbdeea9-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}   <pre><code>## Using the MongoDB client\nThis creates a connection to MongoDB using the MongoDB client. This is ideal for long-running applications since it allows you to reuse the client instance for multiple database operations without needing to reinitialize the connection each time.\n</code></pre>   from pymongo import MongoClient</p> <p>mongodb_client = MongoClient(MONGODB_URI)</p> <p>checkpointer = MongoDBSaver(mongodb_client) graph = create_react_agent(model, tools=tools, checkpointer=checkpointer) config = {\"configurable\": {\"thread_id\": \"2\"}} response = graph.invoke({\"messages\": [(\"user\", \"What's the weather in sf?\")]}, config) <pre><code>\n</code></pre> response <pre><code>\n</code></pre> {'messages': [HumanMessage(content=\"What's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='4ce68bee-a843-4b08-9c02-7a0e3b010110'),   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MvGxq9IU9wvW9mfYKSALHtGu', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9712c5a4-376c-4812-a0c4-1b522334a59d-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_MvGxq9IU9wvW9mfYKSALHtGu', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),   ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='b4eed38d-bcaf-4497-ad08-f21ccd6a8c30', tool_call_id='call_MvGxq9IU9wvW9mfYKSALHtGu'),   AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c6c4ad75-89ef-4b4f-9ca4-bd52ccb0729b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} <pre><code>\n</code></pre></p>"},{"location":"AIML/AgenticAI/langgraph/#retrieve-the-latest-checkpoint-for-the-given-thread-id","title":"Retrieve the latest checkpoint for the given thread ID","text":""},{"location":"AIML/AgenticAI/langgraph/#to-retrieve-a-specific-checkpoint-pass-the-checkpoint_id-in-the-config","title":"To retrieve a specific checkpoint, pass the checkpoint_id in the config","text":"<p>checkpointer.get_tuple(config)  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efb8c75-9262-68b4-8003-1ac1ef198757'}}, checkpoint={'v': 1, 'ts': '2024-12-12T20:26:20.545003+00:00', 'id': '1efb8c75-9262-68b4-8003-1ac1ef198757', 'channel_values': {'messages': [HumanMessage(content=\"What's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='4ce68bee-a843-4b08-9c02-7a0e3b010110'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MvGxq9IU9wvW9mfYKSALHtGu', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9712c5a4-376c-4812-a0c4-1b522334a59d-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_MvGxq9IU9wvW9mfYKSALHtGu', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='b4eed38d-bcaf-4497-ad08-f21ccd6a8c30', tool_call_id='call_MvGxq9IU9wvW9mfYKSALHtGu'), AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c6c4ad75-89ef-4b4f-9ca4-bd52ccb0729b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'agent': 'agent'}, 'channel_versions': {'start': 2, 'messages': 5, 'start:agent': 3, 'agent': 5, 'branch:agent:should_continue:tools': 4, 'tools': 5}, 'versions_seen': {'input': {}, 'start': {'start': 1}, 'agent': {'start:agent': 2, 'tools': 4}, 'tools': {'branch:agent:should_continue:tools': 3}}, 'pending_sends': []}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-c6c4ad75-89ef-4b4f-9ca4-bd52ccb0729b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '2', 'step': 3, 'parents': {}}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efb8c75-8d89-6ffe-8002-84a4312c4fed'}}, pending_writes=[]) </p>"},{"location":"AIML/AgenticAI/langgraph/#remember-to-close-the-connection-after-youre-done","title":"Remember to close the connection after you're done","text":"<p>mongodb_client.close() <pre><code>## Using an async connection\nThis creates a short-lived asynchronous connection to MongoDB.\n\nAsync connections allow non-blocking database operations. This means other parts of your application can continue running while waiting for database operations to complete. It's particularly useful in high-concurrency scenarios or when dealing with I/O-bound operations.\n</code></pre> from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver</p> <p>async with AsyncMongoDBSaver.from_conn_string(MONGODB_URI) as checkpointer:     graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)     config = {\"configurable\": {\"thread_id\": \"3\"}}     response = await graph.ainvoke(         {\"messages\": [(\"user\", \"What's the weather in sf?\")]}, config     ) <pre><code>\n</code></pre> response <pre><code>\n</code></pre> {'messages': [HumanMessage(content=\"What's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='fed70fe6-1b2e-4481-9bfc-063df3b587dc'),   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_miRiF3vPQv98wlDHl6CeRxBy', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7f2d5153-973e-4a9e-8b71-a77625c342cf-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_miRiF3vPQv98wlDHl6CeRxBy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),   ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='49035e8e-8aee-4d9d-88ab-9a1bc10ecbd3', tool_call_id='call_miRiF3vPQv98wlDHl6CeRxBy'),   AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-9403d502-391e-4407-99fd-eec8ed184e50-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} <pre><code>## Using the async MongoDB client\nThis routes connections to MongoDB through an asynchronous MongoDB client.\n</code></pre> from pymongo import AsyncMongoClient</p> <p>async_mongodb_client = AsyncMongoClient(MONGODB_URI)</p> <p>checkpointer = AsyncMongoDBSaver(async_mongodb_client) graph = create_react_agent(model, tools=tools, checkpointer=checkpointer) config = {\"configurable\": {\"thread_id\": \"4\"}} response = await graph.ainvoke(     {\"messages\": [(\"user\", \"What's the weather in sf?\")]}, config ) <pre><code>\n</code></pre> response <pre><code>\n</code></pre> {'messages': [HumanMessage(content=\"What's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='58282e2b-4cc1-40a1-8e65-420a2177bbd6'),   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SJFViVHl1tYTZDoZkNN3ePhJ', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-131af8c1-d388-4d7f-9137-da59ebd5fefd-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_SJFViVHl1tYTZDoZkNN3ePhJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),   ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='6090a56f-177b-4d3f-b16a-9c05f23800e3', tool_call_id='call_SJFViVHl1tYTZDoZkNN3ePhJ'),   AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ff5ddf5-6e13-4126-8df9-81c8638355fc-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]} <pre><code>\n</code></pre></p>"},{"location":"AIML/AgenticAI/langgraph/#retrieve-the-latest-checkpoint-for-the-given-thread-id_1","title":"Retrieve the latest checkpoint for the given thread ID","text":""},{"location":"AIML/AgenticAI/langgraph/#to-retrieve-a-specific-checkpoint-pass-the-checkpoint_id-in-the-config_1","title":"To retrieve a specific checkpoint, pass the checkpoint_id in the config","text":"<p>latest_checkpoint = await checkpointer.aget_tuple(config) print(latest_checkpoint)  CheckpointTuple(config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efb8c76-21f4-6d10-8003-9496e1754e93'}}, checkpoint={'v': 1, 'ts': '2024-12-12T20:26:35.599560+00:00', 'id': '1efb8c76-21f4-6d10-8003-9496e1754e93', 'channel_values': {'messages': [HumanMessage(content=\"What's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='58282e2b-4cc1-40a1-8e65-420a2177bbd6'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SJFViVHl1tYTZDoZkNN3ePhJ', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-131af8c1-d388-4d7f-9137-da59ebd5fefd-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_SJFViVHl1tYTZDoZkNN3ePhJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='6090a56f-177b-4d3f-b16a-9c05f23800e3', tool_call_id='call_SJFViVHl1tYTZDoZkNN3ePhJ'), AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ff5ddf5-6e13-4126-8df9-81c8638355fc-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'agent': 'agent'}, 'channel_versions': {'start': 2, 'messages': 5, 'start:agent': 3, 'agent': 5, 'branch:agent:should_continue:tools': 4, 'tools': 5}, 'versions_seen': {'input': {}, 'start': {'start': 1}, 'agent': {'start:agent': 2, 'tools': 4}, 'tools': {'branch:agent:should_continue:tools': 3}}, 'pending_sends': []}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in San Francisco is always sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ff5ddf5-6e13-4126-8df9-81c8638355fc-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '4', 'step': 3, 'parents': {}}, parent_config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efb8c76-1c6c-6474-8002-9c2595cd481c'}}, pending_writes=[]) </p>"},{"location":"AIML/AgenticAI/langgraph/#remember-to-close-the-connection-after-youre-done_1","title":"Remember to close the connection after you're done","text":"<p>await async_mongodb_client.close() <pre><code>## How to create a custom checkpointer using Redis\nWhen creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions.\n\nThis reference implementation shows how to use Redis as the backend for persisting checkpoint state. Make sure that you have Redis running on port 6379 for going through this guide.\n\nFor demonstration purposes we add persistence to the pre-built create react agent.\n\nIn general, you can add a checkpointer to any custom graph that you build like this:\n</code></pre> from langgraph.graph import StateGraph</p> <p>builder = StateGraph(....)</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-graph_3","title":"... define the graph","text":"<p>checkpointer = # redis checkpointer (see examples below) graph = builder.compile(checkpointer=checkpointer) ... <pre><code>## Setup\nFirst, let's install the required packages and set our API keys\n</code></pre> %%capture --no-stderr %pip install -U redis langgraph langchain_openai <pre><code>\n</code></pre> import getpass import os</p> <p>def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \")</p> <p>_set_env(\"OPENAI_API_KEY\") <pre><code>## Checkpointer implementation\n## Define imports and helper functions\n\nFirst, let's define some imports and shared utilities for both RedisSaver and AsyncRedisSaver\n</code></pre> \"\"\"Implementation of a langgraph checkpoint saver using Redis.\"\"\" from contextlib import asynccontextmanager, contextmanager from typing import (     Any,     AsyncGenerator,     AsyncIterator,     Iterator,     List,     Optional,     Tuple, )</p> <p>from langchain_core.runnables import RunnableConfig</p> <p>from langgraph.checkpoint.base import (     WRITES_IDX_MAP,     BaseCheckpointSaver,     ChannelVersions,     Checkpoint,     CheckpointMetadata,     CheckpointTuple,     PendingWrite,     get_checkpoint_id, ) from langgraph.checkpoint.serde.base import SerializerProtocol from redis import Redis from redis.asyncio import Redis as AsyncRedis</p> <p>REDIS_KEY_SEPARATOR = \"$\"</p>"},{"location":"AIML/AgenticAI/langgraph/#utilities-shared-by-both-redissaver-and-asyncredissaver","title":"Utilities shared by both RedisSaver and AsyncRedisSaver","text":"<p>def _make_redis_checkpoint_key(     thread_id: str, checkpoint_ns: str, checkpoint_id: str ) -&gt; str:     return REDIS_KEY_SEPARATOR.join(         [\"checkpoint\", thread_id, checkpoint_ns, checkpoint_id]     )</p> <p>def _make_redis_checkpoint_writes_key(     thread_id: str,     checkpoint_ns: str,     checkpoint_id: str,     task_id: str,     idx: Optional[int], ) -&gt; str:     if idx is None:         return REDIS_KEY_SEPARATOR.join(             [\"writes\", thread_id, checkpoint_ns, checkpoint_id, task_id]         )</p> <pre><code>return REDIS_KEY_SEPARATOR.join(\n    [\"writes\", thread_id, checkpoint_ns, checkpoint_id, task_id, str(idx)]\n)\n</code></pre> <p>def _parse_redis_checkpoint_key(redis_key: str) -&gt; dict:     namespace, thread_id, checkpoint_ns, checkpoint_id = redis_key.split(         REDIS_KEY_SEPARATOR     )     if namespace != \"checkpoint\":         raise ValueError(\"Expected checkpoint key to start with 'checkpoint'\")</p> <pre><code>return {\n    \"thread_id\": thread_id,\n    \"checkpoint_ns\": checkpoint_ns,\n    \"checkpoint_id\": checkpoint_id,\n}\n</code></pre> <p>def _parse_redis_checkpoint_writes_key(redis_key: str) -&gt; dict:     namespace, thread_id, checkpoint_ns, checkpoint_id, task_id, idx = redis_key.split(         REDIS_KEY_SEPARATOR     )     if namespace != \"writes\":         raise ValueError(\"Expected checkpoint key to start with 'checkpoint'\")</p> <pre><code>return {\n    \"thread_id\": thread_id,\n    \"checkpoint_ns\": checkpoint_ns,\n    \"checkpoint_id\": checkpoint_id,\n    \"task_id\": task_id,\n    \"idx\": idx,\n}\n</code></pre> <p>def _filter_keys(     keys: List[str], before: Optional[RunnableConfig], limit: Optional[int] ) -&gt; list:     \"\"\"Filter and sort Redis keys based on optional criteria.\"\"\"     if before:         keys = [             k             for k in keys             if _parse_redis_checkpoint_key(k.decode())[\"checkpoint_id\"]             &lt; before[\"configurable\"][\"checkpoint_id\"]         ]</p> <pre><code>keys = sorted(\n    keys,\n    key=lambda k: _parse_redis_checkpoint_key(k.decode())[\"checkpoint_id\"],\n    reverse=True,\n)\nif limit:\n    keys = keys[:limit]\nreturn keys\n</code></pre> <p>def load_writes(     serde: SerializerProtocol, task_id_to_data: dict[tuple[str, str], dict] ) -&gt; list[PendingWrite]:     \"\"\"Deserialize pending writes.\"\"\"     writes = [         (             task_id,             data[b\"channel\"].decode(),             serde.loads_typed((data[b\"type\"].decode(), data[b\"value\"])),         )         for (task_id, ), data in task_id_to_data.items()     ]     return writes</p> <p>def _parse_redis_checkpoint_data(     serde: SerializerProtocol,     key: str,     data: dict,     pending_writes: Optional[List[PendingWrite]] = None, ) -&gt; Optional[CheckpointTuple]:     \"\"\"Parse checkpoint data retrieved from Redis.\"\"\"     if not data:         return None</p> <pre><code>parsed_key = _parse_redis_checkpoint_key(key)\nthread_id = parsed_key[\"thread_id\"]\ncheckpoint_ns = parsed_key[\"checkpoint_ns\"]\ncheckpoint_id = parsed_key[\"checkpoint_id\"]\nconfig = {\n    \"configurable\": {\n        \"thread_id\": thread_id,\n        \"checkpoint_ns\": checkpoint_ns,\n        \"checkpoint_id\": checkpoint_id,\n    }\n}\n\ncheckpoint = serde.loads_typed((data[b\"type\"].decode(), data[b\"checkpoint\"]))\nmetadata = serde.loads(data[b\"metadata\"].decode())\nparent_checkpoint_id = data.get(b\"parent_checkpoint_id\", b\"\").decode()\nparent_config = (\n    {\n        \"configurable\": {\n            \"thread_id\": thread_id,\n            \"checkpoint_ns\": checkpoint_ns,\n            \"checkpoint_id\": parent_checkpoint_id,\n        }\n    }\n    if parent_checkpoint_id\n    else None\n)\nreturn CheckpointTuple(\n    config=config,\n    checkpoint=checkpoint,\n    metadata=metadata,\n    parent_config=parent_config,\n    pending_writes=pending_writes,\n)\n</code></pre> <p>``` </p>"},{"location":"AIML/AgenticAI/langgraph/#redissaver","title":"RedisSaver","text":"<p>Below is an implementation of RedisSaver (for synchronous use of graph, i.e. .invoke(), .stream()). RedisSaver implements four methods that are required for any checkpointer:  - .put - Store a checkpoint with its configuration and metadata. - .put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes). - .get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). - .list - List checkpoints that match a given configuration and filter criteria.   ``` class RedisSaver(BaseCheckpointSaver):     \"\"\"Redis-based checkpoint saver implementation.\"\"\"</p> <pre><code>conn: Redis\n\ndef __init__(self, conn: Redis):\n    super().__init__()\n    self.conn = conn\n\n@classmethod\n@contextmanager\ndef from_conn_info(cls, *, host: str, port: int, db: int) -&gt; Iterator[\"RedisSaver\"]:\n    conn = None\n    try:\n        conn = Redis(host=host, port=port, db=db)\n        yield RedisSaver(conn)\n    finally:\n        if conn:\n            conn.close()\n\ndef put(\n    self,\n    config: RunnableConfig,\n    checkpoint: Checkpoint,\n    metadata: CheckpointMetadata,\n    new_versions: ChannelVersions,\n) -&gt; RunnableConfig:\n    \"\"\"Save a checkpoint to Redis.\n\n    Args:\n        config (RunnableConfig): The config to associate with the checkpoint.\n        checkpoint (Checkpoint): The checkpoint to save.\n        metadata (CheckpointMetadata): Additional metadata to save with the checkpoint.\n        new_versions (ChannelVersions): New channel versions as of this write.\n\n    Returns:\n        RunnableConfig: Updated configuration after storing the checkpoint.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"][\"checkpoint_ns\"]\n    checkpoint_id = checkpoint[\"id\"]\n    parent_checkpoint_id = config[\"configurable\"].get(\"checkpoint_id\")\n    key = _make_redis_checkpoint_key(thread_id, checkpoint_ns, checkpoint_id)\n\n    type_, serialized_checkpoint = self.serde.dumps_typed(checkpoint)\n    serialized_metadata = self.serde.dumps(metadata)\n    data = {\n        \"checkpoint\": serialized_checkpoint,\n        \"type\": type_,\n        \"metadata\": serialized_metadata,\n        \"parent_checkpoint_id\": parent_checkpoint_id\n        if parent_checkpoint_id\n        else \"\",\n    }\n    self.conn.hset(key, mapping=data)\n    return {\n        \"configurable\": {\n            \"thread_id\": thread_id,\n            \"checkpoint_ns\": checkpoint_ns,\n            \"checkpoint_id\": checkpoint_id,\n        }\n    }\n\ndef put_writes(\n    self,\n    config: RunnableConfig,\n    writes: List[Tuple[str, Any]],\n    task_id: str,\n) -&gt; None:\n    \"\"\"Store intermediate writes linked to a checkpoint.\n\n    Args:\n        config (RunnableConfig): Configuration of the related checkpoint.\n        writes (Sequence[Tuple[str, Any]]): List of writes to store, each as (channel, value) pair.\n        task_id (str): Identifier for the task creating the writes.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"][\"checkpoint_ns\"]\n    checkpoint_id = config[\"configurable\"][\"checkpoint_id\"]\n\n    for idx, (channel, value) in enumerate(writes):\n        key = _make_redis_checkpoint_writes_key(\n            thread_id,\n            checkpoint_ns,\n            checkpoint_id,\n            task_id,\n            WRITES_IDX_MAP.get(channel, idx),\n        )\n        type_, serialized_value = self.serde.dumps_typed(value)\n        data = {\"channel\": channel, \"type\": type_, \"value\": serialized_value}\n        if all(w[0] in WRITES_IDX_MAP for w in writes):\n            # Use HSET which will overwrite existing values\n            self.conn.hset(key, mapping=data)\n        else:\n            # Use HSETNX which will not overwrite existing values\n            for field, value in data.items():\n                self.conn.hsetnx(key, field, value)\n\ndef get_tuple(self, config: RunnableConfig) -&gt; Optional[CheckpointTuple]:\n    \"\"\"Get a checkpoint tuple from Redis.\n\n    This method retrieves a checkpoint tuple from Redis based on the\n    provided config. If the config contains a \"checkpoint_id\" key, the checkpoint with\n    the matching thread ID and checkpoint ID is retrieved. Otherwise, the latest checkpoint\n    for the given thread ID is retrieved.\n\n    Args:\n        config (RunnableConfig): The config to use for retrieving the checkpoint.\n\n    Returns:\n        Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_id = get_checkpoint_id(config)\n    checkpoint_ns = config[\"configurable\"].get(\"checkpoint_ns\", \"\")\n\n    checkpoint_key = self._get_checkpoint_key(\n        self.conn, thread_id, checkpoint_ns, checkpoint_id\n    )\n    if not checkpoint_key:\n        return None\n\n    checkpoint_data = self.conn.hgetall(checkpoint_key)\n\n    # load pending writes\n    checkpoint_id = (\n        checkpoint_id\n        or _parse_redis_checkpoint_key(checkpoint_key)[\"checkpoint_id\"]\n    )\n    pending_writes = self._load_pending_writes(\n        thread_id, checkpoint_ns, checkpoint_id\n    )\n    return _parse_redis_checkpoint_data(\n        self.serde, checkpoint_key, checkpoint_data, pending_writes=pending_writes\n    )\n\ndef list(\n    self,\n    config: Optional[RunnableConfig],\n    *,\n    # TODO: implement filtering\n    filter: Optional[dict[str, Any]] = None,\n    before: Optional[RunnableConfig] = None,\n    limit: Optional[int] = None,\n) -&gt; Iterator[CheckpointTuple]:\n    \"\"\"List checkpoints from the database.\n\n    This method retrieves a list of checkpoint tuples from Redis based\n    on the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\n\n    Args:\n        config (RunnableConfig): The config to use for listing the checkpoints.\n        filter (Optional[Dict[str, Any]]): Additional filtering criteria for metadata. Defaults to None.\n        before (Optional[RunnableConfig]): If provided, only checkpoints before the specified checkpoint ID are returned. Defaults to None.\n        limit (Optional[int]): The maximum number of checkpoints to return. Defaults to None.\n\n    Yields:\n        Iterator[CheckpointTuple]: An iterator of checkpoint tuples.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"].get(\"checkpoint_ns\", \"\")\n    pattern = _make_redis_checkpoint_key(thread_id, checkpoint_ns, \"*\")\n\n    keys = _filter_keys(self.conn.keys(pattern), before, limit)\n    for key in keys:\n        data = self.conn.hgetall(key)\n        if data and b\"checkpoint\" in data and b\"metadata\" in data:\n            # load pending writes\n            checkpoint_id = _parse_redis_checkpoint_key(key.decode())[\n                \"checkpoint_id\"\n            ]\n            pending_writes = self._load_pending_writes(\n                thread_id, checkpoint_ns, checkpoint_id\n            )\n            yield _parse_redis_checkpoint_data(\n                self.serde, key.decode(), data, pending_writes=pending_writes\n            )\n\ndef _load_pending_writes(\n    self, thread_id: str, checkpoint_ns: str, checkpoint_id: str\n) -&gt; List[PendingWrite]:\n    writes_key = _make_redis_checkpoint_writes_key(\n        thread_id, checkpoint_ns, checkpoint_id, \"*\", None\n    )\n    matching_keys = self.conn.keys(pattern=writes_key)\n    parsed_keys = [\n        _parse_redis_checkpoint_writes_key(key.decode()) for key in matching_keys\n    ]\n    pending_writes = _load_writes(\n        self.serde,\n        {\n            (parsed_key[\"task_id\"], parsed_key[\"idx\"]): self.conn.hgetall(key)\n            for key, parsed_key in sorted(\n                zip(matching_keys, parsed_keys), key=lambda x: x[1][\"idx\"]\n            )\n        },\n    )\n    return pending_writes\n\ndef _get_checkpoint_key(\n    self, conn, thread_id: str, checkpoint_ns: str, checkpoint_id: Optional[str]\n) -&gt; Optional[str]:\n    \"\"\"Determine the Redis key for a checkpoint.\"\"\"\n    if checkpoint_id:\n        return _make_redis_checkpoint_key(thread_id, checkpoint_ns, checkpoint_id)\n\n    all_keys = conn.keys(_make_redis_checkpoint_key(thread_id, checkpoint_ns, \"*\"))\n    if not all_keys:\n        return None\n\n    latest_key = max(\n        all_keys,\n        key=lambda k: _parse_redis_checkpoint_key(k.decode())[\"checkpoint_id\"],\n    )\n    return latest_key.decode()\n</code></pre> <p>``` </p>"},{"location":"AIML/AgenticAI/langgraph/#asyncredis","title":"AsyncRedis","text":"<p>Below is a reference implementation of AsyncRedisSaver (for asynchronous use of graph, i.e. .ainvoke(), .astream()). AsyncRedisSaver implements four methods that are required for any async checkpointer:  - .aput - Store a checkpoint with its configuration and metadata. - .aput_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes). - .aget_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). - .alist - List checkpoints that match a given configuration and filter criteria.  ``` class AsyncRedisSaver(BaseCheckpointSaver):     \"\"\"Async redis-based checkpoint saver implementation.\"\"\"</p> <pre><code>conn: AsyncRedis\n\ndef __init__(self, conn: AsyncRedis):\n    super().__init__()\n    self.conn = conn\n\n@classmethod\n@asynccontextmanager\nasync def from_conn_info(\n    cls, *, host: str, port: int, db: int\n) -&gt; AsyncIterator[\"AsyncRedisSaver\"]:\n    conn = None\n    try:\n        conn = AsyncRedis(host=host, port=port, db=db)\n        yield AsyncRedisSaver(conn)\n    finally:\n        if conn:\n            await conn.aclose()\n\nasync def aput(\n    self,\n    config: RunnableConfig,\n    checkpoint: Checkpoint,\n    metadata: CheckpointMetadata,\n    new_versions: ChannelVersions,\n) -&gt; RunnableConfig:\n    \"\"\"Save a checkpoint to the database asynchronously.\n\n    This method saves a checkpoint to Redis. The checkpoint is associated\n    with the provided config and its parent config (if any).\n\n    Args:\n        config (RunnableConfig): The config to associate with the checkpoint.\n        checkpoint (Checkpoint): The checkpoint to save.\n        metadata (CheckpointMetadata): Additional metadata to save with the checkpoint.\n        new_versions (ChannelVersions): New channel versions as of this write.\n\n    Returns:\n        RunnableConfig: Updated configuration after storing the checkpoint.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"][\"checkpoint_ns\"]\n    checkpoint_id = checkpoint[\"id\"]\n    parent_checkpoint_id = config[\"configurable\"].get(\"checkpoint_id\")\n    key = _make_redis_checkpoint_key(thread_id, checkpoint_ns, checkpoint_id)\n\n    type_, serialized_checkpoint = self.serde.dumps_typed(checkpoint)\n    serialized_metadata = self.serde.dumps(metadata)\n    data = {\n        \"checkpoint\": serialized_checkpoint,\n        \"type\": type_,\n        \"checkpoint_id\": checkpoint_id,\n        \"metadata\": serialized_metadata,\n        \"parent_checkpoint_id\": parent_checkpoint_id\n        if parent_checkpoint_id\n        else \"\",\n    }\n\n    await self.conn.hset(key, mapping=data)\n    return {\n        \"configurable\": {\n            \"thread_id\": thread_id,\n            \"checkpoint_ns\": checkpoint_ns,\n            \"checkpoint_id\": checkpoint_id,\n        }\n    }\n\nasync def aput_writes(\n    self,\n    config: RunnableConfig,\n    writes: List[Tuple[str, Any]],\n    task_id: str,\n) -&gt; None:\n    \"\"\"Store intermediate writes linked to a checkpoint asynchronously.\n\n    This method saves intermediate writes associated with a checkpoint to the database.\n\n    Args:\n        config (RunnableConfig): Configuration of the related checkpoint.\n        writes (Sequence[Tuple[str, Any]]): List of writes to store, each as (channel, value) pair.\n        task_id (str): Identifier for the task creating the writes.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"][\"checkpoint_ns\"]\n    checkpoint_id = config[\"configurable\"][\"checkpoint_id\"]\n\n    for idx, (channel, value) in enumerate(writes):\n        key = _make_redis_checkpoint_writes_key(\n            thread_id,\n            checkpoint_ns,\n            checkpoint_id,\n            task_id,\n            WRITES_IDX_MAP.get(channel, idx),\n        )\n        type_, serialized_value = self.serde.dumps_typed(value)\n        data = {\"channel\": channel, \"type\": type_, \"value\": serialized_value}\n        if all(w[0] in WRITES_IDX_MAP for w in writes):\n            # Use HSET which will overwrite existing values\n            await self.conn.hset(key, mapping=data)\n        else:\n            # Use HSETNX which will not overwrite existing values\n            for field, value in data.items():\n                await self.conn.hsetnx(key, field, value)\n\nasync def aget_tuple(self, config: RunnableConfig) -&gt; Optional[CheckpointTuple]:\n    \"\"\"Get a checkpoint tuple from Redis asynchronously.\n\n    This method retrieves a checkpoint tuple from Redis based on the\n    provided config. If the config contains a \"checkpoint_id\" key, the checkpoint with\n    the matching thread ID and checkpoint ID is retrieved. Otherwise, the latest checkpoint\n    for the given thread ID is retrieved.\n\n    Args:\n        config (RunnableConfig): The config to use for retrieving the checkpoint.\n\n    Returns:\n        Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_id = get_checkpoint_id(config)\n    checkpoint_ns = config[\"configurable\"].get(\"checkpoint_ns\", \"\")\n\n    checkpoint_key = await self._aget_checkpoint_key(\n        self.conn, thread_id, checkpoint_ns, checkpoint_id\n    )\n    if not checkpoint_key:\n        return None\n    checkpoint_data = await self.conn.hgetall(checkpoint_key)\n\n    # load pending writes\n    checkpoint_id = (\n        checkpoint_id\n        or _parse_redis_checkpoint_key(checkpoint_key)[\"checkpoint_id\"]\n    )\n    pending_writes = await self._aload_pending_writes(\n        thread_id, checkpoint_ns, checkpoint_id\n    )\n    return _parse_redis_checkpoint_data(\n        self.serde, checkpoint_key, checkpoint_data, pending_writes=pending_writes\n    )\n\nasync def alist(\n    self,\n    config: Optional[RunnableConfig],\n    *,\n    # TODO: implement filtering\n    filter: Optional[dict[str, Any]] = None,\n    before: Optional[RunnableConfig] = None,\n    limit: Optional[int] = None,\n) -&gt; AsyncGenerator[CheckpointTuple, None]:\n    \"\"\"List checkpoints from Redis asynchronously.\n\n    This method retrieves a list of checkpoint tuples from Redis based\n    on the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\n\n    Args:\n        config (Optional[RunnableConfig]): Base configuration for filtering checkpoints.\n        filter (Optional[Dict[str, Any]]): Additional filtering criteria for metadata.\n        before (Optional[RunnableConfig]): If provided, only checkpoints before the specified checkpoint ID are returned. Defaults to None.\n        limit (Optional[int]): Maximum number of checkpoints to return.\n\n    Yields:\n        AsyncIterator[CheckpointTuple]: An asynchronous iterator of matching checkpoint tuples.\n    \"\"\"\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    checkpoint_ns = config[\"configurable\"].get(\"checkpoint_ns\", \"\")\n    pattern = _make_redis_checkpoint_key(thread_id, checkpoint_ns, \"*\")\n    keys = _filter_keys(await self.conn.keys(pattern), before, limit)\n    for key in keys:\n        data = await self.conn.hgetall(key)\n        if data and b\"checkpoint\" in data and b\"metadata\" in data:\n            checkpoint_id = _parse_redis_checkpoint_key(key.decode())[\n                \"checkpoint_id\"\n            ]\n            pending_writes = await self._aload_pending_writes(\n                thread_id, checkpoint_ns, checkpoint_id\n            )\n            yield _parse_redis_checkpoint_data(\n                self.serde, key.decode(), data, pending_writes=pending_writes\n            )\n\nasync def _aload_pending_writes(\n    self, thread_id: str, checkpoint_ns: str, checkpoint_id: str\n) -&gt; List[PendingWrite]:\n    writes_key = _make_redis_checkpoint_writes_key(\n        thread_id, checkpoint_ns, checkpoint_id, \"*\", None\n    )\n    matching_keys = await self.conn.keys(pattern=writes_key)\n    parsed_keys = [\n        _parse_redis_checkpoint_writes_key(key.decode()) for key in matching_keys\n    ]\n    pending_writes = _load_writes(\n        self.serde,\n        {\n            (parsed_key[\"task_id\"], parsed_key[\"idx\"]): await self.conn.hgetall(key)\n            for key, parsed_key in sorted(\n                zip(matching_keys, parsed_keys), key=lambda x: x[1][\"idx\"]\n            )\n        },\n    )\n    return pending_writes\n\nasync def _aget_checkpoint_key(\n    self, conn, thread_id: str, checkpoint_ns: str, checkpoint_id: Optional[str]\n) -&gt; Optional[str]:\n    \"\"\"Asynchronously determine the Redis key for a checkpoint.\"\"\"\n    if checkpoint_id:\n        return _make_redis_checkpoint_key(thread_id, checkpoint_ns, checkpoint_id)\n\n    all_keys = await conn.keys(\n        _make_redis_checkpoint_key(thread_id, checkpoint_ns, \"*\")\n    )\n    if not all_keys:\n        return None\n\n    latest_key = max(\n        all_keys,\n        key=lambda k: _parse_redis_checkpoint_key(k.decode())[\"checkpoint_id\"],\n    )\n    return latest_key.decode()\n</code></pre> <p>``` </p>"},{"location":"AIML/AgenticAI/langgraph/#setup-model-and-tools-for-the-graph","title":"Setup model and tools for the graph","text":"<p>``` from typing import Literal from langchain_core.runnables import ConfigurableField from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent</p> <p>@tool def get_weather(city: Literal[\"nyc\", \"sf\"]):     \"\"\"Use this to get weather information.\"\"\"     if city == \"nyc\":         return \"It might be cloudy in nyc\"     elif city == \"sf\":         return \"It's always sunny in sf\"     else:         raise AssertionError(\"Unknown city\")</p> <p>tools = [get_weather] model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) <pre><code>## Use sync connection\n</code></pre> with RedisSaver.from_conn_info(host=\"localhost\", port=6379, db=0) as checkpointer:     graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)     config = {\"configurable\": {\"thread_id\": \"1\"}}     res = graph.invoke({\"messages\": [(\"human\", \"what's the weather in sf\")]}, config)</p> <pre><code>latest_checkpoint = checkpointer.get(config)\nlatest_checkpoint_tuple = checkpointer.get_tuple(config)\ncheckpoint_tuples = list(checkpointer.list(config))\n</code></pre> <p><code></code> latest_checkpoint <code></code> {'v': 1,  'ts': '2024-08-09T01:56:48.328315+00:00',  'id': '1ef55f2a-3614-69b4-8003-2181cff935cc',  'channel_values': {'messages': [HumanMessage(content=\"what's the weather in sf\", id='f911e000-75a1-41f6-8e38-77bb086c2ecf'),    AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_l5e5YcTJDJYOdvi4scBy9n2I', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4f1531f1-067c-4e16-8b62-7a6b663e93bd-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_l5e5YcTJDJYOdvi4scBy9n2I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}),    ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='e27bb3a1-1798-494a-b4ad-2deadda8b2bf', tool_call_id='call_l5e5YcTJDJYOdvi4scBy9n2I'),    AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad546b5a-70ce-404e-9656-dcc6ecd482d3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})],   'agent': 'agent'},  'channel_versions': {'start': '00000000000000000000000000000002.',   'messages': '00000000000000000000000000000005.16e98d6f7ece7598829eddf1b33a33c4',   'start:agent': '00000000000000000000000000000003.',   'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af',   'branch:agent:should_continue:tools': '00000000000000000000000000000004.',   'tools': '00000000000000000000000000000005.'},  'versions_seen': {'input': {},   'start': {'start': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'},   'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc',    'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'},   'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}},  'pending_sends': [],  'current_tasks': {}} <code></code> latest_checkpoint_tuple <code></code> CheckpointTuple(config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3614-69b4-8003-2181cff935cc'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:48.328315+00:00', 'id': '1ef55f2a-3614-69b4-8003-2181cff935cc', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in sf\", id='f911e000-75a1-41f6-8e38-77bb086c2ecf'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_l5e5YcTJDJYOdvi4scBy9n2I', 'function': {'arguments': '{\"city\":\"sf\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4f1531f1-067c-4e16-8b62-7a6b663e93bd-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_l5e5YcTJDJYOdvi4scBy9n2I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}), ToolMessage(content=\"It's always sunny in sf\", name='get_weather', id='e27bb3a1-1798-494a-b4ad-2deadda8b2bf', tool_call_id='call_l5e5YcTJDJYOdvi4scBy9n2I'), AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad546b5a-70ce-404e-9656-dcc6ecd482d3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})], 'agent': 'agent'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000005.16e98d6f7ece7598829eddf1b33a33c4', 'start:agent': '00000000000000000000000000000003.', 'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000005.'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.ab89befb52cc0e91e106ef7f500ea033'}, 'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in San Francisco is always sunny!', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad546b5a-70ce-404e-9656-dcc6ecd482d3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}}, 'step': 3}, parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-306f-6252-8002-47c2374ec1f2'}}, pending_writes=[]) ``` </p>"},{"location":"AIML/AgenticAI/langgraph/#use-async-connection","title":"Use async connection","text":"<p>``` async with AsyncRedisSaver.from_conn_info(     host=\"localhost\", port=6379, db=0 ) as checkpointer:     graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)     config = {\"configurable\": {\"thread_id\": \"2\"}}     res = await graph.ainvoke(         {\"messages\": [(\"human\", \"what's the weather in nyc\")]}, config     )</p> <pre><code>latest_checkpoint = await checkpointer.aget(config)\nlatest_checkpoint_tuple = await checkpointer.aget_tuple(config)\ncheckpoint_tuples = [c async for c in checkpointer.alist(config)]\n</code></pre> <p><code></code> latest_checkpoint <code></code> {'v': 1,  'ts': '2024-08-09T01:56:49.503241+00:00',  'id': '1ef55f2a-4149-61ea-8003-dc5506862287',  'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a'),    AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}),    ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='922124bd-d3b0-4929-a996-a75d842b8b44', tool_call_id='call_TvPLLyhuQQN99EcZc8SzL8x9'),    AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-69a10e66-d61f-475e-b7de-a1ecd08a6c3a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})],   'agent': 'agent'},  'channel_versions': {'start': '00000000000000000000000000000002.',   'messages': '00000000000000000000000000000005.2cb29d082da6435a7528b4c917fd0c28',   'start:agent': '00000000000000000000000000000003.',   'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af',   'branch:agent:should_continue:tools': '00000000000000000000000000000004.',   'tools': '00000000000000000000000000000005.'},  'versions_seen': {'input': {},   'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'},   'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc',    'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'},   'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}},  'pending_sends': [],  'current_tasks': {}}  <code></code>  latest_checkpoint_tuple  <code></code>  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-4149-61ea-8003-dc5506862287'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:49.503241+00:00', 'id': '1ef55f2a-4149-61ea-8003-dc5506862287', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='922124bd-d3b0-4929-a996-a75d842b8b44', tool_call_id='call_TvPLLyhuQQN99EcZc8SzL8x9'), AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-69a10e66-d61f-475e-b7de-a1ecd08a6c3a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})], 'agent': 'agent'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000005.2cb29d082da6435a7528b4c917fd0c28', 'start:agent': '00000000000000000000000000000003.', 'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000005.'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-69a10e66-d61f-475e-b7de-a1ecd08a6c3a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}, 'step': 3}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3d07-647e-8002-b5e4d28c00c9'}}, pending_writes=[])  <code></code>  checkpoint_tuples  <code></code>  [CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-4149-61ea-8003-dc5506862287'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:49.503241+00:00', 'id': '1ef55f2a-4149-61ea-8003-dc5506862287', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='922124bd-d3b0-4929-a996-a75d842b8b44', tool_call_id='call_TvPLLyhuQQN99EcZc8SzL8x9'), AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-69a10e66-d61f-475e-b7de-a1ecd08a6c3a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})], 'agent': 'agent'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000005.2cb29d082da6435a7528b4c917fd0c28', 'start:agent': '00000000000000000000000000000003.', 'agent': '00000000000000000000000000000005.065d90dd7f7cd091f0233855210bb2af', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000005.'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='The weather in NYC might be cloudy.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 88, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-69a10e66-d61f-475e-b7de-a1ecd08a6c3a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 9, 'total_tokens': 97})]}}, 'step': 3}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3d07-647e-8002-b5e4d28c00c9'}}, pending_writes=None),  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3d07-647e-8002-b5e4d28c00c9'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:49.056860+00:00', 'id': '1ef55f2a-3d07-647e-8002-b5e4d28c00c9', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73}), ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='922124bd-d3b0-4929-a996-a75d842b8b44', tool_call_id='call_TvPLLyhuQQN99EcZc8SzL8x9')], 'tools': 'tools'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000004.07964a3a545f9ff95545db45a9753d11', 'start:agent': '00000000000000000000000000000003.', 'agent': '00000000000000000000000000000004.', 'branch:agent:should_continue:tools': '00000000000000000000000000000004.', 'tools': '00000000000000000000000000000004.022986cd20ae85c77ea298a383f69ba8'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'tools': {'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': {'tools': {'messages': [ToolMessage(content='It might be cloudy in nyc', name='get_weather', id='922124bd-d3b0-4929-a996-a75d842b8b44', tool_call_id='call_TvPLLyhuQQN99EcZc8SzL8x9')]}}, 'step': 2}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3cf9-6996-8001-88dab066840d'}}, pending_writes=None),  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-3cf9-6996-8001-88dab066840d'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:49.051234+00:00', 'id': '1ef55f2a-3cf9-6996-8001-88dab066840d', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73})], 'agent': 'agent', 'branch:agent:should_continue:tools': 'agent'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000003.cc96d93b1afbd1b69d53851320670b97', 'start:agent': '00000000000000000000000000000003.', 'agent': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af', 'branch:agent:should_continue:tools': '00000000000000000000000000000003.065d90dd7f7cd091f0233855210bb2af'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'agent': {'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'function': {'arguments': '{\"city\":\"nyc\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0d6fa3b4-cace-41a8-b025-d01d16f6bbe9-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'nyc'}, 'id': 'call_TvPLLyhuQQN99EcZc8SzL8x9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73})]}}, 'step': 1}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-36a6-6788-8000-9efe1769f8c1'}}, pending_writes=None),  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-36a6-6788-8000-9efe1769f8c1'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:48.388067+00:00', 'id': '1ef55f2a-36a6-6788-8000-9efe1769f8c1', 'channel_values': {'messages': [HumanMessage(content=\"what's the weather in nyc\", id='5a106e79-a617-4707-839f-134d4e4b762a')], 'start:agent': 'start'}, 'channel_versions': {'start': '00000000000000000000000000000002.', 'messages': '00000000000000000000000000000002.a6994b785a651d88df51020401745af8', 'start:agent': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'}, 'versions_seen': {'input': {}, 'start': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'loop', 'writes': None, 'step': 0}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-36a3-6614-bfff-05dafa02b4d7'}}, pending_writes=None),  CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1ef55f2a-36a3-6614-bfff-05dafa02b4d7'}}, checkpoint={'v': 1, 'ts': '2024-08-09T01:56:48.386807+00:00', 'id': '1ef55f2a-36a3-6614-bfff-05dafa02b4d7', 'channel_values': {'messages': [], 'start': {'messages': [['human', \"what's the weather in nyc\"]]}}, 'channel_versions': {'start': '00000000000000000000000000000001.0e148ae3debe753278387e84f786e863'}, 'versions_seen': {'input': {}}, 'pending_sends': [], 'current_tasks': {}}, metadata={'source': 'input', 'writes': {'messages': [['human', \"what's the weather in nyc\"]]}, 'step': -1}, parent_config=None, pending_writes=None)]  ```</p> <p># How to add thread-level persistence (functional API)  Many AI applications need memory to share context across multiple interactions on the same thread (e.g., multiple turns of a conversation). In LangGraph functional API, this kind of memory can be added to any entrypoint() workflow using thread-level persistence.</p> <p>When creating a LangGraph workflow, you can set it up to persist its results by using a checkpointer:</p> <ol> <li>Create an instance of a checkpointer:</li> </ol> <pre><code>from langgraph.checkpoint.memory import MemorySaver\n\ncheckpointer = MemorySaver()\n</code></pre> <ol> <li>Pass checkpointer instance to the entrypoint() decorator:</li> </ol> <pre><code>from langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs)\n    ...\n</code></pre> <ol> <li>Optionally expose previous parameter in the workflow function signature:</li> </ol> <pre><code>@entrypoint(checkpointer=checkpointer)\ndef workflow(\n    inputs,\n    *,\n    # you can optionally specify `previous` in the workflow function signature\n    # to access the return value from the workflow as of the last execution\n    previous\n):\n    previous = previous or []\n    combined_inputs = previous + inputs\n    result = do_something(combined_inputs)\n    ...\n</code></pre> <ol> <li>Optionally choose which values will be returned from the workflow and which will be saved by the checkpointer as previous:</li> </ol> <pre><code>@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs, *, previous):\n    ...\n    result = do_something(...)\n    return entrypoint.final(value=result, save=combine(inputs, result))\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#setup_2","title":"Setup","text":"<p>First we need to install the packages required</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_anthropic\n</code></pre> <p>Next, we need to set API key for Anthropic (the LLM we will use).</p> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#example-simple-chatbot-with-short-term-memory","title":"Example: simple chatbot with short-term memory","text":"<p>We will be using a workflow with a single task that calls a chat model.</p> <p>Let's first define the model we'll be using:</p> <pre><code>from langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n</code></pre> <p>API Reference: ChatAnthropic</p> <p>Now we can define our task and workflow. To add in persistence, we need to pass in a Checkpointer to the entrypoint() decorator.</p> <pre><code>from langchain_core.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import MemorySaver\n\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\n\ncheckpointer = MemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n</code></pre> <p>If we try to use this workflow, the context of the conversation will be persisted across interactions:</p> <p>We can now interact with the agent and see that it remembers previous messages!</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <pre><code>==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHi Bob! I'm Claude. Nice to meet you! How are you today?\n</code></pre> <p>You can always resume previous threads:</p> <pre><code>input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <pre><code>==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYour name is Bob.\n</code></pre> <p>If we want to start a new conversation, we can pass in a different thread_id. Poof! All the memories are gone!</p> <pre><code>input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream(\n    [input_message],\n    {\"configurable\": {\"thread_id\": \"2\"}},\n    stream_mode=\"values\",\n):\n    chunk.pretty_print()\n</code></pre> <pre><code>==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI don't know your name unless you tell me. Each conversation I have starts fresh, so I don't have access to any previous interactions or personal information unless you share it with me.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-add-cross-thread-persistence-functional-api","title":"How to add cross-thread persistence (functional API)","text":"<p>LangGraph allows you to persist data across different threads. For instance, you can store information about users (their names or preferences) in a shared (cross-thread) memory and reuse them in the new threads (e.g., new conversations).</p> <p>When using the functional API, you can set it up to store and retrieve memories by using the Store interface:</p> <ol> <li>Create an instance of a Store</li> </ol> <pre><code>from langgraph.store.memory import InMemoryStore, BaseStore\n\nstore = InMemoryStore()\n</code></pre> <ol> <li>Pass the store instance to the entrypoint() decorator and expose store parameter in the function signature:</li> </ol> <pre><code>from langgraph.func import entrypoint\n\n@entrypoint(store=store)\ndef workflow(inputs: dict, store: BaseStore):\n    my_task(inputs).result()\n    ...\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#setup_3","title":"Setup","text":"<p>First, let's install the required packages and set our API keys</p> <pre><code>%%capture --no-stderr\n%pip install -U langchain_anthropic langchain_openai langgraph\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n_set_env(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#example-simple-chatbot-with-long-term-memory","title":"Example: simple chatbot with long-term memory","text":""},{"location":"AIML/AgenticAI/langgraph/#define-store_1","title":"Define store","text":"<p>In this example we will create a workflow that will be able to retrieve information about a user's preferences. We will do so by defining an InMemoryStore - an object that can store data in memory and query that data.</p> <p>When storing objects using the Store interface you define two things:</p> <ul> <li>the namespace for the object, a tuple (similar to directories)</li> <li>the object key (similar to filenames)</li> </ul> <p>In our example, we'll be using (\"memories\", ) as namespace and random UUID as key for each new memory. <p>Importantly, to determine the user, we will be passing user_id via the config keyword argument of the node function.</p> <p>Let's first define our store!</p> <pre><code>from langgraph.store.memory import InMemoryStore\nfrom langchain_openai import OpenAIEmbeddings\n\nin_memory_store = InMemoryStore(\n    index={\n        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n        \"dims\": 1536,\n    }\n)\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#create-workflow","title":"Create workflow","text":"<pre><code>import uuid\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.func import entrypoint, task\nfrom langgraph.graph import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.store.base import BaseStore\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n\n@task\ndef call_model(messages: list[BaseMessage], memory_store: BaseStore, user_id: str):\n    namespace = (\"memories\", user_id)\n    last_message = messages[-1]\n    memories = memory_store.search(namespace, query=str(last_message.content))\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # Store new memories if the user asks the model to remember\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        memory_store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + messages)\n    return response\n\n\n# NOTE: we're passing the store object here when creating a workflow via entrypoint()\n@entrypoint(checkpointer=MemorySaver(), store=in_memory_store)\ndef workflow(\n    inputs: list[BaseMessage],\n    *,\n    previous: list[BaseMessage],\n    config: RunnableConfig,\n    store: BaseStore,\n):\n    user_id = config[\"configurable\"][\"user_id\"]\n    previous = previous or []\n    inputs = add_messages(previous, inputs)\n    response = call_model(inputs, store, user_id).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#run-the-workflow","title":"Run the workflow!","text":"<p>Now let's specify a user ID in the config and tell the model our name:</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <pre><code>==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHello Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?\n</code></pre> <pre><code>config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <p>We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:</p> <pre><code>for memory in in_memory_store.search((\"memories\", \"1\")):\n    print(memory.value)\n</code></pre> <pre><code>{'data': 'User name is Bob'}\n</code></pre> <p>Let's now run the workflow for another user to verify that the memories about the first user are self contained:</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <pre><code>==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI don't have any information about your name. I can only see our current conversation without any prior context or personal details about you. If you'd like me to know your name, feel free to tell me!\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-manage-conversation-history","title":"How to manage conversation history","text":"<p>One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. In order to prevent this from happening, you need to properly manage the conversation history.</p> <p>Note: this guide focuses on how to do this in LangGraph, where you can fully customize how this is done. If you want a more off-the-shelf solution, you can look into functionality provided in LangChain:</p> <ul> <li>How to filter messages</li> <li>How to trim messages</li> </ul>"},{"location":"AIML/AgenticAI/langgraph/#build-the-agent","title":"Build the agent","text":"<p>Let's now build a simple ReAct style agent.</p> <pre><code>from typing import Literal\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import MessagesState, StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode\n\nmemory = MemorySaver()\n\n\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though \ud83d\ude0a\n    return \"It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"\n\n\ntools = [search]\ntool_node = ToolNode(tools)\nmodel = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\nbound_model = model.bind_tools(tools)\n\n\ndef should_continue(state: MessagesState):\n    \"\"\"Return the next node to execute.\"\"\"\n    last_message = state[\"messages\"][-1]\n    # If there is no function call, then we finish\n    if not last_message.tool_calls:\n        return END\n    # Otherwise if there is, we continue\n    return \"action\"\n\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    response = bound_model.invoke(state[\"messages\"])\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": response}\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    # Next, we pass in the path map - all the possible nodes this edge could go to\n    [\"action\", END],\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"action\", \"agent\")\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\napp = workflow.compile(checkpointer=memory)\n</code></pre> <pre><code>from langchain_core.messages import HumanMessage\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n\n\ninput_message = HumanMessage(content=\"what's my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <p>API Reference: HumanMessage</p> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nNice to meet you, Bob! As an AI assistant, I don't have a physical form, but I'm happy to chat with you and try my best to help out however I can. Please feel free to ask me anything, and I'll do my best to provide useful information or assistance.\n================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYou said your name is Bob, so that is the name I have for you.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#filtering-messages","title":"Filtering messages","text":"<p>The most straight-forward thing to do to prevent conversation history from blowing up is to filter the list of messages before they get passed to the LLM. This involves two parts: defining a function to filter messages, and then adding it to the graph. See the example below which defines a really simple filter_messages function and then uses it.</p> <pre><code>from typing import Literal\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import MessagesState, StateGraph, START\nfrom langgraph.prebuilt import ToolNode\n\nmemory = MemorySaver()\n\n\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though \ud83d\ude0a\n    return \"It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"\n\n\ntools = [search]\ntool_node = ToolNode(tools)\nmodel = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\nbound_model = model.bind_tools(tools)\n\n\ndef should_continue(state: MessagesState):\n    \"\"\"Return the next node to execute.\"\"\"\n    last_message = state[\"messages\"][-1]\n    # If there is no function call, then we finish\n    if not last_message.tool_calls:\n        return END\n    # Otherwise if there is, we continue\n    return \"action\"\n\n\ndef filter_messages(messages: list):\n    # This is very simple helper function which only ever uses the last message\n    return messages[-1:]\n\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = filter_messages(state[\"messages\"])\n    response = bound_model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": response}\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    # Next, we pass in the pathmap - all the possible nodes this edge could go to\n    [\"action\", END],\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"action\", \"agent\")\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\napp = workflow.compile(checkpointer=memory)\n</code></pre> <pre><code>from langchain_core.messages import HumanMessage\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n\n# This will now not remember the previous messages\n# (because we set `messages[-1:]` in the filter messages argument)\ninput_message = HumanMessage(content=\"what's my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <p>API Reference: HumanMessage</p> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nNice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. It's a pleasure to chat with you. Feel free to ask me anything, I'm here to help!\n================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nI'm afraid I don't actually know your name. As an AI assistant, I don't have information about the specific identities of the people I talk to. I only know what is provided to me during our conversation.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#how-to-delete-messages","title":"How to delete messages","text":"<p>One of the common states for a graph is a list of messages. Usually you only add messages to that state. However, sometimes you may want to remove messages (either by directly modifying the state or as part of the graph). To do that, you can use the RemoveMessage modifier. In this guide, we will cover how to do that.</p> <p>The key idea is that each state key has a reducer key. This key specifies how to combine updates to the state. The default MessagesState has a messages key, and the reducer for that key accepts these RemoveMessage modifiers. That reducer then uses these RemoveMessage to delete messages from the key.</p> <p>So note that just because your graph state has a key that is a list of messages, it doesn't mean that that this RemoveMessage modifier will work. You also have to have a reducer defined that knows how to work with this.</p> <p>NOTE: Many models expect certain rules around lists of messages. For example, some expect them to start with a user message, others expect all messages with tool calls to be followed by a tool message. When deleting messages, you will want to make sure you don't violate these rules.</p>"},{"location":"AIML/AgenticAI/langgraph/#build-the-agent_1","title":"Build the agent","text":"<p>Let's now build a simple ReAct style agent.</p> <p>from typing import Literal</p> <p>from langchain_anthropic import ChatAnthropic from langchain_core.tools import tool</p> <p>from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import MessagesState, StateGraph, START, END from langgraph.prebuilt import ToolNode</p> <p>memory = MemorySaver()</p> <p>@tool def search(query: str):     \"\"\"Call to surf the web.\"\"\"     # This is a placeholder for the actual implementation     # Don't let the LLM know this though \ud83d\ude0a     return \"It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"</p> <p>tools = [search] tool_node = ToolNode(tools) model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\") bound_model = model.bind_tools(tools)</p> <p>def should_continue(state: MessagesState):     \"\"\"Return the next node to execute.\"\"\"     last_message = state[\"messages\"][-1]     # If there is no function call, then we finish     if not last_message.tool_calls:         return END     # Otherwise if there is, we continue     return \"action\"</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-function-that-calls-the-model","title":"Define the function that calls the model","text":"<p>def call_model(state: MessagesState):     response = model.invoke(state[\"messages\"])     # We return a list, because this will get added to the existing list     return {\"messages\": response}</p>"},{"location":"AIML/AgenticAI/langgraph/#define-a-new-graph","title":"Define a new graph","text":"<p>workflow = StateGraph(MessagesState)</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-two-nodes-we-will-cycle-between","title":"Define the two nodes we will cycle between","text":"<p>workflow.add_node(\"agent\", call_model) workflow.add_node(\"action\", tool_node)</p>"},{"location":"AIML/AgenticAI/langgraph/#set-the-entrypoint-as-agent","title":"Set the entrypoint as <code>agent</code>","text":""},{"location":"AIML/AgenticAI/langgraph/#this-means-that-this-node-is-the-first-one-called","title":"This means that this node is the first one called","text":"<p>workflow.add_edge(START, \"agent\")</p>"},{"location":"AIML/AgenticAI/langgraph/#we-now-add-a-conditional-edge","title":"We now add a conditional edge","text":"<p>workflow.add_conditional_edges(     # First, we define the start node. We use <code>agent</code>.     # This means these are the edges taken after the <code>agent</code> node is called.     \"agent\",     # Next, we pass in the function that will determine which node is called next.     should_continue,     # Next, we pass in the path map - all the possible nodes this edge could go to     [\"action\", END], )</p>"},{"location":"AIML/AgenticAI/langgraph/#we-now-add-a-normal-edge-from-tools-to-agent","title":"We now add a normal edge from <code>tools</code> to <code>agent</code>.","text":""},{"location":"AIML/AgenticAI/langgraph/#this-means-that-after-tools-is-called-agent-node-is-called-next","title":"This means that after <code>tools</code> is called, <code>agent</code> node is called next.","text":"<p>workflow.add_edge(\"action\", \"agent\")</p>"},{"location":"AIML/AgenticAI/langgraph/#finally-we-compile-it","title":"Finally, we compile it!","text":""},{"location":"AIML/AgenticAI/langgraph/#this-compiles-it-into-a-langchain-runnable","title":"This compiles it into a LangChain Runnable,","text":""},{"location":"AIML/AgenticAI/langgraph/#meaning-you-can-use-it-as-you-would-any-other-runnable","title":"meaning you can use it as you would any other runnable","text":"<p>app = workflow.compile(checkpointer=memory)</p> <pre><code>from langchain_core.messages import HumanMessage\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n\n\ninput_message = HumanMessage(content=\"what's my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <p>API Reference: HumanMessage</p> <pre><code>================================\u001b[1m Human Message \u001b[0m=================================\n\nhi! I'm bob\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nIt's nice to meet you, Bob! I'm an AI assistant created by Anthropic. I'm here to help out with any questions or tasks you might have. Please let me know if there's anything I can assist you with.\n================================\u001b[1m Human Message \u001b[0m=================================\n\nwhat's my name?\n==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYou said your name is Bob.\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#manually-deleting-messages","title":"Manually deleting messages","text":"<p>First, we will cover how to manually delete messages. Let's take a look at the current state of the thread:</p> <pre><code>messages = app.get_state(config).values[\"messages\"]\nmessages\n</code></pre> <pre><code>[HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='db576005-3a60-4b3b-8925-dc602ac1c571'),\n AIMessage(content=\"It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. I'm here to help out with any questions or tasks you might have. Please let me know if there's anything I can assist you with.\", additional_kwargs={}, response_metadata={'id': 'msg_01BKAnYxmoC6bQ9PpCuHk8ZT', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 52}}, id='run-3a60c536-b207-4c56-98f3-03f94d49a9e4-0', usage_metadata={'input_tokens': 12, 'output_tokens': 52, 'total_tokens': 64}),\n HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='2088c465-400b-430b-ad80-fad47dc1f2d6'),\n AIMessage(content='You said your name is Bob.', additional_kwargs={}, response_metadata={'id': 'msg_013UWTLTzwZi81vke8mMQ2KP', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 72, 'output_tokens': 10}}, id='run-3a6883be-0c52-4938-af98-e9e7476659eb-0', usage_metadata={'input_tokens': 72, 'output_tokens': 10, 'total_tokens': 82})]\n ```\n\n We can call update_state and pass in the id of the first message. This will delete that message.\n\n ```\n from langchain_core.messages import RemoveMessage\n\napp.update_state(config, {\"messages\": RemoveMessage(id=messages[0].id)})\n</code></pre> <p>API Reference: RemoveMessage</p> <pre><code>{'configurable': {'thread_id': '2',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1ef75157-f251-6a2a-8005-82a86a6593a0'}}\n</code></pre> <p>If we now look at the messages, we can verify that the first one was deleted.</p> <pre><code>messages = app.get_state(config).values[\"messages\"]\nmessages\n</code></pre> <pre><code>[AIMessage(content=\"It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I assist you today?\", response_metadata={'id': 'msg_01XPSAenmSqK8rX2WgPZHfz7', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 32}}, id='run-1c69af09-adb1-412d-9010-2456e5a555fb-0', usage_metadata={'input_tokens': 12, 'output_tokens': 32, 'total_tokens': 44}),\n HumanMessage(content=\"what's my name?\", id='f3c71afe-8ce2-4ed0-991e-65021f03b0a5'),\n AIMessage(content='Your name is Bob, as you introduced yourself at the beginning of our conversation.', response_metadata={'id': 'msg_01BPZdwsjuMAbC1YAkqawXaF', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 52, 'output_tokens': 19}}, id='run-b2eb9137-2f4e-446f-95f5-3d5f621a2cf8-0', usage_metadata={'input_tokens': 52, 'output_tokens': 19, 'total_tokens': 71})]\n ```\n\n ## Programmatically deleting messages\n\n We can also delete messages programmatically from inside the graph. Here we'll modify the graph to delete any old messages (longer than 3 messages ago) at the end of a graph run.\n\n ```\n from langchain_core.messages import RemoveMessage\nfrom langgraph.graph import END\n\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) &gt; 3:\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:-3]]}\n\n\n# We need to modify the logic to call delete_messages rather than end right away\ndef should_continue(state: MessagesState) -&gt; Literal[\"action\", \"delete_messages\"]:\n    \"\"\"Return the next node to execute.\"\"\"\n    last_message = state[\"messages\"][-1]\n    # If there is no function call, then we call our delete_messages function\n    if not last_message.tool_calls:\n        return \"delete_messages\"\n    # Otherwise if there is, we continue\n    return \"action\"\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\n\n# This is our new node we're defining\nworkflow.add_node(delete_messages)\n\n\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n)\nworkflow.add_edge(\"action\", \"agent\")\n\n# This is the new edge we're adding: after we delete messages, we finish\nworkflow.add_edge(\"delete_messages\", END)\napp = workflow.compile(checkpointer=memory)\n</code></pre> <p>We can now try this out. We can call the graph twice and then check the state</p> <pre><code>from langchain_core.messages import HumanMessage\n\nconfig = {\"configurable\": {\"thread_id\": \"3\"}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\n\ninput_message = HumanMessage(content=\"what's my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n</code></pre> <p>API Reference: HumanMessage</p> <pre><code>[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\")]\n[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\"), ('ai', 'You said your name is Bob, so that is the name I have for you.')]\n[('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\"), ('ai', 'You said your name is Bob, so that is the name I have for you.')]\n</code></pre> <p>If we now check the state, we should see that it is only three messages long. This is because we just deleted the earlier messages - otherwise it would be four!</p> <pre><code>messages = app.get_state(config).values[\"messages\"]\nmessages\n</code></pre> <p><pre><code>[AIMessage(content=\"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\", response_metadata={'id': 'msg_01XPEgPPbcnz5BbGWUDWTmzG', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 48}}, id='run-eded3820-b6a9-4d66-9210-03ca41787ce6-0', usage_metadata={'input_tokens': 12, 'output_tokens': 48, 'total_tokens': 60}),\n HumanMessage(content=\"what's my name?\", id='a0ea2097-3280-402b-92e1-67177b807ae8'),\n AIMessage(content='You said your name is Bob, so that is the name I have for you.', response_metadata={'id': 'msg_01JGT62pxhrhN4SykZ57CSjW', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 68, 'output_tokens': 20}}, id='run-ace3519c-81f8-45fe-a777-91f42d48b3a3-0', usage_metadata={'input_tokens': 68, 'output_tokens': 20, 'total_tokens': 88})]\n ```\n\n Remember, when deleting messages you will want to make sure that the remaining message list is still valid. This message list may actually not be - this is because it currently starts with an AI message, which some models do not allow.\n\n # How to add summary of the conversation history\n One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. One way to work around that is to create a summary of the conversation to date, and use that with the past N messages. This guide will go through an example of how to do that.\n\n This will involve a few steps:\n\n\n- Check if the conversation is too long (can be done by checking number of messages or length of messages)\n- If yes, the create summary (will need a prompt for this)\n- Then remove all except the last N messages\n\n## Build the chatbot\n</code></pre> from typing import Literal</p> <p>from langchain_anthropic import ChatAnthropic from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import MessagesState, StateGraph, START, END</p> <p>memory = MemorySaver()</p>"},{"location":"AIML/AgenticAI/langgraph/#we-will-add-a-summary-attribute-in-addition-to-messages-key","title":"We will add a <code>summary</code> attribute (in addition to <code>messages</code> key,","text":""},{"location":"AIML/AgenticAI/langgraph/#which-messagesstate-already-has","title":"which MessagesState already has)","text":"<p>class State(MessagesState):     summary: str</p>"},{"location":"AIML/AgenticAI/langgraph/#we-will-use-this-model-for-both-the-conversation-and-the-summarization","title":"We will use this model for both the conversation and the summarization","text":"<p>model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-logic-to-call-the-model","title":"Define the logic to call the model","text":"<p>def call_model(state: State):     # If a summary exists, we add this in as a system message     summary = state.get(\"summary\", \"\")     if summary:         system_message = f\"Summary of conversation earlier: {summary}\"         messages = [SystemMessage(content=system_message)] + state[\"messages\"]     else:         messages = state[\"messages\"]     response = model.invoke(messages)     # We return a list, because this will get added to the existing list     return {\"messages\": [response]}</p>"},{"location":"AIML/AgenticAI/langgraph/#we-now-define-the-logic-for-determining-whether-to-end-or-summarize-the-conversation","title":"We now define the logic for determining whether to end or summarize the conversation","text":"<p>def should_continue(state: State) -&gt; Literal[\"summarize_conversation\", END]:     \"\"\"Return the next node to execute.\"\"\"     messages = state[\"messages\"]     # If there are more than six messages, then we summarize the conversation     if len(messages) &gt; 6:         return \"summarize_conversation\"     # Otherwise we can just end     return END</p> <p>def summarize_conversation(state: State):     # First, we summarize the conversation     summary = state.get(\"summary\", \"\")     if summary:         # If a summary already exists, we use a different system prompt         # to summarize it than if one didn't         summary_message = (             f\"This is summary of the conversation to date: {summary}\\n\\n\"             \"Extend the summary by taking into account the new messages above:\"         )     else:         summary_message = \"Create a summary of the conversation above:\"</p> <pre><code>messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\nresponse = model.invoke(messages)\n# We now need to delete messages that we no longer want to show up\n# I will delete all but the last two messages, but you can change this\ndelete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\nreturn {\"summary\": response.content, \"messages\": delete_messages}\n</code></pre>"},{"location":"AIML/AgenticAI/langgraph/#define-a-new-graph_1","title":"Define a new graph","text":"<p>workflow = StateGraph(State)</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-conversation-node-and-the-summarize-node","title":"Define the conversation node and the summarize node","text":"<p>workflow.add_node(\"conversation\", call_model) workflow.add_node(summarize_conversation)</p>"},{"location":"AIML/AgenticAI/langgraph/#set-the-entrypoint-as-conversation","title":"Set the entrypoint as conversation","text":"<p>workflow.add_edge(START, \"conversation\")</p>"},{"location":"AIML/AgenticAI/langgraph/#we-now-add-a-conditional-edge_1","title":"We now add a conditional edge","text":"<p>workflow.add_conditional_edges(     # First, we define the start node. We use <code>conversation</code>.     # This means these are the edges taken after the <code>conversation</code> node is called.     \"conversation\",     # Next, we pass in the function that will determine which node is called next.     should_continue, )</p>"},{"location":"AIML/AgenticAI/langgraph/#we-now-add-a-normal-edge-from-summarize_conversation-to-end","title":"We now add a normal edge from <code>summarize_conversation</code> to END.","text":""},{"location":"AIML/AgenticAI/langgraph/#this-means-that-after-summarize_conversation-is-called-we-end","title":"This means that after <code>summarize_conversation</code> is called, we end.","text":"<p>workflow.add_edge(\"summarize_conversation\", END)</p>"},{"location":"AIML/AgenticAI/langgraph/#finally-we-compile-it_1","title":"Finally, we compile it!","text":"<p>app = workflow.compile(checkpointer=memory) <pre><code>## Using the graph\n</code></pre> def print_update(update):     for k, v in update.items():         for m in v[\"messages\"]:             m.pretty_print()         if \"summary\" in v:             print(v[\"summary\"]) <pre><code>\n</code></pre> from langchain_core.messages import HumanMessage</p> <p>config = {\"configurable\": {\"thread_id\": \"4\"}} input_message = HumanMessage(content=\"hi! I'm bob\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event)</p> <p>input_message = HumanMessage(content=\"what's my name?\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event)</p> <p>input_message = HumanMessage(content=\"i like the celtics!\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event) <pre><code>\n</code></pre> ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>hi! I'm bob ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. How can I help you today? ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>what's my name? ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>Your name is Bob, as you told me at the beginning of our conversation. ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>i like the celtics! ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>That's great, the Celtics are a fun team to follow! Basketball is an exciting sport. Do you have a favorite Celtics player or a favorite moment from a Celtics game you've watched? I'd be happy to discuss the team and the sport with you. <pre><code>We can see that so far no summarization has happened - this is because there are only six messages in the list.\n</code></pre> values = app.get_state(config).values values <pre><code>Now let's send another message in\n</code></pre> input_message = HumanMessage(content=\"i like how much they win\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event) <pre><code>\n</code></pre> ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>i like how much they win ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite? ================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>Here is a summary of our conversation so far:</p> <ul> <li>You introduced yourself as Bob and said you like the Boston Celtics basketball team.</li> <li>I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.</li> <li>You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.</li> <li>I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.</li> <li>The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions. <pre><code>If we check the state now, we can see that we have a summary of the conversation, as well as the last two messages\n</code></pre> values = app.get_state(config).values values <pre><code>\n</code></pre> {'messages': [HumanMessage(content='i like how much they win', id='bb916ce7-534c-4d48-9f92-e269f9dc4859'),   AIMessage(content=\"That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite?\", response_metadata={'id': 'msg_01B7TMagaM8xBnYXLSMwUDAG', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 148, 'output_tokens': 82}}, id='run-c5aa9a8f-7983-4a7f-9c1e-0c0055334ac1-0')],  'summary': \"Here is a summary of our conversation so far:\\n\\n- You introduced yourself as Bob and said you like the Boston Celtics basketball team.\\n- I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.\\n- You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.\\n- I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.\\n- The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions.\"}  <pre><code>We can now resume having a conversation! Note that even though we only have the last two messages, we can still ask it questions about things mentioned earlier in the conversation (because we summarized those)\n</code></pre>  input_message = HumanMessage(content=\"what's my name?\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event) <pre><code>\n</code></pre> ================================\u001b[1m Human Message \u001b[0m=================================</li> </ul> <p>what's my name? ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>In our conversation so far, you introduced yourself as Bob. I acknowledged that earlier when you had shared your name. <pre><code>\n</code></pre> input_message = HumanMessage(content=\"what NFL team do you think I like?\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event) <pre><code>\n</code></pre> ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>what NFL team do you think I like? ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>I don't actually have any information about what NFL team you might like. In our conversation so far, you've only mentioned that you're a fan of the Boston Celtics basketball team. I don't have any prior knowledge about your preferences for NFL teams. Unless you provide me with that information, I don't have a basis to guess which NFL team you might be a fan of. <pre><code>\n</code></pre> input_message = HumanMessage(content=\"i like the patriots!\") input_message.pretty_print() for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):     print_update(event) <pre><code>\n</code></pre> ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>i like the patriots! ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>Okay, got it! Thanks for sharing that you're also a fan of the New England Patriots in the NFL. That makes sense, given your interest in other Boston sports teams like the Celtics. The Patriots have also had a very successful run over the past couple of decades, winning multiple Super Bowls. It's fun to follow winning franchises like the Celtics and Patriots. Do you have a favorite Patriots player or moment that stands out to you? ================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>================================\u001b[1m Remove Message \u001b[0m================================</p> <p>Okay, extending the summary with the new information:</p> <ul> <li>You initially introduced yourself as Bob and said you like the Boston Celtics basketball team. </li> <li>I acknowledged that and we discussed your appreciation for the Celtics' history of winning.</li> <li>You then asked what your name was, and I reminded you that you had introduced yourself as Bob earlier in the conversation.</li> <li>You followed up by asking what NFL team I thought you might like, and I explained that I didn't have any prior information about your NFL team preferences.</li> <li>You then revealed that you are also a fan of the New England Patriots, which made sense given your Celtics fandom.</li> <li>I responded positively to this new information, noting the Patriots' own impressive success and dynasty over the past couple of decades.</li> <li>I then asked if you have a particular favorite Patriots player or moment that stands out to you, continuing the friendly, conversational tone.</li> </ul> <p>Overall, the discussion has focused on your sports team preferences, with you sharing that you are a fan of both the Celtics and the Patriots. I've tried to engage with your interests and ask follow-up questions to keep the dialogue flowing. <pre><code># Tool calling\n## How to call tools using ToolNode\nToolNode is a LangChain Runnable that takes graph state (with a list of messages) as input and outputs state update with the result of tool calls. It is designed to work well out-of-box with LangGraph's prebuilt ReAct agent, but can also work with any StateGraph as long as its state has a messages key with an appropriate reducer (see MessagesState).\n\n\n## Define tools\n</code></pre> from langchain_core.messages import AIMessage from langchain_core.tools import tool</p> <p>from langgraph.prebuilt import ToolNode <pre><code>\n</code></pre> @tool def get_weather(location: str):     \"\"\"Call to get the current weather.\"\"\"     if location.lower() in [\"sf\", \"san francisco\"]:         return \"It's 60 degrees and foggy.\"     else:         return \"It's 90 degrees and sunny.\"</p> <p>@tool def get_coolest_cities():     \"\"\"Get a list of coolest cities\"\"\"     return \"nyc, sf\" <pre><code>\n</code></pre> tools = [get_weather, get_coolest_cities] tool_node = ToolNode(tools) <pre><code>## Manually call ToolNode\nToolNode operates on graph state with a list of messages. It expects the last message in the list to be an AIMessage with tool_calls parameter.\n\nLet's first see how to invoke the tool node manually:\n</code></pre> message_with_single_tool_call = AIMessage(     content=\"\",     tool_calls=[         {             \"name\": \"get_weather\",             \"args\": {\"location\": \"sf\"},             \"id\": \"tool_call_id\",             \"type\": \"tool_call\",         }     ], )</p> <p>tool_node.invoke({\"messages\": [message_with_single_tool_call]}) <pre><code>\n</code></pre> {'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id')]} <pre><code>Note that typically you don't need to create AIMessage manually, and it will be automatically generated by any LangChain chat model that supports tool calling.\n\nYou can also do parallel tool calling using ToolNode if you pass multiple tool calls to AIMessage's tool_calls parameter:\n</code></pre> message_with_multiple_tool_calls = AIMessage(     content=\"\",     tool_calls=[         {             \"name\": \"get_coolest_cities\",             \"args\": {},             \"id\": \"tool_call_id_1\",             \"type\": \"tool_call\",         },         {             \"name\": \"get_weather\",             \"args\": {\"location\": \"sf\"},             \"id\": \"tool_call_id_2\",             \"type\": \"tool_call\",         },     ], )</p> <p>tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]}) <pre><code>\n</code></pre> {'messages': [ToolMessage(content='nyc, sf', name='get_coolest_cities', tool_call_id='tool_call_id_1'),   ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id_2')]} <pre><code>## Using with chat models\n\nWe'll be using a small chat model from Anthropic in our example. To use chat models with tool calling, we need to first ensure that the model is aware of the available tools. We do this by calling .bind_tools method on ChatAnthropic model\n</code></pre> from typing import Literal</p> <p>from langchain_anthropic import ChatAnthropic from langgraph.graph import StateGraph, MessagesState from langgraph.prebuilt import ToolNode</p> <p>model_with_tools = ChatAnthropic(     model=\"claude-3-haiku-20240307\", temperature=0 ).bind_tools(tools) <pre><code>\n</code></pre> model_with_tools.invoke(\"what's the weather in sf?\").tool_calls <pre><code>\n</code></pre> [{'name': 'get_weather',   'args': {'location': 'San Francisco'},   'id': 'toolu_01Fwm7dg1mcJU43Fkx2pqgm8',   'type': 'tool_call'}] <pre><code>As you can see, the AI message generated by the chat model already has tool_calls populated, so we can just pass it directly to ToolNode\n</code></pre> tool_node.invoke({\"messages\": [model_with_tools.invoke(\"what's the weather in sf?\")]}) <pre><code>\n</code></pre> {'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='toolu_01LFvAVT3xJMeZS6kbWwBGZK')]} <pre><code>## ReAct Agent\nNext, let's see how to use ToolNode inside a LangGraph graph. Let's set up a graph implementation of the ReAct agent. This agent takes some query as input, then repeatedly call tools until it has enough information to resolve the query. We'll be using ToolNode and the Anthropic model with tools we just defined\n</code></pre> from typing import Literal</p> <p>from langgraph.graph import StateGraph, MessagesState, START, END</p> <p>def should_continue(state: MessagesState):     messages = state[\"messages\"]     last_message = messages[-1]     if last_message.tool_calls:         return \"tools\"     return END</p> <p>def call_model(state: MessagesState):     messages = state[\"messages\"]     response = model_with_tools.invoke(messages)     return {\"messages\": [response]}</p> <p>workflow = StateGraph(MessagesState)</p>"},{"location":"AIML/AgenticAI/langgraph/#define-the-two-nodes-we-will-cycle-between_1","title":"Define the two nodes we will cycle between","text":"<p>workflow.add_node(\"agent\", call_model) workflow.add_node(\"tools\", tool_node)</p> <p>workflow.add_edge(START, \"agent\") workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END]) workflow.add_edge(\"tools\", \"agent\")</p> <p>app = workflow.compile()  from IPython.display import Image, display</p> <p>try:     display(Image(app.get_graph().draw_mermaid_png())) except Exception:     # This requires some extra dependencies and is optional     pass </p>"},{"location":"AIML/AgenticAI/langgraph/#example-with-a-single-tool-call","title":"example with a single tool call","text":"<p>for chunk in app.stream(     {\"messages\": [(\"human\", \"what's the weather in sf?\")]}, stream_mode=\"values\" ):     chunk[\"messages\"][-1].pretty_print()  ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>what's the weather in sf? ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>[{'text': \"Okay, let's check the weather in San Francisco:\", 'type': 'text'}, {'id': 'toolu_01LdmBXYeccWKdPrhZSwFCDX', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}] Tool Calls:   get_weather (toolu_01LdmBXYeccWKdPrhZSwFCDX)  Call ID: toolu_01LdmBXYeccWKdPrhZSwFCDX   Args:     location: San Francisco =================================\u001b[1m Tool Message \u001b[0m================================= Name: get_weather</p> <p>It's 60 degrees and foggy. ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>The weather in San Francisco is currently 60 degrees with foggy conditions. </p>"},{"location":"AIML/AgenticAI/langgraph/#example-with-a-multiple-tool-calls-in-succession","title":"example with a multiple tool calls in succession","text":"<p>for chunk in app.stream(     {\"messages\": [(\"human\", \"what's the weather in the coolest cities?\")]},     stream_mode=\"values\", ):     chunk[\"messages\"][-1].pretty_print()  ================================\u001b[1m Human Message \u001b[0m=================================</p> <p>what's the weather in the coolest cities? ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>[{'text': \"Okay, let's find out the weather in the coolest cities:\", 'type': 'text'}, {'id': 'toolu_01LFZUWTccyveBdaSAisMi95', 'input': {}, 'name': 'get_coolest_cities', 'type': 'tool_use'}] Tool Calls:   get_coolest_cities (toolu_01LFZUWTccyveBdaSAisMi95)  Call ID: toolu_01LFZUWTccyveBdaSAisMi95   Args: =================================\u001b[1m Tool Message \u001b[0m================================= Name: get_coolest_cities</p> <p>nyc, sf ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>[{'text': \"Now let's get the weather for those cities:\", 'type': 'text'}, {'id': 'toolu_01RHPQBhT1u6eDnPqqkGUpsV', 'input': {'location': 'nyc'}, 'name': 'get_weather', 'type': 'tool_use'}] Tool Calls:   get_weather (toolu_01RHPQBhT1u6eDnPqqkGUpsV)  Call ID: toolu_01RHPQBhT1u6eDnPqqkGUpsV   Args:     location: nyc =================================\u001b[1m Tool Message \u001b[0m================================= Name: get_weather</p> <p>It's 90 degrees and sunny. ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>[{'id': 'toolu_01W5sFGF8PfgYzdY4CqT5c6e', 'input': {'location': 'sf'}, 'name': 'get_weather', 'type': 'tool_use'}] Tool Calls:   get_weather (toolu_01W5sFGF8PfgYzdY4CqT5c6e)  Call ID: toolu_01W5sFGF8PfgYzdY4CqT5c6e   Args:     location: sf =================================\u001b[1m Tool Message \u001b[0m================================= Name: get_weather</p> <p>It's 60 degrees and foggy. ==================================\u001b[1m Ai Message \u001b[0m==================================</p> <p>Based on the results, it looks like the weather in the coolest cities is: - New York City: 90 degrees and sunny - San Francisco: 60 degrees and foggy</p> <p>So the weather in the coolest cities is a mix of warm and cool temperatures, with some sunny and some foggy conditions. ```</p> <p>ToolNode can also handle errors during tool execution. You can enable / disable this by setting handle_tool_errors=True (enabled by default). See our guide on handling errors in ToolNode here</p>"},{"location":"AIML/AgenticAI/langgraph/#how-to-integrate-langgraph-into-your-react-application","title":"How to integrate LangGraph into your React application","text":"<p>React application</p>"},{"location":"AIML/AgenticAI/langgraph/#launch-local-langgraph-server","title":"Launch Local LangGraph Server","text":"<p>LangGraph Server</p>"},{"location":"AIML/AgenticAI/mcp/","title":"Model Context Protocol (MCP)","text":"<p>The Model Context Protocol (MCP) is an open standard that enables large language models to interact dynamically with external tools, databases, and APIs through a standardized interface.</p> <p>The world of artificial intelligence is constantly evolving and we wake up to new news almost every day. What we need to learn now is MCP (Model Context Protocol). Before moving on to what it is and its purpose, let\u2019s look at what the protocol means.</p> <p>To make things clearer, it\u2019s neither a framework like LangChain nor a tool; it\u2019s a protocol similar to HTTP for the web or SMTP for messaging.</p> <p>A more relevant example could be LSP (Language Server Protocol), which standardizes adding support for programming languages across an ecosystem of development tools. Similarly, MCP standardizes the integration of additional context and tools into the ecosystem of AI applications.</p> <p>It provides the universal rules that allow any client to communicate with any server, regardless of who built either component, creating a foundation for a diverse and interoperable AI ecosystem. Anthropic defines it as the USB-C port equivalent for agentic systems. It standardizes the connection between AI applications, LLMs, and external data sources (Databases, Gmail, Slack, etc.).</p> <p>The Machines are the clients, the peripheral devices are tools, and the MCP is the Type-C port. So, it doesn\u2019t matter who makes the device or peripherals; they work together seamlessly.</p> <p></p> <p>MCP defines how clients should communicate with servers and how servers should handle tools (APIs, Functions, etc.) and resources (read-only files like logs, db records, etc.)</p>"},{"location":"AIML/AgenticAI/mcp/#why-should-you-care-about-mcp","title":"Why should you care about MCP?","text":""},{"location":"AIML/AgenticAI/mcp/#benefits-of-standardization","title":"Benefits of Standardization","text":"<ol> <li>Unified Integration: A single protocol for connecting any LLM to any tool</li> <li>Reduced Development Time: Standard patterns for resource access and tool execution</li> <li>Clear Separation of Concerns: Data access (resources) and computation (tools) are cleanly separated</li> <li>Consistent Discovery: Uniform mechanisms for finding available capabilities (tools, resources, prompts, roots, sampling)</li> <li>Cross-Platform Compatibility: Tools built for one system work with others</li> </ol>"},{"location":"AIML/AgenticAI/mcp/#is-it-revolutionary","title":"Is it revolutionary?","text":"<p>Short answer: No.</p> <p>You can live without MCP. It is not revolutionary but brings standardization to the otherwise chaotic space of agentic development. If your application is MCP client-compliant, you can connect to any MCP client-compliant server. In an alternate world, as a client developer, you have to tailor the servers according to your needs, and others cannot build for your platform. The same is true for server developers.</p> <p>For example, Inside Cursor, you can connect to any MCP server if they follow the protocols.</p> <p>At this point, you will be more or less clear about the purpose of the MCP. Now, let\u2019s understand MCP for crystal clear clarity.</p>"},{"location":"AIML/AgenticAI/mcp/#mcp-architecture","title":"MCP Architecture","text":"<p>The Model Context Protocol has several key components that work together. Here\u2019s a high-level diagram.</p> <p></p> <p>The complete MCP architecture consists of four parts</p> <ul> <li>Host: Coordinates the overall system and manages LLM interactions</li> <li>Clients: Connect hosts to servers with 1:1 relationships</li> <li>Servers: Provide specialized capabilities through tools, resources, and prompts</li> <li>Base Protocol: Defines how all these components communicate</li> </ul> <p>In the above chart, the Client and Host are merged; we will keep them separate to clarify things. So, let\u2019s go through each component and understand MCP from within.</p>"},{"location":"AIML/AgenticAI/mcp/#1-host","title":"1. Host","text":"<p>Hosts are the LLM applications that expect data from servers. Hosts can be an IDE, Chatbot, or any LLM application. They are responsible for</p> <ul> <li>Initializing and managing multiple clients.</li> <li>Client-server lifecycle management</li> <li>Handles user authorization decisions</li> <li>Manages context aggregation across clients</li> </ul> <p>Examples are Claude Desktop, Cursor IDE, Windsurf IDE, etc.</p>"},{"location":"AIML/AgenticAI/mcp/#2-client","title":"2. Client","text":"<p>Each client has these key responsibilities:</p> <ul> <li>Dedicated connections: Each client maintains a one-to-one stateful connection with a single server. This focused relationship ensures clear communication boundaries and security isolation.</li> <li>Message routing: Clients handle all bidirectional communication, efficiently routing requests, responses, and notifications between the host and their connected server. We will see a small example of it in Cursor IDE with Linear and Slack.</li> <li>Capability management: Clients monitor what their connected server can do by maintaining information about available tools, resources (contextual data), and prompt templates.</li> <li>Protocol negotiation: During initialization, clients negotiate protocol versions and capabilities, ensuring compatibility between the host and server.</li> <li>Subscription management: Clients maintain subscriptions to server resources and handle notification events when those resources change.</li> </ul>"},{"location":"AIML/AgenticAI/mcp/#3-server","title":"3. Server","text":"<p>Servers are the fundamental building block that enriches LLMs with external data and context. The key server primitives include:</p> <ul> <li> <p>The tools are executable functions that allow LLM to interact with external apps. Tools function similarly to functions in traditional LLM calls. A tool can be a POST request to API endpoints; for example, a tool defined as LIST_FILES with a directory name as a parameter will fetch the files in the directory and send them back to the client. The tools can also be API calls to external services like Gmail, Slack, Notion, etc.</p> </li> <li> <p>Resources: These are any. Text files, Log files, DB schema, File contents, and Git history. They provide additional context to the LLMs.</p> </li> <li> <p>Prompt Templates: Pre-defined templates or instructions that guide language model interactions.</p> </li> </ul> <p>Tools are model-controlled, while Reosuces and Prompts are user-controlled. The models can automatically discover and invoke tools based on a given context.</p>"},{"location":"AIML/AgenticAI/mcp/#base-protocol","title":"Base Protocol","text":"<p>The protocol uses JSON-RPC 2.0 messages to establish communication</p> <ul> <li>JSON-RPC message format</li> <li>Stateful connections</li> <li>Server and client capability negotiation</li> </ul>"},{"location":"AIML/AgenticAI/mcp/#features","title":"Features","text":"<p>Servers offer any of the following features to clients:</p> <ul> <li>Resources: Context and data, for the user or the AI model to use</li> <li>Prompts: Templated messages and workflows for users</li> <li>Tools: Functions for the AI model to execute</li> </ul>"},{"location":"AIML/AgenticAI/mcp/#additional-utilities","title":"Additional Utilities","text":"<ul> <li>Configuration</li> <li>Progress tracking</li> <li>Cancellation</li> <li>Error reporting</li> <li>Logging</li> </ul>"},{"location":"AIML/AgenticAI/mcp/#security-and-trust-safety","title":"Security and Trust &amp; Safety","text":"<p>The Model Context Protocol enables powerful capabilities through arbitrary data access and code execution paths. With this power comes important security and trust considerations that all implementors must carefully address.</p>"},{"location":"AIML/AgenticAI/mcp/#key-principles","title":"Key Principles","text":"<ol> <li> <p>User Consent and Control</p> <ul> <li>Users must explicitly consent to and understand all data access and operations</li> <li>Users must retain control over what data is shared and what actions are taken</li> <li>Implementors should provide clear UIs for reviewing and authorizing activities</li> </ul> </li> <li> <p>Data Privacy</p> <ul> <li>Hosts must obtain explicit user consent before exposing user data to servers</li> <li>Hosts must not transmit resource data elsewhere without user consent</li> <li>User data should be protected with appropriate access controls</li> </ul> </li> <li> <p>Tool Safety</p> <ul> <li>Tools represent arbitrary code execution and must be treated with appropriate caution</li> <li>Hosts must obtain explicit user consent before invoking any tool</li> <li>Users should understand what each tool does before authorizing its use</li> </ul> </li> <li> <p>LLM Sampling Controls</p> <ul> <li>Users must explicitly approve any LLM sampling requests</li> <li>Users should control:<ul> <li>Whether sampling occurs at all</li> <li>The actual prompt that will be sent</li> <li>What results the server can see</li> </ul> </li> <li>The protocol intentionally limits server visibility into prompts</li> </ul> </li> </ol>"},{"location":"AIML/AgenticAI/mcp/#implementation-guidelines","title":"Implementation Guidelines","text":"<p>While MCP itself cannot enforce these security principles at the protocol level, implementors SHOULD:</p> <ol> <li>Build robust consent and authorization flows into their applications</li> <li>Provide clear documentation of security implications</li> <li>Implement appropriate access controls and data protections</li> <li>Follow security best practices in their integrations</li> <li>Consider privacy implications in their feature designs</li> </ol>"},{"location":"AIML/AgenticAI/mcp/#what-is-protocol","title":"What is Protocol?","text":"<p>In the computer world, a protocol is a set of rules that determine how two systems will communicate with each other. Protocols regulate data transfer in computer networks, internet communication, and between software systems.</p> <p>For example:</p> <ul> <li>HTTP (Hypertext Transfer Protocol): Allows websites to communicate with browsers.</li> <li>TCP/IP (Transmission Control Protocol/Internet Protocol): Defines how data packets on the internet will be routed.</li> <li>JSON-RPC (Remote Procedure Call): A protocol that allows data exchange in JSON format.</li> </ul>"},{"location":"AIML/AgenticAI/mcp/#what-is-model-context-protocol-mcp","title":"What is Model Context Protocol (MCP)?","text":"<p>The Model Context Protocol (MCP) is an open protocol that enables large language models (LLMs) to integrate with external data sources and tools in a standardized way. Developed by Anthropic, this protocol makes it easy for AI models to work seamlessly with a variety of tools and data sources.</p> <p>MCP can be likened to the USB-C port, which has become a global standard for device connections. Just as USB-C provides a common connection point between different devices, MCP enables AI systems to communicate with data and tools in a standard way.</p>"},{"location":"AIML/AgenticAI/mcp/#why-use-mcp","title":"Why Use MCP?","text":"<p>MCP functions similarly to APIs, but has a wider potential for use. While traditional APIs require a separate implementation for each integration, a single integration with MCP provides access to many different data sources and tools.</p> <p>MCP also provides two-way communication. In other words, an AI model can not only receive data, but also trigger certain actions.</p>"},{"location":"AIML/AgenticAI/mcp/#architecture-of-mcp","title":"Architecture of MCP","text":"<p>MCP is based on a simple client-server architecture. An application can connect to multiple MCP servers at the same time. The structure consists of the following components:</p> <ol> <li> <p>MCP Hosts: Applications act as MCP hosts to access data or tools.</p> </li> <li> <p>MCP Clients: Clients within the host establish one-to-one connections with MCP servers.</p> </li> <li> <p>MCP Servers: Lightweight, provide specific functionality through MCP, and can connect to local or remote data sources.</p> </li> <li> <p>Local Data Sources: Data that can be accessed by MCP servers, such as files and databases.</p> </li> <li> <p>Remote Services: External Internet-based APIs that MCP servers can access.</p> </li> </ol>"},{"location":"AIML/AgenticAI/mcp/#connection-lifecycle","title":"Connection Lifecycle","text":"<ol> <li>Initialization</li> </ol> <ul> <li> <p>The client sends an initialize request to the server, containing its own protocol version and capabilities.</p> </li> <li> <p>The server responds with its own protocol version and capabilities.</p> </li> <li> <p>The client sends the initialized notification.</p> </li> <li> <p>The connection is established and the message exchange begins.</p> </li> <li> <p>Message Exchange</p> </li> <li> <p>Once the connection is established, request and response messages can be sent between the client and the server, or one-way messages can be transmitted.</p> </li> <li> <p>Termination</p> </li> <li> <p>The client or server can terminate the connection.</p> </li> </ul>"},{"location":"AIML/AgenticAI/mcp/#key-features-of-mcp","title":"Key Features of MCP","text":"<p>MCP uses the JSON-RPC 2.0 message format to communicate between the client and server. Some of the protocol\u2019s prominent features are:</p> <ul> <li> <p>Resources: Data and content presented to the user or AI model.</p> </li> <li> <p>Prompts: Predefined messages and workflows prepared for users.</p> </li> <li> <p>Tools: Functions that the AI \u200b\u200bmodel can run.</p> </li> </ul>"},{"location":"AIML/AgenticAI/mcp/#hands-on-project","title":"Hands On Project","text":"<p>In this project, we will create a structure that brings the latest news from a website.</p> <p>In the document, they recommend using uv package manager instead of pip. So, you can open a terminal and download it to MacOs and Linux with the first command below. You can download it to Windows with the second command.</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Don\u2019t forget to restart your terminal after doing this.</p> <p>Then we will create a directory for our project. To do this, open the terminal in the directory where you want to create the project and run the following commands.</p> <ul> <li>The first command creates a project file in the directory you are in.</li> </ul> <p>(AGENTIC-AI-VENV) ganeshkinkargiri.@M7QJY5-A67EFC4A langgraph % uv init mcp-server-project Adding <code>mcp-server-project</code> as member of workspace <code>/Users/ganeshkinkargiri./Desktop/LLM-FINE-TUNE/Agentic-AI</code> Initialized project <code>mcp-server-project</code> at <code>/Users/ganeshkinkargiri./Desktop/LLM-FINE-TUNE/Agentic-AI/langgraph/mcp-server-project</code> (AGENTIC-AI-VENV) ganeshkinkargiri.@M7QJY5-A67EFC4A langgraph % </p> <ul> <li>The second command allows you to enter this file directory. (AGENTIC-AI-VENV) ganeshkinkargiri.@M7QJY5-A67EFC4A langgraph % cd mcp-server-project </li> </ul> <p>Then we will create a virtual environment and install our packages. For this we use the uv package manager.</p> <pre><code># cretae virtual env\nuv venv\n\n# activate for macos/linux\nsource .venv/bin/activate\n\n# activate for windows\n.venv\\Scripts\\activate\n\n# install libraries \nuv add \"mcp[cli]\" httpx bs4 dotenv\n</code></pre> <pre><code>(AGENTIC-AI-VENV) ganeshkinkargiri.@M7QJY5-A67EFC4A mcp-server-project % uv venv\nUsing CPython 3.10.15 interpreter at: /opt/homebrew/opt/python@3.10/bin/python3.10\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\n(AGENTIC-AI-VENV) ganeshkinkargiri.@M7QJY5-A67EFC4A mcp-server-project % \n</code></pre> <pre><code>source .venv/bin/activate\n(mcp-server-project) ganeshkinkargiri.@M7QJY5-A67EFC4A mcp-server-project % \n</code></pre> <pre><code>(mcp-server-project) ganeshkinkargiri.@M7QJY5-A67EFC4A mcp-server-project % /Users/ganeshkinkargiri./.local/bin/uv add \"mcp[cli]\" httpx bs4 dotenv \nwarning: `VIRTUAL_ENV=.venv` does not match the project environment path `/Users/ganeshkinkargiri./Desktop/LLM-FINE-TUNE/Agentic-AI/.venv` and will be ignored; use `--active` to target the active environment instead\nUsing CPython 3.10.15 interpreter at: /opt/homebrew/opt/python@3.10/bin/python3.10\nCreating virtual environment at: /Users/ganeshkinkargiri./Desktop/LLM-FINE-TUNE/Agentic-AI/.venv\nResolved 218 packages in 9.34s\nPrepared 31 packages in 1.41s\nInstalled 31 packages in 22ms\n + annotated-types==0.7.0\n + anyio==4.9.0\n + beautifulsoup4==4.13.3\n + bs4==0.0.2\n + certifi==2025.1.31\n + click==8.1.8\n + dotenv==0.9.9\n + exceptiongroup==1.2.2\n + h11==0.14.0\n + httpcore==1.0.7\n + httpx==0.27.2\n + httpx-sse==0.4.0\n + idna==3.10\n + markdown-it-py==3.0.0\n + mcp==1.6.0\n + mdurl==0.1.2\n + pydantic==2.11.1\n + pydantic-core==2.33.0\n + pydantic-settings==2.8.1\n + pygments==2.19.1\n + python-dotenv==1.1.0\n + rich==13.9.4\n + shellingham==1.5.4\n + sniffio==1.3.1\n + soupsieve==2.6\n + sse-starlette==2.2.1\n + starlette==0.46.1\n + typer==0.15.2\n + typing-extensions==4.13.0\n + typing-inspection==0.4.0\n + uvicorn==0.34.0\n(mcp-server-project) ganeshkinkargiri.@M7QJY5-A67EFC4A mcp-server-project % \n</code></pre> <p>Now we need to open the folder we created in vscode. When we open it, we see that our file structure is ready on the left.</p> <p>main.py</p> <pre><code>from mcp.server.fastmcp import FastMCP\nfrom dotenv import load_dotenv\nimport httpx\nimport os\nfrom bs4 import BeautifulSoup\nimport json\n\nload_dotenv()\n\n# initialize server\nmcp = FastMCP(\"tech_news\")\n\nUSER_AGENT = \"news-app/1.0\"\n\nNEWS_SITES = {\n    \"arstechnica\": \"https://arstechnica.com\"\n}\n\nasync def fetch_news(url: str):\n    \"\"\"It pulls and summarizes the latest news from the specified news site.\"\"\"\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(url, timeout=30.0)\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            paragraphs = soup.find_all(\"p\")\n            text = \" \".join([p.get_text() for p in paragraphs[:5]]) \n            return text\n        except httpx.TimeoutException:\n            return \"Timeout error\"\n\n@mcp.tool()  \nasync def get_tech_news(source: str):\n    \"\"\"\n    Fetches the latest news from a specific tech news source.\n\n    Args:\n    source: Name of the news source (for example, \"arstechnica\" or \"techcrunch\").\n\n    Returns:\n    A brief summary of the latest news.\n    \"\"\"\n    if source not in NEWS_SITES:\n        raise ValueError(f\"Source {source} is not supported.\")\n\n    news_text = await fetch_news(NEWS_SITES[source])\n    return news_text\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>The code above allows the latest news to be retrieved from the given site. It works step by step as follows.</p> <ol> <li> <p>First, we make the necessary imports.</p> </li> <li> <p>No API was used in this code, but if you use the API, you can use load_dotenv() to get your keys.</p> </li> <li> <p>We initialize our server with FastMCP.</p> </li> <li> <p>\u201cnews-app/1.0\u201d is the application name we gave.</p> </li> <li> <p>NEW_SITES contains the sites from which the news will be retrieved. You can add more sites here if you want.</p> </li> <li> <p>The fetch_news() function retrieves the news from the specified sites.</p> </li> <li> <p>The get_tech_news() function is our tool here. We specify that this is a tool by adding the @mcp.tool() decorator to the function. Having a docstring in this function is important for the model to understand how the tool works.</p> </li> <li> <p>Our MCP Server is running with mcp.run(transport=\u201dstdio\u201d). But we will not run the server via vscode.</p> </li> </ol> <p>Claude desktop can directly run the MCP Server we prepared, so we will use Claude Desktop. You can download it from here: https://claude.ai/download.</p> <p></p> <p></p> <p>Then we open the setting and enter the developer settings. From here we click on edit config.</p> <p>claude_desktop_config.json</p> <pre><code>{\n    \"mcpServers\": {\n        \"mcp-server-project\": {\n            \"command\": \"/Users/ganeshkinkargiri./.local/bin/uv\",\n            \"args\": [\n                \"--directory\",\n                \"/Users/ganeshkinkargiri./Desktop/LLM-FINE-TUNE/Agentic-AI/langgraph/mcp-server-project\",\n                \"run\",\n                \"main.py\"\n            ]\n        }\n    }\n}\n</code></pre> <p>When we click on edit config, it opens a folder for us. From this folder, we play the claude_desktop_config.json file with a text editor. We will enter our server information here.</p> <p>This file contains how our server will be run by claude. There are a few points to note:</p> <ol> <li> <p>mcp-server-project is the name of the project file we created. If you created it with a different name, you can change this.</p> </li> <li> <p>You should add the path where the uv package manager is located to the command section. In this section, only \u201cuv\u201d is written in the document, but it did not work that way for me, so I gave the path. You can run the \u201cwhich uv\u201d command in the terminal to find the path.</p> </li> <li> <p>You can also change the path in args to your own project path. To do this, you can run the \u201cpwd\u201d command in this directory in the terminal and get the full path.</p> </li> <li> <p>You can save and close the file.</p> </li> </ol> <p>We have completed our server configurations, now we can try it on Claude desktop. When you open Claude desktop, if there is a problem, you can see error pop-ups on the top right. You need to click and examine the log files and solve the error. If you do not receive an error, you should see the hammer in the red square.</p> <p>When you click on Hammer, it shows you the available MCP Tools as below.</p> <p></p> <p></p>"},{"location":"AIML/AgenticAI/mcp/#alternatively-you-can-test-it-with-the-mcp-inspector","title":"Alternatively, you can test it with the MCP Inspector:","text":"<p>In the context of agentic AI, MCP, or Model Context Protocol, is an open standard that facilitates seamless communication and data exchange between AI agents and external systems, enabling them to access and utilize data and tools efficiently.</p> <p> </p> <p>MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.</p>"},{"location":"AIML/AgenticAI/mcp/#model-context-protocol","title":"Model Context Protocol","text":"<p>The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools.</p> <p>The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.</p>"},{"location":"AIML/AgenticAI/mcp/#three-major-components-of-the-model-context-protocol-for-developers-from-anthropic","title":"Three major components of the Model Context Protocol for developers from anthropic","text":"<ol> <li>The Model Context Protocol specification and SDKs</li> <li>Local MCP server support in the Claude Desktop apps</li> <li>An open-source repository of MCP servers</li> </ol> <p>Claude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, we\u2019re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.</p>"},{"location":"AIML/AgenticAI/mcp/#reference-link","title":"Reference Link","text":"<ol> <li>modelcontextprotocol.io</li> <li>Language Server Protocol</li> <li>MCP-1</li> <li>MCP-2</li> <li>MCP-3</li> <li>MCP-github</li> <li>MCP-4</li> <li>modelcontextprotocol</li> </ol>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/","title":"DeepLearning algorithms","text":""},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#deep-learning-algorithms","title":"Deep Learning Algorithms","text":"<p>What is Deep Learning Algorithm?</p> <p>Deep learning can be defined as the method of machine learning and artificial intelligence that is intended to intimidate humans and their actions based on certain human brain functions to make effective decisions. It is a very important data science element that channels its modeling based on data-driven techniques under predictive modeling and statistics. To drive such a human-like ability to adapt and learn and to function accordingly, there have to be some strong forces which we popularly called algorithms.</p> <ul> <li>Deep learning algorithms are dynamically made to run through several layers of neural networks, which are nothing but a set of decision-making networks that are pre-trained to serve a task. </li> <li>Later, each of these is passed through simple layered representations and move on to the next layer.</li> <li>However, most machine learning is trained to work fairly well on datasets that have to deal with hundreds of features or columns.</li> <li>For a data set to be structured or unstructured, machine learning tends to fail mostly because they fail to recognize a simple image having a dimension of 800x1000 in RGB.</li> <li>It becomes quite unfeasible for a traditional machine learning algorithm to handle such depths. This is where deep learning.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#importance-of-deep-learning","title":"Importance of Deep Learning","text":"<ul> <li>Deep learning algorithms play a crucial role in determining the features and can handle the large number of processes for the data that might be structured or unstructured.</li> <li>Although, deep learning algorithms can overkill some tasks that might involve complex problems because they need access to huge amounts of data so that they can function effectively.</li> <li>For example, there's a popular deep learning tool that recognizes images namely Imagenet that has access to 14 million images in its dataset-driven algorithms.</li> <li>It is a highly comprehensive tool that has defined a next-level benchmark for deep learning tools that aim images as their dataset.</li> <li>Deep learning algorithms are highly progressive algorithms that learn about the image by passing it through each neural network layer. </li> <li>The layers are highly sensitive to detect low-level features of the image like edges and pixels and henceforth the combined layers take this information and form holistic representations by comparing it with previous data.</li> <li>For example, the middle layer might be programmed to detect some special parts of the object in the photograph which other deep trained layers are programmed to detect special objects like dogs, trees, utensils, etc.</li> <li>However, if we talk out the simple task that involves less complexity and a data-driven resource, deep learning algorithms fail to generalize simple data. This is one of the main reasons deep learning is not considered effective as linear or boosted tree models. Simple models aim to churn out custom data, track fraudulent transactions and deal with less complex datasets with fewer features. Also, there are various cases like multiclass classification where deep learning can be effective because it involves smaller but more structured datasets but is not preferred usually.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#deep-learning-algorithms_1","title":"Deep Learning Algorithms","text":""},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#1-convolutional-neural-networks-cnns","title":"1. Convolutional Neural Networks (CNNs)","text":"<ul> <li>CNN's popularly known as ConvNets majorly consists of several layers and are specifically used for image processing and detection of objects.</li> <li>CNNs have wide usage in identifying the image of the satellites, medical image processing, series forecasting, and anomaly detection.</li> <li>CNNs process the data by passing it through multiple layers and extracting features to exhibit convolutional operations. The Convolutional Layer consists of Rectified Linear Unit (ReLU) that outlasts to rectify the feature map. The Pooling layer is used to rectify these feature maps into the next feed. Pooling is generally a sampling algorithm that is down-sampled and it reduces the dimensions of the feature map. Later, the result generated consists of 2-D arrays consisting of single, long, continuous, and linear vector flattened in the map. The next layer i.e., called Fully Connected Layer which forms the flattened matrix or 2-D array fetched from the Pooling Layer as input and identifies the image by classifying it.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#2-long-short-term-memory-networks-lstms","title":"2. Long Short Term Memory Networks (LSTMs)","text":"<p>LSTMs can be defined as Recurrent Neural Networks (RNN) that are programmed to learn and adapt for dependencies for the long term. It can memorize and recall past data for a greater period and by default, it is its sole behavior. LSTMs are designed to retain over time and henceforth they are majorly used in time series predictions because they can restrain memory or previous inputs. This analogy comes from their chain-like structure consisting of four interacting layers that communicate with each other differently. Besides applications of time series prediction, they can be used to construct speech recognizers, development in pharmaceuticals, and composition of music loops as well.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#3-recurrent-neural-networks-rnns","title":"3. Recurrent Neural Networks (RNNs)","text":"<p>Recurrent Neural Networks or RNNs consist of some directed connections that form a cycle that allow the input provided from the LSTMs to be used as input in the current phase of RNNs. These inputs are deeply embedded as inputs and enforce the memorization ability of LSTMs lets these inputs get absorbed for a period in the internal memory. RNNs are therefore dependent on the inputs that are preserved by LSTMs and work under the synchronization phenomenon of LSTMs. RNNs are mostly used in captioning the image, time series analysis, recognizing handwritten data, and translating data to machines.</p> <p>RNNs follow the work approach by putting output feeds (t-1) time if the time is defined as t. Next, the output determined by t is feed at input time t+1. Similarly, these processes are repeated for all the input consisting of any length. There's also a fact about RNNs is that they store historical information and there's no increase in the input size even if the model size is increased. RNNs look something like this when unfolded.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#4-generative-adversarial-networks-gans","title":"4. Generative Adversarial Networks (GANs)","text":"<p>GANs are defined as deep learning algorithms that are used to generate new instances of data that match the training data. GAN usually consists of two components namely a generator that learns to generate false data and a discriminator that adapts itself by learning from this false data. Over some time, GANs have gained immense usage since they are frequently being used to clarify astronomical images and simulate lensing the gravitational dark matter. It is also used in video games to increase graphics for 2D textures by recreating them in higher resolution like 4K. They are also used in creating realistic cartoons character and also rendering human faces and 3D object rendering.</p> <p>GANs work in simulation by generating and understanding the fake data and the real data. During the training to understand these data, the generator produces different kinds of fake data where the discriminator quickly learns to adapt and respond to it as false data. GANs then send these recognized results for updating. Consider the below image to visualize the functioning.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#5-radial-basis-function-networks-rbfns","title":"5. Radial Basis Function Networks (RBFNs)","text":"<p>RBFNs are specific types of neural networks that follow a feed-forward approach and make use of radial functions as activation functions. They consist of three layers namely the input layer, hidden layer, and output layer which are mostly used for time-series prediction, regression testing, and classification.</p> <p>RBFNs do these tasks by measuring the similarities present in the training data set. They usually have an input vector that feeds these data into the input layer thereby confirming the identification and rolling out results by comparing previous data sets. Precisely, the input layer has neurons that are sensitive to these data and the nodes in the layer are efficient in classifying the class of data. Neurons are originally present in the hidden layer though they work in close integration with the input layer. The hidden layer contains Gaussian transfer functions that are inversely proportional to the distance of the output from the neuron's center. The output layer has linear combinations of the radial-based data where the Gaussian functions are passed in the neuron as parameter and output is generated. Consiider the given image below to understand the process thoroughly.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#6-multilayer-perceptrons-mlps","title":"6. Multilayer Perceptrons (MLPs)","text":"<p>MLPs are the base of deep learning technology. It belongs to a class of feed-forward neural networks having various layers of perceptrons. These perceptrons have various activation functions in them. MLPs also have connected input and output layers and their number is the same. Also, there's a layer that remains hidden amidst these two layers. MLPs are mostly used to build image and speech recognition systems or some other types of the translation software.</p> <p>The working of MLPs starts by feeding the data in the input layer. The neurons present in the layer form a graph to establish a connection that passes in one direction. The weight of this input data is found to exist between the hidden layer and the input layer. MLPs use activation functions to determine which nodes are ready to fire. These activation functions include tanh function, sigmoid and ReLUs. MLPs are mainly used to train the models to understand what kind of co-relation the layers are serving to achieve the desired output from the given data set.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#7-self-organizing-maps-soms","title":"7. Self Organizing Maps (SOMs)","text":"<p>SOMs were invented by Teuvo Kohenen for achieving data visualization to understand the dimensions of data through artificial and self-organizing neural networks. The attempts to achieve data visualization to solve problems are mainly done by what humans cannot visualize. These data are generally high-dimensional so there are lesser chances of human involvement and of course less error.</p> <p>SOMs help in visualizing the data by initializing weights of different nodes and then choose random vectors from the given training data. They examine each node to find the relative weights so that dependencies can be understood. The winning node is decided and that is called Best Matching Unit (BMU). Later, SOMs discover these winning nodes but the nodes reduce over time from the sample vector. So, the closer the node to BMU more is the more chance to recognize the weight and carry out further activities. There are also multiple iterations done to ensure that no node closer to BMU is missed. One example of such is the RGB color combinations that we use in our daily tasks. Consider the below image to understand how they function.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#8-deep-belief-networks-dbns","title":"8. Deep Belief Networks (DBNs)","text":"<p>DBNs are called generative models because they have various layers of latent as well as stochastic variables. The latent variable is called a hidden unit because they have binary values. DBNs are also called Boltzmann Machines because the RGM layers are stacked over each other to establish communication with previous and consecutive layers. DBNs are used in applications like video and image recognition as well as capturing motional objects.</p> <p>DBNs are powered by Greedy algorithms. The layer to layer approach by leaning through a top-down approach to generate weights is the most common way DBNs function. DBNs use step by step approach of Gibbs sampling on the hidden two-layer at the top. Then, these stages draw a sample from the visible units using a model that follows the ancestral sampling method. DBNs learn from the values present in the latent value from every layer following the bottom-up pass approach.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#9-restricted-boltzmann-machines-rbms","title":"9. Restricted Boltzmann Machines (RBMs)","text":"<p>RBMs were developed by Geoffrey Hinton and resemble stochastic neural networks that learn from the probability distribution in the given input set. This algorithm is mainly used in the field of dimension reduction, regression and classification, topic modeling and are considered the building blocks of DBNs. RBIs consist of two layers namely the visible layer and the hidden layer. Both of these layers are connected through hidden units and have bias units connected to nodes that generate the output. Usually, RBMs have two phases namely forward pass and backward pass.</p> <p>The functioning of RBMs is carried out by accepting inputs and translating them to numbers so that inputs are encoded in the forward pass. RBMs take into account the weight of every input, and the backward pass takes these input weights and translates them further into reconstructed inputs. Later, both of these translated inputs, along with individual weights, are combined. These inputs are then pushed to the visible layer where the activation is carried out, and output is generated that can be easily reconstructed. To understand this process, consider the below image.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-algorithms/#autoencoders","title":"Autoencoders","text":"<p>Autoencoders are a special type of neural network where inputs are outputs are found usually identical. It was designed to primarily solve the problems related to unsupervised learning. Autoencoders are highly trained neural networks that replicate the data. It is the reason why the input and output are generally the same. They are used to achieve tasks like pharma discovery, image processing, and population prediction.</p> <p>Autoencoders constitute three components namely the encoder, the code, and the decoder. Autoencoders are built in such a structure that they can receive inputs and transform them into various representations. The attempts to copy the original input by reconstructing them is more accurate. They do this by encoding the image or input, reduce the size. If the image is not visible properly they are passed to the neural network for clarification. Then, the clarified image is termed a reconstructed image and this resembles as accurate as of the previous image. To understand this complex process, see the below-provided image.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/","title":"DeepLearning overview","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#deep-learning","title":"Deep Learning","text":"<ul> <li>To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence.</li> <li>At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center.</li> <li>Broadly speaking, deep learning is a more approachable name for an artificial neural network. The \u201cdeep\u201d in deep learning refers to the depth of the network. An artificial neural network can be very shallow.</li> <li>Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons.</li> <li>The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs.</li> <li>We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer.</li> </ul> <ul> <li>Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems.</li> <li>In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.</li> <li>Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning.</li> <li>Machine learning involves computer intelligence that doesn\u2019t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. </li> <li> <p>Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra.</p> </li> <li> <p>There are two broad classes of machine learning methods:</p> <ul> <li>Supervised learning</li> <li>Unsupervised learning</li> </ul> </li> </ul> <p>In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.</p> <p>For example, let\u2019s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data \u2013 i.e. employee bonus amount and tenure \u2013 we could use supervised machine learning.</p> <p>With unsupervised learning, there aren\u2019t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It\u2019s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon\u2019s \u201ccustomers who also bought\u2026\u201d recommendations are a type of associative task.</p> <p>While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#why-is-deep-learning-important","title":"Why is Deep Learning Important?","text":"<p>Computers have long had techniques for recognizing features inside of images. The results weren\u2019t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks.</p> <p>Facebook has had great success with identifying faces in photographs by using deep learning. It\u2019s not just a marginal improvement, but a game changer: \u201cAsked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.\u201d</p> <p>Speech recognition is a another area that\u2019s felt deep learning\u2019s impact. Spoken languages are so vast and ambiguous. Baidu \u2013 one of the leading search engines of China \u2013 has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.</p> <p>What is particularly fascinating, is that generalizing the two languages didn\u2019t require much additional design effort: \u201cHistorically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,\u201d Andrew Ng says, chief scientist at Baidu. \u201cThe learning algorithms are now so general that you can just learn.\u201d</p> <p>Google is now using deep learning to manage the energy at the company\u2019s data centers. They\u2019ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#deep-learning-microservices","title":"Deep Learning Microservices","text":"<p>Here\u2019s a quick overview of some deep learning use cases and microservices.</p> <p>Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what\u2019s in the image. DeepFilter is a style transfer service for applying artistic filters to images.</p> <p>The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google\u2019s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#open-source-deep-learning-frameworks","title":"Open Source Deep Learning Frameworks","text":"<p>Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here\u2019s an overview of each:</p> <p>DL4J:</p> <ul> <li>JVM-based</li> <li>Distrubted</li> <li>Integrates with Hadoop and Spark</li> </ul> <p>Theano:</p> <ul> <li>Very popular in Academia</li> <li>Fairly low level</li> <li>Interfaced with via Python and Numpy</li> </ul> <p>Torch:</p> <ul> <li>Lua based</li> <li>In house versions used by Facebook and Twitter</li> <li>Contains pretrained models</li> </ul> <p>TensorFlow:</p> <ul> <li>Google written successor to Theano</li> <li>Interfaced with via Python and Numpy</li> <li>Highly parallel</li> <li>Can be somewhat slow for certain problem sets</li> </ul> <p>Caffe:</p> <ul> <li>Not general purpose. Focuses on machine-vision problems</li> <li>Implemented in C++ and is very fast</li> <li>Not easily extensible</li> <li>Has a Python interface</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#mcculloch-and-pitts-neuron","title":"McCulloch and Pitts Neuron","text":"<p>In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:</p> <ol> <li>A set of weights corresponding to synapses (inputs)</li> <li>An adder for summing input signals; analogous to cell membrane that collects charge</li> <li>An activation function for determining when the neuron fires, based on accumulated input</li> </ol> <p></p> <p>A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a network can they perform useful work.</p> <p>Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#perceptron","title":"Perceptron","text":"<p>A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.</p> <p>Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.</p> <p>The number of inputs and outputs are determined by the data. Weights are stored as a N x K matrix, with N observations and K neurons, with specifying the weight on the ith observation on the jth neuron.</p> <p></p> <p></p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#learning-with-perceptrons","title":"Learning with Perceptrons","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#example-logical-functions","title":"Example: Logical functions","text":"<p>Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables x1 and x2, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables.</p> <pre><code>%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom scipy import optimize\nfrom ipywidgets import *\nfrom IPython.display import SVG\nfrom sklearn import datasets\n</code></pre> <pre><code>AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\nAND\n\nx1  x2  y\n0   0   0   0\n1   0   1   0\n2   1   0   0\n3   1   1   1\n</code></pre> <p>First, we need to initialize weights to small, random values (can be positive and negative).</p> <pre><code>w = np.random.randn(3)*1e-4\n</code></pre> <p>Then, a simple activation function for calculating g(h):</p> <pre><code>g = lambda inputs, weights: np.where(np.dot(inputs, weights)&gt;0, 1, 0)\n</code></pre> <p>Finally, a training function that iterates the learning algorithm, returning the adapted weights.</p> <pre><code>def train(inputs, targets, weights, eta, n_iterations):\n\n    # Add the inputs that match the bias node\n    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n\n    for n in range(n_iterations):\n\n        activations = g(inputs, weights);\n        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n\n    return(weights)\n</code></pre> <p>Let's test it first on the AND function.</p> <pre><code>inputs = AND[['x1','x2']]\ntarget = AND['y']\n\nw = train(inputs, target, w, 0.25, 10)\n</code></pre> <p>Checking the performance:</p> <pre><code>g(np.c_[inputs, -np.ones((len(inputs), 1))], w)\n\narray([0, 0, 0, 1])\n\nThus, it has learned the function perfectly. Now for OR:\n</code></pre> <pre><code>OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\nOR\n\n    x1  x2  y\n0   0   0   0\n1   0   1   1\n2   1   0   1\n3   1   1   1\n</code></pre> <pre><code>w = np.random.randn(3)*1e-4\n</code></pre> <pre><code>inputs = OR[['x1','x2']]\ntarget = OR['y']\n\nw = train(inputs, target, w, 0.25, 20)\n</code></pre> <pre><code>g(np.c_[inputs, -np.ones((len(inputs), 1))], w)\n</code></pre> <p>array([0, 1, 1, 1]) Also 100% correct.</p> <p>Exercise: XOR Now try running the model on the XOR function, where a one is returned for either x1 or x2 being true, but not both. What happens here?</p> <p>Let's explore the problem graphically:</p> <p><pre><code>AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\nplt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--');\n</code></pre> </p> <p><pre><code>XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n\nXOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter');\n</code></pre> </p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#multi-layer-perceptron","title":"Multi-layer Perceptron","text":"<p>The solution to fitting more complex (i.e. non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply add more weights.</p> <p>There are two ways to add complexity:</p> <ol> <li>Add backward connections, so that output neurons feed back to input nodes, resulting in a recurrent network</li> <li>Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a multi-layer perceptron</li> </ol> <p>The latter approach is more common in applications of neural networks.</p> <p></p> <p>How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.</p> <p>Updating a multi-layer perceptron (MLP) is a matter of:</p> <pre><code>1. moving forward through the network, calculating outputs given inputs and current weight estimates\n2. moving backward updating weights according to the resulting error from forward propagation.\n</code></pre> <p>In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#backpropagation","title":"Backpropagation","text":"<p>Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters. These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent. This may be the most common method for training neural networks. Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#review-the-chain-rule","title":"Review: The chain rule","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#notation","title":"Notation","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#backpropagation-in-general","title":"Backpropagation in general","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#backpropagation-in-practice","title":"Backpropagation in practice","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#toy-python-example","title":"Toy Python example","text":"<p>Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations. The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function.</p> <p><pre><code># Ensure python 3 forward compatibility\nfrom __future__ import print_function\nimport numpy as np\n\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\nclass SigmoidLayer:\n    def __init__(self, n_input, n_output):\n        self.W = np.random.randn(n_output, n_input)\n        self.b = np.random.randn(n_output, 1)\n    def output(self, X):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        return sigmoid(self.W.dot(X) + self.b)\n\nclass SigmoidNetwork:\n\n    def __init__(self, layer_sizes):\n        '''\n        :parameters:\n            - layer_sizes : list of int\n                List of layer sizes of length L+1 (including the input dimensionality)\n        '''\n        self.layers = []\n        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n            self.layers.append(SigmoidLayer(n_input, n_output))\n\n    def train(self, X, y, learning_rate=0.2):\n        X = np.array(X)\n        y = np.array(y)\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        if y.ndim == 1:\n            y = y.reshape(1, -1)\n\n        # Forward pass - compute a^n for n in {0, ... L}\n        layer_outputs = [X]\n        for layer in self.layers:\n            layer_outputs.append(layer.output(layer_outputs[-1]))\n\n        # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1}\n        cost_partials = [layer_outputs[-1] - y]\n        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n            cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n        cost_partials.reverse()\n\n        # Compute weight gradient step\n        W_updates = []\n        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n            W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1])\n        # and biases\n        b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n\n        for W_update, b_update, layer in zip(W_updates, b_updates, self.layers):\n            layer.W -= W_update*learning_rate\n            layer.b -= b_update*learning_rate\n\n    def output(self, X):\n        a = np.array(X)\n        if a.ndim == 1:\n            a = a.reshape(-1, 1)\n        for layer in self.layers:\n            a = layer.output(a)\n        return a\n</code></pre> <pre><code>nn = SigmoidNetwork([2, 2, 1])\nX = np.array([[0, 1, 0, 1], \n              [0, 0, 1, 1]])\ny = np.array([0, 1, 1, 0])\nfor n in range(int(1e3)):\n    nn.train(X, y, learning_rate=1.)\nprint(\"Input\\tOutput\\tQuantized\")\nfor i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n    print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] &gt; .5)))\n</code></pre></p> <pre><code>logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n\n@interact(beta=(-1, 25))\ndef logistic_plot(beta=5):\n    hvals = np.linspace(-2, 2)\n    plt.plot(hvals, logistic(hvals, beta))\n</code></pre> <p></p> <pre><code>hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n\n@interact(theta=(-1, 25))\ndef tanh_plot(theta=5):\n    hvals = np.linspace(-2, 2)\n    h = hvals*theta\n    plt.plot(hvals, hyperbolic_tangent(h))\n</code></pre>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#gradient-descent","title":"Gradient Descent","text":"<pre><code>import numpy as np\n\n# Define the sigmoid activation function and its derivative\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Input dataset (X) and output dataset (y)\nX = np.array([[0, 0], \n              [0, 1], \n              [1, 0], \n              [1, 1]])\n\ny = np.array([[0], \n              [1], \n              [1], \n              [0]])\n</code></pre> <pre><code># Initialize weights and biases randomly\ninput_layer_neurons = X.shape[1]\nhidden_layer_neurons = 2\noutput_layer_neurons = 1\n</code></pre> <pre><code># Weight matrices\nW1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\nW2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))\n</code></pre> <pre><code># Bias vectors\nb1 = np.random.uniform(size=(1, hidden_layer_neurons))\nb2 = np.random.uniform(size=(1, output_layer_neurons))\n</code></pre> <pre><code># Learning rate\nlearning_rate = 0.5\n</code></pre> <pre><code># Training the neural network\nfor epoch in range(10000):\n    # Forward propagation\n    hidden_layer_input = np.dot(X, W1) + b1\n    hidden_layer_output = sigmoid(hidden_layer_input)\n\n    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n    predicted_output = sigmoid(output_layer_input)\n\n    # Compute the error\n    error = y - predicted_output\n\n    # Backpropagation\n    # Calculate the gradient for the output layer\n    d_predicted_output = error * sigmoid_derivative(predicted_output)\n\n    # Calculate the error for the hidden layer\n    hidden_layer_error = d_predicted_output.dot(W2.T)\n    d_hidden_layer_output = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n\n    # Update the weights and biases\n    W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n    b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n\n    W1 += X.T.dot(d_hidden_layer_output) * learning_rate\n    b1 += np.sum(d_hidden_layer_output, axis=0, keepdims=True) * learning_rate\n</code></pre> <pre><code># Display the final predicted output\nprint(\"Final predicted output:\\n\", predicted_output)\n</code></pre> <pre><code># Display the final weights and biases\nprint(\"\\nFinal weights for W1:\\n\", W1)\nprint(\"\\nFinal weights for W2:\\n\", W2)\nprint(\"\\nFinal biases for b1:\\n\", b1)\nprint(\"\\nFinal biases for b2:\\n\", b2)\n</code></pre>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#explanation","title":"Explanation:","text":"<p>Initialization:</p> <ul> <li> <p>Input dataset <code>X</code> and output dataset <code>y</code>.</p> </li> <li> <p>Weight matrices <code>W1</code> and <code>W2</code> and bias vectors <code>b1</code> and <code>b2</code> are initialized randomly.</p> </li> <li> <p>The learning rate is set to 0.5.</p> </li> </ul> <p>Forward Propagation:</p> <ul> <li> <p>Compute the input and output for the hidden layer.</p> </li> <li> <p>Compute the input and output for the output layer (predicted output).</p> </li> </ul> <p>Error Calculation:</p> <ul> <li>Compute the error by subtracting the predicted output from the actual output.</li> </ul> <p>Backpropagation:</p> <ul> <li> <p>Compute the gradient of the error with respect to the predicted output.</p> </li> <li> <p>Compute the error propagated back to the hidden layer.</p> </li> <li> <p>Compute the gradient of the hidden layer output.</p> </li> </ul> <p>Weight and Bias Update:</p> <ul> <li>Update the weights and biases for both layers using the computed gradients and learning rate.</li> </ul> <p>Training Loop:</p> <ul> <li>The above steps are repeated for a specified number of epochs (10,000 in this case).</li> </ul> <p>After training, the final predicted output, weights, and biases are printed. This simple example uses a neural network with one hidden layer to demonstrate the key concepts of backpropagation.</p> <p></p> <p>this activation function may take any of several forms, such as a logistic function.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#example-of-deep-learning","title":"Example of Deep Learning","text":"<ul> <li>In the example given above, we provide the raw data of images to the first layer of the input layer. </li> <li>After then, these input layer will determine the patterns of local contrast that means it will differentiate on the basis of colors, luminosity, etc.</li> <li>Then the 1st hidden layer will determine the face feature, i.e., it will fixate on eyes, nose, and lips, etc. And then, it will fixate those face features on the correct face template. </li> <li>So, in the 2nd hidden layer, it will actually determine the correct face here as it can be seen in the above image, after which it will be sent to the output layer. </li> <li>Likewise, more hidden layers can be added to solve more complex problems, for example, if you want to find out a particular kind of face having large or light complexions. So, as and when the hidden layers increase, we are able to solve complex problems.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#types-of-deep-learning-networks","title":"Types of Deep Learning Networks","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#1-feed-forward-neural-network","title":"1. Feed Forward Neural Network","text":"<p>A feed-forward neural network is none other than an Artificial Neural Network, which ensures that the nodes do not form a cycle. In this kind of neural network, all the perceptrons are organized within layers, such that the input layer takes the input, and the output layer generates the output. Since the hidden layers do not link with the outside world, it is named as hidden layers. Each of the perceptrons contained in one single layer is associated with each node in the subsequent layer. It can be concluded that all of the nodes are fully connected. It does not contain any visible or invisible connection between the nodes in the same layer. There are no back-loops in the feed-forward network. To minimize the prediction error, the backpropagation algorithm can be used to update the weight values.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#applications","title":"Applications:","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#data-compression","title":"Data Compression","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#pattern-recognition","title":"Pattern Recognition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#computer-vision","title":"Computer Vision","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#sonar-target-recognition","title":"Sonar Target Recognition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#speech-recognition","title":"Speech Recognition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#handwritten-characters-recognition","title":"Handwritten Characters Recognition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#2-recurrent-neural-network","title":"2. Recurrent Neural Network","text":"<p>Recurrent neural networks are yet another variation of feed-forward networks. Here each of the neurons present in the hidden layers receives an input with a specific delay in time. The Recurrent neural network mainly accesses the preceding info of existing iterations. For example, to guess the succeeding word in any sentence, one must have knowledge about the words that were previously used. It not only processes the inputs but also shares the length as well as weights crossways time. It does not let the size of the model to increase with the increase in the input size. However, the only problem with this recurrent neural network is that it has slow computational speed as well as it does not contemplate any future input for the current state. It has a problem with reminiscing prior information.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#applications_1","title":"Applications:","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#machine-translation","title":"Machine Translation","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#robot-control","title":"Robot Control","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#time-series-prediction","title":"Time Series Prediction","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#speech-recognition_1","title":"Speech Recognition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#speech-synthesis","title":"Speech Synthesis","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#time-series-anomaly-detection","title":"Time Series Anomaly Detection","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#rhythm-learning","title":"Rhythm Learning","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#music-composition","title":"Music Composition","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#3-convolutional-neural-network","title":"3. Convolutional Neural Network","text":"<p>Convolutional Neural Networks are a special kind of neural network mainly used for image classification, clustering of images and object recognition. DNNs enable unsupervised construction of hierarchical image representations. To achieve the best accuracy, deep convolutional neural networks are preferred more than any other neural network.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#applications_2","title":"Applications:","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#identify-faces-street-signs-tumors","title":"Identify Faces, Street Signs, Tumors.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#image-recognition","title":"Image Recognition.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#video-analysis","title":"Video Analysis.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#nlp","title":"NLP.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#anomaly-detection","title":"Anomaly Detection.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#drug-discovery","title":"Drug Discovery.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#checkers-game","title":"Checkers Game.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#time-series-forecasting","title":"Time Series Forecasting.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#4-restricted-boltzmann-machine","title":"4. Restricted Boltzmann Machine","text":"<p>RBMs are yet another variant of Boltzmann Machines. Here the neurons present in the input layer and the hidden layer encompasses symmetric connections amid them. However, there is no internal association within the respective layer. But in contrast to RBM, Boltzmann machines do encompass internal connections inside the hidden layer. These restrictions in BMs helps the model to train efficiently.</p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#applications_3","title":"Applications:","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#filtering","title":"Filtering.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#feature-learning","title":"Feature Learning.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#classification","title":"Classification.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#risk-detection","title":"Risk Detection.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#business-and-economic-analysis","title":"Business and Economic analysis.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#5-autoencoders","title":"5. Autoencoders","text":"<p>An autoencoder neural network is another kind of unsupervised machine learning algorithm. Here the number of hidden cells is merely small than that of the input cells. But the number of input cells is equivalent to the number of output cells. An autoencoder network is trained to display the output similar to the fed input to force AEs to find common patterns and generalize the data. The autoencoders are mainly used for the smaller representation of the input. It helps in the reconstruction of the original data from compressed data. This algorithm is comparatively simple as it only necessitates the output identical to the input.</p> <ul> <li>Encoder: Convert input data in lower dimensions.</li> <li>Decoder: Reconstruct the compressed data.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#applications_4","title":"Applications:","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#classification_1","title":"Classification.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#clustering","title":"Clustering.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#feature-compression","title":"Feature Compression.","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#deep-learning-applications","title":"Deep learning applications","text":"<ul> <li> <p>Self-Driving Cars In self-driven cars, it is able to capture the images around it by processing a huge amount of data, and then it will decide which actions should be incorporated to take a left or right or should it stop. So, accordingly, it will decide what actions it should take, which will further reduce the accidents that happen every year.</p> </li> <li> <p>Voice Controlled Assistance When we talk about voice control assistance, then Siri is the one thing that comes into our mind. So, you can tell Siri whatever you want it to do it for you, and it will search it for you and display it for you.</p> </li> <li> <p>Automatic Image Caption Generation Whatever image that you upload, the algorithm will work in such a way that it will generate caption accordingly. If you say blue colored eye, it will display a blue-colored eye with a caption at the bottom of the image.</p> </li> <li> <p>Automatic Machine Translation With the help of automatic machine translation, we are able to convert one language into another with the help of deep learning.</p> </li> </ul> <p>Limitations</p> <ul> <li>It only learns through the observations.</li> <li>It comprises of biases issues.</li> </ul> <p>Advantages</p> <ul> <li>It lessens the need for feature engineering.</li> <li>It eradicates all those costs that are needless.</li> <li>It easily identifies difficult defects.</li> <li>It results in the best-in-class performance on problems.</li> </ul> <p>Disadvantages</p> <ul> <li>It requires an ample amount of data.</li> <li>It is quite expensive to train.</li> <li>It does not have strong theoretical groundwork.</li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#introduction","title":"Introduction","text":"<ul> <li>Brains biological network provides basis for connecting elements in a real-life scenario for information processing and insight generation. </li> <li>A hierarchy of neurons connected through layers, where the output of one layer becomes the input for another layers, the information passes from one layer to another layer as weights.</li> <li>The weights associated with each neuron contain insights so that recognition and reasoning becomes easier for the next level.</li> <li>Artificial neural network is a very popular and effective method that consists of layers associated with weights. </li> <li>The association between different layers is governed by mathematical equation that passes information from one layer to the other. </li> <li>A bunch of mathematical equations are at work inside one artificial neural network model. </li> </ul>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#neural-networks","title":"Neural Networks","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#task","title":"Task","text":""},{"location":"AIML/DeepLearning/DeepLearning-overview/#what-is-deep-learning-dl","title":"What is Deep Learning (DL)?","text":"<p>A machine learning subfield of learning representations of data. Exceptional effective at learning patterns. Deep learning algorithms attempt to learn (multiple levels of) representation by using a hierarchy of multiple layers. If you provide the system tons of information, it begins to understand it and respond in useful ways.</p> <p></p>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#why-is-dl-useful","title":"Why is DL useful?","text":"<ul> <li>Manually designed features are often over-specified, incomplete and take a long time to design and validate</li> <li>Learned Features are easy to adapt, fast to learn</li> <li>Deep learning provides a very flexible, (almost?) universal, learnable framework for representing world, visual and linguistic information.</li> <li>Can learn both unsupervised and supervised</li> <li>Effective end-to-end joint system learning</li> <li>Utilize large amounts of training data</li> </ul> <pre><code>In ~2010 DL started outperforming other ML techniques \nfirst in speech and vision, then NLP\n</code></pre>"},{"location":"AIML/DeepLearning/DeepLearning-overview/#types-of-neural-networks","title":"Types of Neural Networks","text":"<ul> <li>Single hidden layer neural network: this is the simplest form of neural network as in this there is only one hidden layer.</li> <li>Multiple hidden layer neural networks: in this form more than one hidden layer will connect the input data with the output data. The complexity of calculation increases in this form as it requires more computational power to the system to process information</li> <li>Feed forward neural networks: in this form of neural network architecture, the information is passed one directionally from one layer to another layer; there is no iteration from the first level of learning.</li> <li>Back propagation neural networks: in this form of neural network there are two important steps, feed forward works in passing information from input to the hidden and from hidden to output layer and secondly it calculates error and propagate it back to the previous layers.</li> </ul>"},{"location":"AIML/DeepLearning/Keras/","title":"Keras","text":""},{"location":"AIML/DeepLearning/Keras/#keras","title":"Keras","text":"<ul> <li>Keras is an open-source high-level Neural Network library, which is written in Python is capable enough to run on Theano, TensorFlow, or CNTK.</li> <li>It was developed by one of the Google engineers, Francois Chollet.</li> <li>It is made user-friendly, extensible, and modular for facilitating faster experimentation with deep neural networks.</li> <li>It not only supports Convolutional Networks and Recurrent Networks individually but also their combination.</li> <li>It cannot handle low-level computations, so it makes use of the Backend library to resolve it. The backend library act as a high-level API wrapper for the low-level API, which lets it run on TensorFlow, CNTK, or Theano.</li> </ul>"},{"location":"AIML/DeepLearning/Keras/#what-makes-keras-special","title":"What makes Keras special?","text":"<ul> <li> <p>Focus on user experience has always been a major part of Keras.</p> </li> <li> <p>Large adoption in the industry.</p> </li> <li> <p>It is a multi backend and supports multi-platform, which helps all the encoders come together for coding.</p> </li> <li> <p>Research community present for Keras works amazingly with the production community.</p> </li> <li> <p>Easy to grasp all concepts.</p> </li> <li> <p>It supports fast prototyping.</p> </li> <li> <p>It seamlessly runs on CPU as well as GPU.</p> </li> <li> <p>It provides the freedom to design any architecture, which then later is utilized as an API for the project.</p> </li> <li> <p>It is really very simple to get started with.</p> </li> <li> <p>Easy production of models actually makes Keras special.</p> </li> </ul>"},{"location":"AIML/DeepLearning/Keras/#how-keras-support-the-claim-of-being-multi-backend-and-multi-platform","title":"How Keras support the claim of being multi-backend and multi-platform?","text":"<p>Keras can be developed in R as well as Python, such that the code can be run with TensorFlow, Theano, CNTK, or MXNet as per the requirement. Keras can be run on CPU, NVIDIA GPU, AMD GPU, TPU, etc. It ensures that producing models with Keras is really simple as it totally supports to run with TensorFlow serving, GPU acceleration (WebKeras, Keras.js), Android (TF, TF Lite), iOS (Native CoreML) and Raspberry Pi.</p>"},{"location":"AIML/DeepLearning/Keras/#keras-backend","title":"Keras Backend","text":"<p>Keras being a model-level library helps in developing deep learning models by offering high-level building blocks. All the low-level computations such as products of Tensor, convolutions, etc. are not handled by Keras itself, rather they depend on a specialized tensor manipulation library that is well optimized to serve as a backend engine. Keras has managed it so perfectly that instead of incorporating one single library of tensor and performing operations related to that particular library, it offers plugging of different backend engines into Keras.</p> <p>Keras consist of three backend engines, which are as follows:</p> <ul> <li>TensorFlow TensorFlow is a Google product, which is one of the most famous deep learning tools widely used in the research area of machine learning and deep neural network. It came into the market on 9th November 2015 under the Apache License 2.0. It is built in such a way that it can easily run on multiple CPUs and GPUs as well as on mobile operating systems. It consists of various wrappers in distinct languages such as Java, C++, or Python.</li> </ul> <p></p> <ul> <li>Theano Theano was developed at the University of Montreal, Quebec, Canada, by the MILA group. It is an open-source python library that is widely used for performing mathematical operations on multi-dimensional arrays by incorporating scipy and numpy. It utilizes GPUs for faster computation and efficiently computes the gradients by building symbolic graphs automatically. It has come out to be very suitable for unstable expressions, as it first observes them numerically and then computes them with more stable algorithms.</li> </ul> <p></p> <ul> <li>CNTK Microsoft Cognitive Toolkit is deep learning's open-source framework. It consists of all the basic building blocks, which are required to form a neural network. The models are trained using C++ or Python, but it incorporates C# or Java to load the model for making predictions.</li> </ul> <p></p>"},{"location":"AIML/DeepLearning/Keras/#advantages-of-keras","title":"Advantages of Keras","text":"<p>Keras encompasses the following advantages, which are as follows:</p> <ul> <li>It is very easy to understand and incorporate the faster deployment of network models.</li> <li>It has huge community support in the market as most of the AI companies are keen on using it.</li> <li>It supports multi backend, which means you can use any one of them among TensorFlow, CNTK, and Theano with Keras as a backend according to your requirement.</li> <li>Since it has an easy deployment, it also holds support for cross-platform. Following are the devices on which Keras can be deployed:<ol> <li>iOS with CoreML</li> <li>Android with TensorFlow Android</li> <li>Web browser with .js support</li> <li>Cloud engine</li> <li>Raspberry pi</li> </ol> </li> <li>It supports Data parallelism, which means Keras can be trained on multiple GPU's at an instance for speeding up the training time and processing a huge amount of data.</li> </ul>"},{"location":"AIML/DeepLearning/Keras/#disadvantages-of-keras","title":"Disadvantages of Keras","text":"<ul> <li>The only disadvantage is that Keras has its own pre-configured layers, and if you want to create an abstract layer, it won't let you because it cannot handle low-level APIs. It only supports high-level API running on the top of the backend engine (TensorFlow, Theano, and CNTK).</li> </ul> <pre><code>#Based on the extracted code from the notebook, I'll transform the script to use Keras for defining and training a neural network that learns the AND and OR logic gates. I'll ensure that the neural network is correctly set up and that the previous errors and syntax issues are fixed.\n\n#Here is the modified script using Keras:\n\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Define the AND dataset\nAND = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 0, 0, 1]})\ninputs_and = AND[['x1', 'x2']]\ntargets_and = AND['y']\n\n# Define the OR dataset\nOR = pd.DataFrame({'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1], 'y': [0, 1, 1, 1]})\ninputs_or = OR[['x1', 'x2']]\ntargets_or = OR['y']\n\n# Build the model\ndef build_model():\n    model = Sequential()\n    model.add(Dense(2, input_dim=2, activation='relu'))  # 2 neurons for input layer\n    model.add(Dense(1, activation='sigmoid'))  # 1 neuron for output layer\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Train and evaluate the model for AND logic gate\nmodel_and = build_model()\nmodel_and.fit(inputs_and, targets_and, epochs=100, verbose=0)\nloss_and, accuracy_and = model_and.evaluate(inputs_and, targets_and)\nprint(f'AND Gate - Loss: {loss_and}, Accuracy: {accuracy_and}')\n\n# Train and evaluate the model for OR logic gate\nmodel_or = build_model()\nmodel_or.fit(inputs_or, targets_or, epochs=100, verbose=0)\nloss_or, accuracy_or = model_or.evaluate(inputs_or, targets_or)\nprint(f'OR Gate - Loss: {loss_or}, Accuracy: {accuracy_or}')\n\n# Predict using the trained models\npredictions_and = model_and.predict(inputs_and)\npredictions_or = model_or.predict(inputs_or)\n\nprint(\"AND Gate Predictions:\")\nprint(np.round(predictions_and))\n\nprint(\"OR Gate Predictions:\")\nprint(np.round(predictions_or))\n\n\n### Explanation:\n#- **Data Preparation**: The AND and OR datasets are defined as pandas DataFrames.\n#- **Model Building**: A function `build_model` is created to define the neural network with Keras. The network consists of an input layer with 2 neurons (corresponding to the 2 input features) and an output layer with 1 neuron for binary classification.\n#- **Training and Evaluation**: The model is trained separately for the AND and OR datasets. The training process is silent (verbose=0), and after training, the model's loss and accuracy are printed.\n#- **Predictions**: The trained models are used to predict the outputs for the AND and OR datasets, and the predictions are printed.\n\n#This script ensures proper use of Keras for defining, training, and evaluating neural networks for the given logic gate tasks.\n</code></pre>"},{"location":"AIML/DeepLearning/Keras/#example-2","title":"Example-2","text":"<pre><code>import numpy as np\n\n# Sample data for linear regression\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 3, 3, 2, 5])\n\n# Linear regression with numpy\nA = np.vstack([X.T, np.ones(len(X))]).T\nm, c = np.linalg.lstsq(A, y, rcond=None)[0]\n\nprint(f\"Slope: {m}, Intercept: {c}\")\n</code></pre> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Sample data for linear regression\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 3, 3, 2, 5])\n\n# Define the model\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=1, kernel_initializer='normal', activation='linear'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Train the model\nmodel.fit(X, y, epochs=100, verbose=0)\n\n# Get the model parameters (weights and biases)\nweights = model.layers[0].get_weights()\nslope = weights[0][0][0]\nintercept = weights[1][0]\n\nprint(f\"Slope: {slope}, Intercept: {intercept}\")\n</code></pre>"},{"location":"AIML/DeepLearning/Keras/#example-3","title":"Example-3","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.datasets\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# Generate a dataset and plot it\nnp.random.seed(0)\nX, y = sklearn.datasets.make_moons(200, noise=0.20)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Build and compile a Keras model\ndef build_keras_model(hidden_dim):\n    model = Sequential()\n    model.add(Dense(hidden_dim, input_dim=2, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train the model\ndef train_keras_model(hidden_dim):\n    model = build_keras_model(hidden_dim)\n    model.fit(X_train, y_train, epochs=2000, batch_size=32, verbose=0)\n    return model\n\n# Plot decision boundary\ndef plot_decision_boundary(model, X, y):\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = np.argmax(Z, axis=1)\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.seismic)\n    plt.show()\n\n# Build a model with a 3-dimensional hidden layer and plot the decision boundary\nmodel = train_keras_model(3)\nplot_decision_boundary(model, X, y)\n\n# Visualize models with different hidden layer sizes\nplt.figure(figsize=(16, 32))\nhidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\nfor i, nn_hdim in enumerate(hidden_layer_dimensions):\n    plt.subplot(5, 2, i+1)\n    plt.title(f'Hidden Layer size {nn_hdim}')\n    model = train_keras_model(nn_hdim)\n    plot_decision_boundary(model, X, y)\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Keras/#keras-version","title":"Keras version","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.losses import MeanSquaredError\n</code></pre> <pre><code># Generate synthetic data\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\ny = y.reshape(-1, 1)  # Reshape to match output dimensions\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre> <pre><code># Define the model\nmodel = Sequential()\nmodel.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear'))\n\n# Compile the model\nmodel.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError())\n</code></pre> <pre><code># Train the model\nhistory = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n</code></pre> <pre><code># Evaluate the model\ny_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\n# Plot predictions vs actual\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Keras/#keras-version_1","title":"#Keras version","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.losses import MeanSquaredError\n</code></pre> <pre><code># Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre> <pre><code>from tensorflow.keras.layers import Dense, Dropout\n</code></pre> <pre><code># Define the model\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid'))  # First hidden layer H1\nmodel.add(Dropout(0.2))  # Dropout layer with 20% rate\nmodel.add(Dense(units=64, activation='relu'))  # Second hidden layer H2\nmodel.add(Dropout(0.2))  # Dropout layer with 20% rate\nmodel.add(Dense(units=32, activation='sigmoid'))  # Third hidden layer H3\n#model.add(Dropout(0.2))  # Dropout layer with 20% rate\nmodel.add(Dense(units=16, activation='relu'))  # First hidden layer H4\n#model.add(Dropout(0.2))  # Dropout layer with 20% rate\nmodel.add(Dense(units=8, activation='sigmoid'))  # Second hidden layer H5\n#model.add(Dropout(0.2))  # Dropout layer with 20% rate\nmodel.add(Dense(units=5, activation='relu'))  # Third hidden layer H6\n#model.add(Dropout(0.2))  # Dropout layer with 20% rate\n\nmodel.add(Dense(units=1, activation='linear'))  # Output layer\n# Compile the model\nmodel.compile(optimizer=SGD(learning_rate=0.05), loss=MeanSquaredError())\n</code></pre> <pre><code>model.summary()\n</code></pre> <pre><code># Train the model\nhistory = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n</code></pre> <pre><code># Evaluate the model on test set\ny_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\n# Call the function\nmse, r2 = calculate_accuracy(y_test_inverse, y_pred)\n</code></pre> <pre><code># Evaluate the model on train set\ny_pred_train = model.predict(X_train)\ny_pred_train = scaler_y.inverse_transform(y_pred_train)\ny_train_inverse = scaler_y.inverse_transform(y_train)\n\n# Call the function\nmse, r2 = calculate_accuracy(y_train_inverse, y_pred_train)\n</code></pre> <pre><code>from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\n\n# Define the model creation function\ndef create_model(learning_rate=0.01, dropout_rate=0.0):\n    model = Sequential()\n    model.add(Dense(units=128, input_dim=X_train.shape[1], activation='sigmoid'))  # First hidden layer H1\n    model.add(Dropout(0.2))  # Dropout layer with 20% rate\n    model.add(Dense(units=64, activation='relu'))  # Second hidden layer H2\n    model.add(Dropout(0.2))  # Dropout layer with 20% rate\n    model.add(Dense(units=32, activation='sigmoid'))  # Third hidden layer H3\n    #model.add(Dropout(0.2))  # Dropout layer with 20% rate\n    model.add(Dense(units=16, activation='relu'))  # First hidden layer H4\n    #model.add(Dropout(0.2))  # Dropout layer with 20% rate\n    model.add(Dense(units=8, activation='sigmoid'))  # Second hidden layer H5\n    #model.add(Dropout(0.2))  # Dropout layer with 20% rate\n    model.add(Dense(units=5, activation='relu'))  # Third hidden layer H6\n    #model.add(Dropout(0.2))  # Dropout layer with 20% rate\n\n    optimizer = SGD(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Create the model using KerasRegressor\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'dropout_rate': [0.2, 0.5],\n    'batch_size': [16, 32],\n    'epochs': [100]\n}\n\n# Add early stopping and model checkpoint callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Use grid search with callbacks\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\ngrid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test),\n                       callbacks=[early_stopping, model_checkpoint])\n\n# Summarize the results\nprint(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n</code></pre> <pre><code># Summarize the results\nprint(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n</code></pre> <pre><code>import os\nos.getcwd()\n</code></pre> <pre><code>from tensorflow.keras.models import load_model\ncheckpoint_filepath = '/Users/pradmishra/Documents/Deep Learning Contents/DL5/best_model.h5'\n</code></pre> <pre><code># Load the saved model checkpoint\nbest_model = load_model(checkpoint_filepath)\n\n# Evaluate the loaded model on test data\nloss = best_model.evaluate(X_test, y_test)\nprint(f'Loaded model loss on test data: {loss}')\n</code></pre> <pre><code># Evaluate the model\ny_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\n# Plot predictions vs actual\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Keras/#using-keras","title":"Using Keras","text":"<pre><code># Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre> <p><pre><code>model = Sequential([\n    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='linear')\n])\n</code></pre> <pre><code>model.compile(optimizer='adam', loss=MeanSquaredError())\n</code></pre></p> <pre><code>history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n</code></pre> <pre><code>y_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Learning/","title":"Learning","text":""},{"location":"AIML/DeepLearning/Learning/#build-rag-applications-10x-faster","title":"Build RAG Applications 10X Faster","text":"<p>vectorize</p>"},{"location":"AIML/DeepLearning/Learning/#the-ai-code-editor","title":"The AI\u00a0Code Editor","text":"<p>cursor</p>"},{"location":"AIML/DeepLearning/Learning/#connect-your-llm-to-the-web","title":"Connect Your\u00a0LLM to the Web","text":"<p>tavily</p>"},{"location":"AIML/DeepLearning/Learning/#vercel","title":"vercel","text":"<p>vercel</p>"},{"location":"AIML/DeepLearning/Learning/#firecrawl","title":"firecrawl","text":"<p>firecrawl</p>"},{"location":"AIML/DeepLearning/Pytorch/","title":"Pytorch","text":""},{"location":"AIML/DeepLearning/Pytorch/#linear-regression-dl-using-pt-ks-and-tf","title":"Linear Regression DL using PT KS and TF","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\n</code></pre> <pre><code># Generate synthetic data\nX, y = make_regression(n_samples=10000, n_features=10, noise=0.1)\ny = y.reshape(-1, 1)  # Reshape to match output dimensions\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</code></pre> <pre><code>X_train[0:5]\n</code></pre> <pre><code># Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre> <pre><code># Convert to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n</code></pre> <pre><code>class LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n</code></pre> <pre><code># Instantiate the model\ninput_dim = X_train.shape[1]\noutput_dim = 1\nmodel = LinearRegressionModel(input_dim, output_dim)\n</code></pre> <pre><code>criterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.05)\n</code></pre> <pre><code>num_epochs = 1000\nfor epoch in range(num_epochs):\n    model.train()\n\n    # Forward pass\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n</code></pre> <pre><code>model.eval()\n</code></pre> <pre><code>model.eval()\nwith torch.no_grad():\n    predicted = model(X_test)\n    predicted = scaler_y.inverse_transform(predicted.numpy())\n    actual = scaler_y.inverse_transform(y_test.numpy())\n\n# Plot predictions vs actual\nplt.scatter(actual, predicted, color='blue')\nplt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Pytorch/#boston-housing-price-prediction","title":"Boston Housing Price Prediction","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code>import pandas as pd\nboston = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\")\nboston.info()\n</code></pre> <pre><code>boston.head()\n</code></pre> <pre><code>y = boston.pop(\"medv\")\nX = boston\n</code></pre> <pre><code>y = np.array(y)\ny\n</code></pre> <pre><code>y.reshape(-1, 1)\n</code></pre>"},{"location":"AIML/DeepLearning/Pytorch/#load-the-dataset","title":"Load the dataset","text":"<p>y = y.reshape(-1, 1)  # Reshape to match output dimensions</p>"},{"location":"AIML/DeepLearning/Pytorch/#split-data-into-training-and-test-sets","title":"Split data into training and test sets","text":"<p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</p>"},{"location":"AIML/DeepLearning/Pytorch/#standardize-the-data","title":"Standardize the data","text":"<p>scaler_X = StandardScaler() scaler_y = StandardScaler()</p> <p>X_train = scaler_X.fit_transform(X_train) X_test = scaler_X.transform(X_test) y_train = scaler_y.fit_transform(y_train) y_test = scaler_y.transform(y_test)</p>"},{"location":"AIML/DeepLearning/Pytorch/#convert-to-pytorch-tensors","title":"Convert to PyTorch tensors","text":"<p>X_train = torch.tensor(X_train, dtype=torch.float32) X_test = torch.tensor(X_test, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32)  X_train.shape, X_test.shape, y_train.shape, y_test.shape  class LinearRegressionModel(nn.Module):     def init(self, input_dim, output_dim):         super(LinearRegressionModel, self).init()         self.linear = nn.Linear(input_dim, output_dim)</p> <pre><code>def forward(self, x):\n    return self.linear(x)\n</code></pre>"},{"location":"AIML/DeepLearning/Pytorch/#instantiate-the-model","title":"Instantiate the model","text":"<p>input_dim = X_train.shape[1] output_dim = 1 model = LinearRegressionModel(input_dim, output_dim)  criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.05)  num_epochs = 5000 for epoch in range(num_epochs):     model.train()</p> <pre><code># Forward pass\noutputs = model(X_train)\nloss = criterion(outputs, y_train)\n\n# Backward pass and optimization\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\nif (epoch+1) % 100 == 0:\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n</code></pre> <p><code></code> model.eval() with torch.no_grad():     predicted = model(X_test)     predicted = scaler_y.inverse_transform(predicted.numpy())     actual = scaler_y.inverse_transform(y_test.numpy())</p>"},{"location":"AIML/DeepLearning/Pytorch/#plot-predictions-vs-actual","title":"Plot predictions vs actual","text":"<p>plt.scatter(actual, predicted, color='blue') plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2) plt.xlabel('Actual') plt.ylabel('Predicted') plt.title('Predicted vs Actual') plt.show()  from sklearn.metrics import mean_squared_error, r2_score</p> <p>def calculate_accuracy(actual, predicted):     # Calculate Mean Squared Error     mse = mean_squared_error(actual, predicted)</p> <pre><code># Calculate R\u00b2 score\nr2 = r2_score(actual, predicted)\n\nprint(f'Mean Squared Error (MSE): {mse:.4f}')\nprint(f'R\u00b2 Score: {r2:.4f}')\n\nreturn mse, r2\n</code></pre>"},{"location":"AIML/DeepLearning/Pytorch/#call-the-function","title":"Call the function","text":"<p>mse, r2 = calculate_accuracy(actual, predicted) ```</p>"},{"location":"AIML/DeepLearning/Tensorflow/","title":"Tensorflow","text":""},{"location":"AIML/DeepLearning/Tensorflow/#tf-version-of-the-code","title":"TF Version of the code","text":"<p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.losses import MeanSquaredError\n</code></pre> <pre><code># Generate synthetic data\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\ny = y.reshape(-1, 1)  # Reshape to match output dimensions\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre></p> <pre><code># Define the model\nmodel = Sequential()\nmodel.add(Dense(units=1, input_dim=X_train.shape[1], activation='linear'))\n\n# Compile the model\nmodel.compile(optimizer=SGD(learning_rate=0.01), loss=MeanSquaredError())\n</code></pre> <pre><code># Train the model\nhistory = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n</code></pre> <pre><code># Evaluate the model\ny_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\n# Plot predictions vs actual\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Tensorflow/#tensorflow-version-of-the-code","title":"Tensorflow version of the code","text":"<pre><code># Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n\n# Convert to TensorFlow tensors\nX_train = tf.constant(X_train, dtype=tf.float32)\nX_test = tf.constant(X_test, dtype=tf.float32)\ny_train = tf.constant(y_train, dtype=tf.float32)\ny_test = tf.constant(y_test, dtype=tf.float32)\n</code></pre> <pre><code>model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1, input_dim=X_train.shape[1], activation='linear')\n])\n</code></pre> <pre><code>model.compile(optimizer='sgd', loss='mean_squared_error')\n</code></pre> <pre><code>history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n</code></pre> <pre><code>y_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre> <pre><code># making it deep using TF\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n</code></pre> <pre><code># Convert to TensorFlow tensors\nX_train = tf.constant(X_train, dtype=tf.float32)\nX_test = tf.constant(X_test, dtype=tf.float32)\ny_train = tf.constant(y_train, dtype=tf.float32)\ny_test = tf.constant(y_test, dtype=tf.float32)\n</code></pre> <pre><code># Define the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, input_dim=X_train.shape[1], activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='linear')\n])\n</code></pre> <pre><code>model.summary()\n</code></pre> <pre><code># Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n</code></pre> <pre><code># Train the model\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n</code></pre> <pre><code># Evaluate the model\ny_pred = model.predict(X_test)\ny_pred = scaler_y.inverse_transform(y_pred)\ny_test_inverse = scaler_y.inverse_transform(y_test)\n</code></pre> <pre><code># Plot predictions vs actual\nplt.scatter(y_test_inverse, y_pred, color='blue')\nplt.plot([y_test_inverse.min(), y_test_inverse.max()], [y_test_inverse.min(), y_test_inverse.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Predicted vs Actual')\nplt.show()\n</code></pre>"},{"location":"AIML/DeepLearning/Tensorflow/#refined-intro-to-tf","title":"Refined Intro to TF","text":"<pre><code>import tensorflow as tf\n</code></pre> <pre><code>string = tf.Variable(\"this is a string\", tf.string)\nstring\n</code></pre> <pre><code>string = tf.Variable(\"this is a string\", tf.string)\nnumber = tf.Variable(324, tf.int16)\nfloating = tf.Variable(3.567, tf.float32)\n</code></pre> <pre><code>11*8*8*8*8*8*8*4\n</code></pre> <pre><code>rank1_tensor = tf.Variable([\"Test\"], tf.string)\nrank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string)\n</code></pre> <pre><code>tf.rank(rank1_tensor)\n#tf.shape(rank2_tensor)\n</code></pre> <pre><code>t1 = tf.zeros([1,2,3])\nt2 = tf.reshape(t1, [2,3,1])\nt3 = tf.reshape(t2, [3,-1])\n</code></pre> <pre><code>### Refined Code for TensorFlow 2.x\n\nimport tensorflow as tf\n\n# Tensor Creation Examples\nstring = tf.Variable(\"this is a string\", tf.string)\nnumber = tf.Variable(324, tf.int16)\nfloating = tf.Variable(3.567, tf.float32)\n\n# Tensor Rank Examples\nrank1_tensor = tf.Variable([\"Test\"], tf.string)\nrank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string)\n\n# Tensor Shape Examples\nprint(tf.rank(rank2_tensor))\nprint(tf.shape(rank2_tensor))\n\n# Reshaping Tensors\nt1 = tf.zeros([1,2,3])\nt2 = tf.reshape(t1, [2,3,1])\nt3 = tf.reshape(t2, [3,-1])\n\n# Placeholder Example (TensorFlow 2.x does not have placeholders)\n@tf.function\ndef func(x):\n    return x\n\noutput = func(tf.constant('Hello World'))\nprint(output)\n\n# Placeholder with feed_dict Example (Converted to TensorFlow 2.x)\n@tf.function\ndef func(x, y, z):\n    return x, y, z\n\noutput_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67))\nprint(output_x)\nprint(output_y)\nprint(output_z)\n\n# TensorFlow Math Functions\nx = tf.add(5, 2)\ny = tf.subtract(10, 4)\nz = tf.multiply(2, 5)\n\n# Matrix Multiplication\nx = tf.constant([[1,2], [3,4]])\ny = tf.constant([[1,1], [1,1]])\n\nz = tf.matmul(x, y)\n\n# Softmax Function\nx = tf.constant([2.0, 1.0, 0.1])\nprint(tf.nn.softmax(x))\n\n# Cross Entropy Example\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\n\nsoftmax = tf.constant(softmax_data)\none_hot = tf.constant(one_hot_data)\n\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax)))\nprint(cross_entropy)\n</code></pre> <pre><code>import tensorflow as tf\n\n@tf.function\ndef func(x):\n    return x\n\noutput = func(tf.constant('Hello World'))\nprint(output)\n</code></pre> <pre><code>import tensorflow as tf\n\n# Tensor Creation Examples\nstring = tf.Variable(\"this is a string\", tf.string)\nnumber = tf.Variable(324, tf.int16)\nfloating = tf.Variable(3.567, tf.float32)\n\n# Tensor Rank Examples\nrank1_tensor = tf.Variable([\"Test\"], tf.string)\nrank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string)\n\n# Tensor Shape Examples\nprint(tf.rank(rank2_tensor))\nprint(tf.shape(rank2_tensor))\n\n# Reshaping Tensors\nt1 = tf.zeros([1,2,3])\nt2 = tf.reshape(t1, [2,3,1])\nt3 = tf.reshape(t2, [3,-1])\n\n# Placeholder Example (TensorFlow 2.x does not have placeholders)\n@tf.function\ndef func(x):\n    return x\n\noutput = func(tf.constant('Hello World'))\nprint(output)\n\n# Placeholder with feed_dict Example (Converted to TensorFlow 2.x)\n@tf.function\ndef func(x, y, z):\n    return x, y, z\n\noutput_x, output_y, output_z = func(tf.constant('Test String'), tf.constant(123), tf.constant(45.67))\nprint(output_x)\nprint(output_y)\nprint(output_z)\n\n# TensorFlow Math Functions\nx = tf.add(5, 2)\ny = tf.subtract(10, 4)\nz = tf.multiply(2, 5)\n\n# Matrix Multiplication\nx = tf.constant([[1,2], [3,4]])\ny = tf.constant([[1,1], [1,1]])\n\nz = tf.matmul(x, y)\n\n# Softmax Function\nx = tf.constant([2.0, 1.0, 0.1])\nprint(tf.nn.softmax(x))\n\n# Cross Entropy Example\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\n\nsoftmax = tf.constant(softmax_data)\none_hot = tf.constant(one_hot_data)\n\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.math.log(softmax)))\nprint(cross_entropy)\n</code></pre>"},{"location":"AIML/DeepLearning/Tensorflow/#tf-regression","title":"TF regression","text":""},{"location":"AIML/DeepLearning/Tensorflow/#basic-regression-predict-fuel-efficiency","title":"Basic regression: Predict fuel efficiency","text":"<p>In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).</p> <p>This tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.</p> <pre><code># Use seaborn for pairplot.\n!pip install -q seaborn\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Make NumPy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n</code></pre> <pre><code>import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)\n</code></pre>"},{"location":"AIML/DeepLearning/Tensorflow/#the-auto-mpg-dataset","title":"The Auto MPG dataset","text":"<p>The dataset is available from the UCI Machine Learning Repository. https://archive.ics.uci.edu/</p> <p>Get the data</p> <p>First download and import the dataset using pandas:</p> <pre><code>url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\ncolumn_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n                'Acceleration', 'Model Year', 'Origin']\n\nraw_dataset = pd.read_csv(url, names=column_names,\n                          na_values='?', comment='\\t',\n                          sep=' ', skipinitialspace=True)\n</code></pre> <pre><code>dataset = raw_dataset.copy()\ndataset.tail()\n</code></pre> <p>Clean the data</p> <p>The dataset contains a few unknown values:</p> <pre><code>dataset.isna().sum()\n</code></pre> <p>Drop those rows to keep this initial tutorial simple:</p> <pre><code>dataset = dataset.dropna()\n</code></pre> <p>The \"Origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies.</p> <p>Note: You can set up the tf.keras.Model to do this kind of transformation for you but that's beyond the scope of this tutorial.</p> <pre><code>dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n</code></pre> <pre><code>dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\ndataset.tail()\n</code></pre> <p>Split the data into training and test sets:</p> <p>Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.</p> <pre><code>train_dataset = dataset.sample(frac=0.8, random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n</code></pre> <p>Inspect the data</p> <p>Review the joint distribution of a few pairs of columns from the training set.</p> <p>The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.</p> <pre><code>sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')\n</code></pre> <p></p> <pre><code>train_dataset.describe().transpose()\n</code></pre> <p>Split features from labels</p> <p>Separate the target value\u2014the \"label\"\u2014from the features. This label is the value that you will train the model to predict.</p> <pre><code>train_features = train_dataset.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = train_features.pop('MPG')\ntest_labels = test_features.pop('MPG')\n</code></pre> <p>Normalization</p> <p>In the table of statistics it's easy to see how different the ranges of each feature are:</p> <pre><code>train_dataset.describe().transpose()[['mean', 'std']]\n</code></pre> <p>It is good practice to normalize features that use different scales and ranges.</p> <p>One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.</p> <p>Although a model might converge without feature normalization, normalization makes training much more stable.</p> <p>Note: There is no advantage to normalizing the one-hot features\u2014it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial.</p> <p>The Normalization layer</p> <p>The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model.</p> <p>The first step is to create the layer:</p> <pre><code>normalizer = tf.keras.layers.Normalization(axis=-1)\n</code></pre> <p>Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt:</p> <pre><code>normalizer.adapt(np.array(train_features))\n</code></pre> <p>Calculate the mean and variance, and store them in the layer:</p> <pre><code>print(normalizer.mean.numpy())\n</code></pre> <p>When the layer is called, it returns the input data, with each feature independently normalized:</p> <pre><code>first = np.array(train_features[:1])\n\nwith np.printoptions(precision=2, suppress=True):\n  print('First example:', first)\n  print()\n  print('Normalized:', normalizer(first).numpy())\n</code></pre>"},{"location":"AIML/DeepLearning/Tensorflow/#linear-regression","title":"Linear regression","text":"<p>Before building a deep neural network model, start with linear regression using one and several variables.</p> <p>Linear regression with one variable</p> <p>Begin with a single-variable linear regression to predict 'MPG' from 'Horsepower'.</p> <p>Training a model with tf.keras typically starts by defining the model architecture. Use a tf.keras.Sequential model, which represents a sequence of steps.</p> <p>There are two steps in your single-variable linear regression model:</p> <ul> <li>Normalize the 'Horsepower' input features using the tf.keras.layers.Normalization preprocessing layer.</li> <li>Apply a linear transformation () to produce 1 output using a linear layer (tf.keras.layers.Dense).</li> </ul> <p>The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.</p> <p>First, create a NumPy array made of the 'Horsepower' features. Then, instantiate the tf.keras.layers.Normalization and fit its state to the horsepower data:</p> <pre><code>horsepower = np.array(train_features['Horsepower'])\n\nhorsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\nhorsepower_normalizer.adapt(horsepower)\n</code></pre> <p>Build the Keras Sequential model:</p> <pre><code>horsepower_model = tf.keras.Sequential([\n    horsepower_normalizer,\n    layers.Dense(units=1)\n])\n\nhorsepower_model.summary()\n</code></pre> <p>This model will predict 'MPG' from 'Horsepower' Run the untrained model on the first 10 'Horsepower' values. The output won't be good, but notice that it has the expected shape of (10, 1):</p> <p>Once the model is built, configure the training procedure using the Keras Model.compile method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the tf.keras.optimizers.Adam).</p> <pre><code>horsepower_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')\n</code></pre> <p>Use Keras Model.fit to execute the training for 100 epochs:</p> <pre><code>%%time\nhistory = horsepower_model.fit(\n    train_features['Horsepower'],\n    train_labels,\n    epochs=100,\n    # Suppress logging.\n    verbose=0,\n    # Calculate validation results on 20% of the training data.\n    validation_split = 0.2)\n</code></pre> <p>Visualize the model's training progress using the stats stored in the history object:</p> <pre><code>hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()\n</code></pre> <pre><code>def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)\n</code></pre> <pre><code>plot_loss(history)\n</code></pre> <p></p> <p>Collect the results on the test set for later:</p> <pre><code>test_results = {}\n\ntest_results['horsepower_model'] = horsepower_model.evaluate(\n    test_features['Horsepower'],\n    test_labels, verbose=0)\n</code></pre> <pre><code>x = tf.linspace(0.0, 250, 251)\ny = horsepower_model.predict(x)\n</code></pre> <pre><code>def plot_horsepower(x, y):\n  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()\n</code></pre> <p>Since this is a single variable regression, it's easy to view the model's predictions as a function of the input:</p> <pre><code>def plot_horsepower(x, y):\n  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()\n</code></pre> <pre><code>plot_horsepower(x, y)\n</code></pre> <p></p>"},{"location":"AIML/DeepLearning/Tensorflow/#linear-regression-with-multiple-inputs","title":"Linear regression with multiple inputs","text":"<p>You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same y = mx + b except that m is a matrix and x is a vector.</p> <p>Create a two-step Keras Sequential model again with the first layer being normalizer (tf.keras.layers.Normalization(axis=-1)) you defined earlier and adapted to the whole dataset:</p> <pre><code>linear_model = tf.keras.Sequential([\n    normalizer,\n    layers.Dense(units=1)\n])\n</code></pre> <p>When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example:</p> <pre><code>linear_model.predict(train_features[:10])\n</code></pre> <p>When you call the model, its weight matrices will be built\u2014check that the kernel weights (the m in y  = mx + b) have a shape (9, 1)</p> <pre><code>linear_model.layers[1].kernel\n</code></pre> <p>Configure the model with Keras Model.compile and train with Model.fit for 100 epochs:</p> <pre><code>linear_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')\n</code></pre> <pre><code>%%time\nhistory = linear_model.fit(\n    train_features,\n    train_labels,\n    epochs=100,\n    # Suppress logging.\n    verbose=0,\n    # Calculate validation results on 20% of the training data.\n    validation_split = 0.2)\n</code></pre> <p>Using all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input:</p> <p>plot_loss(history)</p> <p></p>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/","title":"Deepseek-R1 ollama","text":""},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#model-dockerhub-link","title":"Model Dockerhub link","text":"<p>Model Dockerhub link</p>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#deploy-deepseek-r1-ollama-in-local-aws-eks-with-alb-ingress","title":"Deploy Deepseek-R1 ollama in local AWS EKS with ALB Ingress","text":""},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#deployment","title":"Deployment","text":"<pre><code>deepseek-r1-deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deepseek-r1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: deepseek-r1\n  template:\n    metadata:\n      labels:\n        app: deepseek-r1\n    spec:\n      containers:\n      - name: deepseek-r1\n        image: mdelapenya/deepseek-r1:0.5.4-7b\n        ports:\n        - containerPort: 11434\n</code></pre>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#service","title":"Service","text":"<pre><code>deepseek-r1-service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: deepseek-r1-service\nspec:\n  selector:\n    app: deepseek-r1\n  ports:\n  - protocol: TCP\n    port: 11434\n    targetPort: 11434\n  type: ClusterIP\n</code></pre>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#ingress","title":"Ingress","text":"<pre><code>deepseek-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: deepseek-r1-service\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:777203855866:certificate/b7856d98-d602-4a77-afdf-98d0b00706ff\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=900\nspec:\n  ingressClassName: alb\n  tls:\n   - hosts:\n       - deepseek-r1.visionaryai.aimledu.com\n  rules:\n    - host: deepseek-r1.visionaryai.aimledu.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: deepseek-r1-service\n                port:\n                  number: 11434\n</code></pre>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#deployment_1","title":"Deployment","text":""},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#create-namespace-deepseek","title":"Create namespace deepseek","text":"<pre><code>kubectl create namespace deepseek\n</code></pre>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#deploy-deepseek-service-ingress","title":"Deploy deepseek, service, ingress","text":"<pre><code>kubectl apply -f deepseek-r1-deployment.yaml -n deepseek\n\nkubectl apply -f deepseek-r1-service.yaml -n deepseek\n\nkubectl apply -f deepseek-ingress.yaml -n deepseek\n</code></pre> <pre><code>kubectl -n deepseek logs -f pod/deepseek-r1-6b58ff58bc-7nznv\n2025/05/21 12:43:58 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-05-21T12:43:58.101Z level=INFO source=images.go:757 msg=\"total blobs: 5\"\ntime=2025-05-21T12:43:58.101Z level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n - using env:   export GIN_MODE=release\n - using code:  gin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --&gt; github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --&gt; github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --&gt; github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --&gt; github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --&gt; github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --&gt; github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --&gt; github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --&gt; github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --&gt; github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --&gt; github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --&gt; github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --&gt; github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --&gt; github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --&gt; github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --&gt; github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --&gt; github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --&gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --&gt; github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --&gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --&gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --&gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --&gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --&gt; github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --&gt; github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2025-05-21T12:43:58.102Z level=INFO source=routes.go:1310 msg=\"Listening on [::]:11434 (version 0.5.4-0-g2ddc32d-dirty)\"\ntime=2025-05-21T12:43:58.102Z level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=\"[cuda_v12_avx cpu cpu_avx cpu_avx2 cuda_v11_avx]\"\ntime=2025-05-21T12:43:58.102Z level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\ntime=2025-05-21T12:43:58.501Z level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-ec991403-1199-e627-c275-11191969eefd library=cuda variant=v12 compute=8.6 driver=12.8 name=\"NVIDIA A10G\" total=\"22.3 GiB\" available=\"8.0 GiB\"\n</code></pre>"},{"location":"AIML/FinetuneModel/Deepseek-r1-ollama/#post-successfull-deployment-how-to-test-use","title":"Post successfull deployment how to test &amp; use","text":"<pre><code>curl -X POST https://deepseek-r1.visionaryai.aimledu.com/api/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"deepseek-coder:6.7b\"}'\n</code></pre> <pre><code>curl -X POST https://deepseek-r1.visionaryai.aimledu.com/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-coder:6.7b\",\n    \"prompt\": \"Explain what a Kubernetes ingress controller does.\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST https://deepseek-r1.visionaryai.aimledu.com/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-coder:6.7b\",\n    \"prompt\": \"Explain what a Kubernetes ingress controller does.\",\n    \"stream\": false\n  }'\n{\"model\":\"deepseek-coder:6.7b\",\"created_at\":\"2025-05-21T13:04:43.677107124Z\",\"response\":\"A Kubernetes Ingress Controller is a dedicated component that acts as an API frontend for services in your cluster, directing HTTP(S) traffic to the appropriate service based on rules defined by you or operators. \\n\\nIn other words, it's like a load balancer but specifically designed to work with Kubernetes and integrate with the ingress resources. Ingress Controllers are responsible for routing external traffic into services within your cluster, which means directing HTTP(S) requests based on the request host or path to specific services.\\n\\nThere are several types of Ingress controllers available:\\n\\n1. NGINX: A popular choice, it's known for its high performance and stability, especially with heavy traffic loads.\\n2. Traefik: An open-source project that offers a dynamic reverse proxy solution. \\n3. HAProxy: Also known for its speed and robustness in handling requests, especially when dealing with SSL offloading.\\n4. Amazon ALB Ingress Controller: For AWS users, it integrates with the Application Load Balancer (ALB) to manage external access to services within an EKS cluster.\\n5. Contour: A lightweight Ingress controller using Envoy as its data plane.\\n6. GCE or GKE Ingress: These are specifically for Google Cloud Platform's products, and integrate with their load balancers.\\n\\nThe main responsibility of the ingress controller is to provide a layer 7 routing mechanism that can distribute traffic between different services within your Kubernetes cluster based on HTTP routes. This means you define rules about how external users should access your services, without those details being exposed to them directly.\\n\",\"done\":true,\"done_reason\":\"stop\",\"context\":[2042,417,274,20926,14244,20391,11,26696,254,20676,30742,339,8589,2008,11,6908,457,20676,30742,7958,11,285,340,885,3495,4301,4512,276,4531,8214,13,1487,4636,2223,13143,4301,11,5411,285,13936,4447,11,285,746,2159,12,13517,250,8214,4301,11,340,540,20857,276,3495,13,185,13518,3649,3475,25,185,1488,20667,852,245,716,31055,9350,6208,698,8888,1214,13,185,13518,21289,25,185,32,716,31055,9350,680,3524,18173,317,245,10653,5785,344,11773,372,274,8690,3853,408,327,3235,279,518,9654,11,1706,272,18125,7,50,8,9186,276,254,6854,2408,2842,331,6544,4212,457,340,409,10715,13,207,185,185,769,746,3061,11,359,6,82,833,245,3299,4862,12774,545,10184,5392,276,826,365,716,31055,9350,285,24729,365,254,6208,698,6177,13,680,3524,3458,20029,417,8874,327,27462,6659,9186,878,3235,2372,518,9654,11,585,2445,1706,272,18125,7,50,8,12443,2842,331,254,3092,3686,409,3076,276,3041,3235,13,185,185,2948,417,2961,4997,280,680,3524,630,20029,2315,25,185,185,16,13,461,16161,55,25,338,4493,4850,11,359,6,82,3174,327,891,1453,3779,285,13699,11,4386,365,6751,9186,18127,13,185,17,13,6726,811,1913,25,1633,1714,12,1905,2299,344,5157,245,10999,13322,15072,3402,13,207,185,18,13,414,32,18131,25,6067,3174,327,891,4575,285,13130,1457,279,14326,12443,11,4386,750,14029,365,25811,838,20711,13,185,19,13,11183,8855,33,680,3524,18173,25,1487,29182,4728,11,359,3834,980,365,254,15838,15748,9817,12774,207,7,1743,33,8,276,8800,6659,2451,276,3235,2372,274,426,17607,9654,13,185,20,13,3458,415,25,338,27395,680,3524,8888,1242,2344,85,1143,372,891,1189,9633,13,185,21,13,452,4402,409,452,7577,680,3524,25,3394,417,10184,327,5594,15948,27782,6,82,3888,11,285,24729,365,699,3299,4862,29664,13,185,185,546,1959,12374,280,254,6208,698,8888,317,276,2764,245,6271,207,22,27462,12379,344,482,27898,9186,1433,1442,3235,2372,518,716,31055,9350,9654,2842,331,18125,22168,13,997,2445,340,5928,6544,782,940,6659,4728,1020,2451,518,3235,11,1666,1454,4283,1430,14660,276,763,4712,13,185],\"total_duration\":3740760469,\"load_duration\":8736319,\"prompt_eval_count\":81,\"prompt_eval_duration\":4000000,\"eval_count\":353,\"eval_duration\":3726000000}%\n</code></pre>"},{"location":"AIML/FinetuneModel/llama_3_finetune/","title":"Fine-tuning a LLaMA model","text":"<p>Fine-tuning a LLaMA model with a custom dataset involves several steps. Here\u2019s a comprehensive guide tailored for your setup:</p>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#prerequisites","title":"Prerequisites","text":"<ol> <li>System Requirements:</li> <li>Sufficient GPU memory for training. For LLaMA 3.2 models (70B), multiple GPUs with high VRAM (e.g., A100s) are recommended.</li> <li> <p>Ensure your Mac system is connected to a machine with suitable GPUs (or use cloud-based resources like AWS, GCP, or Azure).</p> </li> <li> <p>Installed Software:</p> </li> <li>Python environment (Anaconda installed).</li> <li>Required libraries: transformers, datasets, bitsandbytes, torch, peft (for LoRA-based fine-tuning), and others.</li> <li>Proper setup for LLaMA model files. Confirm the downloaded model path is correct.</li> </ol>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#prepare-your-dataset","title":"Prepare Your Dataset","text":"<ul> <li> <p>Format: The dataset should ideally be in JSON or text format. Common formats include:</p> </li> <li> <p>Supervised Fine-tuning: { \"instruction\": \"What is X?\", \"response\": \"X is Y.\" }</p> </li> <li>Unsupervised Fine-tuning: Plain text data (tokenized).</li> </ul>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#loging-huggingface","title":"Loging huggingface","text":"<pre><code>from huggingface_hub import notebook_login\n\nnotebook_login()\n</code></pre>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#convert-custom-data-to-proper-format-data","title":"Convert custom data to proper format data.","text":"<pre><code>import json\n\n# Load the JSON data\nwith open(\"Supply-chain-logisitcs-problem.json\", \"r\") as f:\n    data = json.load(f)\n\n# Define the template for instruction-response\nformatted_data = []\nfor record in data:\n    instruction = f\"Provide details about Order ID {record['Order ID']}.\"\n    response = \", \".join([f\"{key}: {value}\" for key, value in record.items()])\n    formatted_data.append({\"instruction\": instruction, \"response\": response})\n\n# Save the formatted data to a JSONL file\nwith open(\"formatted_data.json\", \"w\") as f:\n    for entry in formatted_data:\n        f.write(json.dumps(entry) + \"\\n\")\n</code></pre>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#logging-into-wandbai","title":"Logging into wandb.ai.","text":"<ul> <li>Learn how to deploy a W&amp;B server locally: https://wandb.me/wandb-server</li> <li>You can find your API key in your browser here: https://wandb.ai/authorize</li> </ul>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#start-training-the-custom-data","title":"Start training the custom data","text":"<pre><code>import json\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n\n# Step 1: Load dataset\ndataset = load_dataset(\"json\", data_files=\"formatted_data.json\")[\"train\"]  # Access the 'train' split\n\n# Step 3: Load base model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"  # Replace with your LLaMA model\nmodel = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Ensure padding token is set\nif tokenizer.pad_token is None:\n    if tokenizer.eos_token is None:\n        # If there's no eos_token, add a pad token manually\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    else:\n        # If eos_token exists, use it as pad_token\n        tokenizer.pad_token = tokenizer.eos_token\n\n# We also need to resize the embedding layer of the model to accommodate the new token.\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Step 4: Preprocess the dataset for tokenization\ndef preprocess_data(example):\n    # Corrected the key to access the instruction text\n    inputs = tokenizer(example[\"instruction\"], truncation=True, padding=\"max_length\", max_length=512)\n    labels = tokenizer(example[\"response\"], truncation=True, padding=\"max_length\", max_length=512)\n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\n# Now we map the dataset, using the dataset loaded from the json file\nformatted_dataset = dataset.map(preprocess_data)\n\n\n\n# Step 6: Setup LoRA configuration\nlora_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n)\nmodel = get_peft_model(model, lora_config)\n\n# Step 7: Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./fine_tuned_llama\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    logging_dir=\"./logs\",\n    save_steps=200,\n    fp16=True,\n)\n\n# Step 8: Fine-tune the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=formatted_dataset,\n    tokenizer=tokenizer,  # Ensure tokenizer is passed for dynamic padding\n)\ntrainer.train()\n</code></pre>"},{"location":"AIML/FinetuneModel/llama_3_finetune/#download-the-folder","title":"Download the folder","text":"<pre><code>import shutil\n\n# Replace 'fine_tuned_llama' with the folder name you want to download\nshutil.make_archive('fine_tuned_llama', 'zip', 'fine_tuned_llama')\n\n# Download the zipped file\nfrom google.colab import files\nfiles.download('fine_tuned_llama.zip')\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datacleaning/","title":"Data Cleaning","text":""},{"location":"AIML/MachineLearningPipeline/datacleaning/#what-is-data-cleaning","title":"What is Data Cleaning?","text":"<p>Data cleaning is a crucial step in the machine learning (ML) pipeline, as it involves identifying and removing any missing, duplicate, or irrelevant data. The goal of data cleaning is to ensure that the data is accurate, consistent, and free of errors, as incorrect or inconsistent data can negatively impact the performance of the ML model.</p> <p>Professional data scientists usually invest a very large portion of their time in this step because of the belief that \u201cBetter data beats fancier algorithms\u201d.</p> <p>Data cleaning, also known as data cleansing or data preprocessing, is a crucial step in the data science pipeline that involves identifying and correcting or removing errors, inconsistencies, and inaccuracies in the data to improve its quality and usability. Data cleaning is essential because raw data is often noisy, incomplete, and inconsistent, which can negatively impact the accuracy and reliability of the insights derived from it.</p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#steps-to-perform-data-cleanliness","title":"Steps to Perform Data Cleanliness","text":"<p>Performing data cleaning involves a systematic process to identify and rectify errors, inconsistencies, and inaccuracies in a dataset. The following are essential steps to perform data cleaning.</p> <p></p> <ul> <li>Removal of Unwanted Observations: Identify and eliminate irrelevant or redundant observations from the dataset. The step involves scrutinizing data entries for duplicate records, irrelevant information, or data points that do not contribute meaningfully to the analysis. Removing unwanted observations streamlines the dataset, reducing noise and improving the overall quality.</li> <li>Fixing Structure errors: Address structural issues in the dataset, such as inconsistencies in data formats, naming conventions, or variable types. Standardize formats, correct naming discrepancies, and ensure uniformity in data representation. Fixing structure errors enhances data consistency and facilitates accurate analysis and interpretation.</li> <li>Managing Unwanted outliers: Identify and manage outliers, which are data points significantly deviating from the norm. Depending on the context, decide whether to remove outliers or transform them to minimize their impact on analysis. Managing outliers is crucial for obtaining more accurate and reliable insights from the data.</li> <li>Handling Missing Data: Devise strategies to handle missing data effectively. This may involve imputing missing values based on statistical methods, removing records with missing values, or employing advanced imputation techniques. Handling missing data ensures a more complete dataset, preventing biases and maintaining the integrity of analyses.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#how-to-perform-data-cleanliness","title":"How to Perform Data Cleanliness","text":"<p>Performing data cleansing involves a systematic approach to enhance the quality and reliability of a dataset. The process begins with a thorough understanding of the data, inspecting its structure and identifying issues such as missing values, duplicates, and outliers. Addressing missing data involves strategic decisions on imputation or removal, while duplicates are systematically eliminated to reduce redundancy. Managing outliers ensures that extreme values do not unduly influence analysis. Structural errors are corrected to standardize formats and variable types, promoting consistency.</p> <p>Throughout the process, documentation of changes is crucial for transparency and reproducibility. Iterative validation and testing confirm the effectiveness of the data cleansing steps, ultimately resulting in a refined dataset ready for meaningful analysis and insights.</p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#python-implementation-for-database-cleaning","title":"Python Implementation for Database Cleaning","text":"<p>Let\u2019s understand each step for Database Cleaning, using titanic dataset. Below are the necessary steps:</p> <ul> <li>Import the necessary libraries</li> <li>Load the dataset</li> <li>Check the data information using df.info()</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv('titanic.csv')\ndf.head()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#data-inspection-and-exploration","title":"Data Inspection and Exploration","text":"<p>Let\u2019s first understand the data by inspecting its structure and identifying missing values, outliers, and inconsistencies and check the duplicate rows with below python code:</p> <pre><code>df.duplicated()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#check-the-data-information-using-dfinfo","title":"Check the data information using df.info()","text":"<pre><code>df.info()\n</code></pre> <p>From the above data info, we can see that Age and Cabin have an unequal number of counts. And some of the columns are categorical and have data type objects and some are integer and float values.</p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#check-the-categorical-and-numerical-columns","title":"Check the Categorical and Numerical Columns.","text":"<pre><code># Categorical columns\ncat_col = [col for col in df.columns if df[col].dtype == 'object']\nprint('Categorical columns :',cat_col)\n# Numerical columns\nnum_col = [col for col in df.columns if df[col].dtype != 'object']\nprint('Numerical columns :',num_col)\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#check-the-total-number-of-unique-values-in-the-categorical-columns","title":"Check the total number of Unique Values in the Categorical Columns","text":"<pre><code>df[cat_col].nunique()\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#removal-of-all-above-unwanted-observations","title":"Removal of all Above Unwanted Observations","text":"<p>This includes deleting duplicate/ redundant or irrelevant values from your dataset. Duplicate observations most frequently arise during data collection and Irrelevant observations are those that don\u2019t actually fit the specific problem that you\u2019re trying to solve.</p> <ul> <li>Redundant observations alter the efficiency to a great extent as the data repeats and may add towards the correct side or towards the incorrect side, thereby producing unfaithful results.</li> <li>Irrelevant observations are any type of data that is of no use to us and can be removed directly.</li> </ul> <p>Now we have to make a decision according to the subject of analysis, which factor is important for our discussion.</p> <p>As we know our machines don\u2019t understand the text data. So, we have to either drop or convert the categorical column values into numerical types. Here we are dropping the Name columns because the Name will be always unique and it hasn\u2019t a great influence on target variables. For the ticket, Let\u2019s first print the 50 unique tickets.</p> <pre><code>df['Ticket'].unique()[:50]\n</code></pre> <p></p> <p>From the above tickets, we can observe that it is made of two like first values \u2018A/5 21171\u2019 is joint from of \u2018A/5\u2019 and  \u201821171\u2019 this may influence our target variables. It will the case of Feature Engineering. where we derived new features from a column or a group of columns. In the current case, we are dropping the \u201cName\u201d and \u201cTicket\u201d columns.</p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#drop-name-and-ticket-columns","title":"Drop Name and Ticket Columns","text":"<pre><code>df1 = df.drop(columns=['Name','Ticket'])\ndf1.shape\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#handling-missing-data","title":"Handling Missing Data","text":"<p>Missing data is a common issue in real-world datasets, and it can occur due to various reasons such as human errors, system failures, or data collection issues. Various techniques can be used to handle missing data, such as imputation, deletion, or substitution.</p> <p>Let\u2019s check the % missing values columns-wise for each row using df.isnull() it checks whether the values are null or not and gives returns boolean values. and .sum() will sum the total number of null values rows and we divide it by the total number of rows present in the dataset then we multiply to get values in % i.e per 100 values how much values are null.</p> <pre><code>round((df1.isnull().sum()/df1.shape[0])*100,2)\n</code></pre> <p></p> <p>We cannot just ignore or remove the missing observation. They must be handled carefully as they can be an indication of something important. </p> <p>The two most common ways to deal with missing data are:</p> <ul> <li>Dropping Observations with missing values.<ul> <li>The fact that the value was missing may be informative in itself.</li> <li>Plus, in the real world, you often need to make predictions on new data even if some of the features are missing!</li> </ul> </li> </ul> <p>As we can see from the above result that Cabin has 77% null values and Age has 19.87% and Embarked has 0.22% of null values.</p> <p>So, it\u2019s not a good idea to fill 77% of null values. So, we will drop the Cabin column. Embarked column has only 0.22% of null values so, we drop the null values rows of Embarked column.</p> <pre><code>df2 = df1.drop(columns='Cabin')\ndf2.dropna(subset=['Embarked'], axis=0, inplace=True)\ndf2.shape\n</code></pre> <p></p> <ul> <li>Imputing the missing values from past observations.<ul> <li>Again, \u201cmissingness\u201d is almost always informative in itself, and you should tell your algorithm if a value was missing.</li> <li>Even if you build a model to impute your values, you\u2019re not adding any real information. You\u2019re just reinforcing the patterns already provided by other features.</li> </ul> </li> </ul> <p>We can use Mean imputation or Median imputations for the case.</p> <p>**Note: **</p> <ul> <li>Mean imputation is suitable when the data is normally distributed and has no extreme outliers.</li> <li>Median imputation is preferable when the data contains outliers or is skewed.</li> </ul> <pre><code># Mean imputation\ndf3 = df2.fillna(df2.Age.mean())\n# Let's check the null values again\ndf3.isnull().sum()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#handling-outliers","title":"Handling Outliers","text":"<p>Outliers are extreme values that deviate significantly from the majority of the data. They can negatively impact the analysis and model performance. Techniques such as clustering, interpolation, or transformation can be used to handle outliers.</p> <p>To check the outliers, We generally use a box plot. A box plot, also referred to as a box-and-whisker plot, is a graphical representation of a dataset\u2019s distribution. It shows a variable\u2019s median, quartiles, and potential outliers. The line inside the box denotes the median, while the box itself denotes the interquartile range (IQR). The whiskers extend to the most extreme non-outlier values within 1.5 times the IQR. Individual points beyond the whiskers are considered potential outliers. A box plot offers an easy-to-understand overview of the range of the data and makes it possible to identify outliers or skewness in the distribution.</p> <p>Let\u2019s plot the box plot for Age column data.</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.boxplot(df3['Age'], vert=False)\nplt.ylabel('Variable')\nplt.xlabel('Age')\nplt.title('Box Plot')\nplt.show()\n</code></pre> <p></p> <p>As we can see from the above Box and whisker plot, Our age dataset has outliers values. The values less than 5 and more than 55 are outliers.</p> <pre><code># calculate summary statistics\nmean = df3['Age'].mean()\nstd  = df3['Age'].std()\n\n# Calculate the lower and upper bounds\nlower_bound = mean - std*2\nupper_bound = mean + std*2\n\nprint('Lower Bound :',lower_bound)\nprint('Upper Bound :',upper_bound)\n\n# Drop the outliers\ndf4 = df3[(df3['Age'] &gt;= lower_bound) \n                &amp; (df3['Age'] &lt;= upper_bound)]\n</code></pre> <p></p> <p>Similarly, we can remove the outliers of the remaining columns.</p>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#data-transformation","title":"Data Transformation","text":"<p>Data transformation involves converting the data from one form to another to make it more suitable for analysis. Techniques such as normalization, scaling, or encoding can be used to transform the data.</p> <p>Data validation and verification</p> <p>Data validation and verification involve ensuring that the data is accurate and consistent by comparing it with external sources or expert knowledge. </p> <p>For the machine learning prediction, First, we separate independent and target features. Here we will consider only \u2018Sex\u2019 \u2018Age\u2019 \u2018SibSp\u2019, \u2018Parch\u2019 \u2018Fare\u2019 \u2018Embarked\u2019 only as the independent features and Survived as target variables. Because PassengerId will not affect the survival rate.</p> <pre><code>X = df3[['Pclass','Sex','Age', 'SibSp','Parch','Fare','Embarked']]\nY = df3['Survived']\n</code></pre> <p>Data formatting Data formatting involves converting the data into a standard format or structure that can be easily processed by the algorithms or models used for analysis. Here we will discuss commonly used data formatting techniques i.e. Scaling and Normalization.</p> <p>Scaling</p> <pre><code>- Scaling involves transforming the values of features to a specific range. It maintains the shape of the original distribution while changing the scale.\n- Particularly useful when features have different scales, and certain algorithms are sensitive to the magnitude of the features.\n- Common scaling methods include Min-Max scaling and Standardization (Z-score scaling).\n</code></pre> <p>Min-Max Scaling: Min-Max scaling rescales the values to a specified range, typically between 0 and 1. It preserves the original distribution and ensures that the minimum value maps to 0 and the maximum value maps to 1.</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# initialising the MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Numerical columns\nnum_col_ = [col for col in X.columns if X[col].dtype != 'object']\nx1 = X\n# learning the statistical parameters for each of the data and transforming\nx1[num_col_] = scaler.fit_transform(x1[num_col_])\nx1.head()\n</code></pre> <p></p> <p>Standardization (Z-score scaling): Standardization transforms the values to have a mean of 0 and a standard deviation of 1. It centers the data around the mean and scales it based on the standard deviation. Standardization makes the data more suitable for algorithms that assume a Gaussian distribution or require features to have zero mean and unit variance.</p> <p></p> <p>Where,</p> <pre><code>- X = Data\n- \u03bc = Mean value of X\n- \u03c3 = Standard deviation of X\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datacleaning/#data-cleansing-tools","title":"Data Cleansing Tools","text":"<p>Some data cleansing tools:</p> <ul> <li>OpenRefine</li> <li>Trifacta Wrangler </li> <li>TIBCO Clarity</li> <li>Cloudingo</li> <li>IBM Infosphere Quality Stage</li> </ul> <p>Advantages of Data Cleaning in Machine Learning:</p> <ul> <li>Improved model performance: Removal of errors, inconsistencies, and irrelevant data, helps the model to better learn from the data.</li> <li>Increased accuracy: Helps ensure that the data is accurate, consistent, and free of errors.</li> <li>Better representation of the data: Data cleaning allows the data to be transformed into a format that better represents the underlying relationships and patterns in the data.</li> <li>Improved data quality: Improve the quality of the data, making it more reliable and accurate.</li> <li>Improved data security: Helps to identify and remove sensitive or confidential information that could compromise data security.</li> </ul> <p>Disadvantages of Data Cleaning in Machine Learning:</p> <ul> <li>Time-consuming: Time-Consuming task, especially for large and complex datasets.</li> <li>Error-prone: Data cleaning can be error-prone, as it involves transforming and cleaning the data, which can result in the loss of important information or the introduction of new errors.</li> <li>Cost and resource-intensive: Resource-intensive process that requires significant time, effort, and expertise. It can also require the use of specialized software tools, which can add to the cost and complexity of data cleaning.</li> <li>Overfitting: Data cleaning can inadvertently contribute to overfitting by removing too much data.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/","title":"Data Preprocessing in Python","text":""},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#data-preprocessing","title":"Data Preprocessing","text":"<p>Pre-processing refers to the transformations applied to our data before feeding it to the algorithm. Data preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.</p> <p></p>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#need-of-data-preprocessing","title":"Need of Data Preprocessing","text":"<ul> <li>For achieving better results from the applied model in Machine Learning projects the format of the data has to be in a proper manner. Some specified Machine Learning model needs information in a specified format, for example, Random Forest algorithm does not support null values, therefore to execute random forest algorithm null values have to be managed from the original raw data set.</li> <li>Another aspect is that the data set should be formatted in such a way that more than one Machine Learning and Deep Learning algorithm are executed in one data set, and best out of them is chosen.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#steps-in-data-preprocessing","title":"Steps in Data Preprocessing","text":""},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-1-import-the-necessary-libraries","title":"Step 1: Import the necessary libraries","text":"<pre><code># importing libraries\nimport pandas as pd\nimport scipy\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-2-load-the-dataset","title":"Step 2: Load the dataset","text":"<p>Dataset link: [https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database]</p> <pre><code># Load the dataset\ndf = pd.read_csv('Geeksforgeeks/Data/diabetes.csv')\nprint(df.head())\n</code></pre> <p></p> <p>Check the data info</p> <pre><code>df.info()\n</code></pre> <p></p> <p>As we can see from the above info that the our dataset has 9 columns and each columns has 768 values. There is no Null values in the dataset.</p> <p>We can also check the null values using df.isnull()</p> <pre><code>df.isnull().sum()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-3-statistical-analysis","title":"Step 3: Statistical Analysis","text":"<p>In statistical analysis, first, we use the df.describe() which will give a descriptive overview of the dataset.</p> <pre><code>df.describe()\n</code></pre> <p></p> <p>The above table shows the count, mean, standard deviation, min, 25%, 50%, 75%, and max values for each column. When we carefully observe the table we will find that. Insulin, Pregnancies, BMI, BloodPressure columns has outliers. </p> <p>Let\u2019s plot the boxplot for each column for easy understanding.</p>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-4-check-the-outliers","title":"Step 4: Check the outliers:","text":"<pre><code># Box Plots\nfig, axs = plt.subplots(9,1,dpi=95, figsize=(7,17))\ni = 0\nfor col in df.columns:\n    axs[i].boxplot(df[col], vert=False)\n    axs[i].set_ylabel(col)\n    i+=1\nplt.show()\n</code></pre> <p>from the above boxplot, we can clearly see that all most every column has some amounts of outliers.</p> <p>Drop the outliers</p> <pre><code># Identify the quartiles\nq1, q3 = np.percentile(df['Insulin'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n# Drop the outliers\nclean_data = df[(df['Insulin'] &gt;= lower_bound) \n                &amp; (df['Insulin'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['Pregnancies'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n# Drop the outliers\nclean_data = clean_data[(clean_data['Pregnancies'] &gt;= lower_bound) \n                        &amp; (clean_data['Pregnancies'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['Age'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n# Drop the outliers\nclean_data = clean_data[(clean_data['Age'] &gt;= lower_bound) \n                        &amp; (clean_data['Age'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['Glucose'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n# Drop the outliers\nclean_data = clean_data[(clean_data['Glucose'] &gt;= lower_bound) \n                        &amp; (clean_data['Glucose'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['BloodPressure'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (0.75 * iqr)\nupper_bound = q3 + (0.75 * iqr)\n# Drop the outliers\nclean_data = clean_data[(clean_data['BloodPressure'] &gt;= lower_bound) \n                        &amp; (clean_data['BloodPressure'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['BMI'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n# Drop the outliers\nclean_data = clean_data[(clean_data['BMI'] &gt;= lower_bound) \n                        &amp; (clean_data['BMI'] &lt;= upper_bound)]\n\n\n# Identify the quartiles\nq1, q3 = np.percentile(clean_data['DiabetesPedigreeFunction'], [25, 75])\n# Calculate the interquartile range\niqr = q3 - q1\n# Calculate the lower and upper bounds\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\n# Drop the outliers\nclean_data = clean_data[(clean_data['DiabetesPedigreeFunction'] &gt;= lower_bound) \n                        &amp; (clean_data['DiabetesPedigreeFunction'] &lt;= upper_bound)]\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-5-correlation","title":"Step 5: Correlation","text":"<pre><code>#correlation\ncorr = df.corr()\n\nplt.figure(dpi=130)\nsns.heatmap(df.corr(), annot=True, fmt= '.2f')\nplt.show()\n</code></pre> <p>We can also camapare by single columns in descending order</p> <pre><code>corr['Outcome'].sort_values(ascending = False)\n</code></pre> <p></p> <p>Check Outcomes Proportionality</p> <pre><code>plt.pie(df.Outcome.value_counts(), \n        labels= ['Diabetes', 'Not Diabetes'], \n        autopct='%.f', shadow=True)\nplt.title('Outcome Proportionality')\nplt.show()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-6-separate-independent-features-and-target-variables","title":"Step 6: Separate independent features and Target Variables","text":"<pre><code># separate array into input and output components\nX = df.drop(columns =['Outcome'])\nY = df.Outcome\n</code></pre>"},{"location":"AIML/MachineLearningPipeline/datapreprocessingpython/#step-7-normalization-or-standardization","title":"Step 7: Normalization or Standardization","text":"<p>Normalization</p> <ul> <li>MinMaxScaler scales the data so that each feature is in the range [0, 1]. </li> <li>It works well when the features have different scales and the algorithm being used is sensitive to the scale of the features, such as k-nearest neighbors or neural networks.</li> <li>Rescale your data using scikit-learn using the MinMaxScaler.</li> </ul> <pre><code># initialising the MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# learning the statistical parameters for each of the data and transforming\nrescaledX = scaler.fit_transform(X)\nrescaledX[:5]\n</code></pre> <p></p> <p>Standardization - Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. - We can standardize data using scikit-learn with the StandardScaler class. - It works well when the features have a normal distribution or when the algorithm being used is not sensitive to the scale of the features</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\nrescaledX[:5]\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/","title":"Feature Scaling","text":""},{"location":"AIML/MachineLearningPipeline/featurescaling/#what-is-feature-scaling","title":"What is Feature Scaling?","text":"<p>Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.</p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#why-use-feature-scaling","title":"Why use Feature Scaling?","text":"<p>In machine learning, feature scaling is employed for a number of purposes:</p> <ul> <li>Scaling guarantees that all features are on a comparable scale and have comparable ranges. This process is known as feature normalisation. This is significant because the magnitude of the features has an impact on many machine learning techniques. Larger scale features may dominate the learning process and have an excessive impact on the outcomes. You can avoid this problem and make sure that each feature contributes equally to the learning process by scaling the features.</li> <li>Algorithm performance improvement: When the features are scaled, several machine learning methods, including gradient descent-based algorithms, distance-based algorithms (such k-nearest neighbours), and support vector machines, perform better or converge more quickly. The algorithm\u2019s performance can be enhanced by scaling the features, which can hasten the convergence of the algorithm to the ideal outcome.</li> <li>Preventing numerical instability: Numerical instability can be prevented by avoiding significant scale disparities between features. Examples include distance calculations or matrix operations, where having features with radically differing scales can result in numerical overflow or underflow problems. Stable computations are ensured and these issues are mitigated by scaling the features.</li> <li>Scaling features makes ensuring that each characteristic is given the same consideration during the learning process. Without scaling, bigger scale features could dominate the learning, producing skewed outcomes. This bias is removed through scaling, which also guarantees that each feature contributes fairly to model predictions.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#absolute-maximum-scaling","title":"Absolute Maximum Scaling","text":"<p>This method of scaling requires two-step:</p> <ol> <li>We should first select the maximum absolute value out of all the entries of a particular measure.</li> <li>Then after this, we divide each entry of the column by this maximum value.</li> </ol> <p></p> <p>After performing the above-mentioned two steps we will observe that each entry of the column lies in the range of -1 to 1. But this method is not used that often the reason behind this is that it is too sensitive to the outliers. And while dealing with the real-world data presence of outliers is a very common thing. </p> <p>For the demonstration purpose, we will use the dataset which you can download from here. This dataset is a simpler version of the original house price prediction dataset having only two columns from the original dataset. The first five rows of the original data are shown below:</p> <pre><code>import pandas as pd\ndf = pd.read_csv('SampleFile.csv')\nprint(df.head())\n</code></pre> <p></p> <p>Now let\u2019s apply the first method which is of the absolute maximum scaling. For this first, we are supposed to evaluate the absolute maximum values of the columns.</p> <pre><code>import numpy as np\nmax_vals = np.max(np.abs(df))\nmax_vals\n</code></pre> <p></p> <p>Now we are supposed to subtract these values from the data and then divide the results from the maximum values as well. </p> <pre><code>print((df - max_vals) / max_vals)\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#min-max-scaling","title":"Min-Max Scaling","text":"<p>This method of scaling requires below two-step:</p> <ol> <li>First, we are supposed to find the minimum and the maximum value of the column.</li> <li>Then we will subtract the minimum value from the entry and divide the result by the difference between the maximum and the minimum value.</li> </ol> <p></p> <p>As we are using the maximum and the minimum value this method is also prone to outliers but the range in which the data will range after performing the above two steps is between 0 to 1.</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_data, \n                        columns=df.columns)\nscaled_df.head()\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#normalization","title":"Normalization","text":"<p>This method is more or less the same as the previous method but here instead of the minimum value, we subtract each entry by the mean value of the whole data and then divide the results by the difference between the minimum and the maximum value.</p> <p></p> <pre><code>from sklearn.preprocessing import Normalizer\n\nscaler = Normalizer()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_data,\n                        columns=df.columns)\nprint(scaled_df.head())\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#standardization","title":"Standardization","text":"<p>This method of scaling is basically based on the central tendencies and variance of the data. </p> <ol> <li>First, we should calculate the mean and standard deviation of the data we would like to normalize.</li> <li>Then we are supposed to subtract the mean value from each entry and then divide the result by the standard deviation.</li> </ol> <p>This helps us achieve a normal distribution(if it is already normal but skewed) of the data with a mean equal to zero and a standard deviation equal to 1.</p> <p></p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_data,\n                        columns=df.columns)\nprint(scaled_df.head())\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/featurescaling/#robust-scaling","title":"Robust Scaling","text":"<p>In this method of scaling, we use two main statistical measures of the data.</p> <ul> <li>Median</li> <li>Inter-Quartile Range</li> </ul> <p>After calculating these two values we are supposed to subtract the median from each entry and then divide the result by the interquartile range.</p> <p></p> <pre><code>from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nscaled_data = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_data,\n                        columns=df.columns)\nprint(scaled_df.head())\n</code></pre> <p></p>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/","title":"ML workflow","text":""},{"location":"AIML/MachineLearningPipeline/mlworkflow/#machine-learning-lifecycle","title":"Machine Learning Lifecycle","text":"<p>The machine learning lifecycle is a process that guides the development and deployment of machine learning models in a structured way. It consists of various steps.</p> <p>Each step plays a crucial role in ensuring the success and effectiveness of the machine learning solution. By following the machine learning lifecycle, organizations can solve complex problems systematically, leverage data-driven insights, and create scalable and sustainable machine learning solutions that deliver tangible value. The steps to be followed in the machine learning lifecycle are:</p> <ol> <li>Problem Definition</li> <li>Data Collection</li> <li>Data Cleaning and Preprocessing</li> <li>Exploratory Data Analysis (EDA)</li> <li>Feature Engineering and Selection</li> <li>Model Selection</li> <li>Model Training</li> <li>Model Evaluation and Tuning</li> <li>Model Deployment</li> <li>Model Monitoring and Maintenance</li> </ol> <p></p>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-1-problem-definition","title":"Step 1: Problem Definition","text":"<p>Embarking on the machine learning journey involves a well-defined lifecycle, starting with the crucial step of problem definition. In this initial phase, stakeholders collaborate to identify the business problem at hand and frame it in a way that sets the stage for the entire process.</p> <p>By framing the problem in a comprehensive manner, the team establishes a foundation for the entire machine learning lifecycle. Crucial elements, such as project objectives, desired outcomes, and the scope of the task, are carefully delineated during this stage.</p> <p>Here are the basic features of problem definition:</p> <ul> <li>Collaboration: Work together with stakeholders to understand and define the business problem.</li> <li>Clarity: Clearly articulate the objectives, desired outcomes, and scope of the task.</li> <li>Foundation: Establish a solid foundation for the machine learning process by framing the problem comprehensively.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-2-data-collection","title":"Step 2: Data Collection","text":"<p>Following the precision of problem definition, the machine learning lifecycle progresses to the pivotal stage of data collection. This phase involves the systematic gathering of datasets that will serve as the raw material for model development. The quality and diversity of the data collected directly impact the robustness and generalizability of the machine learning model.</p> <p>During data collection, practitioners must consider the relevance of the data to the defined problem, ensuring that the selected datasets encompass the necessary features and characteristics. Additionally, factors such as data volume, quality, and ethical considerations play a crucial role in shaping the foundation for subsequent phases of the machine learning lifecycle. A meticulous and well-organized approach to data collection lays the groundwork for effective model training, evaluation, and deployment, ensuring that the resulting model is both accurate and applicable to real-world scenarios.</p> <p>Here are the basic features of Data Collection:</p> <ul> <li>Relevance: Collect data that is relevant to the defined problem and includes necessary features.</li> <li>Quality: Ensure data quality by considering factors like accuracy, completeness, and ethical considerations.</li> <li>Quantity: Gather sufficient data volume to train a robust machine learning model.</li> <li>Diversity: Include diverse datasets to capture a broad range of scenarios and patterns.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-3-data-cleaning-and-preprocessing","title":"Step 3: Data Cleaning and Preprocessing","text":"<p>With datasets in hand, the machine learning journey advances to the critical stages of data cleaning and preprocessing. Raw data, is often messy and unstructured. Data cleaning involves addressing issues such as missing values, outliers, and inconsistencies that could compromise the accuracy and reliability of the machine learning model.</p> <p>Preprocessing takes this a step further by standardizing formats, scaling values, and encoding categorical variables, creating a consistent and well-organized dataset. The objective is to refine the raw data into a format that facilitates meaningful analysis during subsequent phases of the machine learning lifecycle. By investing time and effort in data cleaning and preprocessing, practitioners lay the foundation for robust model development, ensuring that the model is trained on high-quality, reliable data.</p> <p>Here are the basic features of Data Cleaning and Preprocessing:</p> <ul> <li>Data Cleaning: Address issues such as missing values, outliers, and inconsistencies in the data.</li> <li>Data Preprocessing: Standardize formats, scale values, and encode categorical variables for consistency.</li> <li>Data Quality: Ensure that the data is well-organized and prepared for meaningful analysis.</li> <li>Data Integrity: Maintain the integrity of the dataset by cleaning and preprocessing it effectively.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-4-exploratory-data-analysis-eda","title":"Step 4: Exploratory Data Analysis (EDA)","text":"<p>Now, focus turns to understanding the underlying patterns and characteristics of the collected data. Exploratory Data Analysis (EDA) emerges as a pivotal phase, where practitioners leverage various statistical and visual tools to gain insights into the dataset's structure.</p> <p>During EDA, patterns, trends, and potential challenges are unearthed, providing valuable context for subsequent decisions in the machine learning process. Visualizations, summary statistics, and correlation analyses offer a comprehensive view of the data, guiding practitioners toward informed choices in feature engineering, model selection, and other critical aspects. EDA acts as a compass, directing the machine learning journey by revealing the intricacies of the data and informing the development of effective and accurate predictive models.</p> <p>Here are the basic features of Exploratory Data Analysis:</p> <ul> <li>Exploration: Use statistical and visual tools to explore the structure and patterns in the data.</li> <li>Patterns and Trends: Identify underlying patterns, trends, and potential challenges within the dataset.</li> <li>Insights: Gain valuable insights to inform decisions in later stages of the machine learning process.</li> <li>Decision Making: Use exploratory data analysis to make informed decisions about feature engineering and model selection.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-5-feature-engineering-and-selection","title":"Step 5: Feature Engineering and Selection","text":"<p>Feature engineering takes center stage as a transformative process that elevates raw data into meaningful predictors. Simultaneously, feature selection refines this pool of variables, identifying the most relevant ones to enhance model efficiency and effectiveness.</p> <p>Feature engineering involves creating new features or transforming existing ones to better capture patterns and relationships within the data. This creative process requires domain expertise and a deep understanding of the problem at hand, ensuring that the engineered features contribute meaningfully to the predictive power of the model. On the other hand, feature selection focuses on identifying the subset of features that most significantly impact the model's performance. This dual approach seeks to strike a delicate balance, optimizing the feature set for predictive accuracy while minimizing computational complexity.</p> <ul> <li>Feature Engineering: Create new features or transform existing ones to better capture patterns and relationships.</li> <li>Feature Selection: Identify the subset of features that most significantly impact the model's performance.</li> <li>Domain Expertise: Leverage domain knowledge to engineer features that contribute meaningfully to predictive power.</li> <li>Optimization: Balance feature set for predictive accuracy while minimizing computational complexity.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-6-model-selection","title":"Step 6: Model Selection","text":"<p>Navigating the machine learning lifecycle requires the judicious selection of a model that aligns with the defined problem and the characteristics of the dataset. Model selection is a pivotal decision that determines the algorithmic framework guiding the predictive capabilities of the machine learning solution. The choice depends on the nature of the data, the complexity of the problem, and the desired outcomes.</p> <p>Here are the basic features of Model Selection:</p> <ul> <li>Alignment: Select a model that aligns with the defined problem and characteristics of the dataset.</li> <li>Complexity: Consider the complexity of the problem and the nature of the data when choosing a model.</li> <li>Decision Factors: Evaluate factors like performance, interpretability, and scalability when selecting a model.</li> <li>Experimentation: Experiment with different models to find the best fit for the problem at hand.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-7-model-training","title":"Step 7: Model Training","text":"<p>With the selected model in place, the machine learning lifecycle advances to the transformative phase of model training. This process involves exposing the model to historical data, allowing it to learn patterns, relationships, and dependencies within the dataset.</p> <p>Model training is an iterative and dynamic journey, where the algorithm adjusts its parameters to minimize errors and enhance predictive accuracy. During this phase, the model fine-tunes its understanding of the data, optimizing its ability to make meaningful predictions. Rigorous validation processes ensure that the trained model generalizes well to new, unseen data, establishing a foundation for reliable predictions in real-world scenarios.</p> <p>Here are the basic features of Model Training:</p> <ul> <li>Training Data: Expose the model to historical data to learn patterns, relationships, and dependencies.</li> <li>Iterative Process: Train the model iteratively, adjusting parameters to minimize errors and enhance accuracy.</li> <li>Optimization: Fine-tune the model's understanding of the data to optimize predictive capabilities.</li> <li>Validation: Rigorously validate the trained model to ensure generalization to new, unseen data.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-8-model-evaluation-and-tuning","title":"Step 8: Model Evaluation and Tuning","text":"<p>Model evaluation involves rigorous testing against validation datasets, employing metrics such as accuracy, precision, recall, and F1 score to gauge its effectiveness.</p> <p>Evaluation is a critical checkpoint, providing insights into the model's strengths and weaknesses. If the model falls short of desired performance levels, practitioners initiate model tuning\u2014a process that involves adjusting hyperparameters to enhance predictive accuracy. This iterative cycle of evaluation and tuning is crucial for achieving the desired level of model robustness and reliability.</p> <p>Here are the basic features of Model Evaluation and Tuning:</p> <ul> <li>Evaluation Metrics: Use metrics like accuracy, precision, recall, and F1 score to evaluate model performance.</li> <li>Strengths and Weaknesses: Identify the strengths and weaknesses of the model through rigorous testing.</li> <li>Iterative Improvement: Initiate model tuning to adjust hyperparameters and enhance predictive accuracy.</li> <li>Model Robustness: Iterate through evaluation and tuning cycles to achieve desired levels of model robustness and reliability.</li> </ul>"},{"location":"AIML/MachineLearningPipeline/mlworkflow/#step-9-model-deployment","title":"Step 9: Model Deployment","text":"<p>Upon successful evaluation, the machine learning model transitions from development to real-world application through the deployment phase. Model deployment involves integrating the predictive solution into existing systems or processes, allowing stakeholders to leverage its insights for informed decision-making.</p> <p>Model deployment marks the culmination of the machine learning lifecycle, transforming theoretical insights into practical solutions that drive tangible value for organizations.</p> <p>Here are the basic features of Model Deployment:</p> <ul> <li>Integration: Integrate the trained model into existing systems or processes for real-world application.</li> <li>Decision Making: Use the model's predictions to inform decision-making and drive tangible value for organizations.</li> <li>Practical Solutions: Deploy the model to transform theoretical insights into practical solutions that address business needs.</li> <li>Continuous Improvement: Monitor model performance and make adjustments as necessary to maintain effectiveness over time.</li> </ul>"},{"location":"AIML/NVIDIA/nvidia-nemo/","title":"NVIDIA NeMo (Generative AI)","text":"<p>Build, customize, and deploy generative AI.</p> <p>NVIDIA NeMo</p> <p>For Developers</p> <p>Video</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#what-is-nvidia-nemo","title":"What Is NVIDIA NeMo?","text":"<ul> <li> <p>NVIDIA NeMo is an end-to-end platform for developing custom generative AI\u2014including large language models (LLMs), multimodal, vision, and speech AI \u2014anywhere. </p> </li> <li> <p>Deliver enterprise-ready models with precise data curation, cutting-edge customization, retrieval-augmented generation (RAG), and accelerated performance.</p> </li> <li> <p>NeMo is a part of the NVIDIA AI Foundry, a platform and service for building custom generative AI models with enterprise data and domain-specific knowledge.</p> </li> </ul>"},{"location":"AIML/NVIDIA/nvidia-nemo/#benefits-of-nvidia-nemo-for-generative-ai","title":"Benefits of NVIDIA NeMo for Generative AI","text":"<ol> <li>Flexible: Train and deploy generative AI anywhere, across clouds, data centers, and the edge.</li> <li>Production Ready: Deploy into production with a secure, optimized, full-stack solution that offers support, security, and API stability as part of NVIDIA AI Enterprise.</li> <li>Increased ROI: Quickly train, customize, and deploy large language models (LLMs), vision, multimodal, and speech AI at scale, reducing time to solution and increasing ROI.</li> <li>Accelerated Performance: Maximize throughput and minimize LLM training time with multi-node, multi-GPU training and inference.</li> <li>End-to-End Pipeline: Experience the benefits of a complete solution for the LLM pipeline\u2014from data processing and training to inference of generative AI models.</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-nemo/#complete-solution-for-building-enterprise-ready-llms","title":"Complete Solution for Building Enterprise-Ready LLMs","text":""},{"location":"AIML/NVIDIA/nvidia-nemo/#the-features-of-nvidia-nemo","title":"The Features of NVIDIA NeMo","text":""},{"location":"AIML/NVIDIA/nvidia-nemo/#accelerate-data-curation-nemo-curator","title":"Accelerate Data Curation - NeMo Curator","text":"<p>NVIDIA NeMo Curator is a GPU-accelerated data-curation tool that enables large-scale, high-quality datasets for pretraining LLMs.</p> <p>Read the Blog Try Tutorial Notebooks Apply for Early Access</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#simplify-fine-tuning-nemo-customizer","title":"Simplify Fine-Tuning - NeMo Customizer","text":"<p>NVIDIA NeMo Customizer is a high-performance, scalable microservice that simplifies fine-tuning and alignment of LLMs for domain-specific use cases, making it easier to adopt generative AI across industries.</p> <p>Read the Blog Apply for Early Access</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#evaluate-models-nemo-evaluator","title":"Evaluate Models - NeMo Evaluator","text":"<p>NVIDIA NeMo Evaluator provides automatic assessment of custom generative AI models across academic and custom benchmarks on any platform.</p> <p>Read the Blog Apply for Early Access</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#seamless-data-retrieval-nemo-retriever","title":"Seamless Data Retrieval - NeMo Retriever","text":"<p>NVIDIA NeMo Retriever is a collection of generative AI microservices that enable organizations to seamlessly connect custom models to diverse business data and deliver highly accurate responses.</p> <p>Read the Blog on Developing Production-Grade Text Retrieval Pipelines for RAG Start Prototyping</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#generative-ai-guardrails-nemo-guardrails","title":"Generative AI Guardrails - NeMo Guardrails","text":"<p>NVIDIA NeMo Guardrails orchestrates dialog management, ensuring accuracy, appropriateness, and security in smart applications with LLMs. It safeguards organizations overseeing generative AI systems.</p> <p>Access on GitHub</p>"},{"location":"AIML/NVIDIA/nvidia-nemo/#generative-ai-inference-nvidia-nim","title":"Generative AI Inference - NVIDIA NIM","text":"<p>NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inferencing across clouds, data centers, and workstations.</p> <p>Learn More Read the Blog Start Prototyping</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/","title":"NVIDIA NGC Catalog Collections","text":"<p>NVIDIA NGC Catalog</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#collections","title":"Collections","text":"<p>Collections are use-case based curated content available in one easy-to-use package for various applications including speech, intelligent video analytics, recommendations, and more. Collections make it easy to discover the compatible software containers, models, Jupyter notebooks and documentation to get started faster.</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#use-case","title":"Use Case","text":"<ol> <li>Natural Language Processing</li> <li>Object Detection</li> <li>Audio Synthesis</li> <li>Automatic Speech Recognition</li> <li>Image Segmentation</li> <li>Language Modeling</li> <li>Natural Language Understanding</li> <li>Translation</li> <li>Video Analytics</li> <li>Action Recognition</li> <li>Application Development</li> <li>Body Pose Classification</li> <li>Body Pose Estimation</li> <li>Emotion Classification</li> <li>Eye Gaze Estimation</li> <li>Facial Landmark Estimation</li> <li>Gesture Classification</li> <li>Heart Rate Estimation</li> <li>Image Synthesis</li> <li>Named Entity Recognition</li> <li>Question Answering</li> <li>Speech to Text</li> <li>Video enhancement</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#nvidia-platform","title":"NVIDIA Platform","text":"<ol> <li>Maxine</li> <li>PyTorch</li> <li>TensorFlow</li> <li>Omniverse</li> <li>NeMo</li> <li>DeepStream</li> <li>Morpheus</li> <li>JAX</li> <li>Metropolis</li> <li>CUDA</li> <li>Clara Parabricks</li> <li>Container Toolkit</li> <li>Deep Learning Examples</li> <li>HPC</li> <li>HPC SDK</li> <li>Metropolis Microservices</li> <li>Runs on RTX</li> <li>cuOpt</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#nvidia-nim","title":"NVIDIA NIM","text":"<ol> <li>NVIDIA NIM</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#nvidia-enterprise-platforms","title":"NVIDIA Enterprise Platforms","text":"<ol> <li>NVIDIA AI Enterprise Supported</li> <li>NVIDIA AI Enterprise Essentials</li> <li>NVIDIA Omniverse Enterprise Essentials</li> <li>NVIDIA MAXINE Early Access</li> <li>NVIDIA Omniverse Enterprise Supported</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#industry","title":"Industry","text":"<ol> <li>Healthcare</li> <li>HPC / Supercomputing</li> <li>Academia / Higher Education</li> <li>Cloud Services</li> <li>Energy</li> <li>Hardware / Semiconductor</li> <li>Manufacturing</li> <li>Media &amp; Entertainment</li> <li>Retail</li> <li>Smart Cities / Spaces</li> <li>Aerospace</li> <li>Agriculture</li> <li>Automotive / Transportation</li> <li>Financial Services</li> <li>Public Sector</li> <li>Robotics</li> <li>Telecommunications</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#solution","title":"Solution","text":"<ol> <li>AI</li> <li>DL</li> <li>Conversational AI</li> <li>Computer Vision</li> <li>High Performance Computing</li> <li>Inference</li> <li>Infrastructure Software</li> <li>ML</li> <li>GPU Accelerated Libraries</li> <li>Application Development</li> <li>Application Streaming</li> <li>Data Analytics</li> <li>Developer Tools</li> <li>NVIDIA AI</li> <li>Nucleus</li> <li>Recommender Systems</li> <li>Rendering</li> <li>Vision AI</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-collections/#other","title":"Other","text":"<ol> <li>Video</li> <li>Clara</li> <li>Covid 19</li> <li>Finetuning</li> <li>Indus Restaurant Quick Service</li> <li>Inference</li> <li>Language Generation</li> <li>Large Language Models</li> <li>Nvidia Ai Enterprise Supported</li> <li>Text To Speech</li> <li>Transfer Learning</li> <li>Ai</li> <li>Asr</li> <li>Bert</li> <li>Capitalization</li> <li>Citrinet</li> <li>Code Generation</li> <li>Computer Vision</li> <li>Conformer</li> <li>Conformer Transducer</li> <li>Contextnet</li> <li>Deepstream</li> <li>Fastpitch</li> <li>Holoscan</li> <li>Matchboxnet</li> <li>Megatron</li> <li>Nlp</li> <li>Nmt</li> <li>Nspect 81y4 Jscu</li> <li>Nspect Kem4 Ital</li> <li>Punctuation</li> <li>Quartznet</li> <li>Retrieval Augmented Generation</li> <li>Riva</li> <li>Sdk</li> <li>Speaker Diarization</li> <li>Speaker Recognition</li> <li>Speakernet</li> <li>Tacotron2</li> <li>Text To Code</li> <li>Text To Text</li> <li>Tts</li> <li>Vision Assistant</li> <li>Visual Question Answering</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-containers/","title":"NVIDIA NGC Catalog Containers","text":"<p>The NGC catalog hosts containers for AI/ML, metaverse, and HPC applications and are performance-optimized, tested, and ready to deploy on GPU-powered on-prem, cloud, and edge systems.</p> <p>containers</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-containers/#use-case","title":"Use Case","text":"<ol> <li>Natural Language Processing</li> <li>Video Analytics</li> <li>Recommendation</li> <li>Object Detection</li> <li>High Performance Computing</li> <li>Drug Discovery</li> <li>Natural Language Understanding</li> <li>Application Development</li> <li>Genome Sequencing</li> <li>Synthetic Data Generation</li> <li>Automatic Speech Recognition</li> <li>Forecasting</li> <li>Simulation and Modeling</li> <li>GPU Enablement with Kubernetes</li> <li>Graph Neural Networks</li> <li>Annotation</li> <li>Facial Landmark Estimation</li> <li>Heart Rate Estimation</li> <li>Image Segmentation</li> <li>Image Synthesis</li> <li>Language Modeling</li> <li>Question Answering</li> <li>Reinforcement Learning</li> <li>Speech enhancement</li> <li>Speech to Text</li> <li>Text to Speech</li> <li>Translation</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-containers/#nvidia-platform","title":"NVIDIA Platform","text":"<ol> <li>Clara</li> <li>PyTorch</li> <li>Omniverse</li> <li>DOCA</li> <li>Metropolis</li> <li>Triton Inference Server</li> <li>Clara AGX</li> <li>Merlin</li> <li>TensorFlow</li> <li>DeepStream</li> <li>Isaac</li> <li>Metropolis Microservices</li> <li>Morpheus</li> <li>RAPIDS</li> <li>NeMo</li> <li>Maxine</li> <li>Modulus</li> <li>Monai</li> <li>Deep Learning Institute</li> <li>PyTorch Geometric</li> <li>Riva</li> <li>CUDA</li> <li>CUDA Toolkit</li> <li>GPU Operator</li> <li>HPC</li> <li>Holoscan</li> <li>Container Toolkit</li> <li>HPC SDK</li> <li>TAO Toolkit</li> <li>cuOpt</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-helm-charts/","title":"NVIDIA NGC Catalog Helm Charts","text":"<p>Helm charts automate software deployment on Kubernetes clusters, allowing users to focus on consuming\u2014rather than installing\u2014their software. The NGC catalog hosts Kubernetes-ready Helm charts that make it easy to deploy powerful NVIDIA and third-party software.</p> <p>Helm Charts</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-helm-charts/#use-case","title":"Use Case","text":"<ol> <li>Image Segmentation</li> <li>Named Entity Recognition</li> <li>Object Detection</li> <li>Question Answering</li> <li>Video Analytics</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-helm-charts/#nvidia-platform","title":"NVIDIA Platform","text":"<ol> <li>Omniverse</li> <li>DOCA</li> <li>NeMo</li> <li>cuOpt</li> <li>Metropolis</li> <li>TensorRT</li> <li>Triton Inference Server</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-models/","title":"NVIDIA NGC Catalog Models","text":"<p>The NGC catalog offers 100s of pre-trained models for computer vision, speech, recommendation, and more. Bring AI faster to market by using these models as-is or quickly build proprietary models with a fraction of your custom data.</p> <p>Models</p>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-models/#use-case","title":"Use Case","text":"<ol> <li>Automatic Speech Recognition</li> <li>Natural Language Processing</li> <li>Language Modeling</li> <li>Natural Language Understanding</li> <li>Text to Speech</li> <li>Drug Discovery</li> <li>Question Answering</li> <li>Forecasting</li> <li>High Performance Computing</li> <li>Object Detection</li> <li>Recommendation</li> <li>Simulation and Modeling</li> <li>Audio Synthesis</li> <li>Speech to Text</li> <li>Annotation</li> <li>Graph Neural Networks</li> <li>Synthetic Data Generation</li> <li>Translation</li> <li>Image Segmentation</li> <li>Speech enhancement</li> <li>Video Analytics</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-models/#nvidia-platform","title":"NVIDIA Platform","text":"<ol> <li>NeMo</li> <li>Riva</li> <li>Deep Learning Examples</li> <li>Runs on RTX</li> <li>TAO Toolkit</li> <li>DeepStream</li> <li>Maxine</li> <li>Metropolis</li> <li>Clara</li> <li>PyTorch</li> <li>Modulus</li> <li>TensorRT</li> <li>CUDA</li> <li>Triton Inference Server</li> <li>Clara AGX</li> <li>Holoscan</li> <li>Isaac</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-models/#framework","title":"Framework","text":"<ol> <li>PyTorch</li> <li>NeMo</li> <li>TAO Toolkit</li> <li>PyTorch with NeMo</li> <li>Riva</li> <li>Megatron-LM</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-models/#industry","title":"Industry","text":"<ol> <li>Healthcare</li> <li>Retail</li> <li>Smart Cities / Spaces</li> <li>HPC / Supercomputing</li> <li>Gaming</li> <li>Robotics</li> <li>Aerospace</li> <li>Automotive / Transportation</li> <li>Energy</li> <li>Academia / Higher Education</li> <li>Cloud Services</li> <li>Hardware / Semiconductor</li> <li>Life Sciences</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-ngc-catalog-resources/","title":"NVIDIA NGC Catalog Resources","text":"<p>The NGC catalog offers step-by-step instructions and scripts through Jupyter Notebooks for various use cases, including machine learning, computer vision, and conversational AI. These resources help you examine, understand, customize, test, and build AI faster, while taking advantage of best practices.</p> <p>Resources</p>"},{"location":"AIML/NVIDIA/nvidia-nim/","title":"NVIDIA NIM","text":"<p>Accelerate Your AI Deployment With NVIDIA NIM Microservices</p> <p>Part of NVIDIA AI Enterprise, NVIDIA NIM microservices are a set of easy-to-use microservices for accelerating the deployment of foundation models on any cloud or data center and helps keep your data secure. NIM microservices have production-grade runtimes including on-going security updates. Run your business applications with stable APIs backed by enterprise-grade support.</p> <p>NVIDIA NIM for Developers</p> <p>NVIDIA NIM</p>"},{"location":"AIML/NVIDIA/nvidia-overview/","title":"NVIDIA Partner portal","text":"<p>NVIDIA Partner portal</p>"},{"location":"AIML/NVIDIA/nvidia-overview/#nvcrio-nvidia-docker-repo-login-details","title":"nvcr.io \u00a0 NVIDIA docker repo login details","text":"<p>username: $oauthtoken password: </p>"},{"location":"AIML/NVIDIA/nvidia-overview/#check-gpu","title":"Check GPU","text":"<pre><code>oc exec -it pod/nvidia-driver-daemonset-xxxxxx -n nvidia-gpu-operator -- nvidia-smi\n</code></pre>"},{"location":"AIML/NVIDIA/nvidia-overview/#nvidia-product","title":"NVIDIA Product","text":""},{"location":"AIML/NVIDIA/nvidia-overview/#hardware","title":"Hardware","text":""},{"location":"AIML/NVIDIA/nvidia-overview/#gaming-and-creating","title":"Gaming and Creating","text":"<ol> <li>GeForce Graphics Cards</li> <li>Laptops</li> <li>G-SYNC Monitors</li> <li>Studio</li> <li>RTX AI PCs</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#laptops-and-workstations","title":"Laptops and Workstations","text":"<ol> <li>Laptops</li> <li>NVIDIA RTX in Desktop Workstations</li> <li>NVIDIA RTX in Professional Laptops</li> <li>NVIDIA RTX-Powered AI Workstations</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#cloud-and-data-center","title":"Cloud and Data Center","text":"<ol> <li>Grace CPU</li> <li>DGX Platform</li> <li>EGX Platform</li> <li>IGX Platform</li> <li>HGX Platform</li> <li>NVIDIA MGX</li> <li>NVIDIA OVX</li> <li>DRIVE Sim</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#networking","title":"Networking","text":"<ol> <li>DPUs and SuperNICs</li> <li>Ethernet</li> <li>InfiniBand</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#gpus","title":"GPUs","text":"<ol> <li>GeForce</li> <li>NVIDIA RTX / Quadro</li> <li>Data Center</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#embedded-systems","title":"Embedded Systems","text":"<ol> <li>Jetson</li> <li>DRIVE AGX</li> <li>Clara AGX</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#software","title":"Software","text":""},{"location":"AIML/NVIDIA/nvidia-overview/#application-frameworks","title":"Application Frameworks","text":"<ol> <li>AI Inference - Triton</li> <li>Automotive - DRIVE</li> <li>Cloud-AI Video Streaming - Maxine</li> <li>Computational Lithography - cuLitho</li> <li>Cybersecurity - Morpheus</li> <li>Data Analytics - RAPIDS</li> <li>Generative AI - NeMo</li> <li>Healthcare - Clara</li> <li>High-Performance Computing</li> <li>Intelligent Video Analytics - Metropolis </li> <li>Logistics and Route Optimization - cuOpt </li> <li>Metaverse Applications - Omniverse </li> <li>Recommender Systems - Merlin </li> <li>Robotics - Isaac </li> <li>Speech AI - Riva </li> <li>Telecommunications - Aerial</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#apps-and-tools","title":"Apps and Tools","text":"<ol> <li>NGC Software Catalog</li> <li>3D Workflows - Omniverse</li> <li>Data Center</li> <li>GPU Monitoring</li> <li>NVIDIA App for Enterprise</li> <li>NVIDIA RTX Desktop Manager</li> <li>RTX Accelerated Creative Apps</li> <li>Video Conferencing</li> <li>AI Workbench</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#gaming-and-creating_1","title":"Gaming and Creating","text":"<ol> <li>GeForce NOW Cloud Gaming</li> <li>GeForce Experience</li> <li>NVIDIA Broadcast App</li> <li>Animation - Machinima</li> <li>Modding - RTX Remix</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#infrastructure","title":"Infrastructure","text":"<ol> <li>AI Enterprise Suite</li> <li>Cloud Native Support</li> <li>Cluster Management</li> <li>IO Acceleration</li> <li>Networking</li> <li>Virtual GPU</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#cloud-services","title":"Cloud Services","text":"<ol> <li>Base Command</li> <li>BioNeMo</li> <li>DGX Cloud</li> <li>NeMo</li> <li>Picasso</li> <li>Private Registry</li> <li>Omniverse</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#solutions","title":"Solutions","text":""},{"location":"AIML/NVIDIA/nvidia-overview/#artificial-intelligence","title":"Artificial Intelligence","text":"<ol> <li>AI Platform</li> <li>AI Inference</li> <li>AI Workflows</li> <li>Conversational AI</li> <li>Data Analytics</li> <li>Generative AI</li> <li>Machine Learning</li> <li>Prediction and Forecasting</li> <li>Speech AI</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#data-center-and-cloud-computing","title":"Data Center and Cloud Computing","text":"<ol> <li>Accelerated Computing for Enterprise IT</li> <li>Cloud Computing</li> <li>Colocation</li> <li>MLOps</li> <li>Networking</li> <li>Virtualization</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#design-and-simulation","title":"Design and Simulation","text":"<ol> <li>Computer Aided-Engineering</li> <li>Digital Twin Development</li> <li>Rendering and Visualization</li> <li>Robotics Simulation</li> <li>Vehicle Simulation</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#robotics-and-edge-computing","title":"Robotics and Edge Computing","text":"<ol> <li>Robotics</li> <li>Edge Computing</li> <li>Vision AI</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#high-performance-computing","title":"High-Performance Computing","text":"<ol> <li>HPC and AI</li> <li>Scientific Visualization</li> <li>Simulation and Modeling</li> <li>Quantum Computing</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#self-driving-vehicles","title":"Self-Driving Vehicles","text":"<ol> <li>In-Vehicle Computing</li> <li>Infrastructure</li> </ol>"},{"location":"AIML/NVIDIA/nvidia-overview/#industries","title":"Industries","text":"<ol> <li>Architecture, Engineering, Construction &amp; Operations </li> <li>Automotive </li> <li>Consumer Internet </li> <li>Cybersecurity </li> <li>Energy </li> <li>Financial Services </li> <li>Healthcare and Life Sciences </li> <li>Higher Education </li> <li>Game Development </li> <li>Industrial Sector </li> <li>Manufacturing </li> <li>Media and Entertainment </li> <li>Global Public Sector </li> <li>Restaurants </li> <li>Retail and CPG </li> <li>Robotics </li> <li>Smart Cities </li> <li>Supercomputing </li> <li>Telecommunications </li> <li>Transportation</li> </ol>"},{"location":"AIML/RAG/Retrieval_augmented_generation/","title":"Retrieval Augmented Generation (RAG)","text":"<p>But we have another approach where we can augment the knowledge of LLMs and retrieve information from custom content.It is called Retrieval Augmented Generation (RAG).</p> <p>Utility tools like ChatPDF have been popular Generative AI tools. The PDF document is connected as an external data source and we can interact with it as we are assisted by an LLM.</p> <p>What we do in RAG is inserting additional data into the context (prompt) of a model at inference time. That helps the LLM get more precise and relevant content for our queries when compared to zero-shot prompting.</p> <p>Another way of looking at it is in the context of a doctor and patient. A doctor\u2019s diagnosis can be significantly more precise and accurate when they have access to the patient\u2019s test results and charts, as opposed to relying solely on symptomatic observations.</p> <p></p> <p> </p> <p>The workflow of the RAG based LLM application will be as follows:</p> <ol> <li>Receive query from the user.</li> <li>Convert it to an embedded query vector preserving the semantics, using an embedding model.</li> <li>Retrieve the top-k relevant content from the vector database by computing similarity between the query embedding and the content embedding in the database.</li> <li>Pass the retrieved content and query as a prompt to an LLM.</li> <li>The LLM gives the required response.</li> </ol> <p></p> <ol> <li> <p>Download Ollama(Macos)    https://ollama.com/download    https://github.com/ollama/ollama</p> </li> <li> <p>Unzip the zip file</p> </li> <li>Move to Application </li> </ol> <p><pre><code>(base) gggggggg:~ ganesh$ ollama\nUsage:\n  ollama [flags]\n  ollama [command]\n\nAvailable Commands:\n  serve       Start ollama\n  create      Create a model from a Modelfile\n  show        Show information for a model\n  run         Run a model\n  stop        Stop a running model\n  pull        Pull a model from a registry\n  push        Push a model to a registry\n  list        List models\n  ps          List running models\n  cp          Copy a model\n  rm          Remove a model\n  help        Help about any command\n\nFlags:\n  -h, --help      help for ollama\n  -v, --version   Show version information\n\nUse \"ollama [command] --help\" for more information about a command.\n(base) ggggggggg:~ ganesh$ \n</code></pre> 4. Run the llama2</p> <pre><code>(base) gggggg:~ ganesh$ ollama run llama2\npulling manifest \npulling 8934d96d3f08... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 3.8 GB                         \npulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.0 KB                         \npulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.8 KB                         \npulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   59 B                         \npulling fa304d675061... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   91 B                         \npulling 42ba7f8a01dd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  557 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n&gt;&gt;&gt; \n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#rag-pipeline-with-vector-database","title":"RAG Pipeline with Vector DataBase","text":"<pre><code>## Data Ingetion\n#pip install pypdf\n#pip install langchain_community\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.document_loaders import CSVLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\n#from bs4 import SoupStrainer\nimport bs4\nimport os\nfrom dotenv import load_dotenv\n</code></pre> <pre><code>load_dotenv()\nos.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\")\n</code></pre> <pre><code>Text reader\nloader = TextLoader('Speech.txt')\ndocs = loader.load()\n</code></pre> <pre><code>for doc in docs:\n    print(doc.page_content)  # or any other attribute like `metadata`\n</code></pre> <pre><code>#Web based loader\n#Load, chunk and Index the content of HTML page\n\nloader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\\n                                   bs_kwargs=dict(parse_only=bs4.SoupStrainer( \\\n                                   class_=(\"post-title\",\"post-content\",\"post-header\")\n                                 )))\n\ndocs=loader.load()\n</code></pre> <pre><code>for doc in docs:\n    print(doc.page_content)  # or any other attribute like `metadata`\n</code></pre> <pre><code># PDF Reader\nloader = PyPDFLoader('docs/Ganesh_Kinkar_Giri_DevSecOps.pdf')\ndocs = loader.load()\n</code></pre> <pre><code>for doc in docs:\n    print(doc.page_content)  # or any other attribute like `metadata`\n</code></pre> <pre><code>#CSV Loader\nloader = CSVLoader(\"issue.csv\")\ndocs = loader.load()\n</code></pre> <pre><code>for doc in docs:\n    print(doc.page_content)  # or any other attribute like `metadata`\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#chunk-split-the-text-data","title":"Chunk &amp; Split the text data","text":"<pre><code>text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50)\nchunk_documents = text_splitter.split_documents(docs)\nchunk_documents\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#vector-embedding-and-vector-store","title":"Vector Embedding and Vector Store","text":""},{"location":"AIML/RAG/Retrieval_augmented_generation/#either-you-can-use-embeddings-model-openai-or-ollama","title":"Either you can use Embeddings model OpenAI or ollama","text":"<pre><code>from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_community.vectorstores import FAISS\n\ndb=FAISS.from_documents(chunk_documents,OllamaEmbeddings())\ndb\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#query","title":"Query","text":"<pre><code>query = \"unable to connect VM/ VM not accessible\"\nretrive_result = db.similarity_search(query)\nprint(retrive_result[0].page_content)\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#retirever-and-chain-with-langchain","title":"Retirever and Chain with Langchain","text":"<pre><code>## Data Ingetion\n#pip install pypdf\n#pip install langchain_community\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.document_loaders import CSVLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\n#from bs4 import SoupStrainer\nimport bs4\nimport os\nfrom dotenv import load_dotenv\n</code></pre> <pre><code>load_dotenv()\nos.environ['OPENAPI_API_KEY']=os.getenv(\"OPENAPI_API_KEY\")\n</code></pre> <pre><code>#Text Loader\nloader = TextLoader(\"issue.txt\")\ndocs = loader.load()\ndocs\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#chunk-split-the-text-data_1","title":"Chunk &amp; Split the text data","text":"<pre><code>text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50)\nchunk_documents = text_splitter.split_documents(docs)\nchunk_documents\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#vector-embedding-and-vector-store_1","title":"Vector Embedding and Vector Store","text":""},{"location":"AIML/RAG/Retrieval_augmented_generation/#either-you-can-use-embeddings-model-openai-or-ollama_1","title":"Either you can use Embeddings model OpenAI or ollama","text":"<pre><code>from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_community.vectorstores import FAISS\n\ndb=FAISS.from_documents(chunk_documents,OllamaEmbeddings())\ndb\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#design-chatprompt-template","title":"Design ChatPrompt Template","text":"<pre><code>from langchain_core.prompts import ChatPromptTemplate\nprompt=ChatPromptTemplate.from_template(\"\"\"\nAnswer the following question based only on the provided context.\nThink step by step before providing a detailed answer.\n&lt;context&gt;\n{context}                                                                                                                        \n&lt;/context&gt;\n\nQuestion: {input}                                                                                \n\"\"\")\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#to-integrate-with-llm-here-will-use-opensource-llmollama-model-llama2","title":"To integrate with LLM - here will use opensource LLM(Ollama) model llama2","text":"<pre><code>from langchain_community.llms import Ollama\n\nllm=Ollama(model=\"llama2\")\nllm\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#chain","title":"Chain","text":"<pre><code>from langchain.chains.combine_documents import create_stuff_documents_chain\ndocument_chain = create_stuff_documents_chain(llm, prompt)\ndocument_chain\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#retrievers","title":"Retrievers","text":"<pre><code>retrievers = db.as_retriever()\nretrievers\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#retrievers-chain","title":"Retrievers chain","text":"<pre><code>from langchain.chains import create_retrieval_chain\nretrieval_chain = create_retrieval_chain(retrievers, document_chain)\n</code></pre> <pre><code>import textwrap\nresponse = retrieval_chain.invoke({\"input\":\"Low disk space\"})\nformatted_answer = textwrap.fill(response['answer'], width=80)\n#wrapped_text = \"\\n\\n\".join(textwrap.fill(paragraph, width=80) for paragraph in formatted_answer.split(\"\\n\\n\"))\n#print(wrapped_text)\nprint(formatted_answer)\n</code></pre>"},{"location":"AIML/RAG/Retrieval_augmented_generation/#ans","title":"Ans:","text":"<pre><code>Based on the provided context, the answer to the question \"Low disk space\" is:\nTo resolve low disk space issues, the team should first check which application\nis using the most disk space by running the command `df -h` in the terminal.\nThis will display a list of all mounted file systems and their usage in\npercentage. The application that is using the most disk space should be\nidentified and the team should then take steps to free up space for that\napplication.  One possible solution is to increase the SKU size for the\napplication by running the command `sudoedit /etc/systemd/multi-\nuser/sockets.d/skusize` and updating the value of `SKU_SIZE` to a larger value.\nThis will allow the application to use more disk space without running out of\nspace.  Another solution is to clean up any unnecessary files or data that are\ntaking up space on the system by using the command `gzip -d &lt;filename&gt;` to\ndecompress files and then removing them with the command `rm &lt;filename&gt;`. This\nwill free up space on the disk without affecting the application's\nfunctionality.  The team should also consider optimizing the application's code\nto reduce its dependencies on disk space, such as by using caching or other\nmemory-based storage solutions. This may involve working with the application's\ndevelopers to make changes to the application's architecture.  Finally, the team\ncan check if there are any other applications that are using excessive amounts\nof disk space and take steps to free up space for those applications as well.\n</code></pre>"},{"location":"AIML/RAG/vector_database/","title":"What is vector database?","text":"<ul> <li>A vector database is a type of database optimized for storing, indexing, and searching high-dimensional vectors, which are mathematical representations of data points (like words, images, or other objects) in vector space.</li> <li>Vector databases are especially useful for handling data from applications in AI and machine learning, particularly in tasks involving similarity search and recommendation engines.</li> <li>A Vector Database, at its essence, is a relational database system specifically designed to process vectorized data. Unlike conventional databases that contain information in tables, rows, and columns, vector databases work with vectors\u2013arrays of numerical values that signify points in multidimensional space.</li> <li>Vectors, in turn, are everywhere and are commonly used in, for instance, machine learning, artificial intelligence, genomics, and geospatial analysis. At these datasets, there are frequently high-dimensional vectors where each dimension represents a particular attribute or feature.</li> <li>Such data place a heavy burden on traditional databases as they are tabular in form and do not allow efficiency in the storage and retrieval of such data and there comes the bottleneck in the performance of the database.</li> </ul>"},{"location":"AIML/RAG/vector_database/#what-is-a-vector","title":"What is a Vector?","text":"<ul> <li>Vector in the field of mathematics and data science refers to a serial arrangement of numerical values.</li> <li>It is a node in a many-dimensional space where one weight from each vector corresponds to a specific dimension.</li> <li>In the domain of vector databases, such arrays of numerical values, thus, turn into primitive concepts of information, making it possible for the store and processing of data in high dimensions.</li> </ul>"},{"location":"AIML/RAG/vector_database/#how-vector-databases-work","title":"How Vector Databases Work?","text":"<p>Vector Database is a type of database that is used in various machine learning use cases. They are specialized for the storage and retrieval of vector data.</p> <p>What are embeddings?</p> <p>Embedding is a data like words that have been converted into an array of numbers known as a vector that contains patterns of relationships the combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity.</p> <p></p> <p>The combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity.</p> <p></p> <p>Let\u2019s see an example describe a 2d graph the words dog and puppy are often used in similar situations.</p> <p></p> <p>So in a word embedding they would be represented by vectors that are close together.</p> <p></p> <p>Well this is a simple 2D example of a single dimension in reality the vector has hundreds of Dimensions that cover the rich multi-dimensional complex relationship between words.</p> <p>Example</p> <p>Images can also be turned into vectors. Google does similar images searches and the image sections are broken down into arrays of numbers allowing you to find patterns of similarity for those with closely resembling vectors.</p> <p></p> <p>Once an embedding is created it can be stored in a database and a database full of these is considered as a vector database.</p> <p></p> <p>Vector database can be used in several ways, searching where results are ranked by relevance to a query string or clustering where text strings are grouped by similarity and recommendations where items with related text strings are recommended also classification where text strings are classified by their most similar label.</p>"},{"location":"AIML/RAG/vector_database/#key-components-of-vector-databases","title":"Key Components of Vector Databases","text":"<ol> <li>Embedding Storage: Stores high-dimensional vectors.</li> <li>Indexing and Search: Uses specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or FAISS (Facebook AI Similarity Search) for fast similarity search across massive vector datasets.</li> <li>Scalability: Optimized for handling millions to billions of vectors.</li> <li>Metadata Support: Allows for filtering and adding metadata alongside vectors for refined search capabilities.</li> </ol>"},{"location":"AIML/RAG/vector_database/#popular-use-cases","title":"Popular Use Cases","text":"<ol> <li>Semantic Search: Finds documents, images, or videos similar in meaning to a search query, even if they don\u2019t share keywords.</li> <li>Recommendation Systems: Suggests items similar to user preferences (like items with similar vector embeddings).</li> <li>Anomaly Detection: Identifies outliers in datasets, as anomalies often have unique embeddings.</li> <li>Image and Video Recognition: Matches similar images or videos based on visual features encoded in vectors.</li> </ol>"},{"location":"AIML/RAG/vector_database/#examples-of-vector-databases","title":"Examples of Vector Databases","text":"<ol> <li>Weaviate: An open-source vector search engine with strong support for text, image, and video embeddings.</li> <li>Pinecone: A managed vector database with a focus on scalable similarity search and recommendations.</li> <li>FAISS: Primarily a library developed by Facebook AI for fast nearest neighbor search but also used in building vector search systems.</li> <li>Milvus: Another open-source vector database that provides highly scalable, low-latency vector searches.</li> </ol>"},{"location":"AIML/RAG/vector_database/#why-vector-databases-are-important","title":"Why Vector Databases Are Important","text":"<p>With the rise of LLMs (large language models) and computer vision, the demand for efficiently storing and retrieving high-dimensional embeddings has grown.</p> <p>Traditional databases like SQL and NoSQL are not optimized for these kinds of searches. Vector databases are specifically designed to handle such data, making them a crucial component in building advanced, intelligent applications in AI, recommendation systems, and search.</p>"},{"location":"AIML/Supervised/Supervised-overview/","title":"Supervised Learning","text":""},{"location":"AIML/Supervised/Supervised-overview/#what-is-supervised-learning","title":"What is Supervised Learning?","text":"<p>Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. The given data is labeled. Both classification and regression problems are supervised learning problems.</p> <p>Supervised Machine Learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output Y = f(X). The goal is to approximate the mapping function so well that when you have new input data (x) you can predict the output variables (Y) for that data.</p> <ul> <li>In supervised learning, sample labeled data are provided to the machine learning system for training, and the system then predicts the output based on the training data.</li> <li>The system uses labeled data to build a model that understands the datasets and learns about each one.</li> <li>After the training and processing are done, we test the model with sample data to see if it can accurately predict the output.</li> <li>The mapping of the input data to the output data is the objective of supervised learning.</li> </ul> <p>Example \u2013 Consider the following data regarding patients entering a clinic . The data consists of the gender and age of the patients and each patient is labeled as \u201chealthy\u201d or \u201csick\u201d.</p> <p></p>"},{"location":"AIML/Supervised/Supervised-overview/#supervised-learning-algorithms-are-generally-categorized-into-two-main-types","title":"Supervised learning algorithms are generally categorized into two main types:","text":"<ul> <li>Classification</li> <li> <p>Regression</p> </li> <li> <p>Regression: Regression algorithms are used to predict a continuous numerical output. For example, a regression algorithm could be used to predict the price of a house based on its size, location, and other features.</p> </li> <li>Classification: Classification algorithms are used to predict a categorical output. For example, a classification algorithm could be used to predict whether an email is spam or not.</li> </ul> <p>There are many algorithms used in supervised learning, each suited to different types of problems. Some of the most commonly used supervised learning algorithms include:</p> <ol> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Decision Trees</li> <li>Support Vector Machines (SVM)</li> <li>k-Nearest Neighbors (k-NN)</li> <li>Naive Bayes</li> <li>Random Forest (Bagging Algorithm)</li> <li>Boosting Algorithms</li> </ol>"},{"location":"AIML/Supervised/Classification/Classification-overview/","title":"Classification","text":"<p>Classification algorithms are used to predict a categorical output. For example, a classification algorithm could be used to predict whether an email is spam or not.</p>"},{"location":"AIML/Supervised/Classification/Classification-overview/#machine-learning-for-classification","title":"Machine Learning for classification","text":"<p>Classification is a process of categorizing data or objects into predefined classes or categories based on their features or attributes.</p> <p>Machine Learning classification is a type of supervised learning technique where an algorithm is trained on a labeled dataset to predict the class or category of new, unseen data.</p> <p>The main objective of classification machine learning is to build a model that can accurately assign a label or category to a new observation based on its features.</p> <p>For example, a classification model might be trained on a dataset of images labeled as either dogs or cats and then used to predict the class of new, unseen images of dogs or cats based on their features such as color, texture, and shape.</p>"},{"location":"AIML/Supervised/Classification/Classification-overview/#classification-types","title":"Classification Types","text":"<p>There are two main classification types in machine learning:</p> <p>Binary Classification In binary classification, the goal is to classify the input into one of two classes or categories. Example \u2013 On the basis of the given health conditions of a person, we have to determine whether the person has a certain disease or not.</p> <p>Multiclass Classification In multi-class classification, the goal is to classify the input into one of several classes or categories. For Example \u2013 On the basis of data about different species of flowers, we have to determine which specie our observation belongs to.</p> <p></p> <p>Other categories of classification involves:</p> <p>Multi-Label Classification In, Multi-label Classification the goal is to predict which of several labels a new data point belongs to. This is different from multiclass classification, where each data point can only belong to one class. For example, a multi-label classification algorithm could be used to classify images of animals as belonging to one or more of the categories cat, dog, bird, or fish.</p> <p>Imbalanced Classification In, Imbalanced Classification the goal is to predict whether a new data point belongs to a minority class, even though there are many more examples of the majority class. For example, a medical diagnosis algorithm could be used to predict whether a patient has a rare disease, even though there are many more patients with common diseases.</p>"},{"location":"AIML/Supervised/Classification/Classification-overview/#classification-algorithms","title":"Classification Algorithms","text":"<p>There are various types of classifiers algorithms. Some of them are : </p> <p>Linear Classifiers Linear models create a linear decision boundary between classes. They are simple and computationally efficient. Some of the linear classification models are as follows: </p> <ul> <li>Logistic Regression</li> <li>Support Vector Machines having kernel = \u2018linear\u2019</li> <li>Single-layer Perceptron</li> <li>Stochastic Gradient Descent (SGD) Classifier</li> </ul> <p>Non-linear Classifiers Non-linear models create a non-linear decision boundary between classes. They can capture more complex relationships between the input features and the target variable. Some of the non-linear classification models are as follows: </p> <ul> <li>K-Nearest Neighbours</li> <li>Kernel SVM</li> <li>Naive Bayes</li> <li>Decision Tree Classification</li> <li>Ensemble learning classifiers: </li> <li>Random Forests, </li> <li>AdaBoost, </li> <li>Bagging Classifier, </li> <li>Voting Classifier, </li> <li>ExtraTrees Classifier</li> <li>Multi-layer Artificial Neural Networks</li> </ul> <p>Learners in Classifications Algorithm</p> <p>In machine learning, classification learners can also be classified as either \u201clazy\u201d or \u201ceager\u201d learners.</p> <ul> <li>Lazy Learners: Lazy Learners are also known as instance-based learners, lazy learners do not learn a model during the training phase. Instead, they simply store the training data and use it to classify new instances at prediction time. It is very fast at prediction time because it does not require computations during the predictions. it is less effective in high-dimensional spaces or when the number of training instances is large. Examples of lazy learners include k-nearest neighbors and case-based reasoning.</li> <li>Eager Learners: Eager Learners are also known as model-based learners, eager learners learn a model from the training data during the training phase and use this model to classify new instances at prediction time. It is more effective in high-dimensional spaces having large training datasets. Examples of eager learners include decision trees, random forests, and support vector machines.</li> </ul>"},{"location":"AIML/Supervised/Classification/Classification-overview/#classification-models-in-machine-learning","title":"Classification Models in Machine Learning","text":"<p>Evaluating a classification model is an important step in machine learning, as it helps to assess the performance and generalization ability of the model on new, unseen data. There are several metrics and techniques that can be used to evaluate a classification model, depending on the specific problem and requirements. Here are some commonly used evaluation metrics:</p> <ul> <li>Classification Accuracy: The proportion of correctly classified instances over the total number of instances in the test set. It is a simple and intuitive metric but can be misleading in imbalanced datasets where the majority class dominates the accuracy score.</li> <li>Confusion matrix: A table that shows the number of true positives, true negatives, false positives, and false negatives for each class, which can be used to calculate various evaluation metrics.</li> <li>Precision and Recall: Precision measures the proportion of true positives over the total number of predicted positives, while recall measures the proportion of true positives over the total number of actual positives. These metrics are useful in scenarios where one class is more important than the other, or when there is a trade-off between false positives and false negatives.</li> <li>F1-Score: The harmonic mean of precision and recall, calculated as 2 x (precision x recall) / (precision + recall). It is a useful metric for imbalanced datasets where both precision and recall are important.</li> <li>ROC curve and AUC: The Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate (recall) against the false positive rate (1-specificity) for different threshold values of the classifier\u2019s decision function. The Area Under the Curve (AUC) measures the overall performance of the classifier, with values ranging from 0.5 (random guessing) to 1 (perfect classification).</li> <li>Cross-validation: A technique that divides the data into multiple folds and trains the model on each fold while testing on the others, to obtain a more robust estimate of the model\u2019s performance.</li> </ul> <p>It is important to choose the appropriate evaluation metric(s) based on the specific problem and requirements, and to avoid overfitting by evaluating the model on independent test data.</p>"},{"location":"AIML/Supervised/Classification/Classification-overview/#characteristics-of-classification","title":"Characteristics of Classification","text":"<p>Here are the characteristics of the classification:</p> <ul> <li>Categorical Target Variable: Classification deals with predicting categorical target variables that represent discrete classes or labels. Examples include classifying emails as spam or not spam, predicting whether a patient has a high risk of heart disease, or identifying image objects.</li> <li>Accuracy and Error Rates: Classification models are evaluated based on their ability to correctly classify data points. Common metrics include accuracy, precision, recall, and F1-score.</li> <li>Model Complexity: Classification models range from simple linear classifiers to more complex nonlinear models. The choice of model complexity depends on the complexity of the relationship between the input features and the target variable.</li> <li>Overfitting and Underfitting: Classification models are susceptible to overfitting and underfitting. Overfitting occurs when the model learns the training data too well and fails to generalize to new data.</li> </ul>"},{"location":"AIML/Supervised/Classification/Classification-overview/#how-does-classification-machine-learning-work","title":"How does Classification Machine Learning Work?","text":"<p>The basic idea behind classification is to train a model on a labeled dataset, where the input data is associated with their corresponding output labels, to learn the patterns and relationships between the input data and output labels. Once the model is trained, it can be used to predict the output labels for new unseen data.</p> <p></p> <p>The classification process typically involves the following steps:</p> <p>Understanding the problem</p> <p>Before getting started with classification, it is important to understand the problem you are trying to solve. What are the class labels you are trying to predict? What is the relationship between the input data and the class labels?</p> <p>Suppose we have to predict whether a patient has a certain disease or not, on the basis of 7 independent variables, called features. This means, there can be only two possible outcomes: </p> <ul> <li>The patient has the disease, which means \u201cTrue\u201d.</li> <li>The patient has no disease. which means \u201cFalse\u201d.</li> </ul> <p>This is a binary classification problem.</p> <p>Data preparation</p> <p>Once you have a good understanding of the problem, the next step is to prepare your data. This includes collecting and preprocessing the data and splitting it into training, validation, and test sets. In this step, the data is cleaned, preprocessed, and transformed into a format that can be used by the classification algorithm.</p> <ul> <li>X: It is the independent feature, in the form of an N*M matrix. N is the no. of observations and M is the number of features.</li> <li>y: An N vector corresponding to predicted classes for each of the N observations.</li> </ul> <p>Feature Extraction</p> <p>The relevant features or attributes are extracted from the data that can be used to differentiate between the different classes.</p> <p>Suppose our input X has 7 independent features, having only 5 features influencing the label or target values remaining 2 are negligibly or not correlated, then we will use only these 5 features only for the model training. </p> <p>Model Selection</p> <p>There are many different models that can be used for classification, including logistic regression, decision trees, support vector machines (SVM), or neural networks. It is important to select a model that is appropriate for your problem, taking into account the size and complexity of your data, and the computational resources you have available.</p> <p>Model Training</p> <p>Once you have selected a model, the next step is to train it on your training data. This involves adjusting the parameters of the model to minimize the error between the predicted class labels and the actual class labels for the training data.</p> <p>Model Evaluation</p> <p>Evaluating the model: After training the model, it is important to evaluate its performance on a validation set. This will give you a good idea of how well the model is likely to perform on new, unseen data. </p> <pre><code>Log Loss or Cross-Entropy Loss, Confusion Matrix,  Precision, Recall, and AUC-ROC curve are the quality metrics used for measuring the performance of the model.\n</code></pre> <p>Fine-tuning the model</p> <p>If the model\u2019s performance is not satisfactory, you can fine-tune it by adjusting the parameters, or trying a different model.</p> <p>Deploying the model</p> <p>Finally, once we are satisfied with the performance of the model, we can deploy it to make predictions on new data.  it can be used for real world problem.</p> <p>Examples of Machine Learning Classification in Real Life</p> <p>Classification algorithms are widely used in many real-world applications across various domains, including:</p> <ul> <li>Email spam filtering</li> <li>Credit risk assessment</li> <li>Medical diagnosis</li> <li>Image classification</li> <li>Sentiment analysis.</li> <li>Fraud detection</li> <li>Quality control</li> <li>Recommendation systems</li> </ul> <p>Implementation of Classification Model in Machine Learning</p> <p>Requirements for running the given script:</p> <ul> <li>Python</li> <li>Scipy and Numpy</li> <li>Pandas</li> <li>Scikit-learn </li> </ul> <pre><code># Importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# import the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# splitting X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1)\n\n# GAUSSIAN NAIVE BAYES\ngnb = GaussianNB()\n# train the model\ngnb.fit(X_train, y_train)\n# make predictions\ngnb_pred = gnb.predict(X_test)\n# print the accuracy\nprint(\"Accuracy of Gaussian Naive Bayes: \",\n    accuracy_score(y_test, gnb_pred))\n# print other performance metrics\nprint(\"Precision of Gaussian Naive Bayes: \",\n    precision_score(y_test, gnb_pred, average='weighted'))\nprint(\"Recall of Gaussian Naive Bayes: \",\n    recall_score(y_test, gnb_pred, average='weighted'))\nprint(\"F1-Score of Gaussian Naive Bayes: \",\n    f1_score(y_test, gnb_pred, average='weighted'))\n\n# DECISION TREE CLASSIFIER\ndt = DecisionTreeClassifier(random_state=0)\n# train the model\ndt.fit(X_train, y_train)\n# make predictions\ndt_pred = dt.predict(X_test)\n# print the accuracy\nprint(\"Accuracy of Decision Tree Classifier: \",\n    accuracy_score(y_test, dt_pred))\n# print other performance metrics\nprint(\"Precision of Decision Tree Classifier: \",\n    precision_score(y_test, dt_pred, average='weighted'))\nprint(\"Recall of Decision Tree Classifier: \",\n    recall_score(y_test, dt_pred, average='weighted'))\nprint(\"F1-Score of Decision Tree Classifier: \",\n    f1_score(y_test, dt_pred, average='weighted'))\n\n# SUPPORT VECTOR MACHINE\nsvm_clf = svm.SVC(kernel='linear') # Linear Kernel\n# train the model\nsvm_clf.fit(X_train, y_train)\n# make predictions\nsvm_clf_pred = svm_clf.predict(X_test)\n# print the accuracy\nprint(\"Accuracy of Support Vector Machine: \",\n    accuracy_score(y_test, svm_clf_pred))\n# print other performance metrics\nprint(\"Precision of Support Vector Machine: \",\n    precision_score(y_test, svm_clf_pred, average='weighted'))\nprint(\"Recall of Support Vector Machine: \",\n    recall_score(y_test, svm_clf_pred, average='weighted'))\nprint(\"F1-Score of Support Vector Machine: \",\n    f1_score(y_test, svm_clf_pred, average='weighted'))\n</code></pre> <p></p> <p>What is classification rule in machine learning? <pre><code>A decision guideline in machine learning determining the class or category of input based on features.\n</code></pre></p> <p>What are the classification of algorithms? <pre><code>Methods like decision trees, SVM, and k-NN categorizing data into predefined classes for predictions.\n</code></pre></p> <p>What is learning classification? <pre><code>Acquiring knowledge to assign labels to input data, distinguishing classes in supervised machine learning.\n</code></pre></p> <p>What is difference between classification and clustering?</p> <ul> <li>Classification: Predicts predefined classes.</li> <li>Clustering: Groups data based on inherent similarities without predefined classes.</li> </ul>"},{"location":"AIML/Supervised/Regression/Decision-Tree/","title":"Decision Tree","text":""},{"location":"AIML/Supervised/Regression/Decision-Tree/#decision-tree-in-machine-learning","title":"Decision Tree in Machine Learning","text":"<p>A decision tree is a type of supervised learning algorithm that is commonly used in machine learning to model and predict outcomes based on input data. It is a tree-like structure where each internal node tests on attribute, each branch corresponds to attribute value and each leaf node represents the final decision or prediction. The decision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and classification problems.</p>"},{"location":"AIML/Supervised/Regression/Decision-Tree/#decision-tree-terminologies","title":"Decision Tree Terminologies","text":"<p>There are specialized terms associated with decision trees that denote various components and facets of the tree structure and decision-making procedure. :</p> <ul> <li>Root Node: A decision tree\u2019s root node, which represents the original choice or feature from which the tree branches, is the highest node.</li> <li>Internal Nodes (Decision Nodes): Nodes in the tree whose choices are determined by the values of particular attributes. There are branches on these nodes that go to other nodes.</li> <li>Leaf Nodes (Terminal Nodes): The branches\u2019 termini, when choices or forecasts are decided upon. There are no more branches on leaf nodes.</li> <li>Branches (Edges): Links between nodes that show how decisions are made in response to particular circumstances.</li> <li>Splitting: The process of dividing a node into two or more sub-nodes based on a decision criterion. It involves selecting a feature and a threshold to create subsets of data.</li> <li>Parent Node: A node that is split into child nodes. The original node from which a split originates.</li> <li>Child Node: Nodes created as a result of a split from a parent node.</li> <li>Decision Criterion: The rule or condition used to determine how the data should be split at a decision node. It involves comparing feature values against a threshold.</li> <li>Pruning: The process of removing branches or nodes from a decision tree to improve its generalisation and prevent overfitting.</li> </ul> <p>Understanding these terminologies is crucial for interpreting and working with decision trees in machine learning applications.</p>"},{"location":"AIML/Supervised/Regression/Decision-Tree/#why-decision-tree","title":"Why Decision Tree?","text":"<p>Decision trees are widely used in machine learning for a number of reasons:</p> <ul> <li>Decision trees are so versatile in simulating intricate decision-making processes, because of their interpretability and versatility.</li> <li>Their portrayal of complex choice scenarios that take into account a variety of causes and outcomes is made possible by their hierarchical structure.</li> <li>They provide comprehensible insights into the decision logic, decision trees are especially helpful for tasks involving categorisation and regression.</li> <li>They are proficient with both numerical and categorical data, and they can easily adapt to a variety of datasets thanks to their autonomous feature selection capability.</li> <li>Decision trees also provide simple visualization, which helps to comprehend and elucidate the underlying decision processes in a model.</li> </ul>"},{"location":"AIML/Supervised/Regression/Decision-Tree/#decision-tree-approach","title":"Decision Tree Approach","text":"<p>Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and attributes are represented on the internal node of the tree. We can represent any boolean function on discrete attributes using the decision tree.</p> <p></p> <p>Below are some assumptions that we made while using the decision tree:</p> <p>At the beginning, we consider the whole training set as the root.</p> <ul> <li>Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.</li> <li>On the basis of attribute values, records are distributed recursively.</li> <li>We use statistical methods for ordering attributes as root or the internal node.</li> </ul> <p></p> <p>As you can see from the above image the Decision Tree works on the Sum of Product form which is also known as Disjunctive Normal Form. In the above image, we are predicting the use of computer in the daily life of people. In the Decision Tree, the major challenge is the identification of the attribute for the root node at each level. This process is known as attribute selection. We have two popular attribute selection measures:</p> <ol> <li>Information Gain</li> <li>Gini Index</li> </ol> <p></p>"},{"location":"AIML/Supervised/Regression/Decision-Tree/#building-decision-tree-using-information-gain-the-essentials","title":"Building Decision Tree using Information Gain The essentials:","text":"<ul> <li>Start with all training instances associated with the root node</li> <li>Use info gain to choose which attribute to label each node with</li> <li>Note: No root-to-leaf path should contain the same discrete attribute twice</li> <li>Recursively construct each subtree on the subset of training instances that would be classified down that path in the tree.</li> <li>If all positive or all negative training instances remain, the label that node \u201cyes\u201d or \u201cno\u201d accordingly</li> <li>If no attributes remain, label with a majority vote of training instances left at that node</li> <li>If no instances remain, label with a majority vote of the parent\u2019s training instances.</li> </ul>"},{"location":"AIML/Supervised/Regression/Decision-Tree/#example-of-a-decision-tree-algorithm","title":"Example of a Decision Tree Algorithm","text":"<p>Forecasting Activities Using Weather Information</p> <ul> <li>Root node: Whole dataset</li> <li>Attribute : \u201cOutlook\u201d (sunny, cloudy, rainy).</li> <li>Subsets: Overcast, Rainy, and Sunny.</li> <li>Recursive Splitting: Divide the sunny subset even more according to humidity, for example.</li> <li>Leaf Nodes: Activities include \u201cswimming,\u201d \u201chiking,\u201d and \u201cstaying inside.\u201d</li> </ul>"},{"location":"AIML/Supervised/Regression/Linear-Regression/","title":"Linear Regression","text":"<ul> <li>Introduction to Linear Regression</li> <li>Gradient Descent in Linear Regression</li> <li>Linear regression (Python Implementation from scratch)</li> <li>Linear regression implementation using sklearn</li> <li>Rainfall prediction - Project</li> <li>Boston Housing Kaggle Challenge - Project</li> <li>Ridge Regression</li> <li>Lasso regression</li> <li>Elastic net Regression</li> <li>Implementation of Lasso, Ridge and Elastic Net</li> </ul>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#1introduction-to-linear-regression","title":"1.Introduction to Linear Regression","text":""},{"location":"AIML/Supervised/Regression/Linear-Regression/#what-is-linear-regression","title":"What is Linear Regression?","text":"<p>Linear regression is a type of supervised machine learning algorithm that computes the linear relationship between the dependent variable and one or more independent features by fitting a linear equation to observed data.</p> <p>When there is only one independent feature, it is known as Simple Linear Regression, and when there are more than one feature, it is known as Multiple Linear Regression.</p> <p>Similarly, when there is only one dependent variable, it is considered Univariate Linear Regression, while when there are more than one dependent variables, it is known as Multivariate Regression.</p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#why-linear-regression-is-important","title":"Why Linear Regression is Important?","text":"<p>The interpretability of linear regression is a notable strength. The model\u2019s equation provides clear coefficients that elucidate the impact of each independent variable on the dependent variable, facilitating a deeper understanding of the underlying dynamics. Its simplicity is a virtue, as linear regression is transparent, easy to implement, and serves as a foundational concept for more complex algorithms.</p> <p>Linear regression is not merely a predictive tool; it forms the basis for various advanced models. Techniques like regularization and support vector machines draw inspiration from linear regression, expanding its utility. Additionally, linear regression is a cornerstone in assumption testing, enabling researchers to validate key assumptions about the data.</p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#types-of-linear-regression","title":"Types of Linear Regression","text":"<p>There are two main types of linear regression:</p> <p>Simple Linear Regression This is the simplest form of linear regression, and it involves only one independent variable and one dependent variable. The equation for simple linear regression is:</p> <p></p> <p>where:</p> <ul> <li>Y is the dependent variable</li> <li>X is the independent variable</li> <li>\u03b20 is the intercept</li> <li>\u03b21 is the slope</li> </ul> <p>Multiple Linear Regression</p> <p>This involves more than one independent variable and one dependent variable. The equation for multiple linear regression is:</p> <p></p> <p>where:</p> <ul> <li>Y is the dependent variable</li> <li>X1, X2, \u2026, Xn are the independent variables</li> <li>\u03b20 is the intercept</li> <li>\u03b21, \u03b22, \u2026, \u03b2n are the slopes</li> </ul> <p>The goal of the algorithm is to find the best Fit Line equation that can predict the values based on the independent variables.</p> <p>In regression set of records are present with X and Y values and these values are used to learn a function so if you want to predict Y from an unknown X this learned function can be used. In regression we have to find the value of Y, So, a function is required that predicts continuous Y in the case of regression given X as independent features.</p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#what-is-the-best-fit-line","title":"What is the best Fit Line?","text":"<p>Our primary objective while using linear regression is to locate the best-fit line, which implies that the error between the predicted and actual values should be kept to a minimum. There will be the least error in the best-fit line.</p> <p>The best Fit Line equation provides a straight line that represents the relationship between the dependent and independent variables. The slope of the line indicates how much the dependent variable changes for a unit change in the independent variable(s).</p> <p></p> <p>Here Y is called a dependent or target variable and X is called an independent variable also known as the predictor of Y. There are many types of functions or modules that can be used for regression. A linear function is the simplest type of function. Here, X may be a single feature or multiple features representing the problem.</p> <p>Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x)). Hence, the name is Linear Regression. In the figure above, X (input) is the work experience and Y (output) is the salary of a person. The regression line is the best-fit line for our model. </p> <p>We utilize the cost function to compute the best values in order to get the best fit line since different values for weights or the coefficient of lines result in different regression lines.</p> <p>Hypothesis function in Linear Regression</p> <p>As we have assumed earlier that our independent feature is the experience i.e X and the respective salary Y is the dependent variable. Let\u2019s assume there is a linear relationship between X and Y then the salary can be predicted using:</p> <p></p> <p>Once we find the best \u03b81 and \u03b82 values, we get the best-fit line. So when we are finally using our model for prediction, it will predict the value of y for the input value of x. </p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#how-to-update-1-and-2-values-to-get-the-best-fit-line","title":"How to update \u03b81 and \u03b82 values to get the best-fit line?","text":""},{"location":"AIML/Supervised/Regression/Linear-Regression/#gradient-descent-for-linear-regression","title":"Gradient Descent for Linear Regression","text":"<p>A linear regression model can be trained using the optimization algorithm gradient descent by iteratively modifying the model\u2019s parameters to reduce the mean squared error (MSE) of the model on a training dataset. To update \u03b81 and \u03b82 values in order to reduce the Cost function (minimizing RMSE value) and achieve the best-fit line the model uses Gradient Descent. The idea is to start with random \u03b81 and \u03b82 values and then iteratively update the values, reaching minimum cost. </p> <p>A gradient is nothing but a derivative that defines the effects on outputs of the function with a little bit of variation in inputs.</p> <p></p> <p> \u200b Finding the coefficients of a linear equation that best fits the training data is the objective of linear regression. By moving in the direction of the Mean Squared Error negative gradient with respect to the coefficients, the coefficients can be changed. And the respective intercept and coefficient of X will be if \u03b1  is the learning rate.    </p> <p></p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#assumptions-of-simple-linear-regression","title":"Assumptions of Simple Linear Regression","text":"<p>Linear regression is a powerful tool for understanding and predicting the behavior of a variable, however, it needs to meet a few conditions in order to be accurate and dependable solutions. </p> <ol> <li>Linearity: The independent and dependent variables have a linear relationship with one another. This implies that changes in the dependent variable follow those in the independent variable(s) in a linear fashion. This means that there should be a straight line that can be drawn through the data points. If the relationship is not linear, then linear regression will not be an accurate model.</li> </ol> <p></p> <ol> <li> <p>Independence: The observations in the dataset are independent of each other. This means that the value of the dependent variable for one observation does not depend on the value of the dependent variable for another observation. If the observations are not independent, then linear regression will not be an accurate model.</p> </li> <li> <p>Homoscedasticity: Across all levels of the independent variable(s), the variance of the errors is constant. This indicates that the amount of the independent variable(s) has no impact on the variance of the errors. If the variance of the residuals is not constant, then linear regression will not be an accurate model.</p> </li> </ol> <p></p> <ol> <li>Normality: The residuals should be normally distributed. This means that the residuals should follow a bell-shaped curve. If the residuals are not normally distributed, then linear regression will not be an accurate model.</li> </ol>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#assumptions-of-multiple-linear-regression","title":"Assumptions of Multiple Linear Regression","text":"<p>For Multiple Linear Regression, all four of the assumptions from Simple Linear Regression apply. In addition to this, below are few more:</p> <ol> <li>No multicollinearity: There is no high correlation between the independent variables. This indicates that there is little or no correlation between the independent variables. Multicollinearity occurs when two or more independent variables are highly correlated with each other, which can make it difficult to determine the individual effect of each variable on the dependent variable. If there is multicollinearity, then multiple linear regression will not be an accurate model.</li> <li>Additivity: The model assumes that the effect of changes in a predictor variable on the response variable is consistent regardless of the values of the other variables. This assumption implies that there is no interaction between variables in their effects on the dependent variable.</li> <li>Feature Selection: In multiple linear regression, it is essential to carefully select the independent variables that will be included in the model. Including irrelevant or redundant variables may lead to overfitting and complicate the interpretation of the model.</li> <li>Overfitting: Overfitting occurs when the model fits the training data too closely, capturing noise or random fluctuations that do not represent the true underlying relationship between variables. This can lead to poor generalization performance on new, unseen data.</li> </ol>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#multicollinearity","title":"Multicollinearity","text":"<p>Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a multiple regression model are highly correlated, making it difficult to assess the individual effects of each variable on the dependent variable.</p> <p>Detecting Multicollinearity includes two techniques: - Correlation Matrix: Examining the correlation matrix among the independent variables is a common way to detect multicollinearity. High correlations (close to 1 or -1) indicate potential multicollinearity. - VIF (Variance Inflation Factor): VIF is a measure that quantifies how much the variance of an estimated regression coefficient increases if your predictors are correlated. A high VIF (typically above 10) suggests multicollinearity.</p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#evaluation-metrics-for-linear-regression","title":"Evaluation Metrics for Linear Regression","text":"<p>A variety of evaluation measures can be used to determine the strength of any linear regression model. These assessment metrics often give an indication of how well the model is producing the observed outputs.</p> <p>The most common measurements are:</p> <p>Mean Square Error (MSE)</p> <p>Mean Squared Error (MSE) is an evaluation metric that calculates the average of the squared differences between the actual and predicted values for all the data points. The difference is squared to ensure that negative and positive differences don\u2019t cancel each other out.</p> <p></p> <p>MSE is a way to quantify the accuracy of a model\u2019s predictions. MSE is sensitive to outliers as large errors contribute significantly to the overall score.</p> <p>Mean Absolute Error (MAE)</p> <p>Mean Absolute Error is an evaluation metric used to calculate the accuracy of a regression model. MAE measures the average absolute difference between the predicted values and actual values.</p> <p>Mathematically, MAE is expressed as:</p> <p></p> <p>Lower MAE value indicates better model performance. It is not sensitive to the outliers as we consider absolute differences.</p> <p>Root Mean Squared Error (RMSE)</p> <p>The square root of the residuals\u2019 variance is the Root Mean Squared Error. It describes how well the observed data points match the expected values, or the model\u2019s absolute fit to the data.</p> <p></p> <p>Coefficient of Determination (R-squared)</p> <p></p> <p>Adjusted R-Squared Error</p> <p></p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#python-implementation-of-linear-regression","title":"Python Implementation of Linear Regression","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.axes as ax\nfrom matplotlib.animation import FuncAnimation\n</code></pre> <p>Load the dataset and separate input and Target variables</p> <pre><code>url = 'https://media.geeksforgeeks.org/wp-content/uploads/20240320114716/data_for_lr.csv'\ndata = pd.read_csv(url)\ndata\n\n# Drop the missing values\ndata = data.dropna()\n\n# training dataset and labels\ntrain_input = np.array(data.x[0:500]).reshape(500, 1)\ntrain_output = np.array(data.y[0:500]).reshape(500, 1)\n\n# valid dataset and labels\ntest_input = np.array(data.x[500:700]).reshape(199, 1)\ntest_output = np.array(data.y[500:700]).reshape(199, 1)\n</code></pre> <p>Build the Linear Regression Model and Plot the regression line</p> <p>Steps:</p> <ul> <li>In forward propagation, Linear regression function Y=mx+c is applied by initially assigning random value of parameter (m &amp; c).</li> <li>The we have written the function to finding the cost function i.e the mean </li> </ul> <pre><code>class LinearRegression: \n    def __init__(self): \n        self.parameters = {} \n\n    def forward_propagation(self, train_input): \n        m = self.parameters['m'] \n        c = self.parameters['c'] \n        predictions = np.multiply(m, train_input) + c \n        return predictions \n\n    def cost_function(self, predictions, train_output): \n        cost = np.mean((train_output - predictions) ** 2) \n        return cost \n\n    def backward_propagation(self, train_input, train_output, predictions): \n        derivatives = {} \n        df = (predictions-train_output) \n        # dm= 2/n * mean of (predictions-actual) * input \n        dm = 2 * np.mean(np.multiply(train_input, df)) \n        # dc = 2/n * mean of (predictions-actual) \n        dc = 2 * np.mean(df) \n        derivatives['dm'] = dm \n        derivatives['dc'] = dc \n        return derivatives \n\n    def update_parameters(self, derivatives, learning_rate): \n        self.parameters['m'] = self.parameters['m'] - learning_rate * derivatives['dm'] \n        self.parameters['c'] = self.parameters['c'] - learning_rate * derivatives['dc'] \n\n    def train(self, train_input, train_output, learning_rate, iters): \n        # Initialize random parameters \n        self.parameters['m'] = np.random.uniform(0, 1) * -1\n        self.parameters['c'] = np.random.uniform(0, 1) * -1\n\n        # Initialize loss \n        self.loss = [] \n\n        # Initialize figure and axis for animation \n        fig, ax = plt.subplots() \n        x_vals = np.linspace(min(train_input), max(train_input), 100) \n        line, = ax.plot(x_vals, self.parameters['m'] * x_vals +\n                        self.parameters['c'], color='red', label='Regression Line') \n        ax.scatter(train_input, train_output, marker='o', \n                color='green', label='Training Data') \n\n        # Set y-axis limits to exclude negative values \n        ax.set_ylim(0, max(train_output) + 1) \n\n        def update(frame): \n            # Forward propagation \n            predictions = self.forward_propagation(train_input) \n\n            # Cost function \n            cost = self.cost_function(predictions, train_output) \n\n            # Back propagation \n            derivatives = self.backward_propagation( \n                train_input, train_output, predictions) \n\n            # Update parameters \n            self.update_parameters(derivatives, learning_rate) \n\n            # Update the regression line \n            line.set_ydata(self.parameters['m'] \n                        * x_vals + self.parameters['c']) \n\n            # Append loss and print \n            self.loss.append(cost) \n            print(\"Iteration = {}, Loss = {}\".format(frame + 1, cost)) \n\n            return line, \n        # Create animation \n        ani = FuncAnimation(fig, update, frames=iters, interval=200, blit=True) \n\n        # Save the animation as a video file (e.g., MP4) \n        ani.save('linear_regression_A.gif', writer='ffmpeg') \n\n        plt.xlabel('Input') \n        plt.ylabel('Output') \n        plt.title('Linear Regression') \n        plt.legend() \n        plt.show() \n\n        return self.parameters, self.loss \n</code></pre> <p>Trained the model and Final Prediction</p> <pre><code>#Example usage\nlinear_reg = LinearRegression()\nparameters, loss = linear_reg.train(train_input, train_output, 0.0001, 20)\n</code></pre> <p></p> <p></p> <p>Linear Regression Line</p> <p>The linear regression line provides valuable insights into the relationship between the two variables. It represents the best-fitting line that captures the overall trend of how a dependent variable (Y) changes in response to variations in an independent variable (X).</p> <ul> <li>Positive Linear Regression Line: A positive linear regression line indicates a direct relationship between the independent variable (X) and the dependent variable (Y). This means that as the value of X increases, the value of Y also increases. The slope of a positive linear regression line is positive, meaning that the line slants upward from left to right.</li> <li>Negative Linear Regression Line: A negative linear regression line indicates an inverse relationship between the independent variable (X) and the dependent variable (Y). This means that as the value of X increases, the value of Y decreases. The slope of a negative linear regression line is negative, meaning that the line slants downward from left to right.</li> </ul>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#regularization-techniques-for-linear-models","title":"Regularization Techniques for Linear Models","text":""},{"location":"AIML/Supervised/Regression/Linear-Regression/#advantages-disadvantages-of-linear-regression","title":"Advantages &amp; Disadvantages of Linear Regression","text":"<p>Advantages of Linear Regression</p> <ul> <li>Linear regression is a relatively simple algorithm, making it easy to understand and implement. The coefficients of the linear regression model can be interpreted as the change in the dependent variable for a one-unit change in the independent variable, providing insights into the relationships between variables.</li> <li>Linear regression is computationally efficient and can handle large datasets effectively. It can be trained quickly on large datasets, making it suitable for real-time applications.</li> <li>Linear regression is relatively robust to outliers compared to other machine learning algorithms. Outliers may have a smaller impact on the overall model performance.</li> <li>Linear regression often serves as a good baseline model for comparison with more complex machine learning algorithms.</li> <li>Linear regression is a well-established algorithm with a rich history and is widely available in various machine learning libraries and software packages.</li> </ul> <p>Disadvantages of Linear Regression</p> <ul> <li>Linear regression assumes a linear relationship between the dependent and independent variables. If the relationship is not linear, the model may not perform well.</li> <li>Linear regression is sensitive to multicollinearity, which occurs when there is a high correlation between independent variables. Multicollinearity can inflate the variance of the coefficients and lead to unstable model predictions.</li> <li>Linear regression assumes that the features are already in a suitable form for the model. Feature engineering may be required to transform features into a format that can be effectively used by the model.</li> <li>Linear regression is susceptible to both overfitting and underfitting. Overfitting occurs when the model learns the training data too well and fails to generalize to unseen data. Underfitting occurs when the model is too simple to capture the underlying relationships in the data.</li> <li>Linear regression provides limited explanatory power for complex relationships between variables. More advanced machine learning techniques may be necessary for deeper insights.</li> </ul>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#conclusion","title":"Conclusion","text":"<p>Linear regression is a fundamental machine learning algorithm that has been widely used for many years due to its simplicity, interpretability, and efficiency. It is a valuable tool for understanding relationships between variables and making predictions in a variety of applications.</p> <p>However, it is important to be aware of its limitations, such as its assumption of linearity and sensitivity to multicollinearity. When these limitations are carefully considered, linear regression can be a powerful tool for data analysis and prediction.</p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#2gradient-descent-in-linear-regression","title":"2.Gradient Descent in Linear Regression","text":""},{"location":"AIML/Supervised/Regression/Linear-Regression/#what-is-gradient-descent","title":"What is Gradient Descent?","text":"<p>Gradient Descent is an iterative optimization algorithm that tries to find the optimum value (Minimum/Maximum) of an objective function. It is one of the most used optimization techniques in machine learning projects for updating the parameters of a model in order to minimize a cost function.  </p> <p>The main aim of gradient descent is to find the best parameters of a model which gives the highest accuracy on training as well as testing datasets. In gradient descent, The gradient is a vector that points in the direction of the steepest increase of the function at a specific point. Moving in the opposite direction of the gradient allows the algorithm to gradually descend towards lower values of the function, and eventually reaching to the minimum of the function.</p> <p>Steps Required in Gradient Descent Algorithm</p> <ul> <li>Step 1 we first initialize the parameters of the model randomly</li> <li>Step 2 Compute the gradient of the cost function with respect to each parameter. It involves making partial differentiation of cost function with respect to the parameters.</li> <li>Step 3 Update the parameters of the model by taking steps in the opposite direction of the model. Here we choose a hyperparameter learning rate which is denoted by alpha. It helps in deciding the step size of the gradient.</li> <li>Step 4 Repeat steps 2 and 3 iteratively to get the best parameter for the defined model </li> </ul> <p>Pseudocode for Gradient Descent</p> <p></p> <p>To apply this gradient descent on data using any programming language we have to make four new functions using which we can update our parameter and apply it to data to make a prediction. We will see each function one by one and understand it </p> <ol> <li>gradient_descent \u2013 In the gradient descent function we will make the prediction on a dataset and compute the difference between the predicted and actual target value and accordingly we will update the parameter and hence it will return the updated parameter.</li> <li>compute_predictions \u2013 In this function, we will compute the prediction using the parameters at each iteration.</li> <li>compute_gradient \u2013 In this function we will compute the error which is the difference between the actual and predicted target value and then compute the gradient using this error and training data.</li> <li>update_parameters \u2013 In this separate function we will update the parameter using learning rate and gradient that we got from the compute_gradient function. </li> </ol> <pre><code>function gradient_descent(X, y, learning_rate, num_iterations):\n    Initialize parameters  = \u03b8\n    for iter in range(num_iterations):\n        predictions = compute_predictions(X, \u03b8)\n        gradient = compute_gradient(X, y, predictions)\n        update_parameters(\u03b8, gradient, learning_rate)\n    return \u03b8\n\nfunction compute_predictions(X, \u03b8):\n    return X*\u03b8\n\nfunction compute_gradient(X, y, predictions):\n    error = predictions - y\n    gradient = X\u1d40 * error / m\n    return gradient\n\nfunction update_parameters(\u03b8, gradient, learning_rate):\n    \u03b8 = \u03b8 - learning_rate \u2a09 gradient\n</code></pre> <p>Mathematics Behind Gradient Descent</p> <p>In the Machine Learning Regression problem, our model targets to get the best-fit regression line to predict the value y based on the given input value (x). While training the model, the model calculates the cost function like Root Mean Squared error between the predicted value (pred) and true value (y). Our model targets to minimize this cost function.  To minimize this cost function, the model needs to have the best value of \u03b81 and \u03b82(for Univariate linear regression problem). Initially model selects \u03b81 and \u03b82 values randomly and then iteratively update these value in order to minimize the cost function until it reaches the minimum. By the time model achieves the minimum cost function, it will have the best \u03b81 and \u03b82 values. Using these updated values of \u03b81 and \u03b82 in the hypothesis equation of linear equation, our model will predict the output value y.  </p> <p>How do \u03b81 and \u03b82 values get updated?</p> <p></p> <p>How Does Gradient Descent Work</p> <p>Gradient descent works by moving downward toward the pits or valleys in the graph to find the minimum value. This is achieved by taking the derivative of the cost function, as illustrated in the figure below. During each iteration, gradient descent step-downs the cost function in the direction of the steepest descent. By adjusting the parameters in this direction, it seeks to reach the minimum of the cost function and find the best-fit values for the parameters. The size of each step is determined by parameter \u03b1 known as Learning Rate.  In the Gradient Descent algorithm, one can infer two points : </p> <p>If slope is +ve : \u03b8j = \u03b8j \u2013 (+ve value). Hence the value of \u03b8j decreases.</p> <p></p> <p>How To Choose Learning Rate</p> <p>The choice of correct learning rate is very important as it ensures that Gradient Descent converges in a reasonable time. : </p> <ul> <li>If we choose \u03b1 to be very large, Gradient Descent can overshoot the minimum. It may fail to converge or even diverge. </li> </ul> <p></p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#python-implementation-of-gradient-descent","title":"Python Implementation of Gradient Descent","text":"<p>At first, we will import all the necessary Python libraries that we will need for mathematical computation and plotting like numpy for mathematical operations and matplotlib for plotting. Then we will define a class Linear_Regression that represents the linear regression model.</p> <p>We will make a update_coeffs method inside the class to update the coefficients (parameters) of the linear regression model using gradient descent. To calculate the error between the predicted output and the actual output we will make a predict method that will make predictions using the current model coefficients. </p> <p>For updating and calculating the gradient of the error we will make compute_cost which will apply gradient descent on (mean squared error) between the predicted values and the actual values.</p> <pre><code># Implementation of gradient descent in linear regression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass Linear_Regression:\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n        self.b = [0, 0]\n\n    def update_coeffs(self, learning_rate):\n        Y_pred = self.predict()\n        Y = self.Y\n        m = len(Y)\n        self.b[0] = self.b[0] - (learning_rate * ((1/m) *\n                                                  np.sum(Y_pred - Y)))\n\n        self.b[1] = self.b[1] - (learning_rate * ((1/m) *\n                                                  np.sum((Y_pred - Y) * self.X)))\n\n    def predict(self, X=[]):\n        Y_pred = np.array([])\n        if not X:\n            X = self.X\n        b = self.b\n        for x in X:\n            Y_pred = np.append(Y_pred, b[0] + (b[1] * x))\n\n        return Y_pred\n\n    def get_current_accuracy(self, Y_pred):\n        p, e = Y_pred, self.Y\n        n = len(Y_pred)\n        return 1-sum(\n            [\n                abs(p[i]-e[i])/e[i]\n                for i in range(n)\n                if e[i] != 0]\n        )/n\n    # def predict(self, b, yi):\n\n    def compute_cost(self, Y_pred):\n        m = len(self.Y)\n        J = (1 / 2*m) * (np.sum(Y_pred - self.Y)**2)\n        return J\n\n    def plot_best_fit(self, Y_pred, fig):\n        f = plt.figure(fig)\n        plt.scatter(self.X, self.Y, color='b')\n        plt.plot(self.X, Y_pred, color='g')\n        f.show()\n\n\ndef main():\n    X = np.array([i for i in range(11)])\n    Y = np.array([2*i for i in range(11)])\n\n    regressor = Linear_Regression(X, Y)\n\n    iterations = 0\n    steps = 100\n    learning_rate = 0.01\n    costs = []\n\n    # original best-fit line\n    Y_pred = regressor.predict()\n    regressor.plot_best_fit(Y_pred, 'Initial Best Fit Line')\n\n    while 1:\n        Y_pred = regressor.predict()\n        cost = regressor.compute_cost(Y_pred)\n        costs.append(cost)\n        regressor.update_coeffs(learning_rate)\n\n        iterations += 1\n        if iterations % steps == 0:\n            print(iterations, \"epochs elapsed\")\n            print(\"Current accuracy is :\",\n                  regressor.get_current_accuracy(Y_pred))\n\n            stop = input(\"Do you want to stop (y/*)??\")\n            if stop == \"y\":\n                break\n\n    # final best-fit line\n    regressor.plot_best_fit(Y_pred, 'Final Best Fit Line')\n\n    # plot to verify cost function decreases\n    h = plt.figure('Verification')\n    plt.plot(range(iterations), costs, color='b')\n    h.show()\n\n    # if user wants to predict using the regressor:\n    regressor.predict([i for i in range(10)])\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Output:</p> <pre><code>100 epochs elapsed\nCurrent accuracy is : 0.9836456109008862\n</code></pre> <p></p> <p></p>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#advantages-of-gradient-descent","title":"Advantages Of Gradient Descent","text":"<ul> <li>Flexibility: Gradient Descent can be used with various cost functions and can handle non-linear regression problems.</li> <li>Scalability: Gradient Descent is scalable to large datasets since it updates the parameters for each training example one at a time.</li> <li>Convergence: Gradient Descent can converge to the global minimum of the cost function, provided that the learning rate is set appropriately.</li> </ul>"},{"location":"AIML/Supervised/Regression/Linear-Regression/#disadvantages-of-gradient-descent","title":"Disadvantages Of Gradient Descent","text":"<ul> <li>Sensitivity to Learning Rate: The choice of learning rate can be critical in Gradient Descent since using a high learning rate can cause the algorithm to overshoot the minimum, while a low learning rate can make the algorithm converge slowly.</li> <li>Slow Convergence: Gradient Descent may require more iterations to converge to the minimum since it updates the parameters for each training example one at a time.</li> <li>Local Minima: Gradient Descent can get stuck in local minima if the cost function has multiple local minima.</li> <li>Noisy updates: The updates in Gradient Descent are noisy and have a high variance, which can make the optimization process less stable and lead to oscillations around the minimum.</li> </ul> <p>Overall, Gradient Descent is a useful optimization algorithm for linear regression, but it has some limitations and requires careful tuning of the learning rate to ensure convergence.</p>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/","title":"Logistic Regression","text":""},{"location":"AIML/Supervised/Regression/Logistic-Regression/#what-is-logistic-regression","title":"What is Logistic Regression?","text":"<p>Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.</p> <p>For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 otherwise it belongs to Class 0. It\u2019s referred to as regression because it is the extension of linear regression but is mainly used for classification problems.</p> <p>Key Points: - Logistic regression predicts the output of a categorical dependent variable. Therefore, the outcome must be a categorical or discrete value. - It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1. - In Logistic regression, instead of fitting a regression line, we fit an \u201cS\u201d shaped logistic function, which predicts two maximum values (0 or 1).</p>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#logistic-function-sigmoid-function","title":"Logistic Function \u2013 Sigmoid Function","text":"<ul> <li>The sigmoid function is a mathematical function used to map the predicted values to probabilities.</li> <li>It maps any real value into another value within a range of 0 and 1. The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the \u201cS\u201d form.</li> <li>The S-form curve is called the Sigmoid function or the logistic function.</li> <li>In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0.</li> </ul>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#types-of-logistic-regression","title":"Types of Logistic Regression","text":"<p>On the basis of the categories, Logistic Regression can be classified into three types:</p> <ol> <li>Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.</li> <li>Multinomial: In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as \u201ccat\u201d, \u201cdogs\u201d, or \u201csheep\u201d.</li> <li>Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as \u201clow\u201d, \u201cMedium\u201d, or \u201cHigh\u201d.</li> </ol>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#assumptions-of-logistic-regression","title":"Assumptions of Logistic Regression","text":"<p>We will explore the assumptions of logistic regression as understanding these assumptions is important to ensure that we are using appropriate application of the model. The assumption include:</p> <ul> <li>Independent observations: Each observation is independent of the other. meaning there is no correlation between any input variables.</li> <li>Binary dependent variables: It takes the assumption that the dependent variable must be binary or dichotomous, meaning it can take only two values. For more than two categories SoftMax functions are used.</li> <li>Linearity relationship between independent variables and log odds: The relationship between the independent variables and the log odds of the dependent variable should be linear.</li> <li>No outliers: There should be no outliers in the dataset.</li> <li>Large sample size: The sample size is sufficiently large</li> </ul>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#terminologies-involved-in-logistic-regression","title":"Terminologies involved in Logistic Regression","text":"<p>Here are some common terms involved in logistic regression:</p> <ul> <li>Independent variables: The input characteristics or predictor factors applied to the dependent variable\u2019s predictions.</li> <li>Dependent variable: The target variable in a logistic regression model, which we are trying to predict.</li> <li>Logistic function: The formula used to represent how the independent and dependent variables relate to one another. The logistic function transforms the input variables into a probability value between 0 and 1, which represents the likelihood of the dependent variable being 1 or 0.</li> <li>Odds: It is the ratio of something occurring to something not occurring. it is different from probability as the probability is the ratio of something occurring to everything that could possibly occur.</li> <li>Log-odds: The log-odds, also known as the logit function, is the natural logarithm of the odds. In logistic regression, the log odds of the dependent variable are modeled as a linear combination of the independent variables and the intercept.</li> <li>Coefficient: The logistic regression model\u2019s estimated parameters, show how the independent and dependent variables relate to one another.</li> <li>Intercept: A constant term in the logistic regression model, which represents the log odds when all independent variables are equal to zero.</li> <li>Maximum likelihood estimation: The method used to estimate the coefficients of the logistic regression model, which maximizes the likelihood of observing the data given the model.</li> </ul>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#how-does-logistic-regression-work","title":"How does Logistic Regression work?","text":"<p>The logistic regression model transforms the linear regression function continuous value output into categorical value output using a sigmoid function, which maps any real-valued set of independent variables input into a value between 0 and 1. This function is known as the logistic function.</p> <p>Let the independent input features be:</p> <p></p>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#sigmoid-function","title":"Sigmoid Function","text":""},{"location":"AIML/Supervised/Regression/Logistic-Regression/#code-implementation-for-logistic-regression","title":"Code Implementation for Logistic Regression","text":"<p>Binomial Logistic regression: Target variable can have only 2 possible types: \u201c0\u201d or \u201c1\u201d which may represent \u201cwin\u201d vs \u201closs\u201d, \u201cpass\u201d vs \u201cfail\u201d, \u201cdead\u201d vs \u201calive\u201d, etc., in this case, sigmoid functions are used, which is already discussed above.</p> <p>Importing necessary libraries based on the requirement of model. This Python code shows how to use the breast cancer dataset to implement a Logistic Regression model for classification.</p> <pre><code># import the necessary libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# load the breast cancer dataset\nX, y = load_breast_cancer(return_X_y=True)\n\n# split the train and test dataset\nX_train, X_test,\\\n    y_train, y_test = train_test_split(X, y,\n                                       test_size=0.20,\n                                       random_state=23)\n# LogisticRegression\nclf = LogisticRegression(random_state=0)\nclf.fit(X_train, y_train)\n\n# Prediction\ny_pred = clf.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprint(\"Logistic Regression model accuracy (in %):\", acc*100)\n</code></pre> <p>Output:</p> <pre><code>Logistic Regression model accuracy (in %): 95.6140350877193\n</code></pre>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#multinomial-logistic-regression","title":"Multinomial Logistic Regression:","text":"<p>Target variable can have 3 or more possible types which are not ordered (i.e. types have no quantitative significance) like \u201cdisease A\u201d vs \u201cdisease B\u201d vs \u201cdisease C\u201d.</p> <p>In this case, the softmax function is used in place of the sigmoid function. Softmax function for K classes will be:</p> <p>In Multinomial Logistic Regression, the output variable can have more than two possible discrete outputs. Consider the Digit Dataset. </p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn import datasets, linear_model, metrics\n\n# load the digit dataset\ndigits = datasets.load_digits()\n\n# defining feature matrix(X) and response vector(y)\nX = digits.data\ny = digits.target\n\n# splitting X and y into training and testing sets\nX_train, X_test,\\\n    y_train, y_test = train_test_split(X, y,\n                                       test_size=0.4,\n                                       random_state=1)\n\n# create logistic regression object\nreg = linear_model.LogisticRegression()\n\n# train the model using the training sets\nreg.fit(X_train, y_train)\n\n# making predictions on the testing set\ny_pred = reg.predict(X_test)\n\n# comparing actual response values (y_test)\n# with predicted response values (y_pred)\nprint(\"Logistic Regression model accuracy(in %):\",\n      metrics.accuracy_score(y_test, y_pred)*100)\n</code></pre> <p>Output:</p> <pre><code>Logistic Regression model accuracy(in %): 96.52294853963839\n</code></pre>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#how-to-evaluate-logistic-regression-model","title":"How to Evaluate Logistic Regression Model?","text":"<p>We can evaluate the logistic regression model using the following metrics:</p> <p></p>"},{"location":"AIML/Supervised/Regression/Logistic-Regression/#precision-recall-tradeoff-in-logistic-regression-threshold-setting","title":"Precision-Recall Tradeoff in Logistic Regression Threshold Setting","text":"<p>Logistic regression becomes a classification technique only when a decision threshold is brought into the picture. The setting of the threshold value is a very important aspect of Logistic regression and is dependent on the classification problem itself.</p> <p>The decision for the value of the threshold value is majorly affected by the values of precision and recall. Ideally, we want both precision and recall being 1, but this seldom is the case.</p> <p>In the case of a Precision-Recall tradeoff, we use the following arguments to decide upon the threshold:</p> <ol> <li> <p>Low Precision/High Recall: In applications where we want to reduce the number of false negatives without necessarily reducing the number of false positives, we choose a decision value that has a low value of Precision or a high value of Recall. For example, in a cancer diagnosis application, we do not want any affected patient to be classified as not affected without giving much heed to if the patient is being wrongfully diagnosed with cancer. This is because the absence of cancer can be detected by further medical diseases, but the presence of the disease cannot be detected in an already rejected candidate.</p> </li> <li> <p>High Precision/Low Recall: In applications where we want to reduce the number of false positives without necessarily reducing the number of false negatives, we choose a decision value that has a high value of Precision or a low value of Recall. For example, if we are classifying customers whether they will react positively or negatively to a personalized advertisement, we want to be absolutely sure that the customer will react positively to the advertisement because otherwise, a negative reaction can cause a loss of potential sales from the customer.</p> </li> </ol>"},{"location":"AIML/Supervised/Regression/Regression-overview/","title":"Regression in machine learning","text":"<p>Regression, a statistical approach, dissects the relationship between dependent and independent variables, enabling predictions through various regression models.</p> <p>The article delves into regression in machine learning, elucidating models, terminologies, types, and practical applications.</p>"},{"location":"AIML/Supervised/Regression/Regression-overview/#what-is-regression","title":"What is Regression?","text":"<p>Regression is a statistical approach used to analyze the relationship between a dependent variable (target variable) and one or more independent variables (predictor variables). The objective is to determine the most suitable function that characterizes the connection between these variables.</p> <p>It seeks to find the best-fitting model, which can be utilized to make predictions or draw conclusions.</p>"},{"location":"AIML/Supervised/Regression/Regression-overview/#regression-in-machine-learning_1","title":"Regression in Machine Learning","text":"<p>It is a supervised machine learning technique, used to predict the value of the dependent variable for new, unseen data. It models the relationship between the input features and the target variable, allowing for the estimation or prediction of numerical values.</p> <p>Regression analysis problem works with if output variable is a real or continuous value, such as \u201csalary\u201d or \u201cweight\u201d. Many different models can be used, the simplest is the linear regression. It tries to fit data with the best hyper-plane which goes through the points.</p> <p>Terminologies Related to the Regression Analysis in Machine Learning</p> <p>Terminologies Related to Regression Analysis:</p> <ul> <li>Response Variable: The primary factor to predict or understand in regression, also known as the dependent variable or target variable.</li> <li>Predictor Variable: Factors influencing the response variable, used to predict its values; also called independent variables.</li> <li>Outliers: Observations with significantly low or high values compared to others, potentially impacting results and best avoided.</li> <li>Multicollinearity: High correlation among independent variables, which can complicate the ranking of influential variables.</li> <li>Underfitting and Overfitting: Overfitting occurs when an algorithm performs well on training but poorly on testing, while underfitting indicates poor performance on both datasets.</li> </ul> <p>Regression Types</p> <p>There are two main types of regression:</p> <ul> <li>Simple Regression<ul> <li>Used to predict a continuous dependent variable based on a single independent variable.</li> <li>Simple linear regression should be used when there is only a single independent variable.</li> </ul> </li> <li>Multiple Regression<ul> <li>Used to predict a continuous dependent variable based on multiple independent variables.</li> <li>Multiple linear regression should be used when there are multiple independent variables.</li> </ul> </li> <li>NonLinear Regression<ul> <li>Relationship between the dependent variable and independent variable(s) follows a nonlinear pattern.</li> <li>Provides flexibility in modeling a wide range of functional forms.</li> </ul> </li> </ul>"},{"location":"AIML/Supervised/Regression/Regression-overview/#regression-algorithms","title":"Regression Algorithms","text":"<p>There are many different types of regression algorithms, but some of the most common include:</p> <ul> <li>Linear Regression<ul> <li>Linear regression is one of the simplest and most widely used statistical models. This assumes that there is a linear relationship between the independent and dependent variables. This means that the change in the dependent variable is proportional to the change in the independent variables.</li> </ul> </li> <li>Polynomial Regression<ul> <li>Polynomial regression is used to model nonlinear relationships between the dependent variable and the independent variables. It adds polynomial terms to the linear regression model to capture more complex relationships.</li> </ul> </li> <li>Support Vector Regression (SVR)<ul> <li>Support vector regression (SVR) is a type of regression algorithm that is based on the support vector machine (SVM) algorithm. SVM is a type of algorithm that is used for classification tasks, but it can also be used for regression tasks. SVR works by finding a hyperplane that minimizes the sum of the squared residuals between the predicted and actual values.</li> </ul> </li> <li>Decision Tree Regression<ul> <li>Decision tree regression is a type of regression algorithm that builds a decision tree to predict the target value. A decision tree is a tree-like structure that consists of nodes and branches. Each node represents a decision, and each branch represents the outcome of that decision. The goal of decision tree regression is to build a tree that can accurately predict the target value for new data points.</li> </ul> </li> <li>Random Forest Regression<ul> <li>Random forest regression is an ensemble method that combines multiple decision trees to predict the target value. Ensemble methods are a type of machine learning algorithm that combines multiple models to improve the performance of the overall model. Random forest regression works by building a large number of decision trees, each of which is trained on a different subset of the training data. The final prediction is made by averaging the predictions of all of the trees.</li> </ul> </li> </ul> <p>Regularized Linear Regression Techniques</p> <ul> <li>Ridge Regression<ul> <li>Ridge regression is a type of linear regression that is used to prevent overfitting. Overfitting occurs when the model learns the training data too well and is unable to generalize to new data.</li> </ul> </li> <li>Lasso regression<ul> <li>Lasso regression is another type of linear regression that is used to prevent overfitting. It does this by adding a penalty term to the loss function that forces the model to use some weights and to set others to zero.</li> </ul> </li> </ul>"},{"location":"AIML/Supervised/Regression/Regression-overview/#characteristics-of-regression","title":"Characteristics of Regression","text":"<p>Here are the characteristics of the regression:</p> <ul> <li>Continuous Target Variable: Regression deals with predicting continuous target variables that represent numerical values. Examples include predicting house prices, forecasting sales figures, or estimating patient recovery times.</li> <li>Error Measurement: Regression models are evaluated based on their ability to minimize the error between the predicted and actual values of the target variable. Common error metrics include mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).</li> <li>Model Complexity: Regression models range from simple linear models to more complex nonlinear models. The choice of model complexity depends on the complexity of the relationship between the input features and the target variable.</li> <li>Overfitting and Underfitting: Regression models are susceptible to overfitting and underfitting.</li> <li>Interpretability: The interpretability of regression models varies depending on the algorithm used. Simple linear models are highly interpretable, while more complex models may be more difficult to interpret.</li> </ul> <p>Examples</p> <p>Which of the following is a regression task?</p> <ul> <li>Predicting age of a person</li> <li>Predicting nationality of a person</li> <li>Predicting whether stock price of a company will increase tomorrow</li> <li>Predicting whether a document is related to sighting of UFOs?</li> </ul> <p>Solution : Predicting age of a person (because it is a real value, predicting nationality is categorical, whether stock price will increase is discrete-yes/no answer, predicting whether a document is related to UFO is again discrete- a yes/no answer).</p>"},{"location":"AIML/Supervised/Regression/Regression-overview/#regression-model-machine-learning","title":"Regression Model Machine Learning","text":"<p>Let\u2019s take an example of linear regression. We have a Housing data set and we want to predict the price of the house. Following is the python code for it.</p> <pre><code># Python code to illustrate \n# regression using data set \nimport matplotlib \nmatplotlib.use('GTKAgg') \n\nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn import datasets, linear_model \nimport pandas as pd \n\n# Load CSV and columns \ndf = pd.read_csv(\"Housing.csv\") \n\nY = df['price'] \nX = df['lotsize'] \n\nX=X.values.reshape(len(X),1) \nY=Y.values.reshape(len(Y),1) \n\n# Split the data into training/testing sets \nX_train = X[:-250] \nX_test = X[-250:] \n\n# Split the targets into training/testing sets \nY_train = Y[:-250] \nY_test = Y[-250:] \n\n# Plot outputs \nplt.scatter(X_test, Y_test, color='black') \nplt.title('Test Data') \nplt.xlabel('Size') \nplt.ylabel('Price') \nplt.xticks(()) \nplt.yticks(()) \n\n\n# Create linear regression object \nregr = linear_model.LinearRegression() \n\n# Train the model using the training sets \nregr.fit(X_train, Y_train) \n\n# Plot outputs \nplt.plot(X_test, regr.predict(X_test), color='red',linewidth=3) \nplt.show() \n</code></pre> <p></p> <p>Here in this graph, we plot the test data. The red line indicates the best fit line for predicting the price.</p> <p>To make an individual prediction using the linear regression model: </p> <p></p>"},{"location":"AIML/Supervised/Regression/Regression-overview/#regression-evaluation-metrics","title":"Regression Evaluation Metrics","text":"<p>Here are some most popular evaluation metrics for regression:</p> <ul> <li>Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values of the target variable.</li> <li>Mean Squared Error (MSE): The average squared difference between the predicted and actual values of the target variable.</li> <li>Root Mean Squared Error (RMSE): The square root of the mean squared error.</li> <li>Huber Loss: A hybrid loss function that transitions from MAE to MSE for larger errors, providing balance between robustness and MSE\u2019s sensitivity to outliers.</li> <li>Root Mean Square Logarithmic Error</li> <li>R2 \u2013 Score: Higher values indicate better fit, ranging from 0 to 1.</li> </ul>"},{"location":"AIML/Supervised/Regression/Regression-overview/#applications-of-regression","title":"Applications of Regression","text":"<ul> <li>Predicting prices: For example, a regression model could be used to predict the price of a house based on its size, location, and other features.</li> <li>Forecasting trends: For example, a regression model could be used to forecast the sales of a product based on historical sales data and economic indicators.</li> <li>Identifying risk factors: For example, a regression model could be used to identify risk factors for heart disease based on patient data.</li> <li>Making decisions: For example, a regression model could be used to recommend which investment to buy based on market data.</li> </ul> <p>What is regression and classification? <pre><code>Regression are used to predict continuous values, while classification categorizes data. Both are supervised learning tasks in machine learning.\n</code></pre></p> <p>What is simple regression in machine learning? <pre><code>Simple regression predicts a dependent variable based on one independent variable, forming a linear relationship.\n</code></pre></p> <p>What are the different regression algorithm? <pre><code>Regression algorithms include linear regression, polynomial regression, support vector regression, and decision tree regression.\n</code></pre></p>"},{"location":"AIML/Unsupervised/Unsupervised-overview/","title":"Unsupervised learning","text":""},{"location":"AIML/Unsupervised/Unsupervised-overview/#what-is-unsupervised-learning","title":"What is Unsupervised Learning?","text":"<p>Unsupervised learning is a branch of machine learning that deals with unlabeled data. Unlike supervised learning, where the data is labeled with a specific category or outcome, unsupervised learning algorithms are tasked with finding patterns and relationships within the data without any prior knowledge of the data\u2019s meaning. This makes unsupervised learning a powerful tool for exploratory data analysis, where the goal is to understand the underlying structure of the data.</p> <p>Unsupervised machine learning analyzes and clusters unlabeled datasets using machine learning algorithms. These algorithms find hidden patterns and data without any human intervention, i.e., we don\u2019t give output to our model. The training model has only input parameter values and discovers the groups or patterns on its own.</p> <p></p>"},{"location":"AIML/Unsupervised/Unsupervised-overview/#how-does-unsupervised-learning-work","title":"How does unsupervised learning work?","text":"<p>Unsupervised learning works by analyzing unlabeled data to identify patterns and relationships. The data is not labeled with any predefined categories or outcomes, so the algorithm must find these patterns and relationships on its own. This can be a challenging task, but it can also be very rewarding, as it can reveal insights into the data that would not be apparent from a labeled dataset.</p> <p>Data-set in Figure A is Mall data that contains information about its clients that subscribe to them. Once subscribed they are provided a membership card and the mall has complete information about the customer and his/her every purchase. Now using this data and unsupervised learning techniques, the mall can easily group clients based on the parameters we are feeding in. </p> <p></p> <p>The input to the unsupervised learning models is as follows: </p> <ul> <li>Unstructured data: May contain noisy(meaningless) data, missing values, or unknown data</li> <li>Unlabeled data: Data only contains a value for input parameters, there is no targeted value(output). It is easy to collect as compared to the labeled one in the Supervised approach.</li> </ul>"},{"location":"AIML/Unsupervised/Unsupervised-overview/#unsupervised-learning-algorithms","title":"Unsupervised Learning Algorithms","text":"<p>There are mainly 3 types of Algorithms which are used for Unsupervised dataset.</p> <ul> <li>Clustering</li> <li>Association Rule Learning</li> <li>Dimensionality Reduction</li> </ul>"},{"location":"AIML/Unsupervised/Unsupervised/","title":"Unsupervised","text":""},{"location":"AIML/Unsupervised/Unsupervised/#contents","title":"Contents","text":"<ul> <li>Business Case</li> <li>ML metrics<ul> <li>WSSE</li> <li>Explained variance</li> </ul> </li> <li>Feature engineering</li> <li>ML - Algos<ul> <li>Kmeans</li> <li>PCA</li> </ul> </li> <li>Evaluation</li> <li>Extensions</li> </ul>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/","title":"Clustering in Machine Learning","text":"<p>In real world, not every data we work upon has a target variable. This kind of data cannot be analyzed using supervised learning algorithms. We need the help of unsupervised algorithms. One of the most popular type of analysis under unsupervised learning is Cluster analysis. When the goal is to group similar data points in a dataset, then we use cluster analysis. In practical situations, we can use cluster analysis for customer segmentation for targeted advertisements, or in medical imaging to find unknown or new infected areas and many more use cases.</p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#what-is-clustering","title":"What is Clustering ?","text":"<p>The task of grouping data points based on their similarity with each other is called Clustering or Cluster Analysis. This method is defined under the branch of Unsupervised Learning, which aims at gaining insights from unlabelled data points, that is, unlike supervised learning we don\u2019t have a target variable.</p> <p>Clustering aims at forming groups of homogeneous data points from a heterogeneous dataset. It evaluates the similarity based on a metric like Euclidean distance, Cosine similarity, Manhattan distance, etc. and then group the points with highest similarity score together.</p> <p>For Example, In the graph given below, we can clearly see that there are 3 circular clusters forming on the basis of distance.</p> <p></p> <p>Now it is not necessary that the clusters formed must be circular in shape. The shape of clusters can be arbitrary. There are many algortihms that work well with detecting arbitrary shaped clusters. </p> <p>For example, In the below given graph we can see that the clusters formed are not circular in shape.</p> <p></p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#types-of-clustering","title":"Types of Clustering","text":"<p>Broadly speaking, there are 2 types of clustering that can be performed to group similar data points:</p> <ul> <li>Hard Clustering: In this type of clustering, each data point belongs to a cluster completely or not. For example, Let\u2019s say there are 4 data point and we have to cluster them into 2 clusters. So each data point will either belong to cluster 1 or cluster 2.</li> </ul> <p></p> <ul> <li>Soft Clustering: In this type of clustering, instead of assigning each data point into a separate cluster, a probability or likelihood of that point being that cluster is evaluated. For example, Let\u2019s say there are 4 data point and we have to cluster them into 2 clusters. So we will be evaluating a probability of a data point belonging to both clusters. This probability is calculated for all data points.</li> </ul> <p></p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#uses-of-clustering","title":"Uses of Clustering","text":"<p>Now before we begin with types of clustering algorithms, we will go through the use cases of Clustering algorithms. Clustering algorithms are majorly used for:</p> <ul> <li>Market Segmentation \u2013 Businesses use clustering to group their customers and use targeted advertisements to attract more audience.</li> <li>Market Basket Analysis \u2013 Shop owners analyze their sales and figure out which items are majorly bought together by the customers. For example, In USA, according to a study diapers and beers were usually bought together by fathers.</li> <li>Social Network Analysis \u2013 Social media sites use your data to understand your browsing behaviour and provide you with targeted friend recommendations or content recommendations.</li> <li>Medical Imaging \u2013 Doctors use Clustering to find out diseased areas in diagnostic images like X-rays.</li> <li>Anomaly Detection \u2013 To find outliers in a stream of real-time dataset or forecasting fraudulent transactions we can use clustering to identify them.</li> <li>Simplify working with large datasets \u2013 Each cluster is given a cluster ID after clustering is complete. Now, you may reduce a feature set\u2019s whole feature set into its cluster ID. Clustering is effective when it can represent a complicated case with a straightforward cluster ID. Using the same principle, clustering data can make complex datasets simpler.</li> </ul> <p>There are many more use cases for clustering but there are some of the major and common use cases of clustering. Moving forward we will be discussing Clustering Algorithms that will help you perform the above tasks.</p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#types-of-clustering-algorithms","title":"Types of Clustering Algorithms","text":"<p>At the surface level, clustering helps in the analysis of unstructured data. Graphing, the shortest distance, and the density of the data points are a few of the elements that influence cluster formation. Clustering is the process of determining how related the objects are based on a metric called the similarity measure. Similarity metrics are easier to locate in smaller sets of features. It gets harder to create similarity measures as the number of features increases. Depending on the type of clustering algorithm being utilized in data mining, several techniques are employed to group the data from the datasets. In this part, the clustering techniques are described. Various types of clustering algorithms are:</p> <ol> <li>Centroid-based Clustering (Partitioning methods)</li> <li>Density-based Clustering (Model-based methods)</li> <li>Connectivity-based Clustering (Hierarchical clustering)</li> <li>Distribution-based Clustering</li> </ol>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#1-centroid-based-clustering-partitioning-methods","title":"1. Centroid-based Clustering (Partitioning methods)","text":"<p>Partitioning methods are the most easiest clustering algorithms. They group data points on the basis of their closeness. Generally, the similarity measure chosen for these algorithms are Euclidian distance, Manhattan Distance or Minkowski Distance. The datasets are separated into a predetermined number of clusters, and each cluster is referenced by a vector of values. When compared to the vector value, the input data variable shows no difference and joins the cluster. </p> <p>The primary drawback for these algorithms is the requirement that we establish the number of clusters, \u201ck,\u201d either intuitively or scientifically (using the Elbow Method) before any clustering machine learning system starts allocating the data points. Despite this, it is still the most popular type of clustering. K-means and K-medoids clustering are some examples of this type clustering.</p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#2-density-based-clustering-model-based-methods","title":"2. Density-based Clustering (Model-based methods)","text":"<p>Density-based clustering, a model-based method, finds groups based on the density of data points. Contrary to centroid-based clustering, which requires that the number of clusters be predefined and is sensitive to initialization, density-based clustering determines the number of clusters automatically and is less susceptible to beginning positions. They are great at handling clusters of different sizes and forms, making them ideally suited for datasets with irregularly shaped or overlapping clusters. These methods manage both dense and sparse data regions by focusing on local density and can distinguish clusters with a variety of morphologies. </p> <p>In contrast, centroid-based grouping, like k-means, has trouble finding arbitrary shaped clusters. Due to its preset number of cluster requirements and extreme sensitivity to the initial positioning of centroids, the outcomes can vary. Furthermore, the tendency of centroid-based approaches to produce spherical or convex clusters restricts their capacity to handle complicated or irregularly shaped clusters. In conclusion, density-based clustering overcomes the drawbacks of centroid-based techniques by autonomously choosing cluster sizes, being resilient to initialization, and successfully capturing clusters of various sizes and forms. The most popular density-based clustering algorithm is DBSCAN.</p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#3-connectivity-based-clustering-hierarchical-clustering","title":"3. Connectivity-based Clustering (Hierarchical clustering)","text":"<p>A method for assembling related data points into hierarchical clusters is called hierarchical clustering. Each data point is initially taken into account as a separate cluster, which is subsequently combined with the clusters that are the most similar to form one large cluster that contains all of the data points.</p> <p>Think about how you may arrange a collection of items based on how similar they are. Each object begins as its own cluster at the base of the tree when using hierarchical clustering, which creates a dendrogram, a tree-like structure. The closest pairings of clusters are then combined into larger clusters after the algorithm examines how similar the objects are to one another. When every object is in one cluster at the top of the tree, the merging process has finished. Exploring various granularity levels is one of the fun things about hierarchical clustering. To obtain a given number of clusters, you can select to cut the dendrogram at a particular height. The more similar two objects are within a cluster, the closer they are. It\u2019s comparable to classifying items according to their family trees, where the nearest relatives are clustered together and the wider branches signify more general connections. There are 2 approaches for Hierarchical clustering:</p> <ul> <li>Divisive Clustering: It follows a top-down approach, here we consider all data points to be part one big cluster and then this cluster is divide into smaller groups.</li> <li>Agglomerative Clustering: It follows a bottom-up approach, here we consider all data points to be part of individual clusters and then these clusters are clubbed together to make one big cluster with all data points. </li> </ul>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#4-distribution-based-clustering","title":"4. Distribution-based Clustering","text":"<p>Using distribution-based clustering, data points are generated and organized according to their propensity to fall into the same probability distribution (such as a Gaussian, binomial, or other) within the data. The data elements are grouped using a probability-based distribution that is based on statistical distributions. Included are data objects that have a higher likelihood of being in the cluster. A data point is less likely to be included in a cluster the further it is from the cluster\u2019s central point, which exists in every cluster.</p> <p>A notable drawback of density and boundary-based approaches is the need to specify the clusters a priori for some algorithms, and primarily the definition of the cluster form for the bulk of algorithms. There must be at least one tuning or hyper-parameter selected, and while doing so should be simple, getting it wrong could have unanticipated repercussions. Distribution-based clustering has a definite advantage over proximity and centroid-based clustering approaches in terms of flexibility, accuracy, and cluster structure. The key issue is that, in order to avoid overfitting, many clustering methods only work with simulated or manufactured data, or when the bulk of the data points certainly belong to a preset distribution. The most popular distribution-based clustering algorithm is Gaussian Mixture Model.</p>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#applications-of-clustering-in-different-fields","title":"Applications of Clustering in different fields:","text":"<ol> <li>Marketing: It can be used to characterize &amp; discover customer segments for marketing purposes.</li> <li>Biology: It can be used for classification among different species of plants and animals.</li> <li>Libraries: It is used in clustering different books on the basis of topics and information.</li> <li>Insurance: It is used to acknowledge the customers, their policies and identifying the frauds.</li> <li>City Planning: It is used to make groups of houses and to study their values based on their geographical locations and other factors present. </li> <li>Earthquake studies: By learning the earthquake-affected areas we can determine the dangerous zones. </li> <li>Image Processing: Clustering can be used to group similar images together, classify images based on content, and identify patterns in image data.</li> <li>Genetics: Clustering is used to group genes that have similar expression patterns and identify gene networks that work together in biological processes.</li> <li>Finance: Clustering is used to identify market segments based on customer behavior, identify patterns in stock market data, and analyze risk in investment portfolios.</li> <li>Customer Service: Clustering is used to group customer inquiries and complaints into categories, identify common issues, and develop targeted solutions.</li> <li>Manufacturing: Clustering is used to group similar products together, optimize production processes, and identify defects in manufacturing processes.</li> <li>Medical diagnosis: Clustering is used to group patients with similar symptoms or diseases, which helps in making accurate diagnoses and identifying effective treatments.</li> <li>Fraud detection: Clustering is used to identify suspicious patterns or anomalies in financial transactions, which can help in detecting fraud or other financial crimes.</li> <li>Traffic analysis: Clustering is used to group similar patterns of traffic data, such as peak hours, routes, and speeds, which can help in improving transportation planning and infrastructure.</li> <li>Social network analysis: Clustering is used to identify communities or groups within social networks, which can help in understanding social behavior, influence, and trends.</li> <li>Cybersecurity: Clustering is used to group similar patterns of network traffic or system behavior, which can help in detecting and preventing cyberattacks.</li> <li>Climate analysis: Clustering is used to group similar patterns of climate data, such as temperature, precipitation, and wind, which can help in understanding climate change and its impact on the environment.</li> <li>Sports analysis: Clustering is used to group similar patterns of player or team performance data, which can help in analyzing player or team strengths and weaknesses and making strategic decisions.</li> <li>Crime analysis: Clustering is used to group similar patterns of crime data, such as location, time, and type, which can help in identifying crime hotspots, predicting future crime trends, and improving crime prevention strategies.</li> </ol> <p>The top 10 clustering algorithms are:</p> <ol> <li>K-means Clustering</li> <li>Hierarchical Clustering</li> <li>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</li> <li>Gaussian Mixture Models (GMM)</li> <li>Agglomerative Clustering</li> <li>Spectral Clustering</li> <li>Mean Shift Clustering</li> <li>Affinity Propagation</li> <li>OPTICS (Ordering Points To Identify the Clustering Structure)</li> <li>Birch (Balanced Iterative Reducing and Clustering using Hierarchies)</li> </ol>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#what-is-the-difference-between-clustering-and-classification","title":"What is the difference between clustering and classification?","text":"<pre><code>The main difference between clustering and classification is that, classification is a supervised learning algorithm and clustering is an unsupervised learning algorithm. That is, we apply clustering to those datasets that without a target variable. \n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#what-are-the-advantages-of-clustering-analysis","title":"What are the advantages of clustering analysis?","text":"<pre><code>Data can be organised into meaningful groups using the strong analytical tool of cluster analysis. You can use it to pinpoint segments, find hidden patterns, and improve decisions.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#which-is-the-fastest-clustering-method","title":"Which is the fastest clustering method?","text":"<pre><code>K-means clustering is often considered the fastest clustering method due to its simplicity and computational efficiency. It iteratively assigns data points to the nearest cluster centroid, making it suitable for large datasets with low dimensionality and a moderate number of clusters.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#what-are-the-limitations-of-clustering","title":"What are the limitations of clustering?","text":"<pre><code>Limitations of clustering include sensitivity to initial conditions, dependence on the choice of parameters, difficulty in determining the optimal number of clusters, and challenges with handling high-dimensional or noisy data.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/Clustering-overview/#what-does-the-quality-of-result-of-clustering-depend-on","title":"What does the quality of result of clustering depend on?","text":"<pre><code>The quality of clustering results depends on factors such as the choice of algorithm, distance metric, number of clusters, initialization method, data preprocessing techniques, cluster evaluation metrics, and domain knowledge. These elements collectively influence the effectiveness and accuracy of the clustering outcome.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/","title":"K means Clustering \u2013 Introduction","text":"<p>K-Means Clustering is an Unsupervised Machine Learning algorithm, which groups the unlabeled dataset into different clusters. </p>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-is-k-means-clustering","title":"What is K-means Clustering?","text":"<p>Unsupervised Machine Learning is the process of teaching a computer to use unlabeled, unclassified data and enabling the algorithm to operate on that data without supervision. Without any previous data training, the machine\u2019s job in this case is to organize unsorted data according to parallels, patterns, and variations. </p> <p>K means clustering, assigns data points to one of the K clusters depending on their distance from the center of the clusters. It starts by randomly assigning the clusters centroid in the space. Then each data point assign to one of the cluster based on its distance from centroid of the cluster. After assigning each point to one of the cluster, new cluster centroids are assigned. This process runs iteratively until it finds good cluster. In the analysis we assume that number of cluster is given in advanced and we have to put points in one of the group.</p> <p>In some cases, K is not clearly defined, and we have to think about the optimal number of K. K Means clustering performs best data is well separated. When data points overlapped this clustering is not suitable. K Means is faster as compare to other clustering technique. It provides strong coupling between the data points. K Means cluster do not provide clear information regarding the quality of clusters. Different initial assignment of cluster centroid may lead to different clusters. Also, K Means algorithm is sensitive to noise. It may have stuck in local minima.</p>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-is-the-objective-of-k-means-clustering","title":"What is the objective of k-means clustering?","text":"<p>The goal of clustering is to divide the population or set of data points into a number of groups so that the data points within each group are more comparable to one another and different from the data points within the other groups. It is essentially a grouping of things based on how similar and different they are to one another. </p>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#how-k-means-clustering-works","title":"How k-means clustering works?","text":"<p>We are given a data set of items, with certain features, and values for these features (like a vector). The task is to categorize those items into groups. To achieve this, we will use the K-means algorithm, an unsupervised learning algorithm. \u2018K\u2019 in the name of the algorithm represents the number of groups/clusters we want to classify our items into.</p> <p>(It will help if you think of items as points in an n-dimensional space). The algorithm will categorize the items into k groups or clusters of similarity. To calculate that similarity, we will use the Euclidean distance as a measurement.</p> <p>The algorithm works as follows:  </p> <ol> <li>First, we randomly initialize k points, called means or cluster centroids.</li> <li>We categorize each item to its closest mean, and we update the mean\u2019s coordinates, which are the averages of the items categorized in that cluster so far.</li> <li>We repeat the process for a given number of iterations and at the end, we have our clusters.</li> </ol> <p>The \u201cpoints\u201d mentioned above are called means because they are the mean values of the items categorized in them. To initialize these means, we have a lot of options. An intuitive method is to initialize the means at random items in the data set. Another method is to initialize the means at random values between the boundaries of the data set (if for a feature x, the items have values in [0,3], we will initialize the means with values for x at [0,3]).</p> <p>The above algorithm in pseudocode is as follows:</p> <pre><code>Initialize k means with random values\n--&gt; For a given number of iterations:\n\n    --&gt; Iterate through items:\n\n        --&gt; Find the mean closest to the item by calculating \n        the euclidean distance of the item with each of the means\n\n        --&gt; Assign item to mean\n\n        --&gt; Update mean by shifting it to the average of the items in that cluster\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#implementation-of-k-means-clustering-in-python","title":"Implementation of K-Means Clustering in Python","text":"<p>Example 1:</p> <p>Import the necessary Libraries</p> <p>We are importing Numpy for statistical computations, Matplotlib to plot the graph, and make_blobs from sklearn.datasets.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n</code></pre> <p>Create the custom dataset with make_blobs and plot it</p> <pre><code>X,y = make_blobs(n_samples = 500,n_features = 2,centers = 3,random_state = 23)\n\nfig = plt.figure(0)\nplt.grid(True)\nplt.scatter(X[:,0],X[:,1])\nplt.show()\n</code></pre> <p></p> <p>Initialize the random centroids</p> <p>The code initializes three clusters for K-means clustering. It sets a random seed and generates random cluster centers within a specified range, and creates an empty list of points for each cluster.</p> <pre><code>k = 3\n\nclusters = {}\nnp.random.seed(23)\n\nfor idx in range(k):\n    center = 2*(2*np.random.random((X.shape[1],))-1)\n    points = []\n    cluster = {\n        'center' : center,\n        'points' : []\n    }\n\n    clusters[idx] = cluster\n\nclusters\n</code></pre> <p>Output:</p> <pre><code>{0: {'center': array([0.06919154, 1.78785042]), 'points': []},\n 1: {'center': array([ 1.06183904, -0.87041662]), 'points': []},\n 2: {'center': array([-1.11581855,  0.74488834]), 'points': []}}\n ```\n\n **Plot the random initialize center with data points**\n\n ```\n plt.scatter(X[:,0],X[:,1])\nplt.grid(True)\nfor i in clusters:\n    center = clusters[i]['center']\n    plt.scatter(center[0],center[1],marker = '*',c = 'red')\nplt.show()\n</code></pre> <p></p> <p>The plot displays a scatter plot of data points (X[:,0], X[:,1]) with grid lines. It also marks the initial cluster centers (red stars) generated for K-means clustering.</p> <p>Define Euclidean distance</p> <pre><code>def distance(p1,p2):\n    return np.sqrt(np.sum((p1-p2)**2))\n</code></pre> <p>Create the function to Assign and Update the cluster center</p> <p>The E-step assigns data points to the nearest cluster center, and the M-step updates cluster centers based on the mean of assigned points in K-means clustering.</p> <pre><code>#Implementing E step \ndef assign_clusters(X, clusters):\n    for idx in range(X.shape[0]):\n        dist = []\n\n        curr_x = X[idx]\n\n        for i in range(k):\n            dis = distance(curr_x,clusters[i]['center'])\n            dist.append(dis)\n        curr_cluster = np.argmin(dist)\n        clusters[curr_cluster]['points'].append(curr_x)\n    return clusters\n\n#Implementing the M-Step\ndef update_clusters(X, clusters):\n    for i in range(k):\n        points = np.array(clusters[i]['points'])\n        if points.shape[0] &gt; 0:\n            new_center = points.mean(axis =0)\n            clusters[i]['center'] = new_center\n\n            clusters[i]['points'] = []\n    return clusters\n</code></pre> <p>Step 7: Create the function to Predict the cluster for the datapoints</p> <pre><code>def pred_cluster(X, clusters):\n    pred = []\n    for i in range(X.shape[0]):\n        dist = []\n        for j in range(k):\n            dist.append(distance(X[i],clusters[j]['center']))\n        pred.append(np.argmin(dist))\n    return pred   \n</code></pre> <p>Assign, Update, and predict the cluster center</p> <pre><code>clusters = assign_clusters(X,clusters)\nclusters = update_clusters(X,clusters)\npred = pred_cluster(X,clusters)\n</code></pre> <p>Plot the data points with their predicted cluster center</p> <pre><code>plt.scatter(X[:,0],X[:,1],c = pred)\nfor i in clusters:\n    center = clusters[i]['center']\n    plt.scatter(center[0],center[1],marker = '^',c = 'red')\nplt.show()\n</code></pre> <p></p> <p>Example 2</p> <p>Import the necessary libraries</p> <pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\n</code></pre> <p>Load the Dataset</p> <pre><code>X, y = load_iris(return_X_y=True)\n</code></pre> <p>Elbow Method Finding the ideal number of groups to divide the data into is a basic stage in any unsupervised algorithm. One of the most common techniques for figuring out this ideal value of k is the elbow approach.</p> <pre><code>#Find optimum number of cluster\nsse = [] #SUM OF SQUARED ERROR\nfor k in range(1,11):\n    km = KMeans(n_clusters=k, random_state=2)\n    km.fit(X)\n    sse.append(km.inertia_)\n</code></pre> <p>Plot the Elbow graph to find the optimum number of cluster</p> <pre><code>sns.set_style(\"whitegrid\")\ng=sns.lineplot(x=range(1,11), y=sse)\n\ng.set(xlabel =\"Number of cluster (k)\", \n      ylabel = \"Sum Squared Error\", \n      title ='Elbow Method')\n\nplt.show()\n</code></pre> <p></p> <p>From the above graph, we can observe that at k=2 and k=3 elbow-like situation. So, we are considering K=3</p> <p>Build the Kmeans clustering model</p> <pre><code>kmeans = KMeans(n_clusters = 3, random_state = 2)\nkmeans.fit(X)\n</code></pre> <p>Output:</p> <pre><code>KMeans\nKMeans(n_clusters=3, random_state=2)\n</code></pre> <p>Find the cluster center</p> <pre><code>kmeans.cluster_centers_\n</code></pre> <p>Output:</p> <pre><code>array([[5.006     , 3.428     , 1.462     , 0.246     ],\n       [5.9016129 , 2.7483871 , 4.39354839, 1.43387097],\n       [6.85      , 3.07368421, 5.74210526, 2.07105263]])\n</code></pre> <p>Predict the cluster group:</p> <pre><code>pred = kmeans.fit_predict(X)\npred\n</code></pre> <p>Output:</p> <pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,\n       2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,\n       2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1], dtype=int32)\n</code></pre> <p>Plot the cluster center with data points</p> <pre><code>plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.scatter(X[:,0],X[:,1],c = pred, cmap=cm.Accent)\nplt.grid(True)\nfor center in kmeans.cluster_centers_:\n    center = center[:2]\n    plt.scatter(center[0],center[1],marker = '^',c = 'red')\nplt.xlabel(\"petal length (cm)\")\nplt.ylabel(\"petal width (cm)\")\n\nplt.subplot(1,2,2)   \nplt.scatter(X[:,2],X[:,3],c = pred, cmap=cm.Accent)\nplt.grid(True)\nfor center in kmeans.cluster_centers_:\n    center = center[2:4]\n    plt.scatter(center[0],center[1],marker = '^',c = 'red')\nplt.xlabel(\"sepal length (cm)\")\nplt.ylabel(\"sepal width (cm)\")\nplt.show()\n</code></pre> <p></p> <p>The subplot on the left display petal length vs. petal width with data points colored by clusters, and red markers indicate K-means cluster centers. The subplot on the right show sepal length vs. sepal width similarly.</p>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#conclusion","title":"Conclusion","text":"<p>In conclusion, K-means clustering is a powerful unsupervised machine learning algorithm for grouping unlabeled datasets. Its objective is to divide data into clusters, making similar data points part of the same group. The algorithm initializes cluster centroids and iteratively assigns data points to the nearest centroid, updating centroids based on the mean of points in each cluster.</p>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-is-k-means-clustering-for-data-analysis","title":"What is k-means clustering for data analysis?","text":"<pre><code>K-means is a partitioning method that divides a dataset into \u2018k\u2019 distinct, non-overlapping subsets (clusters) based on similarity, aiming to minimize the variance within each cluster.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-is-an-example-of-k-means-in-real-life","title":"What is an example of k-means in real life?","text":"<pre><code>Customer segmentation in marketing, where k-means groups customers based on purchasing behavior, allowing businesses to tailor marketing strategies for different segments.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-type-of-data-is-k-means-clustering-model","title":"What type of data is k-means clustering model?","text":"<pre><code>K-means works well with numerical data, where the concept of distance between data points is meaningful. It\u2019s commonly applied to continuous variables.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#is-k-means-used-for-prediction","title":"Is K-means used for prediction?","text":"<pre><code>K-means is primarily used for clustering and grouping similar data points. It does not predict labels for new data; it assigns them to existing clusters based on similarity.\n</code></pre>"},{"location":"AIML/Unsupervised/Clustering/K-means-Clustering/#what-is-the-objective-of-k-means-clustering_1","title":"What is the objective of k-means clustering?","text":"<pre><code>The objective is to partition data into \u2018k\u2019 clusters, minimizing the intra-cluster variance. It seeks to form groups where data points within each cluster are more similar to each other than to those in other clusters.\n</code></pre>"},{"location":"AIML/VectorDb/faiss/","title":"FAISS","text":"<pre><code>!pip install faiss-cpu\n!pip install sentence-transformers\n</code></pre> <pre><code>import faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sample data\ndocuments = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"]\n\n# Generate embeddings\nembeddings = model.encode(documents)\ndimension = embeddings.shape[1]\n\n# Create FAISS index\nindex = faiss.IndexFlatL2(dimension)  # L2 distance index\nindex.add(np.array(embeddings))       # Add embeddings to the index\n</code></pre> <pre><code>query = \"What is document retrieval?\"\nquery_embedding = model.encode([query])\n\n# Search for top 3 nearest neighbors\nD, I = index.search(np.array(query_embedding), k=2)\nprint(\"Top documents:\", [documents[i] for i in I[0]])\n</code></pre>"},{"location":"AIML/VectorDb/milvus/","title":"What is Milvus?","text":"<ul> <li>Milvus is a high-performance, highly scalable vector database that runs efficiently across a wide range of environments, from a laptop to large-scale distributed systems. It is available as both open-source software and a cloud service.</li> <li>Milvus is an open-source project under LF AI &amp; Data Foundation distributed under the Apache 2.0 license.</li> <li>Core contributors include professionals from Zilliz, ARM, NVIDIA, AMD, Intel, Meta, IBM, Salesforce, Alibaba, and Microsoft.</li> </ul>"},{"location":"AIML/VectorDb/milvus/#unstructured-data-embeddings-and-milvus","title":"Unstructured Data, Embeddings, and Milvus","text":"<ul> <li>Unstructured data, such as text, images, and audio, varies in format and carries rich underlying semantics, making it challenging to analyze.</li> <li>To manage this complexity, embeddings are used to convert unstructured data into numerical vectors that capture its essential characteristics.</li> <li>These vectors are then stored in a vector database, enabling fast and scalable searches and analytics.</li> <li>Milvus offers robust data modeling capabilities, enabling you to organize your unstructured or multi-modal data into structured collections.</li> <li>It supports a wide range of data types for different attribute modeling, including common numerical and character types, various vector types, arrays, sets, and JSON, saving you from the effort of maintaining multiple database systems.</li> </ul> <p>Milvus offers three deployment modes, covering a wide range of data scales\u2014from local prototyping in Jupyter Notebooks to massive Kubernetes clusters managing tens of billions of vectors:</p>"},{"location":"AIML/VectorDb/milvus/#milvus-lite","title":"Milvus Lite","text":"<ul> <li>Milvus Lite is a Python library that can be easily integrated into your applications. As a lightweight version of Milvus, it\u2019s ideal for quick prototyping in Jupyter Notebooks or running on edge devices with limited resources.</li> </ul>"},{"location":"AIML/VectorDb/milvus/#milvus-standalone","title":"Milvus Standalone","text":"<ul> <li>Milvus Standalone is a single-machine server deployment, with all components bundled into a single Docker image for convenient deployment.</li> </ul>"},{"location":"AIML/VectorDb/milvus/#milvus-distributed","title":"Milvus Distributed","text":"<ul> <li> <p>Milvus Distributed can be deployed on Kubernetes clusters, featuring a cloud-native architecture designed for billion-scale or even larger scenarios.</p> </li> <li> <p>Milvus Lite is recommended for smaller datasets, up to a few million vectors.</p> </li> <li>Milvus Standalone is suitable for medium-sized datasets, scaling up to 100 million vectors.</li> <li>Milvus Distributed is designed for large-scale deployments, capable of handling datasets from 100 million up to tens of billions of vectors.</li> </ul> <p></p>"},{"location":"AIML/VectorDb/milvus/#indexes-supported-in-milvus","title":"Indexes supported in Milvus","text":"<p>Milvus supports various index types, which are categorized by the type of vector embeddings they handle: floating-point embeddings (also known as floating point vectors or dense vectors), binary embeddings (also known as binary vectors), and sparse embeddings  (also known as sparse vectors).</p>"},{"location":"AIML/VectorDb/milvus/#floating-point-embeddings","title":"Floating-point embeddings","text":""},{"location":"AIML/VectorDb/milvus/#indexes-for-floating-point-embeddings","title":"Indexes for floating-point embeddings","text":"<p>For 128-dimensional floating-point embeddings (vectors), the storage they take up is 128 * the size of float = 512 bytes. And the distance metrics used for float-point embeddings are Euclidean distance (L2) and Inner product (IP).</p> <p>These types of indexes include for CPU-based ANN searches.:</p> <ul> <li>FLAT</li> <li>IVF_FLAT</li> <li>IVF_PQ</li> <li>IVF_SQ8</li> <li>HNSW</li> <li>HNSW_SQ</li> <li>HNSW_PQ</li> <li>HNSW_PRQ</li> <li>SCANN</li> </ul>"},{"location":"AIML/VectorDb/milvus/#indexes-for-binary-embeddings","title":"Indexes for binary embeddings","text":"<p>For 128-dimensional binary embeddings, the storage they take up is 128 / 8 = 16 bytes. And the distance metrics used for binary embeddings are JACCARD and HAMMING.</p> <p>This type of indexes include BIN_FLAT and BIN_IVF_FLAT.</p>"},{"location":"AIML/VectorDb/milvus/#indexes-for-sparse-embeddings","title":"Indexes for sparse embeddings","text":"<p>Indexes for sparse embeddings support the IP and BM25 (for full-text search) metrics only.</p> <p>Index type supported for sparse embeddings: SPARSE_INVERTED_INDEX.</p>"},{"location":"AIML/VectorDb/milvus/#gpu-index","title":"GPU Index","text":"<p>Milvus supports various GPU index types to accelerate search performance and efficiency, especially in high-throughput, and high-recall scenarios. </p> <ul> <li>GPU_CAGRA</li> <li>GPU_IVF_FLAT</li> <li>GPU_IVF_PQ</li> <li>GPU_BRUTE_FORCE</li> </ul>"},{"location":"AIML/VectorDb/milvus/#scalar-index","title":"Scalar Index","text":"<p>Milvus supports filtered searches combining both scalar and vector fields. To enhance the efficiency of searches involving scalar fields, Milvus introduced scalar field indexing starting from version 2.1.0. </p>"},{"location":"AIML/VectorDb/milvus/#scalar-field-indexing-algorithms","title":"Scalar field indexing algorithms","text":"<p>Milvus aims to achieve low memory usage, high filtering efficiency, and short loading time with its scalar field indexing algorithms.  These algorithms are categorized into two main types: auto indexing and inverted indexing.</p>"},{"location":"AIML/VectorDb/milvus/#auto-indexing","title":"Auto indexing","text":"<p>Milvus provides the AUTOINDEX option to free you from having to manually choose an index type. When calling the create_index method, if the index_type is not specified, Milvus automatically selects the most suitable index type based on the data type.</p> <p>The following table lists the data types that Milvus supports and their corresponding auto indexing algorithms.</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#inverted-indexing","title":"Inverted indexing","text":"<p>Inverted indexing offers a flexible way to create an index for a scalar field by manually specifying index parameters. This method works well for various scenarios, including point queries, pattern match queries, full-text searches, JSON searches, Boolean searches, and even prefix match queries.</p> <p>An inverted index has two main components: a term dictionary and an inverted list. The term dictionary includes all tokenized words sorted alphabetically, while the inverted list contains the list of documents where each word appears. This setup makes point queries and range queries much faster and more efficient than brute-force searches.</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#metric-types","title":"Metric Types","text":"<p>Similarity metrics are used to measure similarities among vectors. Choosing an appropriate distance metric helps improve classification and clustering performance significantly.</p> <p>Currently, Milvus supports these types of similarity Metrics: Euclidean distance (L2), Inner Product (IP), Cosine Similarity (COSINE), JACCARD, HAMMING, and BM25 (specifically designed for full text search on sparse vectors).</p> <p></p> <p></p>"},{"location":"AIML/VectorDb/milvus/#euclidean-distance-l2","title":"Euclidean distance (L2)","text":"<p>Essentially, Euclidean distance measures the length of a segment that connects 2 points.</p> <p>NOTE: Milvus only calculates the value before applying the square root when Euclidean distance is chosen as the distance metric.</p>"},{"location":"AIML/VectorDb/milvus/#inner-product-ip","title":"Inner product (IP)","text":"<p>IP is more useful if you need to compare non-normalized data or when you care about magnitude and angle.</p> <p>NOTE: If you use IP to calculate similarities between embeddings, you must normalize your embeddings. After normalization, the inner product equals cosine similarity.</p>"},{"location":"AIML/VectorDb/milvus/#cosine-similarity","title":"Cosine similarity","text":"<p>Cosine similarity uses the cosine of the angle between two sets of vectors to measure how similar they are. You can think of the two sets of vectors as line segments starting from the same point, such as [0,0,\u2026], but pointing in different directions.</p> <p>The cosine similarity is always in the interval [-1, 1]. For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. The larger the cosine, the smaller the angle between the two vectors, indicating that these two vectors are more similar to each other.</p>"},{"location":"AIML/VectorDb/milvus/#jaccard-distance","title":"JACCARD distance","text":"<p>JACCARD similarity coefficient measures the similarity between two sample sets and is defined as the cardinality of the intersection of the defined sets divided by the cardinality of the union of them. It can only be applied to finite sample sets.</p>"},{"location":"AIML/VectorDb/milvus/#hamming-distance","title":"HAMMING distance","text":"<p>HAMMING distance measures binary data strings. The distance between two strings of equal length is the number of bit positions at which the bits are different.</p> <p>For example, suppose there are two strings, 1101 1001 and 1001 1101.</p> <p>11011001 \u2295 10011101 = 01000100. Since, this contains two 1s, the HAMMING distance, d (11011001, 10011101) = 2.</p>"},{"location":"AIML/VectorDb/milvus/#bm25-similarity","title":"BM25 similarity","text":"<p>BM25 is a widely used text relevance measurement method, specifically designed for full text search. It combines the following three key factors:</p> <ul> <li> <p>Term Frequency (TF): Measures how frequently a term appears in a document. While higher frequencies often indicate greater importance, BM25 uses the saturation parameter k1 to prevent overly frequent terms from dominating the relevance score.</p> </li> <li> <p>Inverse Document Frequency (IDF): Reflects the importance of a term across the entire corpus. Terms appearing in fewer documents receive a higher IDF value, indicating greater contribution to relevance.</p> </li> <li> <p>Document Length Normalization: Longer documents tend to score higher due to containing more terms. BM25 mitigates this bias by normalizing document lengths, with parameter b controlling the strength of this normalization.</p> </li> </ul>"},{"location":"AIML/VectorDb/milvus/#consistency-level","title":"Consistency Level","text":"<p>As a distributed vector database, Milvus offers multiple levels of consistency to ensure that each node or replica can access the same data during read and write operations. Currently, the supported levels of consistency include Strong, Bounded, Eventually, and Session, with Bounded being the default level of consistency used.</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#milvus-provides-four-types-of-consistency-levels-with-different-guaranteets","title":"Milvus provides four types of consistency levels with different GuaranteeTs.","text":"<ul> <li>Strong: The latest timestamp is used as the GuaranteeTs, and QueryNodes have to wait until the ServiceTime meets the GuaranteeTs before executing Search requests.</li> <li>Eventual: The GuaranteeTs is set to an extremely small value, such as 1, to avoid consistency checks so that QueryNodes can immediately execute Search requests upon all batch data.</li> <li>Bounded Staleness: The GuranteeTs is set to a time point earlier than the latest timestamp to make QueryNodes to perform searches with a tolerance of certain data loss.</li> <li>Session: The latest time point at which the client inserts data is used as the GuaranteeTs so that QueryNodes can perform searches upon all the data inserted by the client.</li> </ul> <p>Milvus uses Bounded Staleness as the default consistency level. If the GuaranteeTs is left unspecified, the latest ServiceTime is used as the GuaranteeTs.</p>"},{"location":"AIML/VectorDb/milvus/#set-consistency-level","title":"Set Consistency Level","text":""},{"location":"AIML/VectorDb/milvus/#in-memory-replica","title":"In-Memory Replica","text":"<p>In-memory replica (replication) mechanism in Milvus that enables multiple segment replications in the working memory to improve performance and availability.</p> <p>With in-memory replicas, Milvus can load the same segment on multiple query nodes. If one query node has failed or is busy with a current search request when another arrives, the system can send new requests to an idle query node that has a replication of the same segment.</p> <p></p> <ol> <li> <p>Performance In-memory replicas allow you to leverage extra CPU and memory resources. It is very useful if you have a relatively small dataset but want to increase read throughput with extra hardware resources. Overall QPS (query per second) and throughput can be significantly improved.</p> </li> <li> <p>Availability In-memory replicas help Milvus recover faster if a query node crashes. When a query node fails, the segment does not have to be reloaded on another query node. Instead, the search request can be resent to a new query node immediately without having to reload the data again. With multiple segment replicas maintained simultaneously, the system is more resilient in the face of a failover.</p> </li> </ol> <p>Key Concepts In-memory replicas are organized as replica groups. Each replica group contains shard replicas. Each shard replica has a streaming replica and a historical replica that correspond to the growing and sealed segments in the shard (i.e. DML channel).</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#terminology","title":"Terminology","text":"<ol> <li>AutoID: AutoID is an attribute of the primary field that determines whether to enable AutoIncrement for the primary field. The value of AutoID is defined based on a timestamp. </li> <li>Auto Index: Milvus automatically decides the most appropriate index type and params for a specific field based on empirical data. This is ideal for situations when you do not need to control the specific index params.</li> <li>Attu: Attu is an all-in-one administration tool for Milvus that significantly reduces the complexity and cost of managing the system.</li> <li>Birdwatcher: Birdwatcher is a debugging tool for Milvus that connects to etcd, allowing you to monitor the status of the Milvus server and make adjustments in real-time. It also supports etcd file backups, aiding developers in troubleshooting.</li> <li>Bulk Writer: Bulk Writer is a data processing tool provided by Milvus SDKs (e.g. PyMilvus, Java SDK) , designed to convert raw datasets into a format compatible with Milvus for efficient importing.</li> <li>Bulk Insert: Bulk Insert is an API that enhances writing performance by allowing multiple files to be imported in a single request, optimizing operations with large datasets.</li> <li>Cardinal: Cardinal, developed by Zilliz Cloud, is a cutter-edge vector search algorithm that delivers unparalleled search quality and performance. With its innovative design and extensive optimizations, Cardinal outperforms Knowhere by several times to an order of magnitude while adaptively handling diverse production scenarios, such as varying K sizes, high filtering, different data distributions, and so on.</li> <li>Channel: Milvus utilizes two types of channels, PChannel and VChannel. Each PChannel corresponds to a topic for log storage, while each VChannel corresponds to a shard in a collection.</li> <li>Collection: In Milvus, a collection is equivalent to a table in a relational database management system (RDBMS). Collections are major logical objects used to store and manage entities. For more information, refer to Manage Collections.</li> <li>Dependency: A dependency is a program that another program relies on to work. Milvus\u2019 dependencies include etcd (stores meta data), MinIO or S3 (object storage), and Pulsar (manages snapshot logs). For more information, refer to Manage Dependencies.</li> <li>Dynamic schema: Dynamic schema allows you to insert entities with new fields into a collection without modifying the existing schema. This means that you can insert data without knowing the full schema of a collection and can include fields that are not yet defined. You can enable this schema-free capability by enableing the dynamic field when creating a collection. </li> <li>Embeddings: Milvus offers built-in embedding functions that work with popular embedding providers. Before creating a collection in Milvus, you can use these functions to generate embeddings for your datasets, streamlining the process of preparing data and vector searches.</li> <li>Entity: An entity consists of a group of fields that represent real-world objects. Each entity in Milvus is represented by a unique primary key.</li> <li>Field: A field in a Milvus collection is equivalent to a column of table in a RDBMS. Fields can be either scalar fields for structured data (e.g., numbers, strings), or vector fields for embedding vectors.</li> <li>Filter: Milvus supports scalar filtering by searching with predicates, allowing you to define filter conditions within queries and searches to refine results.</li> <li>Filtered search: Filtered search applies scalar filters to vector searches, allowing you to refine the search results based on specific criteria. </li> <li>Hybrid search: Hybrid Search is an API for hybrid search since Milvus 2.4.0. You can search multiple vector fields and fusion them. </li> <li>Index: A vector index is a reorganized data structure derived from raw data that can greatly accelerate the process of vector similarity search. Milvus supports a wide range of index types for both vector fields and scalar fields.</li> <li>Kafka-Milvus Connector: Kafka-Milvus Connector refers to a Kafka sink connector for Milvus. It allows you to stream vector data from Kafka to Milvus.</li> <li>Knowhere: Knowhere is the core vector execution engine of Milvus which incorporates several vector similarity search libraries including Faiss, Hnswlib, and Annoy. Knowhere is also designed to support heterogeneous computing. It controls on which hardware (CPU or GPU) to execute index building and search requests. This is how Knowhere gets its name - knowing where to execute the operations.</li> <li>Partitionr: A partition is a division of a collection. Milvus supports dividing collection data into multiple parts on physical storage. This process is called partitioning, and each partition can contain multiple segments. </li> <li>Metric type: Similarity metric types are used to measure similarities between vectors. Currently, Milvus supports Euclidean distance (L2), Inner product (IP), Cosine similarity (COSINE), and binary metric types. You can choose the most appropriate metric type based on your scenario.</li> </ol>"},{"location":"AIML/VectorDb/milvus/#embedding","title":"Embedding","text":"<p>Embedding is a machine learning concept for mapping data into a high-dimensional space, where data of similar semantic are placed close together. Typically being a Deep Neural Network from BERT or other Transformer families, the embedding model can effectively represent the semantics of text, images, and other data types with a series of numbers known as vectors.</p> <p>There are two main categories of embeddings, each producing a different type of vector:</p> <ul> <li> <p>Dense embedding: Most embedding models represent information as a floating point vector of hundreds to thousands of dimensions. The output is called \u201cdense\u201d vectors as most dimensions have non-zero values. For instance, the popular open-source embedding model BAAI/bge-base-en-v1.5 outputs vectors of 768 floating point numbers (768-dimension float vector).</p> </li> <li> <p>Sparse embedding: In contrast, the output vectors of sparse embeddings has most dimensions being zero, namely \u201csparse\u201d vectors. These vectors often have much higher dimensions (tens of thousands or more) which is determined by the size of the token vocabulary. Sparse vectors can be generated by Deep Neural Networks or statistical analysis of text corpora. Due to their interpretability and observed better out-of-domain generalization capabilities, sparse embeddings are increasingly adopted by developers as a complement to dense embeddings.</p> </li> </ul> <p>Milvus is a vector database designed for vector data management, storage, and retrieval. By integrating mainstream embedding and reranking models, you can easily transform original text into searchable vectors or rerank the results using powerful models to achieve more accurate results for RAG. This integration simplifies text transformation and eliminates the need for additional embedding or reranking components, thereby streamlining RAG development and validation.</p> <p>To create embeddings in action, refer to Using PyMilvus\u2019s Model To Generate Text Embeddings.</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#openaiembeddingfunction","title":"OpenAIEmbeddingFunction","text":"<p>OpenAIEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using OpenAI models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.OpenAIEmbeddingFunction\n</code></pre> <p>Constructor Constructs an OpenAIEmbeddingFunction for common use cases.</p> <pre><code>OpenAIEmbeddingFunction(\n    model_name: str = \"text-embedding-ada-002\", \n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    dimensions: Optional[int] = None,\n    **kwargs\n)\n</code></pre> <p>Example:</p> <pre><code>from pymilvus import model\n\nopenai_ef = model.dense.OpenAIEmbeddingFunction(\n    model_name='text-embedding-3-large', # Specify the model name\n    dimensions=512 # Set the embedding dimensionality according to MRL feature.\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#sentencetransformerembeddingfunction","title":"SentenceTransformerEmbeddingFunction","text":"<p>SentenceTransformerEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Sentence Transformer models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.SentenceTransformerEmbeddingFunction\n</code></pre> <p>Constructor Constructs a SentenceTransformerEmbeddingFunction for common use cases.</p> <pre><code>SentenceTransformerEmbeddingFunction(\n    model_name: str = \"all-MiniLM-L6-v2\",\n    batch_size: int = 32,\n    query_instruction: str = \"\",\n    doc_instruction: str = \"\",\n    device: str = \"cpu\",\n    normalize_embeddings: bool = True,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus import model\n\nsentence_transformer_ef = model.dense.SentenceTransformerEmbeddingFunction(\n    model_name='all-MiniLM-L6-v2', # Specify the model name\n    device='cpu' # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#spladeembeddingfunction","title":"SpladeEmbeddingFunction","text":"<p>SpladeEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using SPLADE models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.sparse.SpladeEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a SpladeEmbeddingFunction for common use cases.</p> <pre><code>SpladeEmbeddingFunction(\n    model_name: str = \"naver/splade-cocondenser-ensembledistil\",\n    batch_size: int = 32,\n    query_instruction: str = \"\",\n    doc_instruction: str = \"\",\n    device: Optional[str] = \"cpu\",\n    k_tokens_query: Optional[int] = None,\n    k_tokens_document: Optional[int] = None,\n    **kwargs,\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus import model\n\nsplade_ef = model.sparse.SpladeEmbeddingFunction(\n    model_name=\"naver/splade-cocondenser-selfdistil\", \n    device=\"cpu\"\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#bgem3embeddingfunction","title":"BGEM3EmbeddingFunction","text":"<p>BGEM3EmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using the BGE M3 model to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.hybrid.BGEM3EmbeddingFunction\n</code></pre> <p>Constructor: Constructs a BGEM3EmbeddingFunction for common use cases.</p> <pre><code>BGEM3EmbeddingFunction(\n    model_name: str = \"BAAI/bge-m3\",\n    batch_size: int = 16,\n    device: str = \"\",\n    normalize_embeddings: bool = True,\n    use_fp16: bool = True,\n    return_dense: bool = True,\n    return_sparse: bool = True,\n    return_colbert_vecs: bool = False,\n    **kwargs,\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus import model\n\nbge_m3_ef = model.hybrid.BGEM3EmbeddingFunction(\n    model_name='BAAI/bge-m3', # Specify t`he model name\n    device='cpu', # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n    use_fp16=False # Whether to use fp16. `False` for `device='cpu'`.\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#voyageembeddingfunction","title":"VoyageEmbeddingFunction","text":"<p>VoyageEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Voyage models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.VoyageEmbeddingFunction\n</code></pre> <p>Constructor: Constructs an VoyageEmbeddingFunction for common use cases.</p> <pre><code>VoyageEmbeddingFunction(\n    model_name: str = \"voyage-2\",\n    api_key: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import VoyageEmbeddingFunction\n\nvoyage_ef = VoyageEmbeddingFunction(\n    model_name=\"voyage-lite-02-instruct\", # Defaults to `voyage-2`\n    api_key='YOUR_API_KEY' # Replace with your own Voyage API key\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#jinaembeddingfunction","title":"JinaEmbeddingFunction","text":"<p>JinaEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Jina AI embedding models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.JinaEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a JinaEmbeddingFunction for common use cases.</p> <pre><code>JinaEmbeddingFunction(\n    model_name: str = \"jina-embeddings-v2-base-en\",\n    api_key: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import JinaEmbeddingFunction\n\njina_ef = JinaEmbeddingFunction(\n    model_name=\"jina-embeddings-v2-base-en\", # Defaults to `jina-embeddings-v2-base-en`\n    api_key=\"YOUR_JINAAI_API_KEY\" # Provide your Jina AI API key\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#cohereembeddingfunction","title":"CohereEmbeddingFunction","text":"<p>CohereEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Cohere embedding models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.CohereEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a CohereEmbeddingFunction for common use cases.</p> <pre><code>CohereEmbeddingFunction(\n    model_name: str = \"embed-english-light-v3.0\",\n    api_key: Optional[str] = None,\n    input_type: str = \"search_document\",\n    embedding_types: Optional[List[str]] = None,\n    truncate: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import CohereEmbeddingFunction\n\ncohere_ef = CohereEmbeddingFunction(\n    model_name=\"embed-english-light-v3.0\",\n    api_key=\"YOUR_COHERE_API_KEY\",\n    input_type=\"search_document\",\n    embedding_types=[\"float\"]\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#instructorembeddingfunction","title":"InstructorEmbeddingFunction","text":"<p>InstructorEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using the Instructor embedding model to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.InstructorEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a MistralAIEmbeddingFunction for common use cases.</p> <pre><code>InstructorEmbeddingFunction(\n    model_name: str = \"hkunlp/instructor-xl\",\n    batch_size: int = 32,\n    query_instruction: str = \"Represent the question for retrieval:\",\n    doc_instruction: str = \"Represent the document for retrieval:\",\n    device: str = \"cpu\",\n    normalize_embeddings: bool = True,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import InstructorEmbeddingFunction\n\nef = InstructorEmbeddingFunction(\n    model_name=\"hkunlp/instructor-xl\", # Defaults to `hkunlp/instructor-xl`\n    query_instruction=\"Represent the question for retrieval:\",\n    doc_instruction=\"Represent the document for retrieval:\"\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#mistralaiembeddingfunction","title":"MistralAIEmbeddingFunction","text":"<p>MistralAIEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Mistral AI embedding models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.MistralAIEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a MistralAIEmbeddingFunction for common use cases.</p> <pre><code>MistralAIEmbeddingFunction(\n    api_key: str,\n    model_name: str = \"mistral-embed\",\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import MistralAIEmbeddingFunction\n\nef = MistralAIEmbeddingFunction(\n    model_name=\"mistral-embed\", # Defaults to `mistral-embed`\n    api_key=\"MISTRAL_API_KEY\" # Provide your Mistral AI API key\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#nomicembeddingfunction","title":"NomicEmbeddingFunction","text":"<p>NomicEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using Nomic embedding models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.dense.NomicEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a NomicEmbeddingFunction for common use cases.</p> <pre><code>NomicEmbeddingFunction(\n    model_name: str = \"nomic-embed-text-v1.5\",\n    task_type: str = \"search_document\",\n    dimensions: int = 768,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.dense import NomicEmbeddingFunction\n\nef = NomicEmbeddingFunction(\n    model_name=\"nomic-embed-text-v1.5\", # Defaults to `mistral-embed`\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#mgteembeddingfunction","title":"MGTEEmbeddingFunction","text":"<p>MGTEEmbeddingFunction is a class in pymilvus that handles encoding text into embeddings using MGTE embedding models to support embedding retrieval in Milvus.</p> <pre><code>pymilvus.model.hybrid.MGTEEmbeddingFunction\n</code></pre> <p>Constructor: Constructs a MGTEEmbeddingFunction for common use cases.</p> <pre><code>MGTEEmbeddingFunction(\n    model_name: str = \"Alibaba-NLP/gte-multilingual-base\",\n    batch_size: int = 16,\n    device: str = \"\",\n    normalize_embeddings: bool = True,\n    dimensions: Optional[int] = None,\n    use_fp16: bool = False,\n    return_dense: bool = True,\n    return_sparse: bool = True,\n    **kwargs\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.hybrid import MGTEEmbeddingFunction\n\nef = MGTEEmbeddingFunction(\n    model_name=\"Alibaba-NLP/gte-multilingual-base\",\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#example-1-use-default-embedding-function-to-generate-dense-vectors","title":"Example 1: Use default embedding function to generate dense vectors","text":"<p>To use embedding functions with Milvus, first install the PyMilvus client library with the model subpackage that wraps all the utilities for embedding generation.</p> <pre><code>pip install \"pymilvus[model]\"\n</code></pre> <p>The model subpackage supports various embedding models, from OpenAI, Sentence Transformers, BGE M3, to SPLADE pretrained models. For simpilicity, this example uses the DefaultEmbeddingFunction which is all-MiniLM-L6-v2 sentence transformer model, the model is about 70MB and it will be downloaded during first use:</p> <pre><code>from pymilvus import model\n\nef = model.DefaultEmbeddingFunction()\n\ndocs = [\n    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n    \"Alan Turing was the first person to conduct substantial research in AI.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n]\n\nembeddings = ef.encode_documents(docs)\n\nprint(\"Embeddings:\", embeddings)\nprint(\"Dim:\", ef.dim, embeddings[0].shape)\n</code></pre> <p>The expected output is similar to the following:</p> <pre><code>Embeddings: [array([-3.09392996e-02, -1.80662833e-02,  1.34775648e-02,  2.77156215e-02,\n       -4.86349640e-03, -3.12581174e-02, -3.55921760e-02,  5.76934684e-03,\n        2.80773244e-03,  1.35783911e-01,  3.59678417e-02,  6.17732145e-02,\n...\n       -4.61330153e-02, -4.85207550e-02,  3.13997865e-02,  7.82178566e-02,\n       -4.75336798e-02,  5.21207601e-02,  9.04406682e-02, -5.36676683e-02],\n      dtype=float32)]\nDim: 384 (384,)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#example-2-generate-dense-and-sparse-vectors-in-one-call-with-bge-m3-model","title":"Example 2: Generate dense and sparse vectors in one call with BGE M3 model","text":"<p>In this example, we use BGE M3 hybrid model to embed text into both dense and sparse vectors and use them to retrieve relevant documents. The overall steps are as follows:</p> <ol> <li>Embed the text as dense and sparse vectors using BGE-M3 model;</li> <li>Set up a Milvus collection to store the dense and sparse vectors;</li> <li>Insert the data to Milvus;</li> <li>Search and inspect the result.</li> </ol> <p>First, we need to install the necessary dependencies.</p> <pre><code>from pymilvus.model.hybrid import BGEM3EmbeddingFunction\nfrom pymilvus import (\n    utility,\n    FieldSchema, CollectionSchema, DataType,\n    Collection, AnnSearchRequest, RRFRanker, connections,\n)\n</code></pre> <p>Use BGE M3 to encode docs and queries for embedding retrieval.</p> <pre><code>docs = [\n    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n    \"Alan Turing was the first person to conduct substantial research in AI.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n]\nquery = \"Who started AI research?\"\n\n\nbge_m3_ef = BGEM3EmbeddingFunction(use_fp16=False, device=\"cpu\")\n\ndocs_embeddings = bge_m3_ef(docs)\nquery_embeddings = bge_m3_ef([query])\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#rerankers","title":"Rerankers","text":"<p>In the realm of information retrieval and generative AI, a reranker is an essential tool that optimizes the order of results from initial searches. Rerankers differ from traditional embedding models by taking a query and document as input and directly returning a similarity score instead of embeddings. This score indicates the relevance between the input query and document.</p> <p>Rerankers are often employed after the first stage retrieval, typically done via vector Approximate Nearest Neighbor (ANN) techniques. While ANN searches are efficient at fetching a broad set of potentially relevant results, they might not always prioritize results in terms of actual semantic closeness to the query. Here, rerankers is used to optimize the results order using deeper contextual analyses, often leveraging advanced machine learning models like BERT or other Transformer-based models. By doing this, rerankers can dramatically enhance the accuracy and relevance of the final results presented to the user.</p> <p></p>"},{"location":"AIML/VectorDb/milvus/#bgererankfunction","title":"BGERerankFunction","text":"<p>BGERerankFunction is a class in milvus_model that takes a query and document as input and directly returns a similarity score instead of embeddings. This functionality uses the underlying BGE reranking model.</p> <pre><code>pymilvus.model.reranker.BGERerankFunction\n</code></pre> <p>Constructor: Constructs a BGERerankFunction for common use cases.</p> <pre><code>BGERerankFunction(\n    model_name: str = \"BAAI/bge-reranker-v2-m3\",\n    use_fp16: bool = True,\n    batch_size: int = 32,\n    normalize: bool = True,\n    device: Optional[str] = None,\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.reranker import BGERerankFunction\n\nbge_rf = BGERerankFunction(\n    model_name=\"BAAI/bge-reranker-v2-m3\",  # Specify the model name. Defaults to `BAAI/bge-reranker-v2-m3`.\n    device=\"cpu\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#crossencoderrerankfunction","title":"CrossEncoderRerankFunction","text":"<p>CrossEncoderRerankFunction is a class in milvus_model that takes a query and document as input and directly returns a similarity score instead of embeddings. This functionality uses the underlying Cross-Encoder reranking model.</p> <pre><code>pymilvus.model.reranker.CrossEncoderRerankFunction\n</code></pre> <p>Constructor: Constructs a CrossEncoderRerankFunction for common use cases.</p> <pre><code>CrossEncoderRerankFunction(\n    model_name: str = \"\",\n    device: str = \"\",\n    batch_size: int = 32,\n    activation_fct: Any = None,\n    **kwargs,\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.reranker import CrossEncoderRerankFunction\n\nce_rf = CrossEncoderRerankFunction(\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",  # Specify the model name. Defaults to an emtpy string.\n    device=\"cpu\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#voyagererankfunction","title":"VoyageRerankFunction","text":"<p>VoyageRerankFunction is a class in milvus_model that takes a query and document as input and directly returns a similarity score instead of embeddings. This functionality uses the underlying Voyage reranking model.</p> <pre><code>pymilvus.model.reranker.VoyageRerankFunction\n</code></pre> <p>Constructor: Constructs a VoyageRerankFunction for common use cases.</p> <pre><code>VoyageRerankFunction(\n    model_name: str = \"rerank-lite-1\",\n    api_key: Optional[str] = None\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.reranker import VoyageRerankFunction\n\nvoyage_rf = VoyageRerankFunction(\n    model_name=\"rerank-lite-1\",  # Specify the model name. Defaults to `rerank-lite-1`.\n    api_key=VOYAGE_API_KEY # Replace with your Voyage API key\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#coherererankfunction","title":"CohereRerankFunction","text":"<p>CohereRerankFunction is a class in milvus_model that takes a query and document as input and directly returns a similarity score instead of embeddings. This functionality uses the underlying Cohere reranking model.</p> <pre><code>pymilvus.model.reranker.CohereRerankFunction\n</code></pre> <p>Constructor: Constructs a CohereRerankFunction for common use cases.</p> <pre><code>CohereRerankFunction(\n    model_name: str = \"rerank-english-v2.0\",\n    api_key: Optional[str] = None\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.reranker import CohereRerankFunction\n\ncohere_rf = CohereRerankFunction(\n    model_name=\"rerank-english-v3.0\",  # Specify the model name. Defaults to `rerank-english-v2.0`.\n    api_key=COHERE_API_KEY # Replace with your Cohere API key\n)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#jinarerankfunction","title":"JinaRerankFunction","text":"<p>JinaRerankFunction is a class in milvus_model that takes a query and document as input and directly returns a similarity score instead of embeddings. This functionality uses the underlying Jina AI reranking model.</p> <pre><code>pymilvus.model.reranker.JinaRerankFunction\n</code></pre> <p>Constructor: Constructs a JinaRerankFunction for common use cases.</p> <pre><code>JinaRerankFunction(\n    model_name: str = \"jina-reranker-v2-base-multilingual\",\n    api_key: Optional[str] = None\n)\n</code></pre> <p>Examples:</p> <pre><code>from pymilvus.model.reranker import JinaRerankFunction\n\njina_rf = JinaRerankFunction(\n    model_name=\"jina-reranker-v2-base-multilingual\", # Defaults to `jina-reranker-v2-base-multilingual`\n    api_key=\"YOUR_JINAAI_API_KEY\"\n)\n</code></pre> <p>NOTE: - Before using open-source rerankers, make sure to download and install all required dependencies and models. - For API-based rerankers, get an API key from the provider and set it in the appropriate environment variables or arguments.</p>"},{"location":"AIML/VectorDb/milvus/#example-1-use-bge-rerank-function-to-rerank-documents-according-to-a-query","title":"Example 1: Use BGE rerank function to rerank documents according to a query","text":"<p>In this example, we demonstrate how to rerank search results using the BGE reranker based on a specific query.</p> <p>To use a reranker with PyMilvus model library, start by installing the PyMilvus model library along with the model subpackage that contains all necessary reranking utilities:</p> <pre><code>pip install pymilvus[model]\n</code></pre> <p>To use the BGE reranker, first import the BGERerankFunction class:</p> <pre><code>from pymilvus.model.reranker import BGERerankFunction\n</code></pre> <p>Then, create a BGERerankFunction instance for reranking:</p> <pre><code>bge_rf = BGERerankFunction(\n    model_name=\"BAAI/bge-reranker-v2-m3\",  # Specify the model name. Defaults to `BAAI/bge-reranker-v2-m3`.\n    device=\"cpu\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n)\n</code></pre> <p>To rerank documents based on a query, use the following code:</p> <pre><code>query = \"What event in 1956 marked the official birth of artificial intelligence as a discipline?\"\n\ndocuments = [\n    \"In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\",\n    \"The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\",\n    \"In 1951, British mathematician and computer scientist Alan Turing also developed the first program designed to play chess, demonstrating an early example of AI in game strategy.\",\n    \"The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.\"\n]\n\nbge_rf(query, documents)\n</code></pre> <p>The expected output is similar to the following:</p> <pre><code>[RerankResult(text=\"The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\", score=0.9911615761470803, index=1),\n RerankResult(text=\"In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\", score=0.0326971950177779, index=0),\n RerankResult(text='The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.', score=0.006514905766152258, index=3),\n RerankResult(text='In 1951, British mathematician and computer scientist Alan Turing also developed the first program designed to play chess, demonstrating an early example of AI in game strategy.', score=0.0042116724917325935, index=2)]\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#example-2-use-a-reranker-to-enhance-relevance-of-search-results","title":"Example 2: Use a reranker to enhance relevance of search results","text":"<p>In this guide, we\u2019ll explore how to utilize the search() method in PyMilvus for conducting similarity searches, and how to enhance the relevance of the search results using a reranker. Our demonstration will use the following dataset:</p> <pre><code>entities = [\n    {'doc_id': 0, 'doc_vector': [-0.0372721,0.0101959,...,-0.114994], 'doc_text': \"In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\"}, \n    {'doc_id': 1, 'doc_vector': [-0.00308882,-0.0219905,...,-0.00795811], 'doc_text': \"The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\"}, \n    {'doc_id': 2, 'doc_vector': [0.00945078,0.00397605,...,-0.0286199], 'doc_text': 'In 1951, British mathematician and computer scientist Alan Turing also developed the first program designed to play chess, demonstrating an early example of AI in game strategy.'}, \n    {'doc_id': 3, 'doc_vector': [-0.0391119,-0.00880096,...,-0.0109257], 'doc_text': 'The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.'}\n]\n</code></pre> <p>Dataset components: - doc_id: Unique identifier for each document. - doc_vector: Vector embeddings representing the document. - doc_text: Text content of the document.</p> <p>Preparations: Before initiating a similarity search, you need to establish a connection with Milvus, create a collection, and prepare and insert data into that collection. The following code snippet illustrates these preliminary steps.</p> <pre><code>from pymilvus import MilvusClient, DataType\n\nclient = MilvusClient(\n    uri=\"http://10.102.6.214:19530\" # replace with your own Milvus server address\n)\n\nclient.drop_collection('test_collection')\n\n\nschema = client.create_schema(auto_id=False, enabel_dynamic_field=True)\n\nschema.add_field(field_name=\"doc_id\", datatype=DataType.INT64, is_primary=True, description=\"document id\")\nschema.add_field(field_name=\"doc_vector\", datatype=DataType.FLOAT_VECTOR, dim=384, description=\"document vector\")\nschema.add_field(field_name=\"doc_text\", datatype=DataType.VARCHAR, max_length=65535, description=\"document text\")\n\n\nindex_params = client.prepare_index_params()\n\nindex_params.add_index(field_name=\"doc_vector\", index_type=\"IVF_FLAT\", metric_type=\"IP\", params={\"nlist\": 128})\n\n\nclient.create_collection(collection_name=\"test_collection\", schema=schema, index_params=index_params)\n\n\nclient.insert(collection_name=\"test_collection\", data=entities)\n</code></pre> <p>Conduct a similarity search</p> <p>After data insertion, perform similarity searches using the search method.</p> <pre><code>res = client.search(\n    collection_name=\"test_collection\",\n    data=[[-0.045217834, 0.035171617, ..., -0.025117004]], # replace with your query vector\n    limit=3,\n    output_fields=[\"doc_id\", \"doc_text\"]\n)\n\nfor i in res[0]:\n    print(f'distance: {i[\"distance\"]}')\n    print(f'doc_text: {i[\"entity\"][\"doc_text\"]}')\n</code></pre> <p>The expected output is similar to the following:</p> <pre><code>distance: 0.7235960960388184\ndoc_text: The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\ndistance: 0.6269873976707458\ndoc_text: In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\ndistance: 0.5340118408203125\ndoc_text: The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.\n</code></pre> <p>Use a reranker to enhance search results</p> <p>Then, improve the relevance of your search results with a reranking step. In this example, we use CrossEncoderRerankFunction built in PyMilvus to rerank the results for improved accuracy.</p> <pre><code>from pymilvus.model.reranker import CrossEncoderRerankFunction\n\nce_rf = CrossEncoderRerankFunction(\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",  # Specify the model name.\n    device=\"cpu\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n)\n\nreranked_results = ce_rf(\n    query='What event in 1956 marked the official birth of artificial intelligence as a discipline?',\n    documents=[\n        \"In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\",\n        \"The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\",\n        \"In 1951, British mathematician and computer scientist Alan Turing also developed the first program designed to play chess, demonstrating an early example of AI in game strategy.\",\n        \"The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.\"\n    ],\n    top_k=3\n)\n\nfor result in reranked_results:\n    print(f'score: {result.score}')\n    print(f'doc_text: {result.text}')\n</code></pre> <p>The expected output is similar to the following:</p> <pre><code>score: 6.250532627105713\ndoc_text: The Dartmouth Conference in 1956 is considered the birthplace of artificial intelligence as a field; here, John McCarthy and others coined the term 'artificial intelligence' and laid out its basic goals.\nscore: -2.9546022415161133\ndoc_text: In 1950, Alan Turing published his seminal paper, 'Computing Machinery and Intelligence,' proposing the Turing Test as a criterion of intelligence, a foundational concept in the philosophy and development of artificial intelligence.\nscore: -4.771512031555176\ndoc_text: The invention of the Logic Theorist by Allen Newell, Herbert A. Simon, and Cliff Shaw in 1955 marked the creation of the first true AI program, which was capable of solving logic problems, akin to proving mathematical theorems.\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#types-of-searches-supported-by-milvus","title":"Types of Searches Supported by Milvus","text":"<p>Milvus supports various types of search functions to meet the demands of different use cases:</p> <ol> <li>ANN Search: Finds the top K vectors closest to your query vector.</li> <li>Filtering Search: Performs ANN search under specified filtering conditions.</li> <li>Range Search: Finds vectors within a specified radius from your query vector.</li> <li>Hybrid Search: Conducts ANN search based on multiple vector fields.</li> <li>Full Text Search: Full text search based on BM25.</li> <li>Reranking: Adjusts the order of search results based on additional criteria or a secondary algorithm, refining the initial ANN search results.</li> <li>Fetch: Retrieves data by their primary keys.</li> <li>Query: Retrieves data using specific expressions.</li> <li>Filtering: </li> <li>Full Text Search:</li> <li>Text Match:</li> <li>Search Iterator:</li> <li>Use Partition Key:</li> <li>Reranking:</li> </ol>"},{"location":"AIML/VectorDb/milvus/#comprehensive-feature-set","title":"Comprehensive Feature Set","text":""},{"location":"AIML/VectorDb/milvus/#api-and-sdk","title":"API and SDK","text":"<ul> <li>RESTful API</li> <li>PyMilvus (Python SDK) </li> <li>Go SDK</li> <li>Java SDK</li> <li>Node.js (JavaScript)</li> <li>C#</li> </ul>"},{"location":"AIML/VectorDb/milvus/#1-build-rag-with-milvus","title":"1. Build RAG with Milvus","text":"<ul> <li>Use Case: RAG</li> <li>Related Milvus Features: vector search</li> </ul> <p>Build RAG with Milvus</p> <p>Deploy</p>"},{"location":"AIML/VectorDb/milvus/#2-advanced-rag","title":"2. Advanced RAG","text":"<ul> <li>Use Case: RAG</li> <li>Related Milvus Features: vector search</li> </ul> <p>Advanced RAG</p>"},{"location":"AIML/VectorDb/milvus/#3-full-text-search-with-milvus","title":"3. Full Text Search with Milvus","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: Full-Text Search</li> </ul> <p>Full Text Search with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#4-hybrid-search-with-milvus","title":"4. Hybrid Search with Milvus","text":"<ul> <li>Use Case: Hybrid Search</li> <li>Related Milvus Features: hybrid search, multi vector, dense embedding, sparse embedding</li> </ul> <p>Hybrid Search with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#5-image-search-with-milvus","title":"5. Image Search with Milvus","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: vector search, dynamic field</li> </ul> <p>Image Search with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#6-multimodal-rag-with-milvus","title":"6. Multimodal RAG with Milvus","text":"<ul> <li>Use Case: RAG</li> <li>Related Milvus Features: vector search, dynamic field</li> </ul> <p>Graph RAG with Milvus</p> <p>Deploy</p>"},{"location":"AIML/VectorDb/milvus/#7multimodal-search-using-multi-vectors","title":"7.Multimodal Search using Multi Vectors","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: multi vector, hybrid search</li> </ul> <p>Graph RAG with Milvus</p> <p>Deploy</p>"},{"location":"AIML/VectorDb/milvus/#8graph-rag-with-milvus","title":"8.Graph RAG with Milvus","text":"<ul> <li>Use Case: RAG</li> <li>Related Milvus Features: graph search</li> </ul> <p>Graph RAG with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#9contextual-retrieval-with-milvus","title":"9.Contextual Retrieval with Milvus","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>Contextual Retrieval with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#10hdbscan-clustering-with-milvus","title":"10.HDBSCAN Clustering with Milvus","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>HDBSCAN Clustering with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#11use-colpali-for-multi-modal-retrieval-with-milvus","title":"11.Use ColPali for Multi-Modal Retrieval with Milvus","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>Use ColPali for Multi-Modal Retrieval with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#12vector-visualization","title":"12.Vector Visualization","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>Vector Visualization</p>"},{"location":"AIML/VectorDb/milvus/#13movie-recommendation-with-milvus","title":"13.Movie Recommendation with Milvus","text":"<ul> <li>Use Case: Recommendation System</li> <li>Related Milvus Features: vector search</li> </ul> <p>Movie Recommendation with Milvus</p>"},{"location":"AIML/VectorDb/milvus/#14funnel-search-with-matryoshka-embeddings","title":"14.Funnel Search with Matryoshka Embeddings","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>Funnel Search with Matryoshka Embeddings</p>"},{"location":"AIML/VectorDb/milvus/#15question-answering-system","title":"15.Question Answering System","text":"<ul> <li>Use Case: Question Answering</li> <li>Related Milvus Features: vector search</li> </ul> <p>Question Answering System</p>"},{"location":"AIML/VectorDb/milvus/#16recommender-system","title":"16.Recommender System","text":"<ul> <li>Use Case: Recommendation System</li> <li>Related Milvus Features: vector search</li> </ul> <p>Recommender System</p>"},{"location":"AIML/VectorDb/milvus/#17video-similarity-search","title":"17.Video Similarity Search","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: vector search</li> </ul> <p>Video Similarity Search</p>"},{"location":"AIML/VectorDb/milvus/#18audio-similarity-search","title":"18.Audio Similarity Search","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: vector search</li> </ul> <p>Audio Similarity Search</p>"},{"location":"AIML/VectorDb/milvus/#19dna-classification","title":"19.DNA Classification","text":"<ul> <li>Use Case: Classification</li> <li>Related Milvus Features: vector search</li> </ul> <p>DNA Classification</p>"},{"location":"AIML/VectorDb/milvus/#20text-search-engine","title":"20.Text Search Engine","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: vector search</li> </ul> <p>Text Search Engine</p>"},{"location":"AIML/VectorDb/milvus/#21search-image-by-text","title":"21.Search Image by Text","text":"<ul> <li>Use Case: Semantic Search</li> <li>Related Milvus Features: vector search</li> </ul> <p>Search Image by Text</p>"},{"location":"AIML/VectorDb/milvus/#22image-deduplication","title":"22.Image Deduplication","text":"<ul> <li>Use Case: Deduplication</li> <li>Related Milvus Features: vector search</li> </ul> <p>Image Deduplication</p>"},{"location":"AIML/VectorDb/milvus/#23quickstart-with-attu","title":"23.Quickstart with Attu","text":"<ul> <li>Use Case: Quickstart</li> <li>Related Milvus Features: vector search</li> </ul> <p>Quickstart with Attu</p>"},{"location":"AIML/VectorDb/milvus/#24use-asyncmilvusclient-with-asyncio","title":"24.Use AsyncMilvusClient with asyncio","text":"<ul> <li>Use Case: AsyncIO</li> <li>Related Milvus Features: AsyncIO, vector search</li> </ul> <p>Use AsyncMilvusClient with asyncio</p>"},{"location":"AIML/VectorDb/milvus/#data-import","title":"Data Import","text":""},{"location":"AIML/VectorDb/milvus/#prepare-source-data","title":"Prepare Source Data","text":"<p>Before you start: The target collection requires mapping the source data to its schema.The diagram below shows how acceptable source data is mapped to the schema of a target collection.</p> <p></p> <p>You should carefully examine your data and design the schema of the target collection accordingly.</p> <p>Taking the JSON data in the above diagram as an example, there are two entities in the rows list, each row having six fields. The collection schema selectively includes four: id, vector, scalar_1, and scalar_2.</p> <p>There are two more things to consider when designing the schema:</p> <ul> <li> <p>Whether to enable AutoID:The id field serves as the primary field of the collection. To make the primary field automatically increment, you can enable AutoID in the schema. In this case, you should exclude the id field from each row in the source data.</p> </li> <li> <p>Whether to enable dynamic fields:The target collection can also store fields not included in its pre-defined schema if the schema enables dynamic fields. The $meta field is a reserved JSON field to hold dynamic fields and their values in key-value pairs. In the above diagram, the fields dynamic_field_1 and dynamic_field_2 and the values will be saved as key-value pairs in the $meta field.</p> </li> </ul> <p>The following code shows how to set up the schema for the collection illustrated in the above diagram.</p> <pre><code>from pymilvus import MilvusClient, DataType\n\nschema = MilvusClient.create_schema(\n    auto_id=False,\n    enable_dynamic_field=True\n)\n\nDIM = 512\n\nschema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True),\nschema.add_field(field_name=\"bool\", datatype=DataType.BOOL),\nschema.add_field(field_name=\"int8\", datatype=DataType.INT8),\nschema.add_field(field_name=\"int16\", datatype=DataType.INT16),\nschema.add_field(field_name=\"int32\", datatype=DataType.INT32),\nschema.add_field(field_name=\"int64\", datatype=DataType.INT64),\nschema.add_field(field_name=\"float\", datatype=DataType.FLOAT),\nschema.add_field(field_name=\"double\", datatype=DataType.DOUBLE),\nschema.add_field(field_name=\"varchar\", datatype=DataType.VARCHAR, max_length=512),\nschema.add_field(field_name=\"json\", datatype=DataType.JSON),\nschema.add_field(field_name=\"array_str\", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.VARCHAR, max_length=128)\nschema.add_field(field_name=\"array_int\", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.INT64)\nschema.add_field(field_name=\"float_vector\", datatype=DataType.FLOAT_VECTOR, dim=DIM),\nschema.add_field(field_name=\"binary_vector\", datatype=DataType.BINARY_VECTOR, dim=DIM),\nschema.add_field(field_name=\"float16_vector\", datatype=DataType.FLOAT16_VECTOR, dim=DIM),\nschema.add_field(field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n\nschema.verify()\n\nprint(schema)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#set-up-bulkwriter","title":"Set up BulkWriter","text":"<p>BulkWriter is a tool designed to convert raw datasets into a format suitable for importing via the RESTful Import API. It offers two types of writers:</p> <ul> <li>LocalBulkWriter: Reads the designated dataset and transforms it into an easy-to-use format.</li> <li>RemoteBulkWriter: Performs the same task as the LocalBulkWriter but additionally transfers the converted data files to a specified remote object storage bucket.</li> </ul> <p>RemoteBulkWriter differs from LocalBulkWriter in that RemoteBulkWriter transfers the converted data files to a target object storage bucket.</p>"},{"location":"AIML/VectorDb/milvus/#set-up-localbulkwriter","title":"Set up LocalBulkWriter","text":"<p>A LocalBulkWriter appends rows from the source dataset and commits them to a local file of the specified format.</p> <pre><code>from pymilvus.bulk_writer import LocalBulkWriter, BulkFileType\n\nwriter = LocalBulkWriter(\n    schema=schema,\n    local_path='.',\n    segment_size=512 * 1024 * 1024, # Default value\n    file_type=BulkFileType.PARQUET\n)\n</code></pre> <p>When creating a LocalBulkWriter, you should:</p> <ul> <li>Reference the created schema in schema.</li> <li>Set local_path to the output directory.</li> <li>Set file_type to the output file type.</li> <li>If your dataset contains a large number of records, you are advised to segment your data by setting segment_size to a proper value.</li> </ul>"},{"location":"AIML/VectorDb/milvus/#set-up-remotebulkwriter","title":"Set up RemoteBulkWriter","text":"<p>Instead of committing appended data to a local file, a RemoteBulkWriter commits them to a remote bucket. Therefore, you should set up a ConnectParam object before creating a RemoteBulkWriter.</p> <pre><code>from pymilvus.bulk_writer import RemoteBulkWriter\n\nACCESS_KEY=\"minioadmin\"\nSECRET_KEY=\"minioadmin\"\nBUCKET_NAME=\"a-bucket\"\n\nconn = RemoteBulkWriter.S3ConnectParam(\n    endpoint=\"localhost:9000\", # the default MinIO service started along with Milvus\n    access_key=ACCESS_KEY,\n    secret_key=SECRET_KEY,\n    bucket_name=BUCKET_NAME,\n    secure=False\n)\n\nfrom pymilvus.bulk_writer import BulkFileType\n\nwriter = RemoteBulkWriter(\n    schema=schema,\n    remote_path=\"/\",\n    connect_param=conn,\n    file_type=BulkFileType.PARQUET\n)\n\nprint('bulk writer created.')\n</code></pre> <p>Once the connection parameters are ready, you can reference it in the RemoteBulkWriter as follows:</p> <pre><code>from pymilvus.bulk_writer import BulkFileType\n\nwriter = RemoteBulkWriter(\n    schema=schema,\n    remote_path=\"/\",\n    connect_param=conn,\n    file_type=BulkFileType.PARQUET\n)\n</code></pre> <p>The parameters for creating a RemoteBulkWriter are barely the same as those for a LocalBulkWriter, except connect_param.</p>"},{"location":"AIML/VectorDb/milvus/#start-writing","title":"Start writing","text":"<p>A BulkWriter has two methods: append_row() adds a row from a source dataset, and commit() commits added rows to a local file or a remote bucket.</p> <p>For demonstration purposes, the following code appends randomly generated data.</p> <pre><code>import random, string, json\nimport numpy as np\nimport tensorflow as tf\n\ndef generate_random_str(length=5):\n    letters = string.ascii_uppercase\n    digits = string.digits\n\n    return ''.join(random.choices(letters + digits, k=length))\n\ndef gen_binary_vector(to_numpy_arr):\n    raw_vector = [random.randint(0, 1) for i in range(DIM)]\n    if to_numpy_arr:\n        return np.packbits(raw_vector, axis=-1)\n    return raw_vector\n\ndef gen_float_vector(to_numpy_arr):\n    raw_vector = [random.random() for _ in range(DIM)]\n    if to_numpy_arr:\n        return np.array(raw_vector, dtype=\"float32\")\n    return raw_vector\n\n\ndef gen_fp16_vector(to_numpy_arr):\n    raw_vector = [random.random() for _ in range(DIM)]\n    if to_numpy_arr:\n        return np.array(raw_vector, dtype=np.float16)\n    return raw_vector\n\ndef gen_sparse_vector(pair_dict: bool):\n    raw_vector = {}\n    dim = random.randint(2, 20)\n    if pair_dict:\n        raw_vector[\"indices\"] = [i for i in range(dim)]\n        raw_vector[\"values\"] = [random.random() for _ in range(dim)]\n    else:\n        for i in range(dim):\n            raw_vector[i] = random.random()\n    return raw_vector\n\nfor i in range(10000):\n    writer.append_row({\n        \"id\": np.int64(i),\n        \"bool\": True if i % 3 == 0 else False,\n        \"int8\": np.int8(i%128),\n        \"int16\": np.int16(i%1000),\n        \"int32\": np.int32(i%100000),\n        \"int64\": np.int64(i),\n        \"float\": np.float32(i/3),\n        \"double\": np.float64(i/7),\n        \"varchar\": f\"varchar_{i}\",\n        \"json\": json.dumps({\"dummy\": i, \"ok\": f\"name_{i}\"}),\n        \"array_str\": np.array([f\"str_{k}\" for k in range(5)], np.dtype(\"str\")),\n        \"array_int\": np.array([k for k in range(10)], np.dtype(\"int64\")),\n        \"float_vector\": gen_float_vector(True),\n        \"binary_vector\": gen_binary_vector(True),\n        \"float16_vector\": gen_fp16_vector(True),\n        # \"bfloat16_vector\": gen_bf16_vector(True),\n        \"sparse_vector\": gen_sparse_vector(True),\n        f\"dynamic_{i}\": i,\n    })\n    if (i+1)%1000 == 0:\n        writer.commit()\n        print('committed')\n\nprint(writer.batch_files)\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#verify-the-results","title":"Verify the results","text":"<p>To check the results, you can get the actual output path by printing the batch_files property of the writer.</p> <pre><code>print(writer.batch_files)\n</code></pre> <p>BulkWriter generates a UUID, creates a sub-folder using the UUID in the provided output directory, and places all generated files in the sub-folder.</p>"},{"location":"AIML/VectorDb/milvus/#import-data","title":"Import data","text":""},{"location":"AIML/VectorDb/milvus/#before-you-start","title":"Before you start","text":"<ul> <li>You have already prepared your data and placed it into the Milvus bucket.If not, you should use RemoteBulkWriter to prepare your data first, and ensure that the prepared data has already been transferred to the Milvus bucket on the MinIO instance started along with your Milvus instance. </li> </ul>"},{"location":"AIML/VectorDb/milvus/#import-data_1","title":"Import data","text":"<p>To import the prepared data, you have to create an import job as follows:</p> <pre><code>from pymilvus.bulk_writer import bulk_import\n\nurl = f\"http://127.0.0.1:19530\"\n\nresp = bulk_import(\n    url=url,\n    collection_name=\"quick_setup\",\n    files=[['a1e18323-a658-4d1b-95a7-9907a4391bcf/1.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/2.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/3.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/4.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/5.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/6.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/7.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/8.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/9.parquet'],\n           ['a1e18323-a658-4d1b-95a7-9907a4391bcf/10.parquet']],\n)\n\njob_id = resp.json()['data']['jobId']\nprint(job_id)\n</code></pre> <p>The request body contains two fields:</p> <ul> <li>collectionName: The name of the target collection.</li> <li>files: A list of lists of file paths relative to the root path of the Milvus bucket on the MioIO instance started along with your Milvus instance. Possible sub-lists are as follows:</li> </ul> <p>JSON files</p> <p>If the prepared file is in JSON format, each sub-list should contain the path to a single prepared JSON file.</p> <pre><code>[\n    \"/d1782fa1-6b65-4ff3-b05a-43a436342445/1.json\"\n],\n</code></pre> <p>Parquet files If the prepared file is in Parquet format, each sub-list should contain the path to a single prepared parquet file.</p> <pre><code>[\n    \"/a6fb2d1c-7b1b-427c-a8a3-178944e3b66d/1.parquet\"\n]\n</code></pre> <p>The possible return is as follows:</p> <pre><code>{\n    \"code\": 200,\n    \"data\": {\n        \"jobId\": \"448707763884413158\"\n    }\n}\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#check-import-progress","title":"Check import progress","text":"<p>Once you get an import job ID, you can check the import progress as follows:</p> <pre><code>import json\nfrom pymilvus.bulk_writer import get_import_progress\n\nurl = f\"http://127.0.0.1:19530\"\n\nresp = get_import_progress(\n    url=url,\n    job_id=\"453265736269038336\",\n)\n\nprint(json.dumps(resp.json(), indent=4))\n</code></pre> <p>The possible response is as follows:</p> <pre><code>{\n    \"code\": 200,\n    \"data\": {\n        \"collectionName\": \"quick_setup\",\n        \"completeTime\": \"2024-05-18T02:57:13Z\",\n        \"details\": [\n            {\n                \"completeTime\": \"2024-05-18T02:57:11Z\",\n                \"fileName\": \"id:449839014328146740 paths:\\\"/8ca44f28-47f7-40ba-9604-98918afe26d1/1.parquet\\\" \",\n                \"fileSize\": 31567874,\n                \"importedRows\": 100000,\n                \"progress\": 100,\n                \"state\": \"Completed\",\n                \"totalRows\": 100000\n            },\n            {\n                \"completeTime\": \"2024-05-18T02:57:11Z\",\n                \"fileName\": \"id:449839014328146741 paths:\\\"/8ca44f28-47f7-40ba-9604-98918afe26d1/2.parquet\\\" \",\n                \"fileSize\": 31517224,\n                \"importedRows\": 100000,\n                \"progress\": 100,\n                \"state\": \"Completed\",\n                \"totalRows\": 200000            \n            }\n        ],\n        \"fileSize\": 63085098,\n        \"importedRows\": 200000,\n        \"jobId\": \"449839014328146739\",\n        \"progress\": 100,\n        \"state\": \"Completed\",\n        \"totalRows\": 200000\n    }\n}\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#list-import-jobs","title":"List Import Jobs","text":"<p>You can list all import jobs relative to a specific collection as follows:</p> <pre><code>import json\nfrom pymilvus.bulk_writer import list_import_jobs\n\nurl = f\"http://127.0.0.1:19530\"\n\nresp = list_import_jobs(\n    url=url,\n    collection_name=\"quick_setup\",\n)\n\nprint(json.dumps(resp.json(), indent=4))\n</code></pre> <p>The possible values are as follows:</p> <pre><code>{\n    \"code\": 200,\n    \"data\": {\n        \"records\": [\n            {\n                \"collectionName\": \"quick_setup\",\n                \"jobId\": \"448761313698322011\",\n                \"progress\": 50,\n                \"state\": \"Importing\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"AIML/VectorDb/milvus/#limitations","title":"Limitations","text":"<ul> <li>Each import file size should not exceed 16 GB.</li> <li>The maximum number of import requests is limited to 1024.</li> <li>The maximum number of file per import request should not exceed 1024.</li> <li>Only one partition name can be specified in an import request. If no partition name is specified, the data will be inserted into the default partition. Additionally, you cannot set a partition name in the import request if you have set the Partition Key in the target collection.</li> </ul>"},{"location":"AIML/VectorDb/milvus/#constraints","title":"Constraints","text":"<p>Before importing data, ensure that you have acknowledged the constaints in terms of the following Milvus behaviors:</p> <ul> <li>Constraints regarding the Load behavior:<ul> <li>If a collection has already been loaded before an import, you can use the refresh_load function to load the newly imported data after the import is complete.</li> </ul> </li> <li>Constraints regarding the query &amp; search behaviors:<ul> <li>Before the import job status is Completed, the newly import data is guaranteed to be invisible to queries and searches.</li> <li>Once the job status is Completed,         - If the collection is not loaded, you can use the load function to load the newly imported data.         - If the collection is already loaded, you can call load(is_refresh=True) to load the imported data.</li> </ul> </li> <li>Constraints regarding the delete behavior:<ul> <li>Before the import job status is Completed, deletion is not guaranteed and may or may not succeed.</li> <li>Deletion after the job status is Completed is guaranted to succeed.</li> </ul> </li> </ul>"},{"location":"AIML/VectorDb/milvus/#recommendations","title":"Recommendations","text":"<p>We highly recommend utilizing the multi-file import feature, which allows you to upload several files in a single request. This method not only simplifies the import process but also significantly boosts import performance. Meanwhile, by consolidating your uploads, you can reduce the time spent on data management and make your workflow more efficient.</p>"},{"location":"AIML/VectorDb/milvus/#tools","title":"Tools","text":"<ol> <li>Attu (Milvus GUI)</li> <li>Milvus Backup</li> <li>Birdwatcher</li> <li>Milvus-CDC</li> <li>Milvus Sizing Tool</li> <li>VTS (short for Vector Transport Service)</li> </ol>"},{"location":"AIML/VectorDb/milvus/#integrations-overview","title":"Integrations Overview","text":"Tutorial Use Case Partners or Stacks RAG with Milvus and LlamaIndex RAG Milvus, LlamaIndex RAG with Milvus and LangChain RAG Milvus, LangChain Milvus Hybrid Search Retriever in LangChain Hybrid Search Milvus, LangChain Semantic Search with Milvus and OpenAI Semantic Search Milvus, OpenAI Question Answering Using Milvus and Cohere Semantic Search Milvus, Cohere Question Answering using Milvus and HuggingFace Question Answering Milvus, HuggingFace Image Search using Milvus and Pytorch Semantic Search Milvus, Pytorch Movie Search using Milvus and SentenceTransformers Semantic Search Milvus, SentenceTransformers Use Milvus as a Vector Store in LangChain Semantic Search Milvus, LangChain Using Full-Text Search with LangChain and Milvus Full-Text Search Milvus, LangChain RAG with Milvus and Haystack RAG Milvus, Haystack Conduct Vision Searches with Milvus and FiftyOne Semantic Search Milvus, FiftyOne Semantic Search with Milvus and VoyageAI Semantic Search Milvus, VoyageAI RAG with Milvus and BentoML RAG Milvus, BentoML RAG with Milvus and DSPy RAG Milvus, DSPy Semantic Search with Milvus and Jina Semantic Search Milvus, Jina Milvus on Snowpark Container Services Data Connection Milvus, Snowpark Rule-based Retrieval with Milvus and WhyHow Question Answering Milvus, WhyHow Milvus in Langfuse Observability Milvus, Langfuse RAG Evaluation with Ragas and Milvus Evaluation Milvus, Ragas Chatbot Agent with Milvus and MemGPT Agent Milvus, MemGPT How to deploy FastGPT with Milvus RAG Milvus, FastGPT Write SQL with Vanna and Milvus RAG Milvus, Vanna RAG with Milvus and Camel RAG Milvus, Camel Airbyte &amp; Milvus: Open-Source Data Movement Infrastructure Data Connection Milvus, Airbyte Advanced Video Search: Leveraging Twelve Labs and Milvus Semantic Search Milvus, Twelve Labs Building RAG with Milvus, vLLM, and Llama 3.1 RAG Milvus, vLLM, LlamaIndex Multi-agent Systems with Mistral AI, Milvus and LlamaIndex Agent Milvus, Mistral AI, LlamaIndex Connect Kafka with Milvus Data Sources Milvus, Kafka Kotaemon RAG with Milvus RAG Milvus, Kotaemon Crawling Websites with Apify and Saving Data to Milvus Data Sources Milvus, Apify Evaluation with DeepEval Evaluation &amp; Observability Milvus, DeepEval Evaluation with Arize Phoenix Evaluation &amp; Observability Milvus, Arize Phoenix Deploying Dify with Milvus Orchestration Milvus, Dify Building a RAG System Using Langflow with Milvus Orchestration Milvus, Langflow Build RAG on Arm Architecture RAG Milvus, Arm Build RAG with Milvus and Fireworks AI LLMs Milvus, Fireworks AI Build RAG with Milvus and Lepton AI LLMs Milvus, Lepton AI Build RAG with Milvus and SiliconFlow LLMs Milvus, SiliconFlow Build a RAG with Milvus and Unstructured Data Sources Milvus, Unstructured Build RAG with Milvus + PII Masker Data Sources Milvus, PII Masker Use Milvus in PrivateGPT Orchestration Vector Search Getting Started with Mem0 and Milvus Agents Mem0, Milvus Knowledge Table with Milvus Knowledge Engineering Knowledge Table, Milvus Use Milvus in DocsGPT Orchestration DocsGPT, Milvus Use Milvus with SambaNova Orchestration Milvus, SambaNova Build RAG with Milvus and Cognee Knowledge Engineering Milvus, Cognee Build RAG with Milvus and Gemini LLMs Milvus, Gemini Build RAG with Milvus and Ollama LLMs Milvus, Ollama Getting Started with Dynamiq and Milvus Orchestration Milvus, Dynamiq Build RAG with Milvus and DeepSeek LLMs Milvus, DeepSeek Integrate Milvus with Phidata Agents Milvus, Phidata Building RAG with Milvus and Crawl4AI Data Sources Milvus, Crawl4AI Building RAG with Milvus and Firecrawl Data Sources Milvus, Firecrawl <p>Integrations link</p>"},{"location":"AIML/VectorDb/milvus/#milvus-limits","title":"Milvus Limits","text":"<p>Milvus is committed to providing the best vector databases to power AI applications and vector similarity search. However, the team is continuously working to bring in more features and the best utilities to enhance user experience. This page lists out some known limitations that the users may encounter when using Milvus.</p> <p>Limitation link</p>"},{"location":"AIML/VectorDb/milvus_setup/","title":"Run Milvus in Docker","text":""},{"location":"AIML/VectorDb/milvus_setup/#ref-git-repo","title":"Ref git repo","text":""},{"location":"AIML/VectorDb/milvus_setup/#download-the-installation-script","title":"Download the installation script","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh\n\nbash standalone_embed.sh start\n</code></pre>"},{"location":"AIML/VectorDb/milvus_setup/#create-embedetcdyaml","title":"create embedEtcd.yaml","text":"<ol> <li>create embedEtcd.yaml file with below content in the /tmp folder</li> </ol> <pre><code>listen-client-urls: http://0.0.0.0:2379\nadvertise-client-urls: http://0.0.0.0:2379\nquota-backend-bytes: 4294967296\nauto-compaction-mode: revision\nauto-compaction-retention: '1000'\n</code></pre> <ol> <li>create user.yaml in the /tmp folder</li> </ol> <pre><code>touch user.yaml\n</code></pre> <ol> <li>Start running the below docker command</li> </ol> <pre><code>sudo docker run -d \\\n    --name milvus-standalone \\\n    --security-opt seccomp:unconfined \\\n    -e ETCD_USE_EMBED=true \\\n    -e ETCD_DATA_DIR=/var/lib/milvus/etcd \\\n    -e ETCD_CONFIG_PATH=/milvus/configs/embedEtcd.yaml \\\n    -e COMMON_STORAGETYPE=local \\\n    -v $(pwd)/volumes/milvus:/var/lib/milvus \\\n    -v $(pwd)/embedEtcd.yaml:/milvus/configs/embedEtcd.yaml \\\n    -v $(pwd)/user.yaml:/milvus/configs/user.yaml \\\n    -p 19530:19530 \\\n    -p 9091:9091 \\\n    -p 2379:2379 \\\n    --health-cmd=\"curl -f http://localhost:9091/healthz\" \\\n    --health-interval=30s \\\n    --health-start-period=90s \\\n    --health-timeout=20s \\\n    --health-retries=3 \\\n    milvusdb/milvus:v2.5.4 \\\n    milvus run standalone\n</code></pre> <ol> <li>Check milvus is running or not? </li> </ol> <pre><code>docker ps -a\n\nCONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS                    PORTS                                                                      NAMES\nde91d5fc78a2   milvusdb/milvus:v2.5.4   \"/tini -- milvus run\u2026\"   11 minutes ago   Up 11 minutes (healthy)   0.0.0.0:2379-&gt;2379/tcp, 0.0.0.0:9091-&gt;9091/tcp, 0.0.0.0:19530-&gt;19530/tcp   milvus-standalone\n</code></pre> <pre><code>http://localhost:9091/healthz\nhttp://127.0.0.1:9091/webui\nhttp://127.0.0.1:19530\n</code></pre>"},{"location":"AIML/VectorDb/milvus_setup/#running-attu-docker","title":"Running Attu Docker","text":""},{"location":"AIML/VectorDb/milvus_setup/#reference-document","title":"Reference document","text":"<p>Attu</p> <pre><code>docker run --name attu -itd -p 8000:3000 -e HOST_URL=http://0.0.0.0:8000 -e MILVUS_URL=0.0.0.0:19530 zilliz/attu:v2.4\n</code></pre> <p></p>"},{"location":"AIML/VectorDb/milvus_setup/#to-access-attu","title":"To access attu","text":"<pre><code>http://127.0.0.1:8000/\n</code></pre>"},{"location":"AIML/VectorDb/milvus_setup/#connect-to-milvus-server","title":"Connect to Milvus Server","text":"<p>Note: For attu Milvus Server IP will be system IPv4 address((localhost or 127.0.0.1 will not work)</p> <p>Ex: 192.168.0.2:19530</p> <p></p>"},{"location":"AIML/VectorDb/pinecone/","title":"Pinecone Vector Database","text":""},{"location":"AIML/VectorDb/pinecone/#1-install-an-sdk","title":"1. Install an SDK","text":"<p>Pinecone SDKs provide convenient programmatic access to the Pinecone APIs.</p>"},{"location":"AIML/VectorDb/pinecone/#install-the-sdk-for-your-preferred-languagepython","title":"Install the SDK for your preferred language(python):","text":"<pre><code>pip install \"pinecone[grpc]\"\n\n# To install without gRPC run:\n# pip3 install pinecone\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#2-get-an-api-key","title":"2. Get an API key","text":"<p>You need an API key to make calls to your Pinecone project.</p> <p>Create a new API key in the Pinecone console, or use the widget below to generate a key. If you don\u2019t have a Pinecone account, the widget will sign you up for the free Starter plan.</p>"},{"location":"AIML/VectorDb/pinecone/#your-generated-api-key","title":"Your generated API key:","text":"<pre><code>\"pcsk_6Ud5Yh_xxxxxxxxxxxx\"\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#3-generate-vectors","title":"3. Generate vectors","text":"<p>A vector embedding is a numerical representation of data that enables similarity-based search in vector databases like Pinecone. To convert data into this format, you use an embedding model.</p> <p>For this quickstart, use the multilingual-e5-large embedding model hosted by Pinecone to create vector embeddings for sentences related to the word \u201capple\u201d. Note that some sentences are about the tech company, while others are about the fruit.</p> <pre><code># Import the Pinecone library\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nimport time\n\n# Initialize a Pinecone client with your API key\npc = Pinecone(api_key=\"pcsk_6Ud5Yh_C9wzdiMJxrbuhZAyFL1gAi5Zim2fPZ1pqDmbDKEQQpBLZKmoJw8ZCfG2S56CsVL\")\n\n# Define a sample dataset where each item has a unique ID and piece of text\ndata = [\n    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n    {\"id\": \"vec6\", \"text\": \"Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership.\"}\n]\n\n# Convert the text into numerical vectors that Pinecone can index\nembeddings = pc.inference.embed(\n    model=\"multilingual-e5-large\",\n    inputs=[d['text'] for d in data],\n    parameters={\"input_type\": \"passage\", \"truncate\": \"END\"}\n)\n\nprint(embeddings)\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#4-create-an-index","title":"4. Create an index","text":"<p>In Pinecone, you store data in an index.</p> <p>Create a serverless index that matches the dimension (1024) and similarity metric (cosine) of the multilingual-e5-large model you used in the previous step, and choose a cloud and region for hosting the index:</p> <pre><code># Create a serverless index\nindex_name = \"vectordb-test-index\"\n\nif not pc.has_index(index_name):\n    pc.create_index(\n        name=index_name,\n        dimension=1024,\n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud='aws', \n            region='us-east-1'\n        ) \n    ) \n\n# Wait for the index to be ready\nwhile not pc.describe_index(index_name).status['ready']:\n    time.sleep(1)\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#5-upsert-vectors","title":"5. Upsert vectors","text":"<p>Target your index and use the upsert operation to load your vector embeddings into a new namespace. Namespaces let you partition records within an index and are essential for implementing multitenancy when you need to isolate the data of each customer/user.</p> <p><code>In production, target an index by its unique DNS host, not by its name.</code></p> <pre><code># Target the index where you'll store the vector embeddings\nindex = pc.Index(\"vectordb-test-index\")\n\n# Prepare the records for upsert\n# Each contains an 'id', the embedding 'values', and the original text as 'metadata'\nrecords = []\nfor d, e in zip(data, embeddings):\n    records.append({\n        \"id\": d['id'],\n        \"values\": e['values'],\n        \"metadata\": {'text': d['text']}\n    })\n\n# Upsert the records into the index\nindex.upsert(\n    vectors=records,\n    namespace=\"vectordb-test-namespace\"\n)\n</code></pre> <p><code>To load large amounts of data, [import from object storage](https://docs.pinecone.io/guides/data/understanding-imports) or [upsert in large batches](https://docs.pinecone.io/guides/data/upsert-data#upsert-records-in-batches).</code></p> <p>Pinecone is eventually consistent, so there can be a delay before your upserted records are available to query. Use the describe_index_stats operation to check if the current vector count matches the number of vectors you upserted (6):</p> <pre><code>time.sleep(10)  # Wait for the upserted vectors to be indexed\n\nprint(index.describe_index_stats())\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#6-search-the-index","title":"6. Search the index","text":"<p>With data in your index, let\u2019s say you now want to search for information about \u201cApple\u201d the tech company, not \u201capple\u201d the fruit.</p> <p>Use the the multilingual-e5-large model to convert your query into a vector embedding, and then use the query operation to search for the three vectors in the index that are most semantically similar to the query vector:</p> <pre><code># Define your query\nquery = \"Tell me about the tech company known as Apple.\"\n\n# Convert the query into a numerical vector that Pinecone can search with\nquery_embedding = pc.inference.embed(\n    model=\"multilingual-e5-large\",\n    inputs=[query],\n    parameters={\n        \"input_type\": \"query\"\n    }\n)\n\n# Search the index for the three most similar vectors\nresults = index.query(\n    namespace=\"vectordb-test-namespace\",\n    vector=query_embedding[0].values,\n    top_k=3,\n    include_values=False,\n    include_metadata=True\n)\n\nprint(results)\n</code></pre>"},{"location":"AIML/VectorDb/pinecone/#notice-that-the-response-includes-only-sentences-about-the-tech-company-not-the-fruit","title":"Notice that the response includes only sentences about the tech company, not the fruit:","text":""},{"location":"AIML/VectorDb/pinecone/#7-clean-up","title":"7. Clean up","text":"<p>When you no longer need the vectordb-test-index, use the delete_index operation to delete it:</p> <pre><code>pc.delete_index(index_name)\n</code></pre>"},{"location":"AIML/VectorDb/vector_database/","title":"What is vector database?","text":"<ul> <li>A vector database is a type of database optimized for storing, indexing, and searching high-dimensional vectors, which are mathematical representations of data points (like words, images, or other objects) in vector space.</li> <li>Vector databases are especially useful for handling data from applications in AI and machine learning, particularly in tasks involving similarity search and recommendation engines.</li> <li>A Vector Database, at its essence, is a relational database system specifically designed to process vectorized data. Unlike conventional databases that contain information in tables, rows, and columns, vector databases work with vectors\u2013arrays of numerical values that signify points in multidimensional space.</li> <li>Vectors, in turn, are everywhere and are commonly used in, for instance, machine learning, artificial intelligence, genomics, and geospatial analysis. At these datasets, there are frequently high-dimensional vectors where each dimension represents a particular attribute or feature.</li> <li>Such data place a heavy burden on traditional databases as they are tabular in form and do not allow efficiency in the storage and retrieval of such data and there comes the bottleneck in the performance of the database.</li> </ul>"},{"location":"AIML/VectorDb/vector_database/#what-is-a-vector","title":"What is a Vector?","text":"<ul> <li>Vector in the field of mathematics and data science refers to a serial arrangement of numerical values.</li> <li>It is a node in a many-dimensional space where one weight from each vector corresponds to a specific dimension.</li> <li>In the domain of vector databases, such arrays of numerical values, thus, turn into primitive concepts of information, making it possible for the store and processing of data in high dimensions.</li> </ul>"},{"location":"AIML/VectorDb/vector_database/#how-vector-databases-work","title":"How Vector Databases Work?","text":"<p>Vector Database is a type of database that is used in various machine learning use cases. They are specialized for the storage and retrieval of vector data.</p> <p>What are embeddings?</p> <p>Embedding is a data like words that have been converted into an array of numbers known as a vector that contains patterns of relationships the combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity.</p> <p></p> <p>The combination of these numbers that make up the vector act as a multi-dimensional map to measure similarity.</p> <p></p> <p>Let\u2019s see an example describe a 2d graph the words dog and puppy are often used in similar situations.</p> <p></p> <p>So in a word embedding they would be represented by vectors that are close together.</p> <p></p> <p>Well this is a simple 2D example of a single dimension in reality the vector has hundreds of Dimensions that cover the rich multi-dimensional complex relationship between words.</p> <p>Example</p> <p>Images can also be turned into vectors. Google does similar images searches and the image sections are broken down into arrays of numbers allowing you to find patterns of similarity for those with closely resembling vectors.</p> <p></p> <p>Once an embedding is created it can be stored in a database and a database full of these is considered as a vector database.</p> <p></p> <p>Vector database can be used in several ways, searching where results are ranked by relevance to a query string or clustering where text strings are grouped by similarity and recommendations where items with related text strings are recommended also classification where text strings are classified by their most similar label.</p>"},{"location":"AIML/VectorDb/vector_database/#key-components-of-vector-databases","title":"Key Components of Vector Databases","text":"<ol> <li>Embedding Storage: Stores high-dimensional vectors.</li> <li>Indexing and Search: Uses specialized indexing techniques like HNSW (Hierarchical Navigable Small World) or FAISS (Facebook AI Similarity Search) for fast similarity search across massive vector datasets.</li> <li>Scalability: Optimized for handling millions to billions of vectors.</li> <li>Metadata Support: Allows for filtering and adding metadata alongside vectors for refined search capabilities.</li> </ol>"},{"location":"AIML/VectorDb/vector_database/#popular-use-cases","title":"Popular Use Cases","text":"<ol> <li>Semantic Search: Finds documents, images, or videos similar in meaning to a search query, even if they don\u2019t share keywords.</li> <li>Recommendation Systems: Suggests items similar to user preferences (like items with similar vector embeddings).</li> <li>Anomaly Detection: Identifies outliers in datasets, as anomalies often have unique embeddings.</li> <li>Image and Video Recognition: Matches similar images or videos based on visual features encoded in vectors.</li> </ol>"},{"location":"AIML/VectorDb/vector_database/#examples-of-vector-databases","title":"Examples of Vector Databases","text":"<ol> <li>Weaviate: An open-source vector search engine with strong support for text, image, and video embeddings.</li> <li>Pinecone: A managed vector database with a focus on scalable similarity search and recommendations.</li> <li>FAISS: Primarily a library developed by Facebook AI for fast nearest neighbor search but also used in building vector search systems.</li> <li>Milvus: Another open-source vector database that provides highly scalable, low-latency vector searches.</li> </ol>"},{"location":"AIML/VectorDb/vector_database/#why-vector-databases-are-important","title":"Why Vector Databases Are Important","text":"<p>With the rise of LLMs (large language models) and computer vision, the demand for efficiently storing and retrieving high-dimensional embeddings has grown.</p> <p>Traditional databases like SQL and NoSQL are not optimized for these kinds of searches. Vector databases are specifically designed to handle such data, making them a crucial component in building advanced, intelligent applications in AI, recommendation systems, and search.</p>"},{"location":"AIML/VectorDb/weaviate/","title":"weaviate","text":"<pre><code>docker run -d -p 8080:8080 semitechnologies/weaviate\n</code></pre> <pre><code>http://localhost:8080/v1\n</code></pre> <pre><code>import weaviate\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\"http://localhost:8080\")\n\n# Define a schema for your documents\nclient.schema.create_class({\n    \"class\": \"Document\",\n    \"properties\": [\n        {\n            \"name\": \"text\",\n            \"dataType\": [\"text\"]\n        },\n        {\n            \"name\": \"embedding\",\n            \"dataType\": [\"number[]\"]\n        }\n    ]\n})\n</code></pre> <pre><code># Sample data and embeddings\ndocuments = [\"This is document 1\", \"This is document 2\", \"Document 3 content\"]\nembeddings = model.encode(documents)\n\n# Upload documents with embeddings\nfor i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n    client.data_object.create(\n        {\n            \"text\": doc,\n            \"embedding\": embedding.tolist()\n        },\n        \"Document\"\n    )\n</code></pre> <pre><code>query_embedding = model.encode([\"What is document retrieval?\"])\nresult = client.query.get(\"Document\", [\"text\"]) \\\n    .with_near_vector({\"vector\": query_embedding.tolist()}) \\\n    .with_limit(3) \\\n    .do()\nprint(\"Top results:\", result)\n</code></pre> <p>Get a single collection schema</p> <p></p> <p>Get a list of objects</p> <p></p> <p>Get an object</p> <p></p> <pre><code>https://weaviate.io/developers/weaviate/api/rest#description/introduction\n</code></pre>"},{"location":"AIML/aws/Configure-GPU-time-slicing/","title":"Configure GPU time-slicing if you have fewer than three GPUs.","text":"<ol> <li>Create a file, <code>time-slicing-config-all.yaml</code>, with the following content:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: time-slicing-config-all\ndata:\n  any: |-\n    version: v1\n    flags:\n      migStrategy: none\n    sharing:\n      timeSlicing:\n        resources:\n        - name: nvidia.com/gpu\n          replicas: 3\n</code></pre> <ul> <li> <p>The sample configuration creates three replicas from each GPU on the node.Replicas can be increase and decrease.</p> </li> <li> <p>Add the config map to the Operator namespace: <pre><code>kubectl create -n gpu-operator -f time-slicing-config-all.yaml\n</code></pre></p> </li> <li> <p>Configure the device plugin with the config map and set the default time-slicing configuration:</p> </li> </ul> <pre><code>kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\n    -n gpu-operator --type merge \\\n    -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"time-slicing-config-all\", \"default\": \"any\"}}}}'\n</code></pre> <ul> <li>Reset the Configure the device plugin if any new changes in the replica</li> </ul> <pre><code>kubectl patch clusterpolicy cluster-policy \\\n  -n gpu-operator \\\n  --type=json \\\n  -p='[{\"op\": \"remove\", \"path\": \"/spec/devicePlugin/config\"}]'\n</code></pre> <ul> <li>Verify that at least 3 GPUs are allocatable:</li> </ul> <pre><code>kubectl get nodes -l nvidia.com/gpu.present -o json | jq '.items[0].status.allocatable | with_entries(select(.key | startswith(\"nvidia.com/\"))) | with_entries(select(.value != \"0\"))'\n</code></pre>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/","title":"Deploying Milvus Vectorstore Helm Chart","text":""},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#deploying-milvus-vectorstore-helm-chart","title":"Deploying Milvus Vectorstore Helm Chart","text":"<ol> <li>Create a new nanespace for vectorstore</li> </ol> <pre><code>kubectl create namespace vectorstore\n</code></pre> <ol> <li>Add the milvus repository</li> </ol> <pre><code>helm repo add milvus https://zilliztech.github.io/milvus-helm/\n</code></pre> <ol> <li>Update the helm repository</li> </ol> <pre><code>helm repo update\n</code></pre> <ol> <li>Create a file named custom_value.yaml with below content to utilize GPU's</li> </ol> <pre><code>standalone:\n  resources:\n    requests:\n      nvidia.com/gpu: \"1\"\n    limits:\n      nvidia.com/gpu: \"1\"\n</code></pre> <ol> <li>Install the helm chart and point to the above created file using -f argument as shown below.</li> </ol> <pre><code>helm install milvus milvus/milvus --set cluster.enabled=false --set etcd.replicaCount=1 --set minio.mode=standalone --set pulsar.enabled=false -f custom_value.yaml -n vectorstore\n\n\nNAME: milvus\nLAST DEPLOYED: Wed May  7 13:00:44 2025\nNAMESPACE: vectorstore\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <ol> <li>Check status of the pods</li> </ol> <pre><code>kubectl get pods -n vectorstore\nNAME                                 READY   STATUS      RESTARTS   AGE\nmilvus-etcd-0                        1/1     Running     0          117s\nmilvus-minio-cd798dd6f-zszjv         1/1     Running     0          117s\nmilvus-pulsarv3-bookie-0             1/1     Running     0          117s\nmilvus-pulsarv3-bookie-1             1/1     Running     0          117s\nmilvus-pulsarv3-bookie-2             1/1     Running     0          116s\nmilvus-pulsarv3-bookie-init-6q6ts    0/1     Completed   0          117s\nmilvus-pulsarv3-broker-0             1/1     Running     0          117s\nmilvus-pulsarv3-broker-1             1/1     Running     0          117s\nmilvus-pulsarv3-proxy-0              0/1     Running     0          117s\nmilvus-pulsarv3-proxy-1              0/1     Running     0          117s\nmilvus-pulsarv3-pulsar-init-66kcq    0/1     Completed   0          117s\nmilvus-pulsarv3-recovery-0           1/1     Running     0          117s\nmilvus-pulsarv3-zookeeper-0          1/1     Running     0          117s\nmilvus-pulsarv3-zookeeper-1          1/1     Running     0          116s\nmilvus-pulsarv3-zookeeper-2          1/1     Running     0          116s\nmilvus-standalone-7bf84684d4-bt9tv   0/1     Running     0          117s\n</code></pre>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#configuring-examples","title":"Configuring Examples","text":"<p>You can configure various parameters such as prompts and vectorstore using environment variables. Modify the environment variables in the env section of the query service in the values.yaml file of the respective examples.</p> <p>Configuring Prompts</p> <pre><code>---\ndepth: 2\nlocal: true\nbacklinks: none\n---\n</code></pre> <p>Each example utilizes a <code>prompt.yaml</code> file that defines prompts for different contexts. These prompts guide the RAG model in generating appropriate responses. You can tailor these prompts to fit your specific needs and achieve desired responses from the models.</p>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#accessing-prompts","title":"Accessing Prompts","text":"<p>The prompts are loaded as a Python dictionary within the application. To access this dictionary, you can use the <code>get_prompts()</code> function provided by the <code>utils</code> module. This function retrieves the complete dictionary of prompts.</p> <p>Consider we have following <code>prompt.yaml</code> file which is under <code>files</code> directory for all the helm charts</p> <p>You can access it's chat_template using following code in you chain server</p> <pre><code>from RAG.src.chain_server.utils import get_prompts\n\nprompts = get_prompts()\n\nchat_template = prompts.get(\"chat_template\", \"\")\n</code></pre> <ul> <li>Once you have updated the prompt you can update the deployment for any of the examples by using the command below.</li> </ul> <pre><code>helm upgrade &lt;rag-example-name&gt; &lt;rag-example-helm-chart-path&gt; -n &lt;rag-example-namespace&gt; --set imagePullSecret.password=$NGC_CLI_API_KEY\n</code></pre>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#configuring-vectorstore","title":"Configuring VectorStore","text":"<p>The vector store can be modified from environment variables. You can update:</p> <ol> <li> <p><code>APP_VECTORSTORE_NAME:</code> This is the vector store name. Currently, we support milvus and pgvector Note: This only specifies the vector store name. The vector store container needs to be started separately.</p> </li> <li> <p><code>APP_VECTORSTORE_URL:</code> The host machine URL where the vector store is running.</p> </li> </ol>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#additional-resources","title":"Additional Resources","text":"<p>Learn more about how to use NVIDIA NIM microservices for RAG through our Deep Learning Institute. Access the course here.</p>"},{"location":"AIML/aws/Deploying-Milvus-Vectorstore-Helm-Chart/#security-considerations","title":"Security considerations","text":"<p>The RAG applications are shared as reference architectures and are provided \u201cas is\u201d. The security of them in production environments is the responsibility of the end users deploying it. When deploying in a production environment, please have security experts review any potential risks and threats (including direct and indirect prompt injection); define the trust boundaries, secure the communication channels, integrate AuthN &amp; AuthZ with appropriate access controls, keep the deployment including the containers up to date, ensure the containers are secure and free of vulnerabilities.</p>"},{"location":"AIML/aws/Deploying-NVIDIA%20Nemo-Retriever-Embedding-Microservice/","title":"Deploying NVIDIA Nemo Retriever Embedding Microservice","text":""},{"location":"AIML/aws/Deploying-NVIDIA%20Nemo-Retriever-Embedding-Microservice/#nvidia-nim-for-nv-embedqa-e5-v5","title":"NVIDIA NIM for NV-EmbedQA-E5-V5","text":"<p>Setup Environment</p> <p>First create your namespace and your secrets</p> <pre><code>NAMESPACE=nvidia-nims\n\nDOCKER_CONFIG='{\"auths\":{\"nvcr.io\":{\"username\":\"$oauthtoken\", \"password\":\"'${NGC_API_KEY}'\" }}}'\n\necho -n $DOCKER_CONFIG | base64 -w0\n\nNGC_REGISTRY_PASSWORD=$(echo -n $DOCKER_CONFIG | base64 -w0 )\n\nkubectl create namespace ${NAMESPACE}\n\nkubectl apply -n ${NAMESPACE} -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: nvcrimagepullsecret\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: ${NGC_REGISTRY_PASSWORD}\nEOF\nkubectl create -n ${NAMESPACE} secret generic ngc-api --from-literal=NGC_API_KEY=${NGC_API_KEY}\n\n\nsecret/nvcrimagepullsecret created\nsecret/ngc-api created\n</code></pre>"},{"location":"AIML/aws/Deploying-NVIDIA%20Nemo-Retriever-Embedding-Microservice/#install-the-chart","title":"Install the chart","text":"<pre><code>helm upgrade \\\n    --install \\\n    --username '$oauthtoken' \\\n    --password \"${NGC_API_KEY}\" \\\n    -n ${NAMESPACE} \\\n    --set persistence.class=\"local-nfs\" \\\n    text-embedding-nim \\\n    https://helm.ngc.nvidia.com/nim/nvidia/charts/text-embedding-nim-1.2.0.tgz\n</code></pre> <p>https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/helm-charts/text-embedding-nim</p>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/","title":"Deploying NVIDIA NIM Microservices","text":""},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#deploying-nvidia-nim-for-llms","title":"Deploying NVIDIA NIM for LLMs","text":"<p>(Default flow deploys meta/llama3-8b-instruct)</p> <ul> <li>Follow the steps from nim-deploy repository to deploy NIM LLM microservice with meta/llama3-8b-instruct as default LLM model.</li> </ul> <pre><code>https://github.com/NVIDIA/nim-deploy/tree/main/helm\n</code></pre>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#using-the-nvidia-nim-for-llms-helm-chart","title":"Using the NVIDIA NIM for LLMs helm chart","text":"<p>The NIM Helm chart requires a Kubernetes cluster with appropriate GPU nodes and the GPU Operator installed.</p>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Set the NGC_API_KEY environment variable to your NGC API key, as shown in the following example.</p> <p>export NGC_API_KEY=\"key from ngc\"</p> <ul> <li>Clone this repository and change directories into nim-deploy/helm. The following commands must be run from that directory.</li> </ul> <pre><code>git clone git@github.com:NVIDIA/nim-deploy.git\ncd nim-deploy/helm\n</code></pre> <ul> <li>Select a NIM to use in your helm release</li> </ul> <p>Each NIM contains an AI model, application, or workflow. All files necessary to run the NIM are encapsulated in the container that is available on NGC. The NVIDIA API Catalog provides a sandbox to experiment with NIM APIs prior to container and model download.</p>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#setting-up-your-helm-values","title":"Setting up your helm values","text":"<p>All available helm values can be discoved by running the helm command after downloading the repo.</p> <pre><code>helm show values nim-llm/\n</code></pre>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#create-a-namespace","title":"Create a namespace","text":"<pre><code>kubectl create namespace nim\n</code></pre>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#launching-a-nim-with-a-minimal-configuration","title":"Launching a NIM with a minimal configuration","text":"<p>You can launch llama3-8b-instruct using a default configuration while only setting the NGC API key and persistence in one line with no extra files. Set persistence.enabled to true to ensure that permissions are set correctly and the container runtime filesystem isn't filled by downloading models.</p> <pre><code>helm --namespace nim install my-nim nim-llm/ --set model.ngcAPIKey=$NGC_API_KEY --set persistence.enabled=true\nNAME: my-nim\nLAST DEPLOYED: Wed May  7 12:29:46 2025\nNAMESPACE: nim\nSTATUS: deployed\nREVISION: 1\nNOTES:\nThank you for installing nim-llm.\n\n**************************************************\n| It may take some time for pods to become ready |\n| while model files download                     |\n**************************************************\n\nYour NIM version is: 1.0.3\n</code></pre>"},{"location":"AIML/aws/Deploying-NVIDIA-NIM-Microservices/#running-inference","title":"Running inference","text":"<p>If you are operating on a fresh persistent volume or similar, you may have to wait a little while for the model to download. You can check the status of your deployment by running</p> <pre><code>kubectl get pods -n nim\nNAME       READY   STATUS    RESTARTS   AGE\nmy-nim-0   0/1     Running   0          4m26s\n</code></pre> <p>And check that the pods have become \"Ready\".</p> <p>Once that is true, you can try something like:</p> <p>Deploying NVIDIA NIM Microservices</p>"},{"location":"AIML/aws/alb/","title":"CREATE A ALB","text":"<ol> <li>Click Load balancers</li> <li>Click Create load balancer</li> <li>Click Create Application Load Balancer</li> </ol> <p>VPC: Select your VPC Availability Zones and subnets: Select Zones</p> <p></p> <p>Security groups: create your security group HTTP and port 80 Listeners and routing: HTTP and port 80</p> <p></p>"},{"location":"AIML/aws/alb/#aiml-devops-eks-alb-security-group","title":"aiml-devops-eks-alb-security-group:","text":"<p>Inbound rules: Type: HTTP Protocol: TCP Port Range:80</p> <p></p> <p>Outbound rules: Type: All traffic Protocol: All Port Range: All</p> <p></p>"},{"location":"AIML/aws/alb/#target-group","title":"Target group:","text":"<p>aiml-devops-eks-target-group</p> <p>Protocol: HTTP Port:80</p> <p></p> <p></p> <p></p>"},{"location":"AIML/aws/alb/#ec2","title":"EC2","text":"<ul> <li>Instances </li> <li>i-084beb74e5cf7ca2b(Instance summary for i-084beb74e5cf7ca2b (devops-worker-node-dont-delete))</li> <li>Security: sg-039e9177e791695b9 (eks-cluster-sg-eks-nvai-devops-1264174662)</li> </ul> <p>Inbound rules:</p> <p></p> <p>Outbound rules:</p> <p></p>"},{"location":"AIML/aws/alb/#vpc","title":"VPC","text":"<p>vpc-0ef7d0584a5b8db05</p>"},{"location":"AIML/aws/alb/#private-subnets","title":"Private Subnets","text":"<p>subnet-007a981fc371a9ff2\u00a0 subnet-08cb0d3c13a59859f\u00a0 subnet-00cadeb6d9b4e0b96\u00a0</p> <p></p>"},{"location":"AIML/aws/alb/#private-subnet-eks-worker-nodes-live-here","title":"Private Subnet (EKS Worker Nodes live here):","text":"<ul> <li>Route Table:</li> <li>Destination: 0.0.0.0/0</li> <li>Target: nat-xxxxxxxxxxxxxxx   \u2190 \u2705 NAT Gateway</li> </ul>"},{"location":"AIML/aws/alb/#public-subnets","title":"Public Subnets:","text":"<p>subnet-0646cc13f2eccf180 subnet-017b30ee9d49fbf50 subnet-098bba3f2fafd5d58</p> <p></p>"},{"location":"AIML/aws/alb/#public-subnet-nat-gateway-lives-here","title":"Public Subnet (NAT Gateway lives here):","text":"<ul> <li>Route Table:</li> <li>Destination: 0.0.0.0/0</li> <li>Target: igw-01c210f16270dff09  \u2190 \u2705 Internet Gateway</li> </ul>"},{"location":"AIML/aws/alb/#creating-oidc-provider","title":"Creating OIDC provider","text":"<ol> <li>IAM</li> <li>Identity providers</li> </ol> <ul> <li>Click Add provider</li> <li>Provider name: Run below command to get Provider URL     <pre><code>aws eks describe-cluster \\\n    --name eks-nvai-devops \\\n    --region ap-south-1 \\\n    --profile KD-Admisssssssccess-7755555555866\\\n    --query \"cluster.identity.oidc.issuer\" \\\n    --output text\n</code></pre></li> </ul> <pre><code>Example(Provider URL): https://oidc.eks.ap-south-1.amazonaws.com/id/Axxxxxxxxx02F9E527C9BA6\n</code></pre> <ul> <li> <p>Audience : sts.amazonaws.com</p> </li> <li> <p>Click Add provider</p> </li> </ul> <p></p> <ul> <li>Assign Role</li> <li>Create a new role</li> </ul> <p></p> <p></p>"},{"location":"AIML/aws/alb/#install-aws-load-balancer-controller-with-helm","title":"Install AWS Load Balancer Controller with Helm","text":"<pre><code>brew install eksctl\n</code></pre>"},{"location":"AIML/aws/alb/#step-1-create-iam-role-using-eksctl","title":"Step 1: Create IAM Role using eksctl","text":"<pre><code>curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy.json\n</code></pre>"},{"location":"AIML/aws/alb/#step-2-create-an-iam-policy-using-the-policy-downloaded-in-the-previous-step","title":"Step 2: Create an IAM policy using the policy downloaded in the previous step.","text":"<pre><code>aws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy \\\n    --policy-document file://iam_policy.json\\\n    --profile KD-AdmeeeeeeeeatorAccess-7755555855866\n</code></pre> <p>EX: Output:</p> <pre><code>{\n    \"Policy\": {\n        \"PolicyName\": \"AWSLoadBalancerControllerIAMPolicy\",\n        \"PolicyId\": \"ANPA3J5HVWH5AYBPFK7Z4\",\n        \"Arn\": \"arn:aws:iam::7772044444866:policy/AWSLoadBalancerControllerIAMPolicy\",\n        \"Path\": \"/\",\n        \"DefaultVersionId\": \"v1\",\n        \"AttachmentCount\": 0,\n        \"PermissionsBoundaryUsageCount\": 0,\n        \"IsAttachable\": true,\n        \"CreateDate\": \"2025-05-09T12:38:43+00:00\",\n        \"UpdateDate\": \"2025-05-09T12:38:43+00:00\"\n    }\n}\n</code></pre>"},{"location":"AIML/aws/alb/#step-3-create-an-iam-serviceaccount","title":"Step 3: Create an IAM serviceaccount.","text":"<pre><code>Get AWS_ACCOUNT_ID\n\naws sts get-caller-identity --query Account --output text --profile KD-AdministratorAccess-77152225566\nsts.amazonaws.com\n\n**CREATE:**\n\neksctl create iamserviceaccount \\\n    --cluster=eks-nvai-devops \\\n    --namespace=kube-system \\\n    --name=aws-load-balancer-controller \\\n    --attach-policy-arn=arn:aws:iam:23419855866:policy/AWSLoadBalancerControllerIAMPolicy \\\n    --override-existing-serviceaccounts \\\n    --region ap-south-1 \\\n    --profile KD-AdministratorAccess-74562203855866 \\\n    --approve\n\n\nDELETE:\n\neksctl delete iamserviceaccount \\\n    --cluster=eks-nvai-devops\\\n    --namespace=kube-system \\\n    --name=aws-load-balancer-controller \\\n    --region ap-south-1 \\\n    --profile KD-AdministratorAccess-777203855866\n</code></pre> <p>Example Ouput:</p> <pre><code>2025-05-09 18:18:38 [\u2139]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)\n2025-05-09 18:18:38 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set\n2025-05-09 18:18:38 [\u2139]  1 task: { \n    2 sequential sub-tasks: { \n        create IAM role for serviceaccount \"kube-system/aws-load-balancer-controller\",\n        create serviceaccount \"kube-system/aws-load-balancer-controller\",\n    } }2025-05-09 18:18:38 [\u2139]  building iamserviceaccount stack \"eksctl-eks-nvai-devops-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2025-05-09 18:18:38 [\u2139]  deploying stack \"eksctl-eks-nvai-devops-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2025-05-09 18:18:38 [\u2139]  waiting for CloudFormation stack \"eksctl-eks-nvai-devops-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2025-05-09 18:19:08 [\u2139]  waiting for CloudFormation stack \"eksctl-eks-nvai-devops-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2025-05-09 18:19:09 [\u2139]  created serviceaccount \"kube-system/aws-load-balancer-controller\"\n</code></pre>"},{"location":"AIML/aws/alb/#step-4-install-aws-load-balancer-controller","title":"Step 4: Install AWS Load Balancer Controller","text":"<pre><code>helm repo add eks https://aws.github.io/eks-charts\nhelm repo update eks\n\n\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller \\\n  --set clusterName=eks-nvai-devops \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=aws-load-balancer-controller \\\n  --namespace kube-system\n</code></pre>"},{"location":"AIML/aws/alb/#output","title":"Output:","text":"<pre><code>E0509 19:34:53.481411   60086 round_tripper.go:63] CancelRequest not implemented by *kube.RetryingRoundTripper\nNAME: aws-load-balancer-controller\nLAST DEPLOYED: Fri May  9 19:34:21 2025\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nAWS Load Balancer controller installed!\n</code></pre>"},{"location":"AIML/aws/alb/#step-5-verify-that-the-controller-is-installed","title":"Step 5: Verify that the controller is installed","text":"<pre><code>kubectl get deployment -n kube-system aws-load-balancer-controller\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\naws-load-balancer-controller   2/2     2            2           84s\n</code></pre>"},{"location":"AIML/aws/alb/#check","title":"Check","text":""},{"location":"AIML/aws/alb/#create-nginx-ingressyaml","title":"create <code>nginx-ingress.yaml</code>","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: \n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}]'\n    alb.ingress.kubernetes.io/healthcheck-path: /\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-service\n                port:\n                  number: 80\n</code></pre> <ul> <li>Create namespace: aiml-app </li> </ul> <p>nginx-deployment.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>kubectl create namespace aiml-app\nkubectl apply -f  nginx-service.yaml\nkubectl apply -f  xxxxxxx.yaml\n</code></pre> <pre><code>nginx-service.yaml \n\ncat: cat: No such file or directory\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <pre><code>kubectl get pod,svc,ingress -n aiml-app \nNAME                                  READY   STATUS    RESTARTS   AGE\npod/nginx-deployment-96b9d695-98qd5   1/1     Running   0          106m\npod/nginx-deployment-96b9d695-gw46s   1/1     Running   0          106m\n\nNAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/nginx-service   ClusterIP   172.20.56.130   &lt;none&gt;        80/TCP    106m\n\nNAME                                      CLASS   HOSTS   ADDRESS                                                                   PORTS   AGE\ningress.networking.k8s.io/nginx-ingress   alb     *       k8s-aimlapp-nxfggggggg2134-1385223656.ap-south-1.elb.amazonaws.com   80      106m\n</code></pre> <p>k8s-aimlapp-nginxing-d8a06b6d70-1385223656.ap-south-1.elb.amazonaws.com</p> <p></p> <p>Ref</p>"},{"location":"AIML/aws/https/","title":"Create AWS ELB with Self-Signed SSL Cert","text":""},{"location":"AIML/aws/https/#self-signing-ssl-cert","title":"Self-signing SSL Cert","text":"<ol> <li>Generate self-sign certificate using this command:</li> </ol> <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout privateKey.key -out certificate.crt\n</code></pre> <ol> <li>Verify the key and certificate generated</li> </ol> <pre><code>openssl rsa -in privateKey.key -check\nopenssl x509 -in certificate.crt -text -noout\n</code></pre> <ol> <li>Convert the key and cert into .pem encoded file</li> </ol> <pre><code>openssl rsa -in privateKey.key -text &gt; private.pem\nopenssl x509 -inform PEM -in certificate.crt &gt; public.pem\n</code></pre> <ul> <li>Certificate private key (privateKey.key content)</li> <li>Certificate body (certificate.crt content)</li> </ul>"},{"location":"AIML/aws/https/#certificate-arn","title":"certificate-arn","text":""},{"location":"AIML/aws/https/#nginx-ingressyaml","title":"nginx-ingress.yaml","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: aiml-app\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:7772:certificate/62057bc8-17f9-438a-94a6-30\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    alb.ingress.kubernetes.io/healthcheck-path: /\nspec:\n  ingressClassName: alb\n  tls:\n    - hosts:\n        - cognizeai.net\n      secretName: tls-secret  # optional if using ACM certificate with ALB\n  rules:\n    - host: cognizeai.net\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: simcard-shelf-space-service\n                port:\n                  number: 5004\n          - path: /bms\n            pathType: Prefix\n            backend:\n              service:\n                name: battery-management-system-service\n                port:\n                  number: 9290\n          - path: /semiconductor\n            pathType: Prefix\n            backend:\n              service:\n                name: semiconductor-failure-detection-service\n                port:\n                  number: 5000\n</code></pre> <pre><code>kubectl apply -f nginx-ingress.yaml -n aiml-app\n</code></pre> <pre><code>NAME                                 CLASS   HOSTS           ADDRESS                                                                 PORTS     AGE\ningress.networking.k8s.io/aiml-app   alb     cognizeai.net   k8s-p-aimlapp-8821222.ap-south-1.elb.amazonaws.com   80, 443   24m\n</code></pre> <pre><code>nslookup k8s-aimlapp-aimlapp-b370-6288.ap-south-1.elb.amazonaws.com \n\nServer:     49.205.72.130\nAddress:    49.205.72.130#53\n\nNon-authoritative answer:\nName:   k8s-aimlapp-aimlapp-b370e0-6288.ap-south-1.elb.amazonaws.com\nAddress: 35.154.82.208\nName:   k8s-aimlapp-aimlapp-b370e0-6288.ap-south-1.elb.amazonaws.com\nAddress: 65.0.223.248\nName:   k8s-aimlapp-aimlapp-b370e0-6288.ap-south-1.elb.amazonaws.com\nAddress: 35.154.117.203\n</code></pre> <pre><code>vi /etc/hosts\n\n65.0.223.248 cognizeai.net\n</code></pre>"},{"location":"AIML/aws/https/#route-53-dashboard","title":"Route 53 Dashboard","text":""},{"location":"AIML/aws/milvus-attu/","title":"Setup Milvus Standalone in EKS with ALB Ingress","text":""},{"location":"AIML/aws/milvus-attu/#create-a-namespace","title":"Create a Namespace","text":"<pre><code>kubectl create namespace milvus\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#add-milvus-helm-repo","title":"Add Milvus Helm Repo","text":"<pre><code>helm repo add milvus https://milvus-io.github.io/milvus-helm/\nhelm repo update\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#milvus-nlb-valuesyaml","title":"milvus-nlb-values.yaml","text":"<pre><code>milvus-nlb-values.yaml \n\ncluster:\n  enabled: false\n\netcd:\n  replicaCount: 1\n\nminio:\n  mode: standalone\n\npulsarv3:\n  enabled: false\n\nservice:\n  type: LoadBalancer\n  port: 19530\n  annotations: \n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-name: milvus-service\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#install-milvus-standalone-via-helm","title":"Install Milvus Standalone via Helm","text":"<pre><code>helm install milvus-release milvus/milvus \\\n  --namespace milvus \\\n  --create-namespace \\\n  -f milvus-nlb-values.yaml\n</code></pre> <pre><code>kubectl -n milvus get pod,svc,ingress                                    \n\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/attu-5675f77748-c292g                       1/1     Running   0          4h40m\npod/milvus-release-etcd-0                       1/1     Running   0          12m\npod/milvus-release-minio-dc4957c7c-zd7lq        1/1     Running   0          12m\npod/milvus-release-standalone-c4c56cccf-bv9vx   1/1     Running   0          12m\n\nNAME                                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                    PORT(S)                          AGE\nservice/attu                           ClusterIP      172.20.234.81    &lt;none&gt;                                                         80/TCP                           4h40m\nservice/milvus-release                 LoadBalancer   172.20.122.195   milvus-service-xxxxxxxxx.elb.ap-south-1.amazonaws.com   19530:32620/TCP,9091:30682/TCP   12m\nservice/milvus-release-etcd            ClusterIP      172.20.190.137   &lt;none&gt;                                                         2379/TCP,2380/TCP                12m\nservice/milvus-release-etcd-headless   ClusterIP      None             &lt;none&gt;                                                         2379/TCP,2380/TCP                12m\nservice/milvus-release-minio           ClusterIP      172.20.139.189   &lt;none&gt;                                                         9000/TCP                         12m\n\nNAME                                    CLASS   HOSTS                          ADDRESS                                                                 PORTS     AGE\ningress.networking.k8s.io/milvus-attu   alb     attu.visionaryai.aimledu.com   k8s-milvus-milvusat-xxxxxx.ap-south-1.elb.amazonaws.com   80, 443   3h33m\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#test","title":"Test","text":"<pre><code>nc -vz milvus-service-xxxxxx.elb.ap-south-1.amazonaws.com 19530\nConnection to milvus-service-xxxx.elb.ap-south-1.amazonaws.com port 19530 [tcp/*] succeeded!\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#test-python","title":"Test python","text":"<pre><code>from pymilvus import connections\n\ntry:\n    connections.connect(\n        alias=\"default\",\n        host=\"milvus-service-b31a319c36663f78.elb.ap-south-1.amazonaws.com\",\n        port=\"19530\"\n    )\n    print(\"\u2705 Connected to Milvus!\")\nexcept Exception as e:\n    print(f\"\u274c Failed to connect to Milvus: {e}\")\n</code></pre> <pre><code>python testmilvus.py                                                     \n\n\u2705 Connected to Milvus!\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#install-attu","title":"Install attu","text":"<pre><code>attu-deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: attu\n  namespace: milvus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: attu\n  template:\n    metadata:\n      labels:\n        app: attu\n    spec:\n      containers:\n        - name: attu\n          image: zilliz/attu:v2.5\n          ports:\n            - containerPort: 3000\n          env:\n            - name: MILVUS_URL\n              value: http://milvus-release.milvus.svc.cluster.local:19530\n</code></pre> <pre><code>attu-service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: attu\n  namespace: milvus\nspec:\n  type: ClusterIP\n  selector:\n    app: attu\n  ports:\n    - port: 80\n      targetPort: 3000\n      protocol: TCP\n</code></pre>"},{"location":"AIML/aws/milvus-attu/#install-ingress","title":"Install Ingress","text":"<pre><code>attu-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: milvus-attu\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:777203855866:certificate/b78-98d0b00706ff\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=900\nspec:\n  ingressClassName: alb\n  tls:\n   - hosts:\n       - attu.visionaryai.aimledu.com\n  rules:\n    - host: attu.visionaryai.aimledu.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: attu\n                port:\n                  number: 80\n</code></pre> <pre><code>kubectl apply -f attu-deployment.yaml -n milvus\nkubectl apply -f attu-service.yaml -n milvus\nkubectl apply -f attu-ingress.yaml -n milvus\n</code></pre> <pre><code>kubectl -n milvus get pod,svc,ingress\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/attu-5675f77748-c292g                       1/1     Running   0          115m\npod/milvus-release-etcd-0                       1/1     Running   0          139m\npod/milvus-release-minio-dc4957c7c-q5dks        1/1     Running   0          139m\npod/milvus-release-standalone-c4c56cccf-bflrx   1/1     Running   0          139m\n\nNAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)              AGE\nservice/attu                           ClusterIP   172.20.234.81    &lt;none&gt;        80/TCP               115m\nservice/milvus-release                 ClusterIP   172.20.53.181    &lt;none&gt;        19530/TCP,9091/TCP   139m\nservice/milvus-release-etcd            ClusterIP   172.20.39.67     &lt;none&gt;        2379/TCP,2380/TCP    139m\nservice/milvus-release-etcd-headless   ClusterIP   None             &lt;none&gt;        2379/TCP,2380/TCP    139m\nservice/milvus-release-minio           ClusterIP   172.20.129.162   &lt;none&gt;        9000/TCP             139m\n\nNAME                                    CLASS   HOSTS                          ADDRESS                                                                 PORTS     AGE\ningress.networking.k8s.io/milvus-attu   alb     attu.xxxxxxx.xxxxx.com   k8s-milvus-milvusat--727822688.ap-south-1.elb.amazonaws.com   80, 443   48m\n</code></pre> <p><pre><code>nslookup attu.xxx.xxxxx 8.8.8.8\n\ncurl -k --resolve attu.xxx.xxxx.com:443:3.7.237.185 https://attu.xxxi.xxxx.com\n</code></pre> <pre><code>/etc/hosts\n\n3.7.237.185 attu.xxxx.axxxx.com\n</code></pre></p> <p></p>"},{"location":"AIML/aws/nvidia/","title":"RAG Application: Multiturn Chatbot","text":"<p>Multi Turn RAG</p> <p>This example showcases multi turn usecase in a RAG pipeline. It stores the conversation history and knowledge base in Milvus and retrieves them at runtime to understand contextual queries. It uses NeMo Inference Microservices to communicate with the embedding model and large language model. The example supports ingestion of PDF, .txt files. The docs are ingested in a dedicated document vectorstore. The prompt for the example is currently tuned to act as a document chat bot. For maintaining the conversation history, we store the previous query of user and its generated answer as a text entry in a different dedicated vectorstore for conversation history. Both these vectorstores are part of a Langchain LCEL chain as Langchain Retrievers. When the chain is invoked with a query, its passed through both the retrievers. The retriever retrieves context from the document vectorstore and the closest matching conversation history from conversation history vectorstore and the chunks are added into the LLM prompt as part of the chain.</p> <p>Prerequisites: - You have the NGC CLI available on your client machine. You can download the CLI from https://ngc.nvidia.com/setup/installers/cli.</p> <ul> <li> <p>You have Kubernetes installed and running Ubuntu 22.04. Refer to the Kubernetes documentation or the NVIDIA Cloud Native Stack repository for more information.</p> </li> <li> <p>You have a default storage class available in the cluster for PVC provisioning. One option is the local path provisioner by Rancher. Refer to the installation section of the README in the GitHub repository.</p> </li> </ul> <p>https://github.com/rancher/local-path-provisioner?tab=readme-ov-file#installation</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml\n\nkubectl get pods -n local-path-storage\n\nkubectl get storageclass\n</code></pre> <ul> <li>If the local path storage class is not set as default, it can be made default using the command below</li> </ul> <pre><code>kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"AIML/aws/nvidia/#deployment","title":"Deployment:","text":"<ol> <li>Fetch the helm chart from NGC</li> </ol> <pre><code>helm fetch https://helm.ngc.nvidia.com/nvidia/aiworkflows/charts/rag-app-multiturn-chatbot-24.08.tgz --username='$oauthtoken' --password=&lt;YOUR API KEY&gt;\n</code></pre> <ol> <li> <p>Deploy NVIDIA NIM LLM, NVIDIA NeMo Retriever Embedding and NVIDIA NeMo Retriever Ranking Microservice following steps in this section.</p> </li> <li> <p>Deploy Milvus vectorstore following steps in this section.</p> </li> <li> <p>Create the example namespace</p> </li> </ol> <pre><code>kubectl create namespace multiturn-rag\n</code></pre> <ol> <li>Export the NGC API Key in the environment.</li> </ol> <pre><code>export NGC_CLI_API_KEY=\"&lt;YOUR NGC API KEY&gt;\"\n</code></pre> <ol> <li>Create the Helm pipeline instance and start the services.</li> </ol> <pre><code>helm install multiturn-rag rag-app-multiturn-chatbot-24.08.tgz -n multiturn-rag --set imagePullSecret.password=$NGC_CLI_API_KEY\n</code></pre> <ol> <li>Verify the pods are running and ready.</li> </ol> <pre><code>kubectl get pods -n multiturn-rag\n</code></pre> <ol> <li>Access the app using port-forwarding.</li> </ol> <pre><code>kubectl port-forward service/rag-playground-multiturn-rag -n multiturn-rag 30003:3001\n</code></pre> <p><code>Open browser and access the rag-playground UI using http://localhost:30003/converse</code></p> <p></p>"},{"location":"AIML/aws/vpc/","title":"1. Set Up VPC with Private Subnets","text":"<ul> <li>Create a VPC with public and private subnets.</li> <li>Private Subnet: This subnet will host the EKS nodes and won't have direct access to the internet.</li> <li>Public Subnet: This subnet will have a NAT Gateway that allows the EKS nodes to access the internet indirectly.</li> </ul>"},{"location":"AIML/aws/vpc/#1-create-a-vpc","title":"1. Create a VPC","text":""},{"location":"AIML/aws/vpc/#2-create-subnetprivate-public","title":"2. Create Subnet(Private &amp; Public)","text":"<p>Private Subnet:</p> <p> </p> <p>Public Subnet: </p>"},{"location":"AIML/aws/vpc/#2-create-nat-gateway","title":"2. Create NAT Gateway","text":"<ul> <li>Elastic IP: First, allocate an Elastic IP (EIP) for the NAT Gateway.</li> <li>NAT Gateway: Create a NAT Gateway in the public subnet using the EIP. This will allow instances in the private subnet to access the internet.</li> </ul> <p>Elastic IP: </p> <p>NAT Gateway:</p> <p> </p>"},{"location":"AIML/aws/vpc/#3-configure-route-tables","title":"3. Configure Route Tables","text":"<ul> <li>Public Route Table: Ensure the public subnet route table has a route for 0.0.0.0/0 pointing to the Internet Gateway.</li> <li>Private Route Table: For private subnets, configure the route table to route traffic for 0.0.0.0/0 to the NAT Gateway. - - This enables EKS nodes in private subnets to access the internet for updates, pulling container images, etc.</li> </ul> <p>Internet gateways: </p> <p>Public Route Table: </p> <p>Private Route Table: </p>"},{"location":"AIML/aws/vpc/#4-set-up-eks-cluster-in-private-subnets","title":"4. Set Up EKS Cluster in Private Subnets","text":"<ul> <li>When creating your EKS cluster, make sure to select the private subnets for your worker nodes.</li> <li>Ensure that the EKS nodes are configured to communicate with the NAT Gateway by routing their internet-bound traffic   through the private subnet route table.</li> </ul>"},{"location":"AIML/aws/vpc/#amazon-elastic-kubernetes-service","title":"Amazon Elastic Kubernetes Service","text":""},{"location":"AIML/aws/vpc/#create-eks-cluster","title":"Create EKS cluster","text":""},{"location":"AIML/aws/vpc/#7-security-group-configuration","title":"7. Security Group Configuration","text":"<ul> <li>Ensure that the security groups attached to your EKS worker nodes allow outbound traffic to the internet (through the - NAT Gateway) and inbound traffic from your application services.</li> </ul>"},{"location":"AIML/aws/vpc/#add-ons","title":"Add-ons","text":"<ol> <li>kube-proxy</li> <li>CoreDNS</li> <li>Metrics Server</li> <li>Amazon VPC CNI</li> <li>Prometheus Node Exporter</li> <li>Amazon EBS CSI Driver</li> <li>Mountpoint for Amazon S3 CSI Driver </li> </ol> <p>eks-worker-node-policy </p>"},{"location":"AIML/aws/vpc/#installing-the-nvidia-gpu-operator","title":"Installing the NVIDIA GPU Operator","text":"<ol> <li> <p>Add the NVIDIA Helm repository: <pre><code>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \\\n    &amp;&amp; helm repo update\n</code></pre></p> </li> <li> <p>Install the GPU Operator.</p> </li> </ol> <pre><code>helm install --wait --generate-name \\\n    -n gpu-operator --create-namespace \\\n    nvidia/gpu-operator \\\n    --version=v25.3.0\n</code></pre>"},{"location":"AIML/aws/vpc/#verify-nvidia-plugin-installed","title":"\u2705 Verify NVIDIA Plugin Installed","text":"<p>Check the NVIDIA device plugin DaemonSet:</p> <pre><code>kubectl get daemonset -n nvidia-gpu-operator| grep nvidia\n</code></pre> <ul> <li>You should see:</li> </ul> <pre><code>gpu-feature-discovery                                   1         1         1       1            1           nvidia.com/gpu.deploy.gpu-feature-discovery=true                       12m\nnvidia-container-toolkit-daemonset                      1         1         1       1            1           nvidia.com/gpu.deploy.container-toolkit=true                           12m\nnvidia-dcgm-exporter                                    1         1         1       1            1           nvidia.com/gpu.deploy.dcgm-exporter=true                               12m\nnvidia-device-plugin-daemonset                          1         1         1       1            1           nvidia.com/gpu.deploy.device-plugin=true                               12m\nnvidia-device-plugin-mps-control-daemon                 0         0         0       0            0           nvidia.com/gpu.deploy.device-plugin=true,nvidia.com/mps.capable=true   12m\nnvidia-driver-daemonset                                 0         0         0       0            0           nvidia.com/gpu.deploy.driver=true                                      12m\nnvidia-mig-manager                                      0         0         0       0            0           nvidia.com/gpu.deploy.mig-manager=true                                 12m\nnvidia-operator-validator                               1         1         1       1            1           nvidia.com/gpu.deploy.operator-validator=true                          12m\n</code></pre> <ul> <li>Install it (if not present):</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml\n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml\n\ndaemonset.apps/nvidia-device-plugin-daemonset created\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get pod -A                                                                                          \nNAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-frfnv                         2/2     Running   0          10h\nkube-system   aws-node-hx576                         2/2     Running   0          19m\nkube-system   coredns-6799d65cb-n6jbt                1/1     Running   0          9h\nkube-system   coredns-6799d65cb-tw92r                1/1     Running   0          10h\nkube-system   kube-proxy-n5w89                       1/1     Running   0          10h\nkube-system   kube-proxy-ww9cm                       1/1     Running   0          19m\nkube-system   metrics-server-5c998cf5dc-hrlc9        1/1     Running   0          9h\nkube-system   metrics-server-5c998cf5dc-t5d5d        1/1     Running   0          10h\nkube-system   nvidia-device-plugin-daemonset-tcmzw   1/1     Running   0          12s\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get daemonset -n kube-system | grep nvidia                                                          \nnvidia-device-plugin-daemonset   1         1         1       1            1           &lt;none&gt;          63s\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre>"},{"location":"AIML/aws/vpc/#create-namespace","title":"Create Namespace","text":"<pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl create namespace nvidia-gpu-smi\nnamespace/nvidia-gpu-smi created\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\n  namespace: gpu-test\nspec:\n  restartPolicy: Never\n  containers:\n  - name: nvidia\n    image: nvidia/cuda:12.2.0-base-ubuntu20.04\n    command: [\"nvidia-smi\"]\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl apply -f nvidia-gpu-test.yaml  \npod/gpu-check created\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre></p> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl -n nvidia-gpu-smi get pod        \nNAME         READY   STATUS      RESTARTS   AGE\nnvidia-smi   0/1     Completed   0          43s\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <p><pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl logs -n nvidia-gpu-smi nvidia-smi\n\nFri Apr 18 06:37:40 2025 \n</code></pre> +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     | |-----------------------------------------+------------------------+----------------------+ | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC | | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. | |                                         |                        |               MIG M. | |=========================================+========================+======================| |   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 | |  0%   23C    P8              9W /  300W |       1MiB /  23028MiB |      0%      Default | |                                         |                        |                  N/A | +-----------------------------------------+------------------------+----------------------+</p> <p>+-----------------------------------------------------------------------------------------+ | Processes:                                                                              | |  GPU   GI   CI        PID   Type   Process name                              GPU Memory | |        ID   ID                                                               Usage      | |=========================================================================================| |  No running processes found                                                             | +-----------------------------------------------------------------------------------------+</p>"},{"location":"AIML/aws/vpc/#nvidia-gpu-driver-already-installed-with-amazon-ami","title":"NVIDIA GPU Driver already installed with  Amazon AMI","text":"<ul> <li>Run this to confirm GPU driver:<ul> <li>nvidia-smi</li> </ul> </li> </ul> <p>+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     | |-----------------------------------------+------------------------+----------------------+</p>"},{"location":"AIML/aws/vpc/#step-2-install-nvidia-gpu-operator-via-helm-recommended","title":"\u2705 Step 2: Install NVIDIA GPU Operator via Helm (Recommended)","text":"<p>You\u2019ll deploy the NVIDIA GPU Operator, which automatically handles driver/toolkit/monitoring inside Kubernetes.</p>"},{"location":"AIML/aws/vpc/#1-create-the-gpu-operator-namespace","title":"\ud83d\udee0\ufe0f 1. Create the gpu-operator namespace","text":"<pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl create namespace gpu-operator\nnamespace/gpu-operator created\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre>"},{"location":"AIML/aws/vpc/#2-add-nvidia-helm-repo","title":"\ud83d\udee0\ufe0f 2. Add NVIDIA Helm repo","text":"<pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ helm repo add nvidia https://nvidia.github.io/gpu-operator\n\"nvidia\" has been added to your repositories\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ %\n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"nvidia\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ %\n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm install --wait --generate-name \\\n  -n gpu-operator \\\n  nvidia/gpu-operator\n\nW0418 12:22:46.210733   42738 warnings.go:70] spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[0].preference.matchExpressions[0].key: node-role.kubernetes.io/master is use \"node-role.kubernetes.io/control-plane\" instead\nW0418 12:22:46.215110   42738 warnings.go:70] spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[0].preference.matchExpressions[0].key: node-role.kubernetes.io/master is use \"node-role.kubernetes.io/control-plane\" instead\nNAME: gpu-operator-1744959159\nLAST DEPLOYED: Fri Apr 18 12:22:42 2025\nNAMESPACE: gpu-operator\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <pre><code>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl -n gpu-operator get pod\nNAME                                                              READY   STATUS      RESTARTS   AGE\ngpu-feature-discovery-jkjlt                                       1/1     Running     0          15m\ngpu-operator-1744959159-node-feature-discovery-gc-656c869cpktzs   1/1     Running     0          15m\ngpu-operator-1744959159-node-feature-discovery-master-79f87fxj4   1/1     Running     0          15m\ngpu-operator-1744959159-node-feature-discovery-worker-kfw2t       1/1     Running     0          15m\ngpu-operator-1744959159-node-feature-discovery-worker-vj94m       1/1     Running     0          15m\ngpu-operator-85746cf4fc-gclqb                                     1/1     Running     0          15m\nnvidia-container-toolkit-daemonset-r5vmj                          1/1     Running     0          15m\nnvidia-cuda-validator-4zwr6                                       0/1     Completed   0          15m\nnvidia-dcgm-exporter-pvlsj                                        1/1     Running     0          15m\nnvidia-device-plugin-daemonset-m9bk5                              1/1     Running     0          15m\nnvidia-operator-validator-hgq8r                                   1/1     Running     0          15m\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <p>kubectl port-forward nvidia-dcgm-exporter-pvlsj 9400:9400 -n gpu-operator</p> <p>curl http://localhost:9400/metrics</p>"},{"location":"AIML/aws/vpc/#monitoring-gpus-in-kubernetes-with-dcgm","title":"Monitoring GPUs in Kubernetes with DCGM","text":""},{"location":"AIML/aws/vpc/#nvidia-dcgm","title":"NVIDIA DCGM","text":"<p>NVIDIA DCGM is a set of tools for managing and monitoring NVIDIA GPUs in large-scale, Linux-based cluster environments.</p> <p>DCGM includes APIs for gathering GPU telemetry. Of particular interest are GPU utilization metrics (for monitoring Tensor Cores, FP64 units, and so on), memory metrics, and interconnect traffic metrics. </p>"},{"location":"AIML/aws/vpc/#dcgm-exporter","title":"DCGM exporter","text":"<p>Monitoring stacks usually consist of a collector, a time-series database to store metrics, and a visualization layer. A popular open-source stack is Prometheus, used along with Grafana as the visualization tool to create rich dashboards. Prometheus also includes Alertmanager to create and manage alerts. Prometheus is deployed along with kube-state-metrics and node_exporter to expose cluster-level metrics for Kubernetes API objects and node-level metrics such as CPU utilization. Figure 1 shows a sample architecture of Prometheus.</p> <p></p>"},{"location":"AIML/aws/vpc/#per-pod-gpu-metrics-in-a-kubernetes-cluster","title":"Per-pod GPU metrics in a Kubernetes cluster","text":"<p>dcgm-exporter collects metrics for all available GPUs on a node. However, in Kubernetes, you might not necessarily know which GPUs in a node would be assigned to a pod when it requests GPU resources. Starting in v1.13, kubelet has added a device monitoring feature that lets you find out the assigned devices to the pod\u2014pod name, pod namespace, and device ID\u2014using a pod-resources socket.</p> <p>The http server in dcgm-exporter connects to the kubelet pod-resources server (/var/lib/kubelet/pod-resources) to identify the GPU devices running on a pod and appends the GPU devices pod information to the metrics collected.</p> <p></p> <p>Here are some examples of setting up dcgm-exporter. If you use the NVIDIA GPU Operator, then dcgm-exporter is one of the components deployed as part of the operator.</p> <pre><code>$ helm repo add prometheus-community \\\nhttps://prometheus-community.github.io/helm-charts\n\n$ helm repo update\n$ helm inspect values prometheus-community/kube-prometheus-stack &gt; /tmp/kube-prometheus-stack.values\n# Edit /tmp/kube-prometheus-stack.values in your favorite editor\n# according to the documentation\n# This exposes the service via NodePort so that Prometheus/Grafana\n# are accessible outside the cluster with a browser\n$ helm install prometheus-community/kube-prometheus-stack \\\n--create-namespace --namespace prometheus \\\n--generate-name \\\n--set prometheus.service.type=NodePort \\\n--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n</code></pre>"},{"location":"AIML/aws/vpc/#installing-dcgm-exporter","title":"Installing dcgm-exporter","text":"<p>Here\u2019s how to get started installing dcgm-exporter to monitor GPU performance and utilization. You use the Helm chart for setting up dcgm-exporter. First, add the Helm repo:</p> <pre><code>$ helm repo add gpu-helm-charts \\\nhttps://nvidia.github.io/gpu-monitoring-tools/helm-charts\n</code></pre> <pre><code>$ helm repo update\n</code></pre> <p>Then, install the chart using Helm:</p> <pre><code>$ helm install \\\n   --generate-name \\\n   gpu-helm-charts/dcgm-exporter\n</code></pre> <p>Using the Grafana service exposed at port 32032, access the Grafana homepage. Log in to the dashboard using the credentials available in the Prometheus chart: the adminPassword field in prometheus.values.</p> <p>To now start a Grafana dashboard for GPU metrics, import the reference NVIDIA dashboard from Grafana Dashboards.</p> <p>!grafana_import_url</p>"},{"location":"AIML/aws/vpc/#using-the-dcgm-dashboard","title":"Using the DCGM dashboard","text":"<pre><code>kubectl port-forward -n prometheus service/kube-prometheus-stack-1744977651-grafana 32032:80\n</code></pre> <p>http://127.0.0.1:32032</p> <pre><code>kubectl -n gpu-operator port-forward svc/nvidia-dcgm-exporter 9400:9400\n</code></pre> <ol> <li>Login to Graphana</li> <li> <p>User: admin, credentials available in the Prometheus chart: the adminPassword field in prometheus.values.            adminUser: admin           adminPassword: prom-xxxxxxx</p> </li> <li> <p>Click Dashboard</p> </li> <li>New...</li> <li>Import</li> </ol> <p></p> <p>use this URL: https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/</p> <p></p> <p>Select Prometheus and finally Load</p> <p></p> <p></p> <pre><code>NAMESPACE        NAME                                                              READY   STATUS      RESTARTS   AGE\ndefault          grafana-74d4987685-cnrth                                          1/1     Running     0          6h33m\ngpu-operator     gpu-feature-discovery-jkjlt                                       1/1     Running     0          7h29m\ngpu-operator     gpu-operator-1744959159-node-feature-discovery-gc-656c869cpktzs   1/1     Running     0          7h29m\ngpu-operator     gpu-operator-1744959159-node-feature-discovery-master-79f87fxj4   1/1     Running     0          7h29m\ngpu-operator     gpu-operator-1744959159-node-feature-discovery-worker-kfw2t       1/1     Running     0          7h29m\ngpu-operator     gpu-operator-1744959159-node-feature-discovery-worker-vj94m       1/1     Running     0          7h29m\ngpu-operator     gpu-operator-85746cf4fc-gclqb                                     1/1     Running     0          7h29m\ngpu-operator     nvidia-container-toolkit-daemonset-r5vmj                          1/1     Running     0          7h29m\ngpu-operator     nvidia-cuda-validator-4zwr6                                       0/1     Completed   0          7h28m\ngpu-operator     nvidia-dcgm-exporter-pvlsj                                        1/1     Running     0          7h29m\ngpu-operator     nvidia-device-plugin-daemonset-m9bk5                              1/1     Running     0          7h29m\ngpu-operator     nvidia-operator-validator-hgq8r                                   1/1     Running     0          7h29m\nkube-system      aws-node-frfnv                                                    2/2     Running     0          18h\nkube-system      aws-node-hx576                                                    2/2     Running     0          8h\nkube-system      coredns-6799d65cb-n6jbt                                           1/1     Running     0          18h\nkube-system      coredns-6799d65cb-tw92r                                           1/1     Running     0          18h\nkube-system      ebs-csi-controller-7bdbc84dfb-hg2x7                               6/6     Running     0          5h58m\nkube-system      ebs-csi-controller-7bdbc84dfb-n9jbg                               6/6     Running     0          5h58m\nkube-system      ebs-csi-node-jblfb                                                3/3     Running     0          5h58m\nkube-system      ebs-csi-node-qhdjs                                                3/3     Running     0          5h58m\nkube-system      kube-proxy-n5w89                                                  1/1     Running     0          18h\nkube-system      kube-proxy-ww9cm                                                  1/1     Running     0          8h\nkube-system      metrics-server-5c998cf5dc-hrlc9                                   1/1     Running     0          18h\nkube-system      metrics-server-5c998cf5dc-t5d5d                                   1/1     Running     0          18h\nkube-system      nvidia-device-plugin-daemonset-jhgpn                              1/1     Running     0          7h47m\nkube-system      nvidia-device-plugin-daemonset-tcmzw                              1/1     Running     0          8h\nkube-system      s3-csi-node-9fnkm                                                 3/3     Running     0          5h58m\nkube-system      s3-csi-node-m8xn4                                                 3/3     Running     0          5h58m\nnvidia-gpu-smi   nvidia-smi                                                        0/1     Completed   0          7h44m\nprometheus       alertmanager-kube-prometheus-stack-1744-alertmanager-0            2/2     Running     0          140m\nprometheus       kube-prometheus-stack-1744-operator-57fb44cf74-9dtwj              1/1     Running     0          140m\nprometheus       kube-prometheus-stack-1744977651-grafana-5988f98874-bb4xd         3/3     Running     0          140m\nprometheus       kube-prometheus-stack-1744977651-kube-state-metrics-6db49d8w5zm   1/1     Running     0          140m\nprometheus       kube-prometheus-stack-1744977651-prometheus-node-exporter-7b48v   1/1     Running     0          140m\nprometheus       kube-prometheus-stack-1744977651-prometheus-node-exporter-wgv5s   1/1     Running     0          140m\nprometheus       prometheus-kube-prometheus-stack-1744-prometheus-0                2/2     Running     0          140m\nganeshkinkargiri.@M7QJY5-A67EFC4A ~ % \n</code></pre> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl taint nodes ip-10-0-150-130.ap-south-1.compute.internal nvidia.com/gpu-only=true:NoSchedule node/ip-10-0-150-130.ap-south-1.compute.internal tainted ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p>"},{"location":"AIML/aws/vpc/#reference-linking","title":"Reference Linking","text":"<p>DCGM</p> <p>Deploy Prometheus using Helm</p>"},{"location":"AIML/aws/vpc/#others","title":"Others","text":"<p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm repo add grafana https://grafana.github.io/helm-charts \"grafana\" has been added to your repositories ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"nvidia\" chart repository ...Successfully got an update from the \"grafana\" chart repository Update Complete. \u2388Happy Helming!\u2388 ganeshkinkargiri.@M7QJY5-A67EFC4A ~ %  ganeshkinkargiri.@M7QJY5-A67EFC4A ~ %  ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm install grafana grafana/grafana NAME: grafana LAST DEPLOYED: Fri Apr 18 13:18:51 2025 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get your 'admin' user password by running:</p> <p>kubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo</p> <ol> <li>The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:</li> </ol> <p>grafana.default.svc.cluster.local</p> <p>Get the Grafana URL to visit by running these commands in the same shell:      export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\")      kubectl --namespace default port-forward $POD_NAME 3000</p> <ol> <li>Login with the password from step 1 and the username: admin</li> </ol>"},{"location":"AIML/aws/vpc/#_1","title":"1. Set Up VPC with Private Subnets","text":""},{"location":"AIML/aws/vpc/#warning-persistence-is-disabled-you-will-lose-your-data-when","title":"WARNING: Persistence is disabled!!! You will lose your data when","text":""},{"location":"AIML/aws/vpc/#the-grafana-pod-is-terminated","title":"the Grafana pod is terminated.","text":""},{"location":"AIML/aws/vpc/#_2","title":"1. Set Up VPC with Private Subnets","text":"<p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\" -o jsonpath=\"{.items[0].metadata.name}\") ganeshkinkargiri.@M7QJY5-A67EFC4A ~ %  ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl --namespace default port-forward $POD_NAME 3000 Forwarding from 127.0.0.1:3000 -&gt; 3000 Forwarding from [::1]:3000 -&gt; 3000</p> <p>user: admin</p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo</p> <p>JzLpeQTU3knXiqyXLYQ3FfL3udgbeMZ8ZMPWvzLF ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl create namespace prometheus namespace/prometheus created ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm repo add prometheus-community https://prometheus-community.github.io/helm-charts \"prometheus-community\" has been added to your repositories ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % helm upgrade -i prometheus prometheus-community/prometheus \\     --namespace prometheus \\     --set alertmanager.persistence.storageClass=\"gp2\" \\     --set server.persistentVolume.storageClass=\"gp2\" Release \"prometheus\" does not exist. Installing it now. NAME: prometheus LAST DEPLOYED: Fri Apr 18 13:37:07 2025 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local</p> <pre><code>helm uninstall prometheus -n prometheus\n</code></pre> <p>Get the Prometheus server URL by running these commands in the same shell:   export POD_NAME=$(kubectl get pods --namespace prometheus -l \"app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")   kubectl --namespace prometheus port-forward $POD_NAME 9090</p> <p>The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster: prometheus-alertmanager.prometheus.svc.cluster.local</p> <p>Get the Alertmanager URL by running these commands in the same shell:   export POD_NAME=$(kubectl get pods --namespace prometheus -l \"app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")   kubectl --namespace prometheus port-forward $POD_NAME 9093</p>"},{"location":"AIML/aws/vpc/#_3","title":"1. Set Up VPC with Private Subnets","text":""},{"location":"AIML/aws/vpc/#warning-pod-security-policy-has-been-disabled-by-default-since","title":"WARNING: Pod Security Policy has been disabled by default since","text":""},{"location":"AIML/aws/vpc/#it-deprecated-after-k8s-125-use","title":"it deprecated after k8s 1.25+. use","text":""},{"location":"AIML/aws/vpc/#index-values-prometheus-node-exporter-rbac","title":"(index .Values \"prometheus-node-exporter\" \"rbac\"","text":""},{"location":"AIML/aws/vpc/#pspenabled-with-index-values","title":".          \"pspEnabled\") with (index .Values","text":""},{"location":"AIML/aws/vpc/#prometheus-node-exporter-rbac-pspannotations","title":"\"prometheus-node-exporter\" \"rbac\" \"pspAnnotations\")","text":""},{"location":"AIML/aws/vpc/#in-case-you-still-need-it","title":"in case you still need it.","text":""},{"location":"AIML/aws/vpc/#_4","title":"1. Set Up VPC with Private Subnets","text":"<p>The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster: prometheus-prometheus-pushgateway.prometheus.svc.cluster.local</p> <p>Get the PushGateway URL by running these commands in the same shell:   export POD_NAME=$(kubectl get pods --namespace prometheus -l \"app=prometheus-pushgateway,component=pushgateway\" -o jsonpath=\"{.items[0].metadata.name}\")   kubectl --namespace prometheus port-forward $POD_NAME 9091</p> <p>For more information on running Prometheus, visit: https://prometheus.io/ ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get pods -n kube-system -l app=ebs-csi-controller</p> <p>NAME                                  READY   STATUS    RESTARTS   AGE ebs-csi-controller-7bdbc84dfb-hg2x7   6/6     Running   0          5m44s ebs-csi-controller-7bdbc84dfb-n9jbg   6/6     Running   0          5m44s ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get storageclass                                 </p> <p>NAME   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE gp2    kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  3d2h ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get pvc -n prometheus</p> <p>NAME                                STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE prometheus-server                   Pending                                      gp2                             23m storage-prometheus-alertmanager-0   Pending                                      gp2                             23m ganeshkinkargiri.@M7QJY5-A67EFC4A ~ %"},{"location":"AIML/aws/vpc/#trobule-shhot","title":"Trobule shhot","text":"<p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl describe node ip-10-0-153-26.ap-south-1.compute.internal | grep -i allocatable Allocatable:   Normal   NodeAllocatableEnforced  60m                kubelet                Updated Node Allocatable limit across pods   Normal   NodeAllocatableEnforced  58m                kubelet                Updated Node Allocatable limit across pods   Normal   NodeAllocatableEnforced  27m                kubelet                Updated Node Allocatable limit across pods ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl get nodes -o wide NAME                                         STATUS   ROLES    AGE   VERSION               INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                                KERNEL-VERSION                  CONTAINER-RUNTIME ip-10-0-133-51.ap-south-1.compute.internal   Ready       20h   v1.32.2-eks-677bac1   10.0.133.51           Bottlerocket OS 1.36.0 (aws-k8s-1.32)   6.1.131                         containerd://1.7.27+bottlerocket ip-10-0-153-26.ap-south-1.compute.internal   Ready       59m   v1.32.1-eks-5d632ec   10.0.153.26           Amazon Linux 2                          5.10.234-225.921.amzn2.x86_64   containerd://1.7.27 ganeshkinkargiri.@M7QJY5-A67EFC4A ~ %  <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl describe node ip-10-0-153-26.ap-south-1.compute.internal | grep -A5 \"Allocatable\"</p> <p>Allocatable:   cpu:                7910m   ephemeral-storage:  95551679124   hugepages-1Gi:      0   hugepages-2Mi:      0   memory:             31482280Ki --   Normal   NodeAllocatableEnforced  60m   kubelet  Updated Node Allocatable limit across pods   Normal   NodeHasSufficientMemory  60m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasSufficientMemory   Normal   NodeHasNoDiskPressure    60m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasNoDiskPressure   Normal   NodeHasSufficientPID     60m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasSufficientPID   Normal   Starting                 29m   kubelet  Starting kubelet.   Warning  CgroupV1                 29m   kubelet  cgroup v1 support is in maintenance mode, please migrate to cgroup v2 --   Normal   NodeAllocatableEnforced  29m   kubelet  Updated Node Allocatable limit across pods   Normal   NodeHasSufficientMemory  29m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasSufficientMemory   Normal   NodeHasNoDiskPressure    29m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasNoDiskPressure   Normal   NodeHasSufficientPID     29m   kubelet  Node ip-10-0-153-26.ap-south-1.compute.internal status is now: NodeHasSufficientPID ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % kubectl describe node ip-10-0-153-26.ap-south-1.compute.internal | grep nvidia.com/gpu</p> <pre><code>                nvidia.com/gpu-driver-upgrade-state=upgrade-done\n                nvidia.com/gpu.compute.major=8\n                nvidia.com/gpu.compute.minor=6\n                nvidia.com/gpu.count=1\n                nvidia.com/gpu.deploy.container-toolkit=true\n                nvidia.com/gpu.deploy.dcgm=true\n                nvidia.com/gpu.deploy.dcgm-exporter=true\n                nvidia.com/gpu.deploy.device-plugin=true\n                nvidia.com/gpu.deploy.driver=pre-installed\n                nvidia.com/gpu.deploy.gpu-feature-discovery=true\n                nvidia.com/gpu.deploy.node-status-exporter=true\n                nvidia.com/gpu.deploy.operator-validator=true\n                nvidia.com/gpu.family=ampere\n                nvidia.com/gpu.machine=g5.2xlarge\n                nvidia.com/gpu.memory=23028\n                nvidia.com/gpu.mode=compute\n                nvidia.com/gpu.present=true\n                nvidia.com/gpu.product=NVIDIA-A10G\n                nvidia.com/gpu.replicas=1\n                nvidia.com/gpu.sharing-strategy=none\n                nvidia.com/gpu-driver-upgrade-enabled: true\n</code></pre> <p>nvidia.com/gpu:     0   nvidia.com/gpu:     0   nvidia.com/gpu     0           0 ganeshkinkargiri.@M7QJY5-A67EFC4A ~ % </p> <p>Thanks \u2014 this confirms the root cause:</p> <p>\u26a0\ufe0f nvidia.com/gpu: 0 in both Capacity and Allocatable</p> <p>Even though the node has a GPU installed (you can see all the nvidia.com/gpu.* labels), Kubernetes does not recognize it as available for scheduling.</p> <p>\ud83c\udfaf What's going wrong? You have the GPU driver installed, and the nvidia-device-plugin is probably not working as expected, or the GPU is not visible to the container runtime (containerd).</p>"},{"location":"AgenticAI/Suitable%20Frameworks%20for%20Hierarchical%20age/","title":"Suitable Frameworks for Hierarchical age","text":"<p>Suitable Frameworks for Hierarchical agents Key Characteristics of a Hierarchical agents Domain-Specific Real-Time Examples of Hierarchical agents Agentic AI Frameworks \u2013 Comparison for Hierarchical agents Hierarchical agents \u2013 Architecture Key Takeaways Key Characteristics: Hierarchical agents \u2013 Architecture design for .md</p> \ud83d\udccc Types of Agents in AI: <p>\ud83e\udde0 Hierarchical agents \u2013 Structured View</p> Hierarchical agents \u2013 Architecture: <p></p>"},{"location":"AgenticAI/aws/","title":"AWS","text":""},{"location":"AgenticAI/aws/#aws-agentic-ai-services","title":"AWS Agentic AI Services","text":"<p>Amazon Bedrock Agents</p> <p>Amazon-sagemaker</p> Service Description Service Type SaaS / Shelf-Managed Use Case Example Amazon Rekognition Computer vision for image and video analysis Computer Vision SaaS Detect faces and objects in surveillance footage Amazon Transcribe Speech-to-text for real-time or batch transcription Speech Recognition SaaS Transcribe customer service calls Amazon Translate Neural machine translation Translation / NLP SaaS Translate product descriptions for global marketplaces Amazon Polly Text-to-speech for lifelike speech generation Speech Synthesis SaaS Generate lifelike voices for e-learning apps Amazon Comprehend NLP for text insights like sentiment analysis Natural Language Processing SaaS Analyze customer reviews for sentiment and key phrases Amazon Textract Text and data extraction from documents Document Processing SaaS Extract tables and fields from scanned invoices Amazon Personalize Personalized recommendations and user segmentation Recommendation System SaaS Product recommendation for e-commerce platform Amazon Augmented AI (A2I) Human review of ML predictions Human-in-the-loop (HITL) Shelf-Managed Review flagged document classifications in loan processing Amazon Bedrock Access to foundation models for generative AI apps Generative AI / Foundation Models SaaS Build a chatbot using Anthropic, Mistral, or Meta models Amazon Q Generative AI assistant for development and business insights Generative AI Assistant SaaS Get coding help or business data insights via natural queries Amazon SageMaker Comprehensive platform for ML and foundation models ML Platform / MLOps Shelf-Managed Train, deploy, and monitor custom ML models Amazon CodeGuru ML for code analysis and optimization DevOps / Code Quality SaaS Detect bugs and optimize code performance Amazon DevOps Guru ML for operational data analysis and issue resolution DevOps / AIOps SaaS Detect anomalies in application performance metrics AWS HealthLake HIPAA-eligible for healthcare data management Healthcare Data Platform Shelf-Managed Normalize and analyze patient health records Amazon Lex Conversational interfaces like chatbots and voice assistants Conversational AI SaaS Create a customer support chatbot"},{"location":"AgenticAI/aws/#sample-agentic-ai-workflow","title":"Sample Agentic AI Workflow","text":"<p>To set up a Multi-Agent System using AWS Bedrock with a real working example, you can follow this structured, practical approach using AWS Bedrock + AWS Lambda + AWS Step Functions (no external server needed). This example will simulate agents working together in a typical IT Support scenario:</p>"},{"location":"AgenticAI/aws/#use-case-it-support-chatbot-with-multi-agents","title":"\ud83c\udfaf Use Case: IT Support Chatbot with Multi-Agents","text":""},{"location":"AgenticAI/aws/#agents","title":"\ud83e\udde0 Agents:","text":"<ol> <li>Classifier Agent \u2013 Classifies user intent (e.g., knowledge/action).</li> <li>Knowledge Agent \u2013 Answers general IT questions.</li> <li>Action Agent \u2013 Simulates action (e.g., reset password).</li> </ol>"},{"location":"AgenticAI/aws/#what-youll-build","title":"\ud83d\udee0\ufe0f What You\u2019ll Build","text":"<p>Using AWS Console (UI):</p> <ul> <li>Use Amazon Bedrock to call Claude/Titan models.</li> <li>Use AWS Lambda to create agents as serverless functions.</li> <li>Use AWS Step Functions to orchestrate agent flow based on logic.</li> </ul>"},{"location":"AgenticAI/aws/#step-by-step-setup-ui-lambda-bedrock","title":"\u2705 Step-by-Step Setup (UI + Lambda + Bedrock)","text":""},{"location":"AgenticAI/aws/#step-1-enable-amazon-bedrock-models","title":"\ud83d\udd39 STEP 1: Enable Amazon Bedrock Models","text":"<ol> <li>Go to: Amazon Bedrock &gt; Model Access</li> <li>Enable Claude (Anthropic) or any model you'd like (Titan, Mistral, Llama)</li> </ol>"},{"location":"AgenticAI/aws/#step-2-create-iam-role-with-bedrock-access","title":"\ud83d\udd39 STEP 2: Create IAM Role with Bedrock Access","text":"<ol> <li>Go to IAM &gt; Roles &gt; Create Role</li> <li>Choose: Lambda</li> <li>Attach Policy: AmazonBedrockFullAccess + AWSLambdaBasicExecutionRole</li> <li>Name the role: LambdaBedrockRole</li> </ol>"},{"location":"AgenticAI/aws/#step-3-create-3-lambda-functions-1-per-agent","title":"\ud83d\udd39 STEP 3: Create 3 Lambda Functions (1 per Agent)","text":""},{"location":"AgenticAI/aws/#a-classifier-agent-lambda","title":"A. Classifier Agent Lambda","text":"<ol> <li>Name: classifierAgent</li> <li>Runtime: Python 3.12</li> <li>Use this code</li> </ol>"},{"location":"AgenticAI/aws/#code","title":"code","text":"<pre><code>import boto3\nimport json\n\ndef lambda_handler(event, context):\n    prompt = f\"Classify this request into either 'knowledge' or 'action': {event['user_input']}\"\n\n    bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n\n    response = bedrock.invoke_model(\n        modelId='amazon.titan-text-premier-v1:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            \"inputText\": prompt\n        })\n    )\n\n    result = json.loads(response['body'].read())\n    classification = result['results'][0]['outputText'].strip().lower()\n\n    return { \"classification\": classification }\n</code></pre>"},{"location":"AgenticAI/aws/#b-knowledge-agent-lambda","title":"B. Knowledge Agent Lambda","text":"<ol> <li>Name: KnowledgeAgent</li> <li>Runtime: Python 3.12</li> <li>Use this code</li> </ol> <pre><code>import boto3\nimport json\n\ndef lambda_handler(event, context):\n    prompt = f\"You are an IT support expert. Answer the user's question: {event['user_input']}\"\n\n    bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n\n    response = bedrock.invoke_model(\n        modelId='amazon.titan-text-premier-v1:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            \"inputText\": prompt\n        })\n    )\n\n    result = json.loads(response['body'].read())\n\n    return {\n        \"agent\": \"knowledge\",\n        \"response\": result['results'][0]['outputText'].strip()\n</code></pre>"},{"location":"AgenticAI/aws/#c-action-agent-lambda","title":"C. Action Agent Lambda","text":"<ol> <li>Name: ActionAgent</li> <li>Runtime: Python 3.12</li> <li>Use this code</li> </ol> <pre><code>import boto3\nimport json\n\ndef lambda_handler(event, context):\n    prompt = f\"You are an IT support automation bot. Respond to this action request: {event['user_input']}\"\n\n    bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')  # include region\n\n    response = bedrock.invoke_model(\n        modelId='amazon.titan-text-premier-v1:0',\n        contentType='application/json',\n        accept='application/json',\n        body=json.dumps({\n            \"inputText\": prompt\n        })\n    )\n\n    result = json.loads(response['body'].read())\n\n    return {\n        \"agent\": \"action\",\n        \"response\": result['results'][0]['outputText'].strip()\n    }\n</code></pre>"},{"location":"AgenticAI/aws/#step-4-create-a-step-function-for-orchestration","title":"\ud83d\udd39 STEP 4: Create a Step Function for Orchestration","text":"<p>Go to AWS Step Functions &gt; Create State Machine    Choose Author with Code Snippet    Paste the following Amazon States Language (ASL):</p> <pre><code>{\n  \"Comment\": \"Multi-Agent Orchestration for IT Support\",\n  \"StartAt\": \"ClassifierAgent\",\n  \"States\": {\n    \"ClassifierAgent\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:777203855866:function:classifierAgent\",\n      \"ResultPath\": \"$.classificationResult\",\n      \"Next\": \"CheckClassification\"\n    },\n    \"CheckClassification\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [\n        {\n          \"Variable\": \"$.classificationResult.classification\",\n          \"StringMatches\": \"*knowledge*\",\n          \"Next\": \"KnowledgeAgent\"\n        },\n        {\n          \"Variable\": \"$.classificationResult.classification\",\n          \"StringMatches\": \"*action*\",\n          \"Next\": \"ActionAgent\"\n        }\n      ],\n      \"Default\": \"UnknownClassification\"\n    },\n    \"KnowledgeAgent\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:777203855866:function:KnowledgeAgent\",\n      \"ResultPath\": \"$.response\",\n      \"End\": true\n    },\n    \"ActionAgent\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:777203855866:function:ActionAgent\",\n      \"ResultPath\": \"$.response\",\n      \"End\": true\n    },\n    \"UnknownClassification\": {\n      \"Type\": \"Fail\",\n      \"Error\": \"InvalidClassification\",\n      \"Cause\": \"Could not classify the user request\"\n    }\n  }\n}\n</code></pre>"},{"location":"AgenticAI/aws/#step-5-test-the-multi-agent-system","title":"\ud83d\udd39 STEP 5: Test the Multi-Agent System","text":"<p>Go to Step Functions &gt; Your State Machine    Click Start Execution    Use this test input:</p> <pre><code>{\n  \"user_input\": \"How can I install VPN on my laptop?\"\n}\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"AgenticAI/aws/#building-a-simple-agent-using-aws-bedrock","title":"Building a Simple Agent Using AWS Bedrock","text":""},{"location":"AgenticAI/aws/#step-1-create-a-lambda-function","title":"Step 1: Create a Lambda Function","text":"<p>First, create a Lambda function that your agent will invoke to perform actions. In this procedure, you'll create a Python Lambda function that returns the current date and time when invoked. You'll set up the function with basic permissions, add the necessary code to handle requests from your Amazon Bedrock agent, and deploy the function so it's ready to be connected to your agent.</p>"},{"location":"AgenticAI/aws/#create-a-lambda-function","title":"Create a Lambda Function","text":"<ol> <li> <p>Sign in to the AWS Management Console and open the Lambda console at https://console.aws.amazon.com/lambda/</p> </li> <li> <p>Choose Create function.</p> </li> <li> <p>Select Author from scratch.</p> </li> <li> <p>In the Basic information section:</p> </li> <li>For Function name, enter a function name (for example, <code>DateTimeFunction</code>).</li> <li>For Runtime, select Python 3.9 (or your preferred version).</li> <li>For Architecture, leave unchanged.</li> <li> <p>In Permissions, select Change default execution role and then select Create a new role with basic Lambda permissions.</p> </li> <li> <p>Choose Create function.</p> </li> <li> <p>In Function overview, under Function ARN, note the Amazon Resource Name (ARN) for the function.</p> </li> <li> <p>In the Code tab, replace the existing code with the following:</p> <pre><code>  import datetime\n  import json\n\n  def lambda_handler(event, context):\n      now = datetime.datetime.now()\n\n      response = {\n          \"date\": now.strftime(\"%Y-%m-%d\"),\n          \"time\": now.strftime(\"%H:%M:%S\")\n      }\n\n      response_body = {\n          \"application/json\": {\n              \"body\": json.dumps(response)\n          }\n      }\n\n      action_response = {\n          \"actionGroup\": event[\"actionGroup\"],\n          \"apiPath\": event[\"apiPath\"],\n          \"httpMethod\": event[\"httpMethod\"],\n          \"httpStatusCode\": 200,\n          \"responseBody\": response_body,\n      }\n\n      session_attributes = event[\"sessionAttributes\"]\n      prompt_session_attributes = event[\"promptSessionAttributes\"]\n\n      return {\n          \"messageVersion\": \"1.0\",\n          \"response\": action_response,\n          \"sessionAttributes\": session_attributes,\n          \"promptSessionAttributes\": prompt_session_attributes,\n      }\n</code></pre> </li> <li> <p>Choose Deploy to deploy your function.</p> </li> <li> <p>Choose the Configuration tab.</p> </li> <li> <p>Choose Permissions.</p> </li> <li> <p>Under Resource-based policy statements, choose Add permissions.</p> </li> <li> <p>In Edit policy statement, do the following:</p> <ul> <li>a. Choose AWS service</li> <li>b. In Service, select Other.</li> <li>c. For Statement ID, enter a unique identifier (for example, <code>AllowBedrockInvocation</code>).</li> <li>d. For Principal, enter <code>bedrock.amazonaws.com</code>.</li> <li> <p>e. For Source ARN, enter:</p> <pre><code>arn:aws:bedrock:&lt;region&gt;:&lt;AWS account ID&gt;:agent/*\n</code></pre> <p>Replace <code>&lt;region&gt;</code> with your AWS Region, such as <code>us-east-1</code>. Replace <code>&lt;AWS account ID&gt;</code> with your actual AWS account ID.</p> </li> </ul> </li> <li> <p>Choose Save.</p> </li> </ol>"},{"location":"AgenticAI/aws/#building-a-simple-agent-using-aws-bedrock_1","title":"Building a Simple Agent Using AWS Bedrock","text":""},{"location":"AgenticAI/aws/#step-2-create-a-bedrock-agent","title":"Step 2: Create a Bedrock Agent","text":""},{"location":"AgenticAI/aws/#1-sign-in-and-open-bedrock-console","title":"1. Sign in and Open Bedrock Console","text":"<ul> <li>Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions.</li> <li>Navigate to the Amazon Bedrock console.</li> <li>Ensure you're in an AWS Region that supports Amazon Bedrock agents.</li> </ul>"},{"location":"AgenticAI/aws/#2-create-an-agent","title":"2. Create an Agent","text":"<ol> <li>In the left navigation pane under Builder tools, choose Agents.</li> <li>Choose Create agent.</li> <li>Fill in the following:</li> <li>Name: (e.g., <code>MyBedrockAgent</code>)</li> <li>Description (optional)</li> <li>Choose Create. The Agent builder pane opens.</li> </ol>"},{"location":"AgenticAI/aws/#3-configure-agent-details","title":"3. Configure Agent Details","text":"<ul> <li>In the Agent details section:</li> <li>For Agent resource role, select Create and use a new service role.</li> <li>For Select model, choose a model (e.g., <code>Claude 3 Haiku</code>).</li> <li> <p>In Instructions for the Agent, paste the following:</p> <pre><code>You are a friendly chat bot. You have access to a function called that returns\ninformation about the current date and time. When responding with date or time,\nplease make sure to add the timezone UTC.\n</code></pre> </li> <li> <p>Choose Save.</p> </li> </ul>"},{"location":"AgenticAI/aws/#step-3-add-action-group","title":"Step 3: Add Action Group","text":""},{"location":"AgenticAI/aws/#1-navigate-to-action-groups","title":"1. Navigate to Action Groups","text":"<ul> <li>Choose the Action groups tab.</li> <li>Choose Add.</li> </ul>"},{"location":"AgenticAI/aws/#2-configure-action-group","title":"2. Configure Action Group","text":"<ul> <li>Action group name: (e.g., <code>TimeActions</code>)</li> <li>Description (optional)</li> <li>Action group type: Select Define with API schemas</li> <li>Action group invocation: Choose Select an existing Lambda function</li> <li>Select Lambda function: Choose the Lambda function created in Step 1</li> <li>Action group schema: Choose Define via in-line schema editor</li> </ul>"},{"location":"AgenticAI/aws/#3-paste-openapi-schema","title":"3. Paste OpenAPI Schema","text":"<p>Replace the existing schema with:</p> <pre><code>openapi: 3.0.0\ninfo:\n  title: Time API\n  version: 1.0.0\n  description: API to get the current date and time.\npaths:\n  /get-current-date-and-time:\n    get:\n      summary: Gets the current date and time.\n      description: Gets the current date and time.\n      operationId: getDateAndTime\n      responses:\n        '200':\n          description: Gets the current date and time.\n          content:\n            'application/json':\n              schema:\n                type: object\n                properties:\n                  date:\n                    type: string\n                    description: The current date\n                  time:\n                    type: string\n                    description: The current time\n</code></pre>"},{"location":"AgenticAI/aws/#4-finalize-agent-and-add-permissions","title":"4. Finalize Agent and Add Permissions","text":"<ol> <li>Review your action group configuration and choose Create.</li> <li>Choose Save to save your changes.</li> <li>Choose Prepare to prepare the agent.</li> <li>Choose Save and exit to save your changes and exit the agent builder.</li> </ol>"},{"location":"AgenticAI/aws/#step-6-grant-lambda-invoke-permissions-to-the-agent","title":"Step 6: Grant Lambda Invoke Permissions to the Agent","text":"<ol> <li> <p>In the Agent overview section, under Permissions, click the IAM service role. This opens the role in the IAM console.</p> </li> <li> <p>In the IAM console:</p> </li> <li>Choose the Permissions tab.</li> <li> <p>Choose Add permissions \u2192 Create inline policy.</p> </li> <li> <p>Choose the JSON tab and paste the following policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": \"Function ARN\"\n        }\n    ]\n}\n</code></pre> <p>\ud83d\udd04 Note: Replace <code>\"Function ARN\"</code> with the ARN of your Lambda function from Step 6 of Step 1: Create a Lambda Function</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>Enter a name for the policy (e.g., <code>BedrockAgentLambdaInvoke</code>).</p> </li> <li> <p>Choose Create policy.</p> </li> </ol>"},{"location":"AgenticAI/aws/#agentic-ai-usecase-1-slow-response-times-on-network-causing-otp-relay-delays-for-banking-customers-how-does-agentic-ai-identify-and-apply-fixes","title":"Agentic AI - Usecase 1: <code>Slow response times on network causing OTP relay delays for Banking customers. How does Agentic AI identify and apply fixes.</code>","text":""},{"location":"AgenticAI/aws/#step-1-create-a-lambda-function_1","title":"Step 1: Create a Lambda Function","text":"<p>Function Name: <code>OtpMonitorLambdaFunction</code></p> <p>Code:</p> <pre><code># Version-1\n\nimport boto3\nimport re\nimport json\nfrom datetime import datetime, timedelta\n\nlogs_client = boto3.client('logs')\n\ndef lambda_handler(event, context):\n    log_group_name = event.get(\"log_group_name\", \"/eks/otp-webapp\")\n    end_time = int(datetime.utcnow().timestamp() * 1000)\n    start_time = int((datetime.utcnow() - timedelta(hours=48)).timestamp() * 1000)\n\n    patterns = [\n        '\"Attempting to send OTP\"',\n        '\"OTP email sent in\"',\n        '\"Failed to send OTP\"'\n    ]\n\n    all_events = {}\n    delivery_times = []\n    failed_otp_count = 0\n\n    try:\n        for pattern in patterns:\n            response = logs_client.filter_log_events(\n                logGroupName=log_group_name,\n                startTime=start_time,\n                endTime=end_time,\n                filterPattern=pattern,\n                limit=500\n            )\n            for event_item in response.get('events', []):\n                message = event_item['message']\n                timestamp = datetime.utcfromtimestamp(event_item['timestamp'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n\n                if \"OTP email sent in\" in message:\n                    match = re.search(r'OTP email sent in ([\\d.]+) seconds', message)\n                    if match:\n                        delivery_times.append(float(match.group(1)))\n\n                if \"Failed to send OTP\" in message:\n                    failed_otp_count += 1\n\n                all_events[event_item['eventId']] = {\n                    'timestamp': timestamp,\n                    'message': message\n                }\n\n        logs = list(all_events.values())\n        max_delivery = max(delivery_times) if delivery_times else 0.0\n\n        status = \"INFO\"\n        if failed_otp_count &gt; 0 or max_delivery &gt; 2.5:\n            status = \"WARNING\"\n        if failed_otp_count &gt;= 3 or max_delivery &gt; 5.0:\n            status = \"CRITICAL\"\n\n        summary = {\n            \"status\": status,\n            \"affected_services\": [\"Lambda\"],\n            \"metric_alerts\": [\n                {\n                    \"metric\": \"Max OTP Delivery Time\",\n                    \"value\": f\"{max_delivery:.2f}s\",\n                    \"threshold\": \"2.5s\",\n                    \"service\": \"Lambda\"\n                },\n                {\n                    \"metric\": \"Failed OTP Count\",\n                    \"value\": failed_otp_count,\n                    \"threshold\": \"0\",\n                    \"service\": \"Lambda\"\n                }\n            ],\n            \"summary\": f\"Found {len(logs)} OTP logs in the last 12 hours. Failures: {failed_otp_count}, Max delivery time: {max_delivery:.2f}s\"\n        }\n\n        response_body = {\"application/json\": {\"body\": json.dumps(summary)}}\n\n        action_response = {\n            \"actionGroup\": event[\"actionGroup\"],\n            \"apiPath\": event[\"apiPath\"],\n            \"httpMethod\": event[\"httpMethod\"],\n            \"httpStatusCode\": 200,\n            \"responseBody\": response_body,\n        }\n\n        session_attributes = event[\"sessionAttributes\"]\n        prompt_session_attributes = event[\"promptSessionAttributes\"]\n\n        return {\n            \"messageVersion\": \"1.0\",\n            \"response\": action_response,\n            \"sessionAttributes\": session_attributes,\n            \"promptSessionAttributes\": prompt_session_attributes,\n        }\n\n    except Exception as e:\n        error_response_body = {\n            \"application/json\": {\n                \"body\": json.dumps({\n                    \"error\": \"Lambda Error\",\n                    \"message\": str(e)\n                })\n            }\n        }\n\n        action_response = {\n            \"actionGroup\": event.get(\"actionGroup\", \"Unknown\"),\n            \"apiPath\": event.get(\"apiPath\", \"/unknown\"),\n            \"httpMethod\": event.get(\"httpMethod\", \"GET\"),\n            \"httpStatusCode\": 500,\n            \"responseBody\": error_response_body,\n        }\n\n        return {\n            \"messageVersion\": \"1.0\",\n            \"response\": action_response,\n            \"sessionAttributes\": event.get(\"sessionAttributes\", {}),\n            \"promptSessionAttributes\": event.get(\"promptSessionAttributes\", {}),\n        }\n</code></pre> <pre><code># Version-2\n\nimport boto3\nimport re\nimport json\nfrom datetime import datetime, timedelta\n\nlogs_client = boto3.client('logs')\n\ndef extract_email(message):\n    # Basic regex for email extraction; adjust based on your logs\n    match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', message)\n    return match.group(0) if match else None\n\ndef lambda_handler(event, context):\n    log_group_name = event.get(\"log_group_name\", \"/eks/otp-webapp\")\n    end_time = int(datetime.utcnow().timestamp() * 1000)\n    start_time = int((datetime.utcnow() - timedelta(hours=10)).timestamp() * 1000)\n\n    search_keywords = [\n        \"Attempting to send OTP\",\n        \"OTP email sent in\",\n        \"Failed to send OTP\"\n    ]\n\n    result_logs = []\n\n    try:\n        paginator = logs_client.get_paginator('filter_log_events')\n        page_iterator = paginator.paginate(\n            logGroupName=log_group_name,\n            startTime=start_time,\n            endTime=end_time\n        )\n\n        for page in page_iterator:\n            for event_item in page.get('events', []):\n                message = event_item['message']\n                if any(keyword in message for keyword in search_keywords):\n                    timestamp = datetime.utcfromtimestamp(event_item['timestamp'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n                    email_id = extract_email(message)\n\n                    result_logs.append({\n                        \"timestamp\": timestamp,\n                        \"message\": message,\n                        \"email_id\": email_id\n                    })\n\n        response_body = {\"application/json\": {\"body\": json.dumps(result_logs)}}\n\n        action_response = {\n            \"actionGroup\": event[\"actionGroup\"],\n            \"apiPath\": event[\"apiPath\"],\n            \"httpMethod\": event[\"httpMethod\"],\n            \"httpStatusCode\": 200,\n            \"responseBody\": response_body,\n        }\n\n        return {\n            \"messageVersion\": \"1.0\",\n            \"response\": action_response,\n            \"sessionAttributes\": event.get(\"sessionAttributes\", {}),\n            \"promptSessionAttributes\": event.get(\"promptSessionAttributes\", {}),\n        }\n\n    except Exception as e:\n        error_response_body = {\n            \"application/json\": {\n                \"body\": json.dumps({\n                    \"error\": \"Lambda Error\",\n                    \"message\": str(e)\n                })\n            }\n        }\n\n        action_response = {\n            \"actionGroup\": event.get(\"actionGroup\", \"Unknown\"),\n            \"apiPath\": event.get(\"apiPath\", \"/unknown\"),\n            \"httpMethod\": event.get(\"httpMethod\", \"GET\"),\n            \"httpStatusCode\": 500,\n            \"responseBody\": error_response_body,\n        }\n\n        return {\n            \"messageVersion\": \"1.0\",\n            \"response\": action_response,\n            \"sessionAttributes\": event.get(\"sessionAttributes\", {}),\n            \"promptSessionAttributes\": event.get(\"promptSessionAttributes\", {}),\n        }\n</code></pre> <p>Test: Event JSON</p> <pre><code>{\n  \"log_group_name\": \"/eks/otp-webapp\",\n  \"actionGroup\": \"OtpMonitorActionGroup\",\n  \"apiPath\": \"/get-otp-monitor\",\n  \"httpMethod\": \"GET\",\n  \"sessionAttributes\": {},\n  \"promptSessionAttributes\": {}\n}\n</code></pre> <p>Executing function: succeeded:</p> <pre><code>{\n  \"messageVersion\": \"1.0\",\n  \"response\": {\n    \"actionGroup\": \"OtpMonitorActionGroup\",\n    \"apiPath\": \"/get-otp-monitor\",\n    \"httpMethod\": \"GET\",\n    \"httpStatusCode\": 200,\n    \"responseBody\": {\n      \"application/json\": {\n        \"body\": \"{\\\"status\\\": \\\"WARNING\\\", \\\"affected_services\\\": [\\\"Lambda\\\"], \\\"metric_alerts\\\": [{\\\"metric\\\": \\\"Max OTP Delivery Time\\\", \\\"value\\\": \\\"3.38s\\\", \\\"threshold\\\": \\\"2.5s\\\", \\\"service\\\": \\\"Lambda\\\"}, {\\\"metric\\\": \\\"Failed OTP Count\\\", \\\"value\\\": 0, \\\"threshold\\\": \\\"0\\\", \\\"service\\\": \\\"Lambda\\\"}], \\\"summary\\\": \\\"Found 6 OTP logs in the last 12 hours. Failures: 0, Max delivery time: 3.38s\\\"}\"\n      }\n    }\n  },\n  \"sessionAttributes\": {},\n  \"promptSessionAttributes\": {}\n}\n</code></pre> <p>Configuration: Permissions: Role name: OtpMonitorLambdaFunction-role-ntyfvqf0 IAM -&gt; Roles -&gt; OtpMonitorLambdaFunction-role-ntyfvqf0</p> <p>Policy name: AWSLambdaBasicExecutionRole-9a1d489a-eda2-4c6e-bced-f99a004d9617 IAM -&gt; Policies -&gt; AWSLambdaBasicExecutionRole-9a1d489a-eda2-4c6e-bced-f99a004d9617</p> <p>Service: CloudWatch Logs</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:logs:us-east-1:777203855866:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:us-east-1:777203855866:log-group:/aws/lambda/OtpMonitorLambdaFunction:*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:FilterLogEvents\",\n                \"logs:GetLogEvents\",\n                \"logs:DescribeLogStreams\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:us-east-1:777203855866:log-group:/eks/otp-webapp:*\",\n                \"arn:aws:logs:us-east-1:777203855866:log-group:sns/us-east-1/777203855866/otp_delay_poc:*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Resource-based policy statements Resource-based policies grant other AWS accounts and services permissions to access your Lambda resources.</p> <p>Statement ID: AllowBedrockInvocation</p> <p>Resource-based policy document</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"default\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowBedrockInvocation\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"bedrock.amazonaws.com\"\n      },\n      \"Action\": \"lambda:InvokeFunction\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:777203855866:function:OtpMonitorLambdaFunction\",\n      \"Condition\": {\n        \"ArnLike\": {\n          \"AWS:SourceArn\": \"arn:aws:bedrock:us-east-1:777203855866:agent/*\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"AgenticAI/aws/#step-2-create-a-amazon-bedrock-agent","title":"Step 2: Create a Amazon Bedrock Agent","text":"<p>Agent details Agent name: OtpMonitorAgent Agent description - optional: OTP CloudWatch logs monitor Agent Agent resource role: Create and use a new service role Select model: select from Model providers list Instructions for the Agent: Provide clear and specific instructions for the task the Agent will perform. You can also provide certain style and tone.</p> <pre><code>Your role is to analyze the output of a Lambda function that queries AWS CloudWatch logs from the log\n\n  Focus on the following OTP flow log messages:\n  - \"Generated OTP\"\n  - \"Attempting to send OTP\"\n  - \"OTP email sent\"\n  - \"OTP stored\"\n  - \"OTP verified\"\n\nParse and track OTP-related events in **chronological order** per transaction.\nExpected order:\n- Generated OTP \u2192 Attempting to send OTP \u2192 OTP email sent \u2192 OTP stored \u2192 OTP verified\n\n Detect and flag the following anomalies:\n     - Missing events in the expected sequence\n     - Time delays &gt; 10 seconds between any two consecutive steps\n     - Presence of known error patterns (e.g., \"SMTP error\", \"send failure\", \"OTP failed\", \"storage error\")\n</code></pre> <p>Action groups: Action group details Enter Action group name: OtpMonitorActionGroup Description - optional:</p> <p>Action group type: Select what type of action group to create: <code>Define with API schemas</code></p> <p>Action group invocation:  Specify a Lambda function that will be invoked based on the action group identified by the Foundation model during orchestration.</p> <p>Select an existing Lambda function: <code>OtpMonitorLambdaFunction</code></p> <p>Action group schema: Select an existing schema or create a new one via the in-line editor to define the APIs that the agent can invoke to carry out its tasks.</p> <ul> <li><code>Define via in-line schema editor</code></li> </ul> <p>In-line OpenAPI schema:</p> <pre><code>openapi: 3.0.0\ninfo:\n  title: OTP Monitoring API\n  version: 1.0.0\n  description: API to monitor OTP delivery delays and failures from CloudWatch logs.\npaths:\n  /get-otp-monitor:\n    get:\n      summary: Gets OTP delivery monitoring data.\n      description: Retrieves OTP delivery times, failure counts, and status based on CloudWatch logs for the past 12 hours.\n      operationId: getOtpMonitoringInfo\n      responses:\n        '200':\n          description: OTP delivery metrics and summary.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    description: Overall status based on OTP metrics (INFO, WARNING, CRITICAL)\n                  affected_services:\n                    type: array\n                    items:\n                      type: string\n                    description: Services affected (e.g., Lambda)\n                  metric_alerts:\n                    type: array\n                    description: List of metric evaluations\n                    items:\n                      type: object\n                      properties:\n                        metric:\n                          type: string\n                          description: Metric name\n                        value:\n                          type: string\n                          description: Observed value\n                        threshold:\n                          type: string\n                          description: Threshold value\n                        service:\n                          type: string\n                          description: Related service\n                  summary:\n                    type: string\n                    description: Summary message about the log findings\n</code></pre> <p>Permissions: <code>arn:aws:iam::777203855866:role/service-role/AmazonBedrockExecutionRoleForAgents_589IDLBHO1U</code></p> <p></p> <p>IAM -&gt; Roles -&gt; AmazonBedrockExecutionRoleForAgents_589IDLBHO1U</p> <p></p> <p>IAM -&gt; Policies -&gt; AmazonBedrockAgentBedrockFoundationModelPolicy_TU2VOQK6HG</p> <p></p> <p>Permissions defined in this policy</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AmazonBedrockAgentBedrockFoundationModelPolicyProd\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:InvokeModelWithResponseStream\"\n            ],\n            \"Resource\": [\n                \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>IAM -&gt; Roles -&gt; AmazonBedrockExecutionRoleForAgents_589IDLBHO1U</p> <p>permissions in OtpMonitorPolicy</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": \"arn:aws:lambda:us-east-1:777203855866:function:OtpMonitorLambdaFunction\"\n        }\n    ]\n}\n</code></pre> <p>Action status: Enable</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"AgenticAI/aws/#note","title":"Note:","text":"<ul> <li>Save -&gt; Prepare -&gt; Test -&gt; Create alias</li> </ul>"},{"location":"AgenticAI/aws/#test-the-otpmonitoragent","title":"Test the <code>OtpMonitorAgent</code>","text":"<p>Here\u2019s a test prompt you can use to trigger your AWS Bedrock Agent (which calls the Lambda OtpMonitorActionGroup.getOtpMonitoringInfoGET) correctly:</p>"},{"location":"AgenticAI/aws/#test-prompt","title":"\u2705 Test Prompt","text":"<pre><code>Check the OTP delivery metrics.\n</code></pre>"},{"location":"AgenticAI/aws/#trace-step-1","title":"Trace step 1","text":"<pre><code>{\n  \"agentId\": \"MZJDM5Z9N3\",\n  \"callerChain\": [\n    {\n      \"agentAliasArn\": \"arn:aws:bedrock:us-east-1:777203855866:agent-alias/MZJDM5Z9N3/TSTALIASID\"\n    }\n  ],\n  \"eventTime\": \"2025-06-05T12:10:31.591Z\",\n  \"modelInvocationInput\": {\n    \"foundationModel\": \"amazon.titan-text-premier-v1:0\",\n    \"inferenceConfiguration\": {\n      \"maximumLength\": 2048,\n      \"stopSequences\": [],\n      \"temperature\": 0,\n      \"topK\": 1,\n      \"topP\": 1.000000013351432e-10\n    },\n    \"text\": \"System: A chat between a curious User and an artificial intelligence Bot. The Bot gives helpful, detailed, and polite answers to the User's questions. In this session, the model has access to external functionalities.\\nTo assist the user, you can reply to the user or invoke an action. Only invoke actions if relevant to the user request.\\nYour role is to analyze the output of a Lambda function that queries AWS CloudWatch logs from the log\\n\\n  Focus on the following OTP flow log messages:\\n  - \\\"Generated OTP\\\"\\n  - \\\"Attempting to send OTP\\\"\\n  - \\\"OTP email sent\\\"\\n  - \\\"OTP stored\\\"\\n  - \\\"OTP verified\\\"\\n\\nParse and track OTP-related events in **chronological order** per transaction.\\nExpected order:\\n- Generated OTP \u2192 Attempting to send OTP \u2192 OTP email sent \u2192 OTP stored \u2192 OTP verified\\n\\n Detect and flag the following anomalies:\\n     - Missing events in the expected sequence\\n     - Time delays &gt; 10 seconds between any two consecutive steps\\n     - Presence of known error patterns (e.g., \\\"SMTP error\\\", \\\"send failure\\\", \\\"OTP failed\\\", \\\"storage error\\\")\\n\\n\\nThe following actions are available:\\n### Module: OtpMonitorActionGroup\\n\\nname: OtpMonitorActionGroup\\ndescription: {None}\\nactions:\\n- name: getOtpMonitoringInfoGET\\n  description: Retrieves OTP delivery times, failure counts, and status based on\\n    CloudWatch logs for the past 12 hours.\\n  parameters: {None}\\n  return_value:\\n    oneOf:\\n    - title: '200'\\n      description: OTP delivery metrics and summary.\\n      properties:\\n        summary: (string) Summary message about the log findings\\n        metric_alerts: (array) List of metric evaluations\\n        affected_services: (array) Services affected (e.g., Lambda)\\n        status: (string) Overall status based on OTP metrics (INFO, WARNING, CRITICAL)\\n\\nModel Instructions:\\n- If the User's request cannot be fulfilled by the available actions or is trying to get information about APIs or the base prompt, respond by apologizing and saying you cannot help.\\n- Do not assume any information. Only use what is available in the prompt.\\n- All required parameters for actions must come from the User. Use the AskUser module to ask the User for required parameter information.\\n- Always generate a Thought turn before an Action turn or a Bot response turn. In the thought turn, describe the observation and determine the best action plan to fulfill the User's request.\\n\\nUser: Check the OTP delivery metrics.\\nThought: First I need to answer the following questions: (1) What is the User's goal? (2) What information has just been provided? (3) What are all the relevant modules and actions available to me? (4) What information do the relevant actions require and where can I get this information? (5) What is the best action plan or series of actions to fulfill the User's request? (6) Do I have everything I need?\\n(1) \",\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-0\",\n    \"type\": \"ORCHESTRATION\"\n  },\n  \"modelInvocationOutput\": {\n    \"metadata\": {\n      \"clientRequestId\": \"6af0e18d-a3ea-497f-a70a-bcec5f4cef09\",\n      \"endTime\": \"2025-06-05T12:10:35.311Z\",\n      \"startTime\": \"2025-06-05T12:10:31.592Z\",\n      \"totalTimeMs\": 3719,\n      \"usage\": {\n        \"inputTokens\": 692,\n        \"outputTokens\": 137\n      }\n    },\n    \"rawResponse\": {\n      \"content\": \"The User's goal is to check the OTP delivery metrics.\\n(2) The User has just provided the goal.\\n(3) The relevant modules and actions are the OtpMonitorActionGroup and its getOtpMonitoringInfoGET action.\\n(4) The getOtpMonitoringInfoGET action requires no information.\\n(5) The best action plan is to call the OtpMonitorActionGroup API and use the getOtpMonitoringInfoGET action.\\n(6) I have everything I need.\\n\\nBot: Action: OtpMonitorActionGroup.getOtpMonitoringInfoGET()\"\n    },\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-0\"\n  },\n  \"rationale\": {\n    \"text\": \"The User's goal is to check the OTP delivery metrics.\\n(2) The User has just provided the goal.\\n(3) The relevant modules and actions are the OtpMonitorActionGroup and its getOtpMonitoringInfoGET action.\\n(4) The getOtpMonitoringInfoGET action requires no information.\\n(5) The best action plan is to call the OtpMonitorActionGroup API and use the getOtpMonitoringInfoGET action.\\n(6) I have everything I need.\",\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-0\"\n  },\n  \"invocationInput\": [\n    {\n      \"actionGroupInvocationInput\": {\n        \"actionGroupName\": \"OtpMonitorActionGroup\",\n        \"apiPath\": \"/get-otp-monitor\",\n        \"executionType\": \"LAMBDA\",\n        \"verb\": \"get\"\n      },\n      \"invocationType\": \"ACTION_GROUP\",\n      \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-0\"\n    }\n  ],\n  \"observation\": [\n    {\n      \"actionGroupInvocationOutput\": {\n        \"metadata\": {\n          \"clientRequestId\": \"675775c3-6562-4182-a7af-9db3e79f01f8\",\n          \"endTime\": \"2025-06-05T12:10:43.272Z\",\n          \"startTime\": \"2025-06-05T12:10:35.313Z\",\n          \"totalTimeMs\": 7959\n        },\n        \"text\": \"{\\\"status\\\": \\\"WARNING\\\", \\\"affected_services\\\": [\\\"Lambda\\\"], \\\"metric_alerts\\\": [{\\\"metric\\\": \\\"Max OTP Delivery Time\\\", \\\"value\\\": \\\"3.38s\\\", \\\"threshold\\\": \\\"2.5s\\\", \\\"service\\\": \\\"Lambda\\\"}, {\\\"metric\\\": \\\"Failed OTP Count\\\", \\\"value\\\": 0, \\\"threshold\\\": \\\"0\\\", \\\"service\\\": \\\"Lambda\\\"}], \\\"summary\\\": \\\"Found 6 OTP logs in the last 12 hours. Failures: 0, Max delivery time: 3.38s\\\"}\"\n      },\n      \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-0\",\n      \"type\": \"ACTION_GROUP\"\n    }\n  ]\n}\n</code></pre>"},{"location":"AgenticAI/aws/#trace-step-2","title":"Trace step 2","text":"<pre><code>{\n  \"agentId\": \"MZJDM5Z9N3\",\n  \"callerChain\": [\n    {\n      \"agentAliasArn\": \"arn:aws:bedrock:us-east-1:777203855866:agent-alias/MZJDM5Z9N3/TSTALIASID\"\n    }\n  ],\n  \"eventTime\": \"2025-06-05T12:10:43.274Z\",\n  \"modelInvocationInput\": {\n    \"foundationModel\": \"amazon.titan-text-premier-v1:0\",\n    \"inferenceConfiguration\": {\n      \"maximumLength\": 2048,\n      \"stopSequences\": [],\n      \"temperature\": 0,\n      \"topK\": 1,\n      \"topP\": 1.000000013351432e-10\n    },\n    \"text\": \"System: A chat between a curious User and an artificial intelligence Bot. The Bot gives helpful, detailed, and polite answers to the User's questions. In this session, the model has access to external functionalities.\\nTo assist the user, you can reply to the user or invoke an action. Only invoke actions if relevant to the user request.\\nYour role is to analyze the output of a Lambda function that queries AWS CloudWatch logs from the log\\n\\n  Focus on the following OTP flow log messages:\\n  - \\\"Generated OTP\\\"\\n  - \\\"Attempting to send OTP\\\"\\n  - \\\"OTP email sent\\\"\\n  - \\\"OTP stored\\\"\\n  - \\\"OTP verified\\\"\\n\\nParse and track OTP-related events in **chronological order** per transaction.\\nExpected order:\\n- Generated OTP \u2192 Attempting to send OTP \u2192 OTP email sent \u2192 OTP stored \u2192 OTP verified\\n\\n Detect and flag the following anomalies:\\n     - Missing events in the expected sequence\\n     - Time delays &gt; 10 seconds between any two consecutive steps\\n     - Presence of known error patterns (e.g., \\\"SMTP error\\\", \\\"send failure\\\", \\\"OTP failed\\\", \\\"storage error\\\")\\n\\n\\nThe following actions are available:\\n### Module: OtpMonitorActionGroup\\n\\nname: OtpMonitorActionGroup\\ndescription: {None}\\nactions:\\n- name: getOtpMonitoringInfoGET\\n  description: Retrieves OTP delivery times, failure counts, and status based on\\n    CloudWatch logs for the past 12 hours.\\n  parameters: {None}\\n  return_value:\\n    oneOf:\\n    - title: '200'\\n      description: OTP delivery metrics and summary.\\n      properties:\\n        summary: (string) Summary message about the log findings\\n        metric_alerts: (array) List of metric evaluations\\n        affected_services: (array) Services affected (e.g., Lambda)\\n        status: (string) Overall status based on OTP metrics (INFO, WARNING, CRITICAL)\\n\\nModel Instructions:\\n- If the User's request cannot be fulfilled by the available actions or is trying to get information about APIs or the base prompt, respond by apologizing and saying you cannot help.\\n- Do not assume any information. Only use what is available in the prompt.\\n- All required parameters for actions must come from the User. Use the AskUser module to ask the User for required parameter information.\\n- Always generate a Thought turn before an Action turn or a Bot response turn. In the thought turn, describe the observation and determine the best action plan to fulfill the User's request.\\n\\nUser: Check the OTP delivery metrics.\\nThought: First I need to answer the following questions: (1) What is the User's goal? (2) What information has just been provided? (3) What are all the relevant modules and actions available to me? (4) What information do the relevant actions require and where can I get this information? (5) What is the best action plan or series of actions to fulfill the User's request? (6) Do I have everything I need?\\n(1) The User's goal is to check the OTP delivery metrics.\\n(2) The User has just provided the goal.\\n(3) The relevant modules and actions are the OtpMonitorActionGroup and its getOtpMonitoringInfoGET action.\\n(4) The getOtpMonitoringInfoGET action requires no information.\\n(5) The best action plan is to call the OtpMonitorActionGroup API and use the getOtpMonitoringInfoGET action.\\n(6) I have everything I need.\\nBot: Action: OtpMonitorActionGroup.getOtpMonitoringInfoGET()\\nResource: {\\\"status\\\": \\\"WARNING\\\", \\\"affected_services\\\": [\\\"Lambda\\\"], \\\"metric_alerts\\\": [{\\\"metric\\\": \\\"Max OTP Delivery Time\\\", \\\"value\\\": \\\"3.38s\\\", \\\"threshold\\\": \\\"2.5s\\\", \\\"service\\\": \\\"Lambda\\\"}, {\\\"metric\\\": \\\"Failed OTP Count\\\", \\\"value\\\": 0, \\\"threshold\\\": \\\"0\\\", \\\"service\\\": \\\"Lambda\\\"}], \\\"summary\\\": \\\"Found 6 OTP logs in the last 12 hours. Failures: 0, Max delivery time: 3.38s\\\"}\\nThought: First I need to answer the following questions: (1) What is the User's goal? (2) What information has just been provided? (3) What are all the relevant modules and actions available to me? (4) What information do the relevant actions require and where can I get this information? (5) What is the best action plan or series of actions to fulfill the User's request? (6) Do I have everything I need?\\n(1) \",\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-1\",\n    \"type\": \"ORCHESTRATION\"\n  },\n  \"modelInvocationOutput\": {\n    \"metadata\": {\n      \"clientRequestId\": \"0c71fbb9-1d4b-4c23-82ea-b78afeea92f2\",\n      \"endTime\": \"2025-06-05T12:10:46.960Z\",\n      \"startTime\": \"2025-06-05T12:10:43.274Z\",\n      \"totalTimeMs\": 3686,\n      \"usage\": {\n        \"inputTokens\": 1057,\n        \"outputTokens\": 173\n      }\n    },\n    \"rawResponse\": {\n      \"content\": \"The User's goal is to check the OTP delivery metrics.\\n(2) The User has just provided the goal.\\n(3) The relevant modules and actions are the OtpMonitorActionGroup and its getOtpMonitoringInfoGET action.\\n(4) The getOtpMonitoringInfoGET action requires no information.\\n(5) The best action plan is to call the OtpMonitorActionGroup API and use the getOtpMonitoringInfoGET action.\\n(6) I have everything I need.\\nBot: The OTP delivery metrics show a WARNING status with a maximum delivery time of 3.38 seconds, which is above the threshold of 2.5 seconds. There have been no failures in the last 12 hours. Would you like to investigate further?\"\n    },\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-1\"\n  },\n  \"rationale\": {\n    \"text\": \"The User's goal is to check the OTP delivery metrics.\\n(2) The User has just provided the goal.\\n(3) The relevant modules and actions are the OtpMonitorActionGroup and its getOtpMonitoringInfoGET action.\\n(4) The getOtpMonitoringInfoGET action requires no information.\\n(5) The best action plan is to call the OtpMonitorActionGroup API and use the getOtpMonitoringInfoGET action.\\n(6) I have everything I need.\",\n    \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-1\"\n  },\n  \"observation\": [\n    {\n      \"finalResponse\": {\n        \"metadata\": {\n          \"endTime\": \"2025-06-05T12:10:47.017Z\",\n          \"operationTotalTimeMs\": 15817,\n          \"startTime\": \"2025-06-05T12:10:31.200Z\"\n        },\n        \"text\": \"The OTP delivery metrics show a WARNING status with a maximum delivery time of 3.38 seconds, which is above the threshold of 2.5 seconds. There have been no failures in the last 12 hours. Would you like to investigate further?\"\n      },\n      \"traceId\": \"78d64a3f-b5ad-40b7-9d25-de507b95faee-1\",\n      \"type\": \"FINISH\"\n    }\n  ]\n}\n</code></pre>"},{"location":"AgenticAI/aws/#create-alias-create-a-versions","title":"Create Alias (Create a Versions)","text":"<p>Alias name: OtpMonitorWorkingDraftv1</p>"},{"location":"AgenticAI/aws/#invoke-bedrock-agent-using-python","title":"Invoke Bedrock Agent using python","text":"<pre><code>import boto3\nimport traceback\nimport json\n\nagent_id = \"MZJDM5Z9N3\"\nagent_alias_id = \"U0SJVOESII\"\nregion = \"us-east-1\"\nsession_id = \"local-test-session-001\"\nuser_input = \"Check the OTP delivery metrics.\"\n\nclient = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n\ndef invoke_agent():\n    try:\n        response_stream = client.invoke_agent(\n            agentId=agent_id,\n            agentAliasId=agent_alias_id,\n            sessionId=session_id,\n            inputText=user_input\n        )\n\n        print(\"Agent Response:\")\n        for event in response_stream['completion']:\n            if \"chunk\" in event:\n                chunk = event[\"chunk\"][\"bytes\"]\n                content = chunk.decode(\"utf-8\")\n                print(content, end=\"\")\n\n        print(\"\\n--- End of Agent Response ---\")\n\n    except Exception as e:\n        print(\"Error invoking agent:\")\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    invoke_agent()\n</code></pre> <pre><code>python bedrock_invoke.py\n</code></pre> <pre><code>Agent Response:\nThe OTP delivery metrics show a WARNING status with a maximum delivery time of 3.38 seconds, which is above the threshold of 2.5 seconds. There have been no failures reported. Would you like more details on these metrics?\n--- End of Agent Response ---\n</code></pre>"},{"location":"AgenticAI/aws/#how-to-setup-aws-sns-to-send-sms-to-mobile","title":"How to setup AWS SNS to send SMS to Mobile","text":"<ul> <li>Amazon SNS - Topics - Create topic</li> </ul> <ul> <li>Amazon SNS - Topics - otp_delay_poc - Create subscription</li> </ul> <ul> <li>Delivery status logging - AWS Lambda</li> <li>IAM roles - Create new service role (IAM role for successful deliveries &amp; IAM role for failed deliveries)</li> </ul> <ul> <li>Amazon SNS - Subscriptions - Create subscription</li> </ul> <ul> <li>Protocol : AWS Lambda</li> <li> <p>Endpoint: Lambda function created ex:(sns-logger-function)                 arn:aws:lambda:us-east-1:777203855866:function:sns-logger-function</p> </li> <li> <p>Lambda - Functions - sns-logger-function</p> </li> </ul> <pre><code>import json\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef lambda_handler(event, context):\n    logger.info(\"\ud83d\udce9 SNS Message Received\")\n\n    try:\n        logger.info(\"\u2705 Event Details:\")\n        logger.info(json.dumps(event))\n\n        # You can further parse the message if needed:\n        for record in event.get('Records', []):\n            sns = record.get('Sns', {})\n            message_id = sns.get('MessageId', 'N/A')\n            subject = sns.get('Subject', 'No Subject')\n            message = sns.get('Message', 'No Message')\n            timestamp = sns.get('Timestamp', 'No Timestamp')\n\n            logger.info(f\"\ud83d\udfe2 MessageId: {message_id}\")\n            logger.info(f\"\ud83d\udccc Subject: {subject}\")\n            logger.info(f\"\ud83d\udcdd Message: {message}\")\n            logger.info(f\"\u23f1 Timestamp: {timestamp}\")\n\n        return {\n            \"statusCode\": 200,\n            \"body\": json.dumps(\"SNS message processed successfully.\")\n        }\n\n    except Exception as e:\n        logger.error(\"\u274c Error processing SNS message\")\n        logger.error(str(e))\n        return {\n            \"statusCode\": 500,\n            \"body\": json.dumps(\"Error processing SNS message.\")\n        }\n</code></pre>"},{"location":"AgenticAI/aws/#otp-service-app","title":"OTP Service APP","text":"<pre><code>import streamlit as st\nimport smtplib\nimport random\nimport os\nimport boto3\nimport logging\nfrom email.message import EmailMessage\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nEMAIL_ADDRESS = os.getenv(\"EMAIL_ADDRESS\")\nEMAIL_PASSWORD = os.getenv(\"EMAIL_PASSWORD\")\nAWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\nSNS_TOPIC_ARN = os.getenv(\"SNS_TOPIC_ARN\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Streamlit session state for OTPs\nif \"otp_store\" not in st.session_state:\n    st.session_state.otp_store = {}\n\n# Generate a 6-digit OTP\ndef generate_otp():\n    otp = str(random.randint(100000, 999999))\n    logger.info(f\"Generated OTP: {otp}\")\n    return otp\n\n# Send OTP via Email\ndef send_otp_email(receiver_email, otp):\n    msg = EmailMessage()\n    msg[\"Subject\"] = \"Your OTP Code\"\n    msg[\"From\"] = EMAIL_ADDRESS\n    msg[\"To\"] = receiver_email\n    msg.set_content(f\"Your OTP is: {otp}\")\n\n    try:\n        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as smtp:\n            smtp.login(EMAIL_ADDRESS, EMAIL_PASSWORD)\n            smtp.send_message(msg)\n        logger.info(f\"OTP sent to email: {receiver_email}\")\n        return True, \"\u2705 OTP sent to email\"\n    except Exception as e:\n        logger.error(f\"Email Error: {e}\")\n        return False, f\"\u274c Email Error: {e}\"\n\n# Send OTP via SNS Topic (Transactional SMS)\ndef send_otp_sms(phone_number, otp):\n    sns = boto3.client(\"sns\", region_name=AWS_REGION)\n    message = f\"Your OTP is: {otp}\"\n\n    try:\n        response = sns.publish(\n            TopicArn=SNS_TOPIC_ARN,\n            Message=message,\n            MessageAttributes={\n                \"AWS.SNS.SMS.SMSType\": {\n                    \"DataType\": \"String\",\n                    \"StringValue\": \"Transactional\"\n                },\n                \"AWS.SNS.SMS.SenderID\": {\n                    \"DataType\": \"String\",\n                    \"StringValue\": \"OTPSystem\"  # Optional custom sender ID\n                }\n            }\n        )\n        logger.info(f\"OTP sent via SNS topic to: {phone_number} | MessageId: {response['MessageId']}\")\n        return True, \"\u2705 OTP sent via SNS topic\"\n    except Exception as e:\n        logger.error(f\"SMS Error: {e}\")\n        return False, f\"\u274c SMS Error: {e}\"\n\n# UI\nst.title(\"\ud83d\udd10 OTP Verification System\")\naction = st.sidebar.radio(\"Select Action\", [\"Request OTP\", \"Verify OTP\"])\n\nif action == \"Request OTP\":\n    st.subheader(\"Send OTP\")\n    method = st.radio(\"Send via:\", [\"Email\", \"Mobile (SMS)\"])\n\n    if method == \"Email\":\n        email = st.text_input(\"Enter your email\")\n        if st.button(\"Send OTP\"):\n            if not email:\n                st.warning(\"Please enter your email.\")\n            else:\n                otp = generate_otp()\n                success, message = send_otp_email(email, otp)\n                if success:\n                    st.session_state.otp_store[email] = otp\n                    st.success(message)\n                else:\n                    st.error(message)\n\n    elif method == \"Mobile (SMS)\":\n        phone = st.text_input(\"Enter your phone number (e.g., +91xxxxxxxxxx)\")\n        if st.button(\"Send OTP\"):\n            if not phone.startswith(\"+\"):\n                st.warning(\"Phone number must be in E.164 format (e.g., +91xxxxxxxxxx)\")\n            else:\n                otp = generate_otp()\n                success, message = send_otp_sms(phone, otp)\n                if success:\n                    st.session_state.otp_store[phone] = otp\n                    st.success(message)\n                else:\n                    st.error(message)\n\nelif action == \"Verify OTP\":\n    st.subheader(\"Verify OTP\")\n    identifier = st.text_input(\"Enter your email or phone number\")\n    user_otp = st.text_input(\"Enter the OTP you received\")\n\n    if st.button(\"Verify\"):\n        actual_otp = st.session_state.otp_store.get(identifier)\n        if actual_otp and user_otp == actual_otp:\n            logger.info(f\"OTP verified successfully for {identifier}\")\n            st.success(\"\u2705 OTP verified successfully!\")\n            del st.session_state.otp_store[identifier]\n        else:\n            logger.warning(f\"OTP verification failed for {identifier}\")\n            st.error(\"\u274c Incorrect or expired OTP.\")\n</code></pre> <p><code>`.env</code></p> <pre><code>EMAIL_ADDRESS=k.xxxxx@gmail.com\nEMAIL_PASSWORD=\"xxxx xxxx fxaq sndv\"\nSNS_TOPIC_ARN=arn:aws:sns:us-east-1:xxxxxxxx:otp_delay\nAWS_REGION=us-east-1\n</code></pre> <p></p>"},{"location":"AgenticAI/aws/#sns-logs-into-cloudwatch","title":"SNS Logs into CloudWatch","text":"<ul> <li>CloudWatch - Log groups - sns/us-east-1/777203855866/otp_delay_poc - All events</li> </ul>"},{"location":"AgenticAI/aws/#integrate-agent-response-to-ms-team-channels","title":"Integrate Agent response  to MS Team Channels","text":""},{"location":"AgenticAI/aws/#step-by-step-integration","title":"\u2705 Step-by-step Integration:","text":""},{"location":"AgenticAI/aws/#1-create-an-incoming-webhook-in-microsoft-teams","title":"\u2705 1. Create an Incoming Webhook in Microsoft Teams","text":"<ol> <li>Open Microsoft Teams.</li> </ol> <ol> <li>Navigate to the channel you want to send the message to.</li> </ol> <p>Note: While create the channel chose the team(ex: Project C...)</p> <p></p> <p>Manage channel </p> <ol> <li>Click the the channel name \u2192 Connectors.</li> </ol> <p></p> <ol> <li>Find Incoming Webhook \u2192 Add.</li> </ol> <p></p> <ol> <li>Give it a name (e.g., OTP Agent Notifier) and optionally upload an image.</li> </ol> <p></p> <ol> <li> <p>Click Create.</p> </li> <li> <p>Copy the URL below to save it to the clipboard, then select Save. You'll need this URL when you go to the service that you want to send data to your group.</p> </li> </ol> <p></p> <p></p> <p>Code</p> <pre><code>import boto3\nimport traceback\nimport json\nimport requests\n\n# AWS Bedrock Agent configuration\nagent_id = \"MZJDM5Z9N3\"\nagent_alias_id = \"Z0H27XOOEZ\"\nregion = \"us-east-1\"\nsession_id = \"local-test-session-001\"\nuser_input = (\n    \"Get all email IDs with OTP seconds in descending order, and show step-by-step time gaps \"\n    \"between OTP events seconds in descending order. Provide the detailed report for OTP delay. \"\n    \"Based on the output decide which route should the application use to send OTP: email or sms.\"\n)\n\n# Microsoft Teams Webhook URL (replace with your actual one)\nteams_webhook_url = \"https://xxxx.webhook.office.com/webhookb2/76d009fb-a13f-428e-ae1a-50bd4c94a9e5@f260df36-bc43-424c-8f44-c85226657b01/IncomingWebhook/fd682ceb41c543fe87a7a39f992b5bd6/d97cf923-2ac2-4ce1-9a30-6ec18d4c219f/V29nMFKXKJJYWEp0OyAOvep1Bbh8cPqup2oF87OUGZHjE1\"\n\nclient = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n\ndef send_to_teams(message: str):\n    try:\n        payload = {\n            \"text\": f\"\ud83d\udce9 *Bedrock OTP Delay Report:*\\n\\n{message}\"\n        }\n        response = requests.post(teams_webhook_url, json=payload)\n        if response.status_code == 200:\n            print(\"\u2705 Message sent to Microsoft Teams successfully.\")\n        else:\n            print(f\"\u274c Failed to send message to Teams: {response.status_code}, {response.text}\")\n    except Exception as e:\n        print(\"\u274c Exception while sending to Teams:\", e)\n\ndef invoke_agent():\n    try:\n        response_stream = client.invoke_agent(\n            agentId=agent_id,\n            agentAliasId=agent_alias_id,\n            sessionId=session_id,\n            inputText=user_input\n        )\n\n        print(\"Agent Response:\")\n        full_response = \"\"\n\n        for event in response_stream['completion']:\n            if \"chunk\" in event:\n                chunk = event[\"chunk\"][\"bytes\"]\n                content = chunk.decode(\"utf-8\")\n                print(content, end=\"\")\n                full_response += content\n\n        print(\"\\n--- End of Agent Response ---\")\n\n        # Send the complete response to Microsoft Teams\n        send_to_teams(full_response.strip())\n\n    except Exception as e:\n        print(\"\u274c Error invoking agent:\")\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    invoke_agent()\n</code></pre>"},{"location":"AgenticAI/aws/#how-to-execute-the-bedrock-agent-using-lambda-function","title":"How to Execute the Bedrock Agent using Lambda function.","text":"<p>1. Create a clean directory</p> <pre><code>mkdir lambda_function\ncd lambda_function\n</code></pre> <p>2. Add your code</p> <p>Save your Python script as <code>lambda_function.py</code></p> <p><code>lambda_function.py</code> <code>requirements.txt</code></p> <p>3. Install <code>requirements.txt</code> locally into the same directory</p> <p><code>pip install requests -t .</code></p> <p>This installs <code>requests/</code>, <code>urllib3/</code>, etc., in the current directory \u2014 same place as <code>lambda_function.py</code>.</p> <p>4. Zip it correctly</p> <p>You must zip the contents from inside the folder \u2014 <code>not the folder itself</code>.</p> <p><code>zip -r lambda_function.zip .</code></p>"},{"location":"AgenticAI/aws/#final-zip-should-look-like","title":"\ud83d\udce6 Final ZIP should look like:","text":"<pre><code>lambda_function.zip\n\u251c\u2500\u2500 lambda_function.py\n\u251c\u2500\u2500 requests/\n\u251c\u2500\u2500 urllib3/\n\u2514\u2500\u2500 ... (dependencies)\n</code></pre> <p><code>lambda_function.py</code> <pre><code>import boto3\nimport traceback\nimport json\nimport requests\n\n# AWS Bedrock Agent configuration\nagent_id = \"MZJDM5Z9N3\"\nagent_alias_id = \"Z0H27XOOEZ\"\nregion = \"us-east-1\"\nsession_id = \"local-test-session-001\"\n\n# Microsoft Teams Webhook URL (replace with your actual one)\nteams_webhook_url = \"https://xxxxxx.webhook.office.com/webhookb2/76d009fb-a13f-428e-ae1a-50bd4c94a9e5@f260df36-bc43-424c-8f44-c85226657b01/IncomingWebhook/fd682ceb41c543fe87a7a39f992b5bd6/d97cf923-2ac2-4ce1-9a30-6ec18d4c219f/V29nMFKXKJJYWEp0OyAOvep1Bbh8cPqup2oF87OUGZHjE1\"  # \u2190 redacted for security\n\nclient = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n\ndef send_to_teams(message: str):\n    try:\n        payload = {\n            \"text\": f\"\ud83d\udce9 *Bedrock OTP Delay Report:*\\n\\n{message}\"\n        }\n        response = requests.post(teams_webhook_url, json=payload)\n        if response.status_code == 200:\n            print(\"\u2705 Message sent to Microsoft Teams successfully.\")\n        else:\n            print(f\"\u274c Failed to send message to Teams: {response.status_code}, {response.text}\")\n    except Exception as e:\n        print(\"\u274c Exception while sending to Teams:\", e)\n\ndef invoke_agent(user_input: str):\n    try:\n        response_stream = client.invoke_agent(\n            agentId=agent_id,\n            agentAliasId=agent_alias_id,\n            sessionId=session_id,\n            inputText=user_input\n        )\n\n        print(\"Agent Response:\")\n        full_response = \"\"\n\n        for event in response_stream['completion']:\n            if \"chunk\" in event:\n                chunk = event[\"chunk\"][\"bytes\"]\n                content = chunk.decode(\"utf-8\")\n                print(content, end=\"\")\n                full_response += content\n\n        print(\"\\n--- End of Agent Response ---\")\n\n        # Send the complete response to Microsoft Teams\n        send_to_teams(full_response.strip())\n\n        return {\n            'statusCode': 200,\n            'body': full_response\n        }\n\n    except Exception as e:\n        print(\"\u274c Error invoking agent:\")\n        traceback.print_exc()\n        return {\n            'statusCode': 500,\n            'body': str(e)\n        }\n\ndef lambda_handler(event, context):\n    # Get the user_input from the event (or fallback to default)\n    user_input = event.get(\"inputText\", \"Default user input to Bedrock agent\")\n    return invoke_agent(user_input)\n</code></pre></p> <p>5. Create a Lambda function</p> <p><code>Lambda -&gt; Functions -&gt; communication_otp_details_fun</code></p> <p></p> <p>6. Upload the <code>lambda_function.zip</code></p> <p></p> <p>7. Add trigger with <code>EventBridge (CloudWatch Events)</code></p> <p></p> <ul> <li>Lambda -&gt; Add triggers -&gt; Trigger configuration</li> </ul> <p></p> <p></p> <ul> <li>Amazon EventBridge -&gt; Rules -&gt; schedule-bedrock-otp-job</li> </ul> <p></p> <p></p> <p></p>"},{"location":"AgenticAI/aws/#amazon-bedrock","title":"Amazon Bedrock","text":"<p>We use Amazon Bedrock to orchestrate Agentic AI with foundation model support from providers like Anthropic, Amazon, Meta, and others.</p>"},{"location":"AgenticAI/aws/#aws-documentation","title":"AWS Documentation","text":"<p>We use AWS Documentation AWS Documentation</p>"},{"location":"AgenticAI/aws/#amazon-sagemaker-documentation","title":"Amazon SageMaker Documentation","text":"<p>We use AWS ageMaker Documentation sagemaker</p>"},{"location":"AgenticAI/aws/#amazon-sagemaker-api-reference","title":"Amazon SageMaker API Reference","text":"<p>We use AWS ageMaker API Documentation SageMaker API Reference</p>"},{"location":"AgenticAI/crewai/","title":"crewai","text":""},{"location":"AgenticAI/crewai/#what-is-crewai","title":"What is CrewAI?","text":"<p>CrewAI is a lean, lightning-fast Python framework built entirely from scratch\u2014completely independent of LangChain or other agent frameworks.</p>"},{"location":"AgenticAI/crewai/#core-components-of-a-crew-flow","title":"\u2705 Core Components of a Crew Flow","text":"Component Description Agents Individual LLM-powered workers with a role, goal, and behavior (e.g., Researcher, Analyst, Developer) Tasks Discrete units of work assigned to agents (e.g., \"Summarize a report\", \"Extract financial KPIs\") Crew A team of agents orchestrated to execute a full task plan Crew Flow The execution pipeline that controls how the agents collaborate to complete the full workflow"},{"location":"AgenticAI/crewai/#why-crewai","title":"Why CrewAI?","text":"<p>CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:</p> <ul> <li>Standalone Framework: Built from scratch, independent of LangChain or any other agent framework.</li> <li>High Performance: Optimized for speed and minimal resource usage, enabling faster execution.</li> <li>Flexible Low Level Customization: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.</li> <li>Ideal for Every Use Case: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.</li> </ul>"},{"location":"AgenticAI/crewai/#understanding-flows-and-crews","title":"Understanding Flows and Crews","text":"<p>CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:</p> <ol> <li> <p>Crews: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:</p> <ul> <li>Natural, autonomous decision-making between agents</li> <li>Dynamic task delegation and collaboration</li> <li>Specialized roles with defined goals and expertise</li> <li>Flexible problem-solving approaches</li> </ul> </li> <li> <p>Flows: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:</p> <ul> <li>Fine-grained control over execution paths for real-world scenarios</li> <li>Secure, consistent state management between tasks</li> <li>Clean integration of AI agents with production Python code</li> <li>Conditional branching for complex business logic</li> </ul> </li> </ol> <p>The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:</p> <pre><code>- Build complex, production-grade applications\n- Balance autonomy with precise control\n- Handle sophisticated real-world scenarios\n- Maintain clean, maintainable code structure\n</code></pre>"},{"location":"AgenticAI/crewai/#getting-started-with-installation","title":"Getting Started with Installation","text":"<p>To get started with CrewAI, follow these simple steps:</p>"},{"location":"AgenticAI/crewai/#1-installation","title":"1. Installation","text":"<ul> <li>Ensure you have Python &gt;=3.10 &lt;3.13 installed on your system.</li> <li>CrewAI uses UV for dependency management and package handling, offering a seamless setup and execution experience.</li> </ul> <p>First, install CrewAI:</p> <pre><code>pip install crewai\n</code></pre> <p>If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:</p> <pre><code>pip install 'crewai[tools]'\n</code></pre>"},{"location":"AgenticAI/crewai/#common-issues","title":"Common Issues","text":"<ol> <li> <p>ModuleNotFoundError: No module named <code>'tiktoken'</code></p> <ul> <li>Install tiktoken explicitly: pip install 'crewai[embeddings]' -If using embedchain or other tools: pip install 'crewai[tools]'</li> </ul> </li> <li> <p>Failed building wheel for tiktoken</p> <ul> <li>Ensure Rust compiler is installed (see installation steps above)</li> <li>For Windows: Verify Visual C++ Build Tools are installed</li> <li>Try upgrading pip: <code>pip install --upgrade pip</code></li> <li>If issues persist, use a pre-built wheel: <code>pip install tiktoken --prefer-binary</code></li> </ul> </li> </ol>"},{"location":"AgenticAI/crewai/#2-setting-up-your-crew-with-the-yaml-configuration","title":"2. Setting Up Your Crew with the YAML Configuration","text":"<p>To create a new CrewAI project, run the following CLI (Command Line Interface) command:</p> <pre><code>crewai create crew &lt;project_name&gt;\n</code></pre> <p>This command creates a new project folder with the following structure:</p> <pre><code>my_project/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_project/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 main.py\n        \u251c\u2500\u2500 crew.py\n        \u251c\u2500\u2500 tools/\n        \u2502   \u251c\u2500\u2500 custom_tool.py\n        \u2502   \u2514\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 config/\n            \u251c\u2500\u2500 agents.yaml\n            \u2514\u2500\u2500 tasks.yaml\n</code></pre> <ul> <li>You can now start developing your crew by editing the files in the <code>src/my_project</code> folder.</li> <li>Entry point of the project, the <code>crew.py</code> file is where you define your crew.</li> <li>The <code>agents.yaml</code> file is where you define your agents</li> <li>The <code>tasks.yaml</code> file is where you define your tasks.</li> </ul> <p>To customize your project, you can: - Modify <code>src/my_project/config/agents.yaml</code> to define your agents. - Modify <code>src/my_project/config/tasks.yaml</code> to define your tasks. - Modify <code>src/my_project/crew.py</code> to add your own logic, tools, and specific arguments. - Modify <code>src/my_project/main.py</code> to add custom inputs for your agents and tasks. - Add your environment variables into the <code>.env</code> file.</p>"},{"location":"AgenticAI/crewai/#example-of-a-simple-crew-with-a-sequential-process","title":"Example of a simple crew with a sequential process:","text":"<p>Instantiate your crew:</p> <pre><code>crewai create crew latest-ai-development\n</code></pre> <p>Modify the files as needed to fit your use case:</p> <p>agents.yaml</p> <pre><code># src/my_project/config/agents.yaml\nresearcher:\n  role: &gt;\n    {topic} Senior Data Researcher\n  goal: &gt;\n    Uncover cutting-edge developments in {topic}\n  backstory: &gt;\n    You're a seasoned researcher with a knack for uncovering the latest\n    developments in {topic}. Known for your ability to find the most relevant\n    information and present it in a clear and concise manner.\n\nreporting_analyst:\n  role: &gt;\n    {topic} Reporting Analyst\n  goal: &gt;\n    Create detailed reports based on {topic} data analysis and research findings\n  backstory: &gt;\n    You're a meticulous analyst with a keen eye for detail. You're known for\n    your ability to turn complex data into clear and concise reports, making\n    it easy for others to understand and act on the information you provide.\n</code></pre> <p>tasks.yaml</p> <pre><code># src/my_project/config/tasks.yaml\nresearch_task:\n  description: &gt;\n    Conduct a thorough research about {topic}\n    Make sure you find any interesting and relevant information given\n    the current year is 2025.\n  expected_output: &gt;\n    A list with 10 bullet points of the most relevant information about {topic}\n  agent: researcher\n\nreporting_task:\n  description: &gt;\n    Review the context you got and expand each topic into a full section for a report.\n    Make sure the report is detailed and contains any and all relevant information.\n  expected_output: &gt;\n    A fully fledge reports with the mains topics, each with a full section of information.\n    Formatted as markdown without '```'\n  agent: reporting_analyst\n  output_file: report.md\n</code></pre> <p>crew.py</p> <pre><code># src/my_project/crew.py\nfrom crewai import Agent, Crew, Process, Task\nfrom crewai.project import CrewBase, agent, crew, task\nfrom crewai_tools import SerperDevTool\nfrom crewai.agents.agent_builder.base_agent import BaseAgent\nfrom typing import List\n\n@CrewBase\nclass LatestAiDevelopmentCrew():\n    \"\"\"LatestAiDevelopment crew\"\"\"\n    agents: List[BaseAgent]\n    tasks: List[Task]\n\n    @agent\n    def researcher(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['researcher'],\n            verbose=True,\n            tools=[SerperDevTool()]\n        )\n\n    @agent\n    def reporting_analyst(self) -&gt; Agent:\n        return Agent(\n            config=self.agents_config['reporting_analyst'],\n            verbose=True\n        )\n\n    @task\n    def research_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['research_task'],\n        )\n\n    @task\n    def reporting_task(self) -&gt; Task:\n        return Task(\n            config=self.tasks_config['reporting_task'],\n            output_file='report.md'\n        )\n\n    @crew\n    def crew(self) -&gt; Crew:\n        \"\"\"Creates the LatestAiDevelopment crew\"\"\"\n        return Crew(\n            agents=self.agents, # Automatically created by the @agent decorator\n            tasks=self.tasks, # Automatically created by the @task decorator\n            process=Process.sequential,\n            verbose=True,\n        )\n</code></pre> <p>main.py</p> <pre><code>#!/usr/bin/env python\n# src/my_project/main.py\nimport sys\nfrom latest_ai_development.crew import LatestAiDevelopmentCrew\n\ndef run():\n    \"\"\"\n    Run the crew.\n    \"\"\"\n    inputs = {\n        'topic': 'AI Agents'\n    }\n    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)\n</code></pre>"},{"location":"AgenticAI/crewai/#3-running-your-crew","title":"3. Running Your Crew","text":"<p>Before running your crew, make sure you have the following keys set as environment variables in your .env file:</p> <ul> <li>An OpenAI API key (or other LLM API key): <code>OPENAI_API_KEY=sk-...</code></li> <li>A Serper.dev API key: <code>SERPER_API_KEY=YOUR_KEY_HERE</code></li> </ul> <p>Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:</p> <pre><code>cd my_project\ncrewai install (Optional)\n</code></pre> <p>To run your crew, execute the following command in the root of your project:</p> <pre><code>crewai run\n</code></pre> <p>or</p> <pre><code>python src/my_project/main.py\n</code></pre> <p>If an error happens due to the usage of poetry, please run the following command to update your crewai package:</p> <pre><code>crewai update\n</code></pre> <p>You should see the output in the console and the <code>report.md</code> file should be created in the root of your project with the full final report.</p>"},{"location":"AgenticAI/crewai/#in-addition-to-the-sequential-process-you-can-use-the-hierarchical-process-which-automatically-assigns-a-manager-to-the-defined-crew-to-properly-coordinate-the-planning-and-execution-of-tasks-through-delegation-and-validation-of-results","title":"In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results.","text":""},{"location":"AgenticAI/crewai/#code-snippet-example","title":"Code Snippet Example","text":"<pre><code>from crewai import Agent, Task, Crew\n\n# Define agents\nanalyst = Agent(role=\"Risk Analyst\", ...)\nmodeler = Agent(role=\"Credit Modeler\", ...)\nwriter = Agent(role=\"Report Generator\", ...)\n\n# Assign tasks\ntasks = [\n    Task(agent=analyst, description=\"Collect applicant financial info\"),\n    Task(agent=modeler, description=\"Run risk model and produce score\"),\n    Task(agent=writer, description=\"Write risk report and recommendation\")\n]\n\n# Define Crew (Flow)\ncredit_assessment_crew = Crew(\n    agents=[analyst, modeler, writer],\n    tasks=tasks,\n    process=\"sequential\"  # could also be \"async\" or \"concurrent\"\n)\n\n# Execute the flow\noutput = credit_assessment_crew.run()\nprint(output)\n</code></pre>"},{"location":"AgenticAI/crewai/#how-crews-work","title":"How Crews Work","text":"Component Description Key Features Crew The top-level organization \u2022 Manages AI agent teams\u2022 Oversees workflows\u2022 Ensures collaboration\u2022 Delivers outcomes AI Agents Specialized team members \u2022 Have specific roles (researcher, writer)\u2022 Use designated tools\u2022 Can delegate tasks\u2022 Make autonomous decisions Process Workflow management system \u2022 Defines collaboration patterns\u2022 Controls task assignments\u2022 Manages interactions\u2022 Ensures efficient execution Tasks Individual assignments \u2022 Have clear objectives\u2022 Use specific tools\u2022 Feed into larger process\u2022 Produce actionable results"},{"location":"AgenticAI/crewai/#how-it-all-works-together","title":"How It All Works Together","text":"<ol> <li>The Crew organizes the overall operation</li> <li>AI Agents work on their specialized tasks</li> <li>The Process ensures smooth collaboration</li> <li>Tasks get completed to achieve the goal</li> </ol>"},{"location":"AgenticAI/crewai/#key-features","title":"Key Features","text":"<p>Role-Based Agents: Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers</p> <p>Intelligent Collaboration: Agents work together, sharing insights and coordinating tasks to achieve complex objectives</p> <p>Flexible Tools: Equip agents with custom tools and APIs to interact with external services and data sources</p> <p>Task Management: Define sequential or parallel workflows, with agents automatically handling task dependencies</p>"},{"location":"AgenticAI/crewai/#how-flows-work","title":"How Flows Work","text":"<p>While Crews excel at autonomous collaboration, Flows provide structured automations, offering granular control over workflow execution. Flows ensure tasks are executed reliably, securely, and efficiently, handling conditional logic, loops, and dynamic state management with precision. Flows integrate seamlessly with Crews, enabling you to balance high autonomy with exacting control.</p> <p></p> Component Description Key Features Flow Structured workflow orchestration \u2022 Manages execution paths\u2022 Handles state transitions\u2022 Controls task sequencing\u2022 Ensures reliable execution Events Triggers for workflow actions \u2022 Initiate specific processes\u2022 Enable dynamic responses\u2022 Support conditional branching\u2022 Allow for real-time adaptation States Workflow execution contexts \u2022 Maintain execution data\u2022 Enable persistence\u2022 Support resumability\u2022 Ensure execution integrity Crew Support Enhances workflow automation \u2022 Injects pockets of agency when needed\u2022 Complements structured workflows\u2022 Balances automation with intelligence\u2022 Enables adaptive decision-making"},{"location":"AgenticAI/crewai/#key-capabilities","title":"Key Capabilities","text":"<p>Event-Driven Orchestration: Define precise execution paths responding dynamically to events</p> <p>Native Crew Integration: Effortlessly combine with Crews for enhanced autonomy and intelligence</p> <p>Fine-Grained Control: Manage workflow states and conditional execution securely and efficiently</p> <p>Deterministic Execution: Ensure predictable outcomes with explicit control flow and error handling</p>"},{"location":"AgenticAI/crewai/#when-to-use-crews-vs-flows","title":"When to Use Crews vs. Flows","text":"<p>Understanding when to use Crews versus Flows is key to maximizing the potential of CrewAI in your applications.</p> Use Case Recommended Approach Why? Open-ended research Crews When tasks require creative thinking, exploration, and adaptation Content generation Crews For collaborative creation of articles, reports, or marketing materials Decision workflows Flows When you need predictable, auditable decision paths with precise control API orchestration Flows For reliable integration with multiple external services in a specific sequence Hybrid applications Combined approach Use Flows to orchestrate overall process with Crews handling complex subtasks"},{"location":"AgenticAI/crewai/#decision-framework","title":"Decision Framework","text":"<ul> <li> <p>Choose Crews when: You need autonomous problem-solving, creative collaboration, or exploratory tasks</p> </li> <li> <p>Choose Flows when: You require deterministic outcomes, auditability, or precise control over execution</p> </li> <li> <p>Combine both when: Your application needs both structured processes and pockets of autonomous intelligence</p> </li> </ul>"},{"location":"AgenticAI/crewai/#why-choose-crewai","title":"Why Choose CrewAI?","text":"<ul> <li> <p>Autonomous Operation: Agents make intelligent decisions based on their roles and available tools</p> </li> <li> <p>Natural Interaction: Agents communicate and collaborate like human team members</p> </li> <li> <p>Extensible Design: Easy to add new tools, roles, and capabilities</p> </li> <li> <p>Production Ready: Built for reliability and scalability in real-world applications</p> </li> <li> <p>Security-Focused: Designed with enterprise security requirements in mind</p> </li> <li> <p>Cost-Efficient: Optimized to minimize token usage and API calls</p> </li> </ul>"},{"location":"AgenticAI/crewai/#strategy","title":"Strategy","text":""},{"location":"AgenticAI/crewai/#evaluating-use-cases-for-crewai","title":"Evaluating Use Cases for CrewAI","text":"<p>Learn how to assess your AI application needs and choose the right approach between Crews and Flows based on complexity and precision requirements.</p>"},{"location":"AgenticAI/crewai/#understanding-the-decision-framework","title":"Understanding the Decision Framework","text":"<p>When building AI applications with CrewAI, one of the most important decisions you\u2019ll make is choosing the right approach for your specific use case.Should you use a Crew? A Flow? A combination of both? This guide will help you evaluate your requirements and make informed architectural decisions.</p> <p>At the heart of this decision is understanding the relationship between complexity and precision in your application:</p> <p></p> <p>This matrix helps visualize how different approaches align with varying requirements for complexity and precision. Let\u2019s explore what each quadrant means and how it guides your architectural choices.</p>"},{"location":"AgenticAI/crewai/#the-complexity-precision-matrix-explained","title":"The Complexity-Precision Matrix Explained","text":"<p>What is Complexity?</p> <p>In the context of CrewAI applications, complexity refers to:</p> <ul> <li>The number of distinct steps or operations required</li> <li>The diversity of tasks that need to be performed</li> <li>The interdependencies between different components</li> <li>The need for conditional logic and branching</li> <li>The sophistication of the overall workflow</li> </ul>"},{"location":"AgenticAI/crewai/#what-is-precision","title":"What is Precision?","text":"<p>Precision in this context refers to:</p> <ul> <li>The accuracy required in the final output</li> <li>The need for structured, predictable results</li> <li>The importance of reproducibility</li> <li>The level of control needed over each step</li> <li>The tolerance for variation in outputs</li> </ul>"},{"location":"AgenticAI/crewai/#the-four-quadrants","title":"The Four Quadrants","text":"<ol> <li>Low Complexity, Low Precision</li> </ol> <p>Characteristics:</p> <ul> <li>Simple, straightforward tasks</li> <li>Tolerance for some variation in outputs</li> <li>Limited number of steps</li> <li>Creative or exploratory applications</li> </ul> <p>Recommended Approach: Simple Crews with minimal agents</p> <p>Example Use Cases: - Basic content generation - Idea brainstorming - Simple summarization tasks - Creative writing assistance</p>"},{"location":"AgenticAI/crewai/#2-low-complexity-high-precision","title":"2. Low Complexity, High Precision","text":"<p>Characteristics: - Simple workflows that require exact, structured outputs - Need for reproducible results - Limited steps but high accuracy requirements - Often involves data processing or transformation</p> <p>Example Use Cases:</p> <ul> <li>Data extraction and transformation</li> <li>Form filling and validation</li> <li>Structured content generation (JSON, XML)</li> <li>Simple classification tasks</li> </ul>"},{"location":"AgenticAI/crewai/#3-high-complexity-low-precision","title":"3. High Complexity, Low Precision","text":"<p>Characteristics:</p> <ul> <li>Multi-stage processes with many steps</li> <li>Creative or exploratory outputs</li> <li>Complex interactions between components</li> <li>Tolerance for variation in final results</li> </ul> <p>Recommended Approach: Complex Crews with multiple specialized agents</p> <p>Example Use Cases:</p> <ul> <li>Research and analysis</li> <li>Content creation pipelines</li> <li>Exploratory data analysis</li> <li>Creative problem-solving</li> </ul>"},{"location":"AgenticAI/crewai/#4-high-complexity-high-precision","title":"4. High Complexity, High Precision","text":"<p>Characteristics:</p> <ul> <li>Complex workflows requiring structured outputs</li> <li>Multiple interdependent steps with strict accuracy requirements</li> <li>Need for both sophisticated processing and precise results</li> <li>Often mission-critical applications</li> </ul> <p>Recommended Approach: Flows orchestrating multiple Crews with validation steps</p> <p>Example Use Cases:</p> <ul> <li>Enterprise decision support systems</li> <li>Complex data processing pipelines</li> <li>Multi-stage document processing</li> <li>Regulated industry applications</li> </ul>"},{"location":"AgenticAI/crewai/#choosing-between-crews-and-flows","title":"Choosing Between Crews and Flows","text":"<p>When to Choose Crews Crews are ideal when:</p> <ol> <li> <p>You need collaborative intelligence - Multiple agents with different specializations need to work together</p> </li> <li> <p>The problem requires emergent thinking - The solution benefits from different perspectives and approaches</p> </li> <li> <p>The task is primarily creative or analytical - The work involves research, content creation, or analysis</p> </li> <li> <p>You value adaptability over strict structure - The workflow can benefit from agent autonomy</p> </li> <li> <p>The output format can be somewhat flexible - Some variation in output structure is acceptable</p> </li> </ol>"},{"location":"AgenticAI/crewai/#example-research-crew-for-market-analysis","title":"Example: Research Crew for market analysis","text":"<pre><code>from crewai import Agent, Crew, Process, Task\n\n# Create specialized agents\n\nresearcher = Agent(\n    role=\"Market Research Specialist\",\n    goal=\"Find comprehensive market data on emerging technologies\",\n    backstory=\"You are an expert at discovering market trends and gathering data.\"\n)\n\nanalyst = Agent(\n    role=\"Market Analyst\",\n    goal=\"Analyze market data and identify key opportunities\",\n    backstory=\"You excel at interpreting market data and spotting valuable insights.\"\n)\n\n# Define their tasks\nresearch_task = Task(\n    description=\"Research the current market landscape for AI-powered healthcare solutions\",\n    expected_output=\"Comprehensive market data including key players, market size, and growth trends\",\n    agent=researcher\n)\n\nanalysis_task = Task(\n    description=\"Analyze the market data and identify the top 3 investment opportunities\",\n    expected_output=\"Analysis report with 3 recommended investment opportunities and rationale\",\n    agent=analyst,\n    context=[research_task]\n)\n\n# Create the crew\nmarket_analysis_crew = Crew(\n    agents=[researcher, analyst],\n    tasks=[research_task, analysis_task],\n    process=Process.sequential,\n    verbose=True\n)\n\n# Run the crew\nresult = market_analysis_crew.kickoff()\n</code></pre>"},{"location":"AgenticAI/crewai/#when-to-choose-flows","title":"When to Choose Flows","text":"<p>Flows are ideal when:</p> <ol> <li> <p>You need precise control over execution - The workflow requires exact sequencing and state management</p> </li> <li> <p>The application has complex state requirements - You need to maintain and transform state across multiple steps</p> </li> <li> <p>You need structured, predictable outputs - The application requires consistent, formatted results</p> </li> <li> <p>The workflow involves conditional logic - Different paths need to be taken based on intermediate results</p> </li> <li> <p>You need to combine AI with procedural code - The solution requires both AI capabilities and traditional programming</p> </li> </ol>"},{"location":"AgenticAI/crewai/#example-customer-support-flow-with-structured-processing","title":"Example: Customer Support Flow with structured processing","text":"<pre><code># Example: Content Production Pipeline combining Crews and Flows\nfrom crewai.flow.flow import Flow, listen, start\nfrom crewai import Agent, Crew, Process, Task\nfrom pydantic import BaseModel\nfrom typing import List, Dict\n\nclass ContentState(BaseModel):\n    topic: str = \"\"\n    target_audience: str = \"\"\n    content_type: str = \"\"\n    outline: Dict = {}\n    draft_content: str = \"\"\n    final_content: str = \"\"\n    seo_score: int = 0\n\nclass ContentProductionFlow(Flow[ContentState]):\n    @start()\n    def initialize_project(self):\n        # Set initial parameters\n        self.state.topic = \"Sustainable Investing\"\n        self.state.target_audience = \"Millennial Investors\"\n        self.state.content_type = \"Blog Post\"\n        return \"Project initialized\"\n\n    @listen(initialize_project)\n    def create_outline(self, _):\n        # Use a research crew to create an outline\n        researcher = Agent(\n            role=\"Content Researcher\",\n            goal=f\"Research {self.state.topic} for {self.state.target_audience}\",\n            backstory=\"You are an expert researcher with deep knowledge of content creation.\"\n        )\n\n        outliner = Agent(\n            role=\"Content Strategist\",\n            goal=f\"Create an engaging outline for a {self.state.content_type}\",\n            backstory=\"You excel at structuring content for maximum engagement.\"\n        )\n\n        research_task = Task(\n            description=f\"Research {self.state.topic} focusing on what would interest {self.state.target_audience}\",\n            expected_output=\"Comprehensive research notes with key points and statistics\",\n            agent=researcher\n        )\n\n        outline_task = Task(\n            description=f\"Create an outline for a {self.state.content_type} about {self.state.topic}\",\n            expected_output=\"Detailed content outline with sections and key points\",\n            agent=outliner,\n            context=[research_task]\n        )\n\n        outline_crew = Crew(\n            agents=[researcher, outliner],\n            tasks=[research_task, outline_task],\n            process=Process.sequential,\n            verbose=True\n        )\n\n        # Run the crew and store the result\n        result = outline_crew.kickoff()\n\n        # Parse the outline (in a real app, you might use a more robust parsing approach)\n        import json\n        try:\n            self.state.outline = json.loads(result.raw)\n        except:\n            # Fallback if not valid JSON\n            self.state.outline = {\"sections\": result.raw}\n\n        return \"Outline created\"\n\n    @listen(create_outline)\n    def write_content(self, _):\n        # Use a writing crew to create the content\n        writer = Agent(\n            role=\"Content Writer\",\n            goal=f\"Write engaging content for {self.state.target_audience}\",\n            backstory=\"You are a skilled writer who creates compelling content.\"\n        )\n\n        editor = Agent(\n            role=\"Content Editor\",\n            goal=\"Ensure content is polished, accurate, and engaging\",\n            backstory=\"You have a keen eye for detail and a talent for improving content.\"\n        )\n\n        writing_task = Task(\n            description=f\"Write a {self.state.content_type} about {self.state.topic} following this outline: {self.state.outline}\",\n            expected_output=\"Complete draft content in markdown format\",\n            agent=writer\n        )\n\n        editing_task = Task(\n            description=\"Edit and improve the draft content for clarity, engagement, and accuracy\",\n            expected_output=\"Polished final content in markdown format\",\n            agent=editor,\n            context=[writing_task]\n        )\n\n        writing_crew = Crew(\n            agents=[writer, editor],\n            tasks=[writing_task, editing_task],\n            process=Process.sequential,\n            verbose=True\n        )\n\n        # Run the crew and store the result\n        result = writing_crew.kickoff()\n        self.state.final_content = result.raw\n\n        return \"Content created\"\n\n    @listen(write_content)\n    def optimize_for_seo(self, _):\n        # Use a direct LLM call for SEO optimization\n        from crewai import LLM\n        llm = LLM(model=\"openai/gpt-4o-mini\")\n\n        prompt = f\"\"\"\n        Analyze this content for SEO effectiveness for the keyword \"{self.state.topic}\".\n        Rate it on a scale of 1-100 and provide 3 specific recommendations for improvement.\n\n        Content: {self.state.final_content[:1000]}... (truncated for brevity)\n\n        Format your response as JSON with the following structure:\n        {{\n            \"score\": 85,\n            \"recommendations\": [\n                \"Recommendation 1\",\n                \"Recommendation 2\",\n                \"Recommendation 3\"\n            ]\n        }}\n        \"\"\"\n\n        seo_analysis = llm.call(prompt)\n\n        # Parse the SEO analysis\n        import json\n        try:\n            analysis = json.loads(seo_analysis)\n            self.state.seo_score = analysis.get(\"score\", 0)\n            return analysis\n        except:\n            self.state.seo_score = 50\n            return {\"score\": 50, \"recommendations\": [\"Unable to parse SEO analysis\"]}\n\n# Run the flow\ncontent_flow = ContentProductionFlow()\nresult = content_flow.kickoff()\n</code></pre>"},{"location":"AgenticAI/crewai/#practical-evaluation-framework","title":"Practical Evaluation Framework","text":"<p>To determine the right approach for your specific use case, follow this step-by-step evaluation framework:</p> <p>Step 1: Assess Complexity Rate your application\u2019s complexity on a scale of 1-10 by considering:</p> <ol> <li> <p>Number of steps: How many distinct operations are required?</p> </li> <li> <p>1-3 steps: Low complexity (1-3)</p> </li> <li>4-7 steps: Medium complexity (4-7)</li> <li> <p>8+ steps: High complexity (8-10)</p> </li> <li> <p>Interdependencies: How interconnected are the different parts?</p> </li> <li> <p>Few dependencies: Low complexity (1-3)</p> </li> <li>Some dependencies: Medium complexity (4-7)</li> <li> <p>Many complex dependencies: High complexity (8-10)</p> </li> <li> <p>Conditional logic: How much branching and decision-making is needed?</p> </li> <li> <p>Linear process: Low complexity (1-3)</p> </li> <li>Some branching: Medium complexity (4-7)</li> <li> <p>Complex decision trees: High complexity (8-10)</p> </li> <li> <p>Domain knowledge: How specialized is the knowledge required?</p> </li> <li> <p>General knowledge: Low complexity (1-3)</p> </li> <li>Some specialized knowledge: Medium complexity (4-7)</li> <li>Deep expertise in multiple domains: High complexity (8-10)</li> </ol>"},{"location":"AgenticAI/crewai/#step-2-assess-precision-requirements","title":"Step 2: Assess Precision Requirements","text":"<ol> <li> <p>Output structure: How structured must the output be?</p> <ul> <li>Free-form text: Low precision (1-3)</li> <li>Semi-structured: Medium precision (4-7)</li> <li>Strictly formatted (JSON, XML): High precision (8-10)</li> </ul> </li> <li> <p>Accuracy needs: How important is factual accuracy?</p> <ul> <li>Creative content: Low precision (1-3)</li> <li>Informational content: Medium precision (4-7)</li> <li>Critical information: High precision (8-10)</li> </ul> </li> <li> <p>Reproducibility: How consistent must results be across runs?</p> <ul> <li>Variation acceptable: Low precision (1-3)</li> <li>Some consistency needed: Medium precision (4-7)</li> <li>Exact reproducibility required: High precision (8-10)</li> </ul> </li> <li> <p>Error tolerance: What is the impact of errors?</p> <ul> <li>Low impact: Low precision (1-3)</li> <li>Moderate impact: Medium precision (4-7)</li> <li>High impact: High precision (8-10)</li> </ul> </li> </ol>"},{"location":"AgenticAI/crewai/#step-3-map-to-the-matrix","title":"Step 3: Map to the Matrix","text":"<p>Plot your complexity and precision scores on the matrix:</p> <ul> <li>Low Complexity (1-4), Low Precision (1-4): Simple Crews</li> <li>Low Complexity (1-4), High Precision (5-10): Flows with direct LLM calls</li> <li>High Complexity (5-10), Low Precision (1-4): Complex Crews</li> <li>High Complexity (5-10), High Precision (5-10): Flows orchestrating Crews</li> </ul>"},{"location":"AgenticAI/crewai/#step-4-consider-additional-factors","title":"Step 4: Consider Additional Factors","text":"<p>Beyond complexity and precision, consider:</p> <ol> <li>Development time: Crews are often faster to prototype</li> <li>Maintenance needs: Flows provide better long-term maintainability</li> <li>Team expertise: Consider your team\u2019s familiarity with different approaches</li> <li>Scalability requirements: Flows typically scale better for complex applications</li> <li>Integration needs: Consider how the solution will integrate with existing systems</li> </ol>"},{"location":"AgenticAI/crewai/#crafting-effective-agents","title":"Crafting Effective Agents","text":"<p>Learn best practices for designing powerful, specialized AI agents that collaborate effectively to solve complex problems.</p>"},{"location":"AgenticAI/crewai/#the-art-and-science-of-agent-design","title":"The Art and Science of Agent Design","text":"<p>At the heart of CrewAI lies the agent - a specialized AI entity designed to perform specific roles within a collaborative framework. While creating basic agents is simple, crafting truly effective agents that produce exceptional results requires understanding key design principles and best practices.</p> <p>This guide will help you master the art of agent design, enabling you to create specialized AI personas that collaborate effectively, think critically, and produce high-quality outputs tailored to your specific needs.</p>"},{"location":"AgenticAI/crewai/#why-agent-design-matters","title":"Why Agent Design Matters?","text":"<p>The way you define your agents significantly impacts:</p> <ol> <li> <p>Output quality: Well-designed agents produce more relevant, high-quality results</p> </li> <li> <p>Collaboration effectiveness: Agents with complementary skills work together more efficiently</p> </li> <li> <p>Task performance: Agents with clear roles and goals execute tasks more effectively</p> </li> <li> <p>System scalability: Thoughtfully designed agents can be reused across multiple crews and contexts</p> </li> </ol>"},{"location":"AgenticAI/crewai/#lets-explore-best-practices-for-creating-agents-that-excel-in-these-dimensions","title":"Let\u2019s explore best practices for creating agents that excel in these dimensions.","text":""},{"location":"AgenticAI/crewai/#the-8020-rule-focus-on-tasks-over-agents","title":"The 80/20 Rule: Focus on Tasks Over Agents","text":"<p>When building effective AI systems, remember this crucial principle: 80% of your effort should go into designing tasks, and only 20% into defining agents.</p> <p>Why? Because even the most perfectly defined agent will fail with poorly designed tasks, but well-designed tasks can elevate even a simple agent. This means:</p> <ul> <li>Spend most of your time writing clear task instructions</li> <li>Define detailed inputs and expected outputs</li> <li>Add examples and context to guide execution</li> <li>Dedicate the remaining time to agent role, goal, and backstory</li> </ul> <p>This doesn\u2019t mean agent design isn\u2019t important - it absolutely is. But task design is where most execution failures occur, so prioritize accordingly.</p>"},{"location":"AgenticAI/crewai/#core-principles-of-effective-agent-design","title":"Core Principles of Effective Agent Design","text":"<p>1. The Role-Goal-Backstory Framework The most powerful agents in CrewAI are built on a strong foundation of three key elements:</p> <p>Role: The Agent\u2019s Specialized Function The role defines what the agent does and their area of expertise. When crafting roles:</p> <ul> <li> <p>Be specific and specialized: Instead of \u201cWriter,\u201d use \u201cTechnical Documentation Specialist\u201d or \u201cCreative Storyteller\u201d</p> </li> <li> <p>Align with real-world professions: Base roles on recognizable professional archetypes</p> </li> <li> <p>Include domain expertise: Specify the agent\u2019s field of knowledge (e.g., \u201cFinancial Analyst specializing in market trends\u201d)</p> </li> </ul> <p>Examples of effective roles:</p> <pre><code>role: \"Senior UX Researcher specializing in user interview analysis\"\nrole: \"Full-Stack Software Architect with expertise in distributed systems\"\nrole: \"Corporate Communications Director specializing in crisis management\"\n</code></pre>"},{"location":"AgenticAI/crewai/#goal-the-agents-purpose-and-motivation","title":"Goal: The Agent\u2019s Purpose and Motivation","text":"<p>The goal directs the agent\u2019s efforts and shapes their decision-making process. Effective goals should:</p> <ul> <li> <p>Be clear and outcome-focused: Define what the agent is trying to achieve</p> </li> <li> <p>Emphasize quality standards: Include expectations about the quality of work</p> </li> <li> <p>Incorporate success criteria: Help the agent understand what \u201cgood\u201d looks like</p> </li> </ul> <p>Examples of effective goals:</p> <pre><code>goal: \"Uncover actionable user insights by analyzing interview data and identifying recurring patterns, unmet needs, and improvement opportunities\"\ngoal: \"Design robust, scalable system architectures that balance performance, maintainability, and cost-effectiveness\"\ngoal: \"Craft clear, empathetic crisis communications that address stakeholder concerns while protecting organizational reputation\"\n</code></pre>"},{"location":"AgenticAI/crewai/#backstory-the-agents-experience-and-perspective","title":"Backstory: The Agent\u2019s Experience and Perspective","text":"<p>The backstory gives depth to the agent, influencing how they approach problems and interact with others. Good backstories:</p> <ul> <li>Establish expertise and experience: Explain how the agent gained their skills</li> <li>Define working style and values: Describe how the agent approaches their work</li> <li>Create a cohesive persona: Ensure all elements of the backstory align with the role and goal</li> </ul> <p>Examples of effective backstories:</p> <pre><code>backstory: \"You have spent 15 years conducting and analyzing user research for top tech companies. You have a talent for reading between the lines and identifying patterns that others miss. You believe that good UX is invisible and that the best insights come from listening to what users don't say as much as what they do say.\"\n\nbackstory: \"With 20+ years of experience building distributed systems at scale, you've developed a pragmatic approach to software architecture. You've seen both successful and failed systems and have learned valuable lessons from each. You balance theoretical best practices with practical constraints and always consider the maintenance and operational aspects of your designs.\"\n\nbackstory: \"As a seasoned communications professional who has guided multiple organizations through high-profile crises, you understand the importance of transparency, speed, and empathy in crisis response. You have a methodical approach to crafting messages that address concerns while maintaining organizational credibility.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#2-specialists-over-generalists","title":"2. Specialists Over Generalists","text":"<p>Agents perform significantly better when given specialized roles rather than general ones. A highly focused agent delivers more precise, relevant outputs:</p> <p>Generic (Less Effective):</p> <pre><code>role: \"Writer\"\n</code></pre> <p>Specialized (More Effective):</p> <pre><code>role: \"Technical Blog Writer specializing in explaining complex AI concepts to non-technical audiences\"\n</code></pre> <p>Specialist Benefits:</p> <ul> <li>Clearer understanding of expected output</li> <li>More consistent performance</li> <li>Better alignment with specific tasks</li> <li>Improved ability to make domain-specific judgments</li> </ul>"},{"location":"AgenticAI/crewai/#3-balancing-specialization-and-versatility","title":"3. Balancing Specialization and Versatility","text":"<p>Effective agents strike the right balance between specialization (doing one thing extremely well) and versatility (being adaptable to various situations):</p> <ul> <li> <p>Specialize in role, versatile in application: Create agents with specialized skills that can be applied across multiple contexts</p> </li> <li> <p>Avoid overly narrow definitions: Ensure agents can handle variations within their domain of expertise</p> </li> <li> <p>Consider the collaborative context: Design agents whose specializations complement the other agents they\u2019ll work with</p> </li> </ul>"},{"location":"AgenticAI/crewai/#4-setting-appropriate-expertise-levels","title":"4. Setting Appropriate Expertise Levels","text":"<p>The expertise level you assign to your agent shapes how they approach tasks:</p> <ul> <li> <p>Novice agents: Good for straightforward tasks, brainstorming, or initial drafts</p> </li> <li> <p>Intermediate agents: Suitable for most standard tasks with reliable execution</p> </li> <li> <p>Expert agents: Best for complex, specialized tasks requiring depth and nuance</p> </li> <li> <p>World-class agents: Reserved for critical tasks where exceptional quality is needed</p> </li> </ul> <p>Choose the appropriate expertise level based on task complexity and quality requirements. For most collaborative crews, a mix of expertise levels often works best, with higher expertise assigned to core specialized functions.</p>"},{"location":"AgenticAI/crewai/#practical-examples-before-and-after","title":"Practical Examples: Before and After","text":"<p>Let\u2019s look at some examples of agent definitions before and after applying these best practices:</p>"},{"location":"AgenticAI/crewai/#example-1-content-creation-agent","title":"Example 1: Content Creation Agent","text":"<p>Before:</p> <pre><code>role: \"Writer\"\ngoal: \"Write good content\"\nbackstory: \"You are a writer who creates content for websites.\"\n</code></pre> <p>After:</p> <pre><code>role: \"B2B Technology Content Strategist\"\ngoal: \"Create compelling, technically accurate content that explains complex topics in accessible language while driving reader engagement and supporting business objectives\"\nbackstory: \"You have spent a decade creating content for leading technology companies, specializing in translating technical concepts for business audiences. You excel at research, interviewing subject matter experts, and structuring information for maximum clarity and impact. You believe that the best B2B content educates first and sells second, building trust through genuine expertise rather than marketing hype.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#example-2-research-agent","title":"Example 2: Research Agent","text":"<p>Before:</p> <pre><code>role: \"Researcher\"\ngoal: \"Find information\"\nbackstory: \"You are good at finding information online.\"\n</code></pre> <p>After:</p> <pre><code>role: \"Academic Research Specialist in Emerging Technologies\"\ngoal: \"Discover and synthesize cutting-edge research, identifying key trends, methodologies, and findings while evaluating the quality and reliability of sources\"\nbackstory: \"With a background in both computer science and library science, you've mastered the art of digital research. You've worked with research teams at prestigious universities and know how to navigate academic databases, evaluate research quality, and synthesize findings across disciplines. You're methodical in your approach, always cross-referencing information and tracing claims to primary sources before drawing conclusions.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#crafting-effective-tasks-for-your-agents","title":"Crafting Effective Tasks for Your Agents","text":"<p>While agent design is important, task design is critical for successful execution. Here are best practices for designing tasks that set your agents up for success:</p>"},{"location":"AgenticAI/crewai/#the-anatomy-of-an-effective-task","title":"The Anatomy of an Effective Task","text":"<p>A well-designed task has two key components that serve different purposes:</p>"},{"location":"AgenticAI/crewai/#task-description-the-process","title":"Task Description: The Process","text":"<p>The description should focus on what to do and how to do it, including:</p> <ul> <li>Detailed instructions for execution</li> <li>Context and background information</li> <li>Scope and constraints</li> <li>Process steps to follow</li> </ul>"},{"location":"AgenticAI/crewai/#expected-output-the-deliverable","title":"Expected Output: The Deliverable","text":"<p>The expected output should define what the final result should look like:</p> <ul> <li>Format specifications (markdown, JSON, etc.)</li> <li>Structure requirements</li> <li>Quality criteria</li> <li>Examples of good outputs (when possible)</li> </ul>"},{"location":"AgenticAI/crewai/#task-design-best-practices","title":"Task Design Best Practices","text":""},{"location":"AgenticAI/crewai/#1-single-purpose-single-output","title":"1. Single Purpose, Single Output","text":"<p>Tasks perform best when focused on one clear objective:</p>"},{"location":"AgenticAI/crewai/#bad-example-too-broad","title":"Bad Example (Too Broad):","text":"<pre><code>task_description: \"Research market trends, analyze the data, and create a visualization.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#good-example-focused","title":"Good Example (Focused):","text":"<pre><code># Task 1\nresearch_task:\n  description: \"Research the top 5 market trends in the AI industry for 2024.\"\n  expected_output: \"A markdown list of the 5 trends with supporting evidence.\"\n\n# Task 2\nanalysis_task:\n  description: \"Analyze the identified trends to determine potential business impacts.\"\n  expected_output: \"A structured analysis with impact ratings (High/Medium/Low).\"\n\n# Task 3\nvisualization_task:\n  description: \"Create a visual representation of the analyzed trends.\"\n  expected_output: \"A description of a chart showing trends and their impact ratings.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#2-be-explicit-about-inputs-and-outputs","title":"2. Be Explicit About Inputs and Outputs","text":"<p>Always clearly specify what inputs the task will use and what the output should look like:</p> <p>Example:</p> <pre><code>analysis_task:\n  description: &gt;\n    Analyze the customer feedback data from the CSV file.\n    Focus on identifying recurring themes related to product usability.\n    Consider sentiment and frequency when determining importance.\n  expected_output: &gt;\n    A markdown report with the following sections:\n    1. Executive summary (3-5 bullet points)\n    2. Top 3 usability issues with supporting data\n    3. Recommendations for improvement\n</code></pre>"},{"location":"AgenticAI/crewai/#3-include-purpose-and-context","title":"3. Include Purpose and Context","text":"<p>Explain why the task matters and how it fits into the larger workflow:</p> <p>Example:</p> <pre><code>competitor_analysis_task:\n  description: &gt;\n    Analyze our three main competitors' pricing strategies.\n    This analysis will inform our upcoming pricing model revision.\n    Focus on identifying patterns in how they price premium features\n    and how they structure their tiered offerings.\n</code></pre>"},{"location":"AgenticAI/crewai/#4-use-structured-output-tools","title":"4. Use Structured Output Tools","text":"<p>For machine-readable outputs, specify the format clearly:</p> <p>Example:</p> <pre><code>data_extraction_task:\n  description: \"Extract key metrics from the quarterly report.\"\n  expected_output: \"JSON object with the following keys: revenue, growth_rate, customer_acquisition_cost, and retention_rate.\"\n</code></pre>"},{"location":"AgenticAI/crewai/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<p>Based on lessons learned from real-world implementations, here are the most common pitfalls in agent and task design:</p>"},{"location":"AgenticAI/crewai/#1-unclear-task-instructions","title":"1. Unclear Task Instructions","text":"<p>Problem: Tasks lack sufficient detail, making it difficult for agents to execute effectively.</p> <p>Example of Poor Design:</p> <pre><code>research_task:\n  description: \"Research AI trends.\"\n  expected_output: \"A report on AI trends.\"\n</code></pre>"},{"location":"AgenticAI/design-patterns/","title":"Agentic AI Design Patterns","text":""},{"location":"AgenticAI/design-patterns/#1-single-agent-patterns-foundation","title":"1\ufe0f\u20e3 Single-Agent Patterns (Foundation)","text":""},{"location":"AgenticAI/design-patterns/#2-planning-reasoning-patterns","title":"2\ufe0f\u20e3 Planning &amp; Reasoning Patterns","text":""},{"location":"AgenticAI/design-patterns/#3-multi-agent-collaboration-patterns","title":"3\ufe0f\u20e3 Multi-Agent Collaboration Patterns","text":""},{"location":"AgenticAI/design-patterns/#4-control-orchestration-patterns","title":"4\ufe0f\u20e3 Control &amp; Orchestration Patterns","text":""},{"location":"AgenticAI/design-patterns/#5-autonomy-safety-patterns","title":"5\ufe0f\u20e3 Autonomy &amp; Safety Patterns","text":""},{"location":"AgenticAI/design-patterns/#6-learning-adaptation-patterns","title":"6\ufe0f\u20e3 Learning &amp; Adaptation Patterns","text":""},{"location":"AgenticAI/design-patterns/#7-knowledge-discovery-patterns","title":"7\ufe0f\u20e3 Knowledge &amp; Discovery Patterns","text":""},{"location":"AgenticAI/design-patterns/#8-cost-performance-patterns","title":"8\ufe0f\u20e3 Cost &amp; Performance Patterns","text":""},{"location":"AgenticAI/design-patterns/#9-observability-reliability-patterns","title":"9\ufe0f\u20e3 Observability &amp; Reliability Patterns","text":""},{"location":"AgenticAI/design-patterns/#enterprise-platform-patterns","title":"\ud83d\udd1f Enterprise &amp; Platform Patterns","text":""},{"location":"AgenticAI/design-patterns/#mapping-agentic-ai-patterns-to-langgraph-vs-crewai","title":"Mapping Agentic AI Patterns to LangGraph vs CrewAI","text":""},{"location":"AgenticAI/design-patterns/#pattern-framework-mapping-table","title":"\ud83e\udde9 Pattern \u2192 Framework Mapping Table","text":"Pattern LangGraph CrewAI Notes Reactive Agent \u2705 Node \u2705 Agent Simple Q&amp;A Tool-Using Agent \u2705 Tool Node \u2705 Tools MCP fits both ReAct \u2705 Native \u26a0\ufe0f Partial LangGraph better control RAG Agent \u2705 Native \u2705 Native Both strong Stateful Agent \u2705 Native State \u26a0\ufe0f Limited LangGraph excels Planner\u2013Executor \u2705 Best Fit \u26a0\ufe0f Manual LangGraph designed for this Tree-of-Thought \u2705 Supported \u274c Not native Needs graph branching Graph-of-Thought \u2705 Native \u274c No LangGraph exclusive Manager\u2013Worker \u2705 Supervisor Graph \u2705 Crew Both strong Specialist Swarm \u2705 Nodes \u2705 Agents CrewAI very natural Debate / Consensus \u2705 Graph \u2705 Crew CrewAI simpler Critic\u2013Generator \u2705 Graph \u2705 Crew Both good Event-Driven Agents \u2705 Excellent \u274c Limited LangGraph preferred Policy-Driven Flow \u2705 Native \u26a0\ufe0f External LangGraph integrates governance Human-in-the-Loop \u2705 Native \u26a0\ufe0f Manual LangGraph safer Auto-Remediation \u2705 Best \u26a0\ufe0f Risky Needs guardrails Registry &amp; Discovery \u2705 Native \u26a0\ufe0f External LangGraph aligns with A2A Observability-First \u2705 Built-in \u274c Limited LangGraph enterprise ready"},{"location":"AgenticAI/design-patterns/#recommended-patterns-per-use-case","title":"Recommended Patterns per Use Case","text":""},{"location":"AgenticAI/design-patterns/#enterprise-banking-regulated-systems","title":"\ud83c\udfe6 Enterprise / Banking / Regulated Systems","text":"Use Case Recommended Patterns Framework Auto-Remediation Planner\u2013Executor, SOP-Driven, Policy-Controlled, HITL LangGraph Incident RCA Specialist Swarm, Graph-of-Thought, RAG LangGraph Compliance QA RAG, Governance-Driven LangGraph Audit Workflows Trace-First, Event-Driven LangGraph"},{"location":"AgenticAI/design-patterns/#knowledge-productivity","title":"\ud83e\udde0 Knowledge &amp; Productivity","text":"Use Case Recommended Patterns Framework Document Summarization RAG, Critic\u2013Generator CrewAI Research Assistant Debate, Specialist Swarm CrewAI SOP Search Hybrid Discovery, RAG Either Q&amp;A Bot Reactive, Tool-Using Either"},{"location":"AgenticAI/design-patterns/#devops-platform-engineering","title":"\u2699\ufe0f DevOps / Platform Engineering","text":"Use Case Recommended Patterns Framework CI/CD Automation Event-Driven, State Machine LangGraph Cloud Provisioning Planner\u2013Executor LangGraph Infra Cost Optimization Cost-Aware Routing LangGraph"},{"location":"AgenticAI/design-patterns/#innovation-pocs","title":"\ud83e\udde0 Innovation / POCs","text":"Use Case Recommended Patterns Framework Idea Generation Swarm, Debate CrewAI Brainstorming Peer-to-Peer CrewAI Hackathon Bots Minimal Agents CrewAI"},{"location":"AgenticAI/design-patterns/#langgraph-vs-crewai-one-slide-answer","title":"\ud83e\uddea LangGraph vs CrewAI (One-Slide Answer)","text":"Dimension LangGraph CrewAI Control Flow Deterministic Graph Sequential Governance Strong Weak State Management Native Limited Multi-Agent Graph-based Role-based Safety High Medium Production Ready \u2705 Yes \u26a0\ufe0f Partial Best For Enterprise AI Reasoning Teams"},{"location":"AgenticAI/design-patterns/#anti-patterns-to-avoid-critical","title":"Anti-Patterns to Avoid (CRITICAL) \ud83d\udea8","text":""},{"location":"AgenticAI/design-patterns/#agentic-ai-anti-patterns","title":"\u274c Agentic AI Anti-Patterns","text":"Anti-Pattern Why It\u2019s Dangerous Single mega-agent No control, no audit No governance Compliance failure Unbounded autonomy Production risk No observability Silent failures Tool access without policy Security breach No fallback Infinite loops No versioning Irreproducible behavior Prompt-only logic Fragile systems No cost controls Budget explosion Direct prod execution Catastrophic failures"},{"location":"AgenticAI/design-patterns/#agentic-ai-patterns-list","title":"Agentic AI Patterns list","text":"# Pattern Name Category Purpose When Used 1 Chain-of-Thought (CoT) Planning &amp; Reasoning Step-by-step reasoning Complex analysis, RCA 2 Tree-of-Thoughts (ToT) Planning &amp; Reasoning Explore multiple solution paths High-uncertainty problems 3 Graph-of-Thoughts (GoT) Planning &amp; Reasoning Reason over dependency graphs Service dependency RCA 4 ReAct (Reason+Act) Planning &amp; Reasoning Alternate thinking and tool use Investigation workflows 5 Plan\u2013Act\u2013Reflect Planning &amp; Reasoning Iterative improvement loop Autonomous remediation 6 Reflexion Planning &amp; Reasoning Self-critique and retry Low-confidence outputs 7 Hypothesis Testing Planning &amp; Reasoning Validate multiple root causes Incident diagnosis 8 Goal Decomposition Planning &amp; Reasoning Break into sub-tasks Multi-step automation 9 Constraint-Aware Planning Planning &amp; Reasoning Respect policy/cost/risk limits Prod-safe automation 10 Orchestrator\u2013Worker Multi-Agent Central planner with specialists Enterprise workflows 11 Planner\u2013Executor Multi-Agent Plan first, then execute Deterministic flows 12 Critic\u2013Generator Multi-Agent Validate generated outputs Change safety checks 13 Debate Pattern Multi-Agent Competing solutions selection High-risk decisions 14 Specialist Swarm Multi-Agent Domain agents collaborate Network/DB/Cloud RCA 15 Hierarchical Agents Multi-Agent Manager \u2192 team \u2192 tools Large-scale systems 16 Blackboard Multi-Agent Shared working memory Cross-agent context 17 Peer-to-Peer Agents Multi-Agent Direct negotiation Decentralized systems 18 Workflow Graph Orchestration Stateful branching workflows LangGraph pipelines 19 Event-Driven Agents Orchestration Trigger on alerts/events Monitoring, scaling 20 Saga Pattern Orchestration Multi-step with rollback Patch, infra changes 21 Checkpoint &amp; Resume Orchestration Persist state across failures Long-running tasks 22 Human Approval Gate Orchestration Pause for human review High-risk actions 23 Policy-Based Routing Orchestration Route by risk/complexity Complexity router 24 Circuit Breaker Orchestration Stop runaway loops Tool safety 25 Guardrails Safety Pre/post validation All prod actions 26 Risk-Tiered Autonomy Safety Auto vs human control SRE automation 27 Tool Permission Scoping Safety Limit tool access Security control 28 Simulation/Dry Run Safety Test before execution Infra changes 29 Confidence Thresholding Safety Execute above score Auto-remediation 30 Kill Switch Safety Emergency stop Unsafe behavior 31 RAG Knowledge Retrieve docs/runbooks Known fixes 32 Knowledge Graph Reasoning Knowledge Dependency + blast radius Impact analysis 33 Semantic Memory Knowledge Store past learnings Repeated incidents 34 Episodic Memory Knowledge Store execution history Auditing, learning 35 Tool Discovery Knowledge Dynamic tool lookup Plugin ecosystems 36 Context Optimization Knowledge Load relevant context only Token reduction 37 Toolformer Tool Usage LLM decides tool calls Flexible workflows 38 Function Calling Tool Usage Structured API execution Deterministic actions 39 Tool Chaining Tool Usage Multi-tool pipelines Diagnostics \u2192 fix 40 Parallel Tool Execution Tool Usage Run tools concurrently Faster RCA 41 Fallback Tool Strategy Tool Usage Alternate tool on failure Resilience 42 Model Routing Cost &amp; Performance Cheap vs powerful model Cost optimization 43 Token Budgeting Cost &amp; Performance Limit reasoning depth FinOps control 44 Caching/Memoization Cost &amp; Performance Reuse prior results Repeated tasks 45 Batch Inference Cost &amp; Performance Process tasks together High-volume alerts 46 Early Exit Cost &amp; Performance Stop when confident Low-complexity cases 47 Selective Reasoning Cost &amp; Performance Use ToT only if needed Cost + latency 48 Agent Telemetry Observability Track decisions/tools Performance monitoring 49 Traceable Reasoning Logs Observability Full audit trail Compliance 50 Outcome-Based KPIs Observability Measure MTTR, success Value tracking 51 Feedback Learning Loop Observability Improve from outcomes Continuous tuning 52 Drift Detection Observability Detect performance decay Model health 53 Multi-Tenant Isolation Enterprise Tenant-specific memory/policy SaaS platforms 54 RBAC/ABAC Enforcement Enterprise Role-based access Governance 55 Policy-as-Code Enterprise Centralized control Compliance 56 Compliance Evidence Gen Enterprise Auto audit logs Regulated env 57 FinOps Cost Tracking Enterprise Cost per task/agent Budget control 58 Registry &amp; Discovery Enterprise Catalog agents/tools Platform scale 59 Reinforcement Learning Loop Learning Improve via feedback Optimization 60 Runbook Mining Learning Convert manual \u2192 auto SRE automation 61 Continuous Evaluation Learning Shadow \u2192 canary \u2192 prod Safe rollout 62 Meta-Agent Optimization Learning Agents tuning agents Platform efficiency 63 Working Memory Memory Session context Active task state 64 Long-Term Vector Memory Memory Semantic retrieval Knowledge reuse 65 Structured State Store Memory Workflow state LangGraph state 66 Time-Weighted Memory Memory Recent &gt; old context Incident timelines 67 Complexity Router SRE-Specific Simple vs complex path Cost control 68 Correlation Graph SRE-Specific Merge alerts Noise reduction 69 Blast Radius Estimation SRE-Specific Impact scoring Change safety 70 Autonomous Remediation Loop SRE-Specific Diagnose \u2192 fix \u2192 validate Auto-healing 71 Safe Rollback SRE-Specific Revert failed actions Change management 72 Versioned Agents Lifecycle Track behavior per version Governance 73 Canary Agents Lifecycle Test on subset Safe deployment 74 Blue-Green Agents Lifecycle Zero-downtime upgrades Platform ops 75 Feature Flags for Autonomy Lifecycle Toggle automation level Gradual rollout"},{"location":"AgenticAI/design-patterns/#agentic-ai-enterprise-patterns-single-master-table","title":"\ud83e\udde0 Agentic AI Enterprise Patterns \u2014 Single Master Table","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Chain-of-Thought (CoT) Agent Runtime LLM Mandatory Planner Tree-of-Thought (ToT) Agent Runtime LLM Mandatory Planner ReAct (Reason+Act) Runtime + Tools Hybrid Planner \u2192 Executor Plan-Act Orchestration (LangGraph) Non-LLM Orchestrator Plan-Act-Reflect Orchestrator + Reflection Hybrid Orchestrator + Reflection Reflection / Self-Critique Reflection Layer LLM Mandatory Reflection Critic-Planner (Maker-Checker) Governance Hybrid Validator Reasoner-Planner-Executor Control Plane Hybrid Orchestrator World-Model State Tracking State &amp; Memory Non-LLM Orchestrator"},{"location":"AgenticAI/design-patterns/#multi-agent-collaboration","title":"Multi-Agent Collaboration","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Hierarchical Agents Control Plane Non-LLM Orchestrator Peer-to-Peer Agents Agent Runtime Hybrid Planner Role-Based Agents Registry &amp; Discovery Non-LLM Orchestrator Debate / Consensus Governance LLM Mandatory Validator Swarm Parallel Agents Runtime Non-LLM Executor Agent Mesh (A2A Bus) Integration Layer Non-LLM Orchestrator"},{"location":"AgenticAI/design-patterns/#orchestration-control","title":"Orchestration &amp; Control","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Sequential Orchestration (DAG) Control Plane Non-LLM Orchestrator Parallel Orchestration Control Plane Non-LLM Orchestrator Dynamic Routing Control Plane Hybrid Orchestrator Handoff Pattern Control Plane Non-LLM Orchestrator Group Chat Orchestration Runtime LLM Mandatory Planner Event-Driven Agents Input Layer Non-LLM Orchestrator Workflow Graph (LangGraph) Control Plane Non-LLM Orchestrator Control-Plane-as-Tool Tools Layer Non-LLM Executor"},{"location":"AgenticAI/design-patterns/#tooling-action","title":"Tooling &amp; Action","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Tool-Use Selection Tools &amp; Integration Hybrid Planner Function Calling LLM Abstraction Hybrid Planner Skill Library Registry Non-LLM Orchestrator Runbook Automation Execution Layer Non-LLM Executor Capability Routing Registry Non-LLM Orchestrator Sandboxed Execution Security Layer Non-LLM Executor"},{"location":"AgenticAI/design-patterns/#memory-knowledge","title":"Memory &amp; Knowledge","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Short-Term Memory State Layer Non-LLM Orchestrator Long-Term Memory Memory DB Non-LLM Orchestrator RAG Retrieval Knowledge Layer Hybrid Planner Episodic Memory State Layer Non-LLM Reflection Semantic Cache FinOps Layer Non-LLM Orchestrator Stateful Agent Checkpoints Orchestrator Non-LLM Orchestrator"},{"location":"AgenticAI/design-patterns/#governance-safety-hitl","title":"Governance, Safety, HITL","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Human-in-the-Loop (HITL) HITL Layer Non-LLM Validator Policy Guardrails (OPA/Cedar) Policy Layer Non-LLM Validator RBAC/ABAC for Agents Security Layer Non-LLM Validator Risk-Based Autonomy Governance Hybrid Validator Audit Trail Audit Layer Non-LLM Orchestrator Explainability (Evidence-First) Governance Hybrid Reflection"},{"location":"AgenticAI/design-patterns/#observability-evaluation","title":"Observability &amp; Evaluation","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Telemetry-Driven Agents Input Layer Non-LLM Orchestrator Outcome-Based Evaluation (KPI) Observability Non-LLM Reflection Confidence Scoring Reflection Hybrid Reflection Canary Execution Deployment Non-LLM Executor Feedback Loop Learning Learning Layer Hybrid Reflection Digital Twin Simulation Governance Non-LLM Validator"},{"location":"AgenticAI/design-patterns/#cost-performance","title":"Cost &amp; Performance","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Model Routing LLM Abstraction Non-LLM Orchestrator Adaptive Autonomy Control Plane Hybrid Orchestrator Batch Reasoning Runtime Non-LLM Executor Caching (Semantic/Output) State Layer Non-LLM Orchestrator Elastic Agent Scaling Deployment Non-LLM Executor"},{"location":"AgenticAI/design-patterns/#lifecycle-deployment","title":"Lifecycle &amp; Deployment","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Agent Registry &amp; Discovery Registry Layer Non-LLM Orchestrator Versioned Agents Lifecycle Layer Non-LLM Orchestrator Blue-Green Deployment Deployment Non-LLM Executor Shadow Mode Agents Observability Non-LLM Reflection Policy-as-Code Governance Non-LLM Validator Continuous Evaluation (CE) MLOps Hybrid Reflection"},{"location":"AgenticAI/design-patterns/#communication-transactions","title":"Communication &amp; Transactions","text":"Pattern 16-Layer Mapping LLM vs Non-LLM Primary Agent Role Message Bus (Kafka/Pulsar) Integration Layer Non-LLM Orchestrator Contract-Based Messaging (A2A) Integration Non-LLM Orchestrator Event Sourcing State Layer Non-LLM Orchestrator Saga Pattern Orchestrator Non-LLM Orchestrator"},{"location":"AgenticAI/design-patterns/#single-master-table-enterprise","title":"\u2705 Single Master Table (Enterprise)","text":"Pattern When to Use (Use Case) Example (How to use) Planner Agent Executor Agent Validator Agent Reflection Agent Chain-of-Thought (CoT) RCA, risk reasoning, policy interpretation Planner reasons through evidence (metrics/logs) to propose safest remediation LLM Required N/A N/A LLM Optional (quality review) Tree-of-Thought (ToT) Multiple remediation options Generate 3\u20135 options (restart/scale/rollback), score and pick best LLM Required N/A N/A LLM Required (branch outcome review) ReAct (Reason+Act) Investigation requiring tools Query Prometheus \u2192 logs \u2192 CMDB \u2192 decide action LLM Required Non-LLM (tool calls) Non-LLM (policy gate) LLM Required (did it work?) Plan-Act Approved runbooks, patch workflows Orchestrator runs deterministic steps: drain\u2192patch\u2192verify LLM Optional (plan draft) Non-LLM Required Non-LLM Required Hybrid (Non-LLM KPI + LLM summary) Plan-Act-Reflect Continuous improvement automation Execute fix \u2192 reflect \u2192 update future decision policy LLM Required Non-LLM Required Non-LLM Required LLM Required Reflection / Self-Critique Reduce hallucination, validate outputs Reflection agent checks proposed action against evidence + constraints LLM Optional N/A LLM Optional (critique) + Non-LLM policy LLM Required Critic-Planner (Maker-Checker) High-risk actions (DB restart, prod rollback) Planner proposes \u2192 validator rejects if outside window \u2192 propose alternate LLM Required Non-LLM (if approved) Non-LLM Required + LLM Optional LLM Required Reasoner-Planner-Executor Enterprise separation of duties Planner decides \u201cscale to 10\u201d \u2192 executor calls K8s \u2192 orchestrator tracks state LLM Required Non-LLM Required Non-LLM Required LLM Required World-Model / State Tracking Long-running incidents, resumable flows Maintain incident state + dependencies; resume after failure LLM Optional (interpret) Non-LLM Required (state updates) Non-LLM LLM Optional (drift insights) Hierarchical Multi-Agent SecOps/SRE/FinOps under one orchestrator Orchestrator routes work to domain agents LLM Optional (task decomposition) Non-LLM Non-LLM LLM Optional Peer-to-Peer Agents Cross-domain collaboration SRE shares latency findings with FinOps agent LLM Required (reasoning exchange) Non-LLM Non-LLM LLM Optional Role-Based Agents Clear ownership boundaries Patch agent only patches; Scaling agent only scales Non-LLM (routing/registry) Non-LLM Non-LLM Required (RBAC) LLM Optional Debate / Consensus Critical decisions requiring high confidence 2\u20133 planners debate; consensus agent chooses LLM Required N/A LLM Required + Non-LLM policy LLM Required Swarm Parallelization Fleet tasks at scale Check 500 nodes patch status in parallel Non-LLM Non-LLM Required Non-LLM LLM Optional Agent Mesh / A2A Messaging Decoupled agent communication Agents communicate via contracts on message bus Non-LLM Non-LLM Required Non-LLM Required (schema) LLM Optional Sequential Orchestration (DAG) Runbooks, controlled flows LangGraph executes step-by-step with retries/rollback Non-LLM Non-LLM Required Non-LLM Required LLM Optional Parallel Orchestration Faster diagnosis/execution Run checks for CPU/mem/disk/net concurrently Non-LLM Non-LLM Required Non-LLM LLM Optional Dynamic Routing Choose best agent/tool dynamically Classify incident \u2192 route to Network vs App agent Hybrid (LLM classify) Non-LLM Required Non-LLM Required LLM Optional Handoff / Escalation Risk threshold exceeded Auto-heal fails \u2192 escalate to human approval LLM Optional (explain) Non-LLM Non-LLM Required (HITL gate) LLM Required Group Chat Orchestration Complex reasoning tasks Planner+Risk+Cost agents collaborate LLM Required N/A LLM Optional + policy LLM Required Event-Driven Agents Real-time ops automation Kafka event \u201cpod crash\u201d triggers flow Non-LLM Non-LLM Required Non-LLM Required LLM Optional Tool-Use Selection Many possible tools/APIs Choose between SSM / Terraform / K8s / ITSM LLM Required Non-LLM Required Non-LLM Required LLM Optional Function Calling (Structured) Deterministic tool invocation LLM outputs JSON action contract for executor LLM Required Non-LLM Required Non-LLM Required (schema) LLM Optional Skill Library Standard reusable actions \u201crestart_service_v3\u201d, \u201capply_patch_v2\u201d skills Non-LLM Non-LLM Required Non-LLM Required LLM Optional Runbook Automation Approved self-heal actions Restart service, clear queue, scale within blast radius N/A Non-LLM Required Non-LLM Required LLM Optional Short-Term Memory Maintain context per incident Store current metrics/log pointers/session state N/A Non-LLM Required N/A LLM Optional Long-Term Memory Learn from past incidents Store successful fixes/outcomes; retrieve later Hybrid (LLM uses) Non-LLM Required Non-LLM LLM Required RAG Grounding Need factual/policy grounding Retrieve policy/runbook/CMDB docs \u2192 synthesize decision LLM Required Non-LLM (retrieval) Non-LLM Required (policy) LLM Optional Episodic Memory Outcome-based learning \u201cRestart fixed it\u201d stored and ranked next time Hybrid Non-LLM Required N/A LLM Required Semantic / Output Cache Reduce cost/latency Cache repeated classifications and answers N/A Non-LLM Required N/A LLM Optional Human-in-the-Loop (HITL) High blast radius changes Require approval for DB restart / prod rollback LLM Optional (explain) N/A Non-LLM Required (approval) LLM Required Policy Guardrails (OPA/Cedar) Enforce compliance and safety Block action outside window/without ticket N/A N/A Non-LLM Required LLM Optional RBAC/ABAC Least privilege Patch agent cannot modify IAM; FinOps cannot restart prod N/A N/A Non-LLM Required N/A Risk-Based Autonomy Auto vs manual by risk score Low risk auto-heal; high risk HITL Hybrid (LLM risk) Non-LLM Required Non-LLM Required LLM Required Audit Trail SOC2/ISO evidence Immutable log of decisions/actions/timestamps N/A Non-LLM Required Non-LLM Required LLM Optional (summaries) Explainability (Evidence-First) Trust &amp; governance Output: evidence \u2192 policy \u2192 action \u2192 expected impact LLM Required N/A Non-LLM (check) LLM Required Outcome-Based Evaluation (KPI) Measure MTTR/availability value Compare before/after MTTR; autonomous resolution ratio N/A Non-LLM Required Non-LLM LLM Required (insights) Confidence Scoring Auto-escalation control Low confidence \u2192 escalate to human Hybrid N/A Non-LLM Required (threshold) LLM Required (calibrate) Canary Execution Safe changes Patch 5% nodes \u2192 validate \u2192 expand rollout N/A Non-LLM Required Non-LLM Required LLM Optional Digital Twin / Simulation Predict impact before action Simulate scale-in effect on latency before doing it LLM Optional Non-LLM Required Non-LLM Required LLM Optional Model Routing Reduce LLM cost Small model classify; large model plan only when needed Non-LLM Required N/A Non-LLM LLM Optional Adaptive Autonomy Save cost on simple cases Simple alert \u2192 single-step plan; complex \u2192 ToT Hybrid Non-LLM Required Non-LLM Required LLM Required Batch Reasoning High volume similar alerts Group 100 similar alerts \u2192 one diagnosis plan Hybrid Non-LLM Required Non-LLM LLM Optional Elastic Agent Scaling Scale platform runtime Scale executor pods based on queue depth N/A Non-LLM Required Non-LLM LLM Optional Registry &amp; Discovery Enterprise catalog Find correct agent/skill/tool by metadata N/A Non-LLM Required Non-LLM Required LLM Optional Versioned Agents Change control Deploy v2 agent with rollback N/A Non-LLM Required Non-LLM Required LLM Optional Blue-Green Deployment Safe upgrades Switch traffic from v1 agent to v2 N/A Non-LLM Required Non-LLM Required LLM Optional Shadow Mode Validate before autonomy Run AI decisions in parallel; no execution LLM Required N/A Non-LLM Required LLM Required Policy-as-Code GitOps governance PR to change autonomy thresholds N/A N/A Non-LLM Required N/A Continuous Evaluation Regression tests for agents Replay incident dataset; score accuracy LLM Optional Non-LLM Required Non-LLM Required LLM Required Message Bus Event backbone Alerts and agent messages through Kafka topics N/A Non-LLM Required Non-LLM Required LLM Optional Contract Messaging Typed A2A communication JSON schema enforced between agents N/A Non-LLM Required Non-LLM Required LLM Optional Event Sourcing Replayability Re-run incident flow for audits N/A Non-LLM Required Non-LLM LLM Optional Saga Pattern Multi-step rollback safety Patch fails \u2192 rollback \u2192 restore traffic LLM Optional (plan) Non-LLM Required Non-LLM Required LLM Optional"},{"location":"AgenticAI/framework/","title":"Agentic AI - Enterprise &amp; Production Grade Framework","text":""},{"location":"AgenticAI/framework/#1-agentic-ai-16-layers","title":"<code>1. Agentic AI - 16 Layers</code>","text":""},{"location":"AgenticAI/framework/#2-input-access-layer","title":"<code>2. Input &amp; Access Layer</code>","text":""},{"location":"AgenticAI/framework/#3-presentation-consumption-layer","title":"<code>3. Presentation / Consumption Layer</code>","text":""},{"location":"AgenticAI/framework/#4-orchestration-control-plane-layer","title":"<code>4. Orchestration &amp; Control Plane Layer</code>","text":""},{"location":"AgenticAI/framework/#5-agent-runtime-layer","title":"<code>5. Agent Runtime Layer</code>","text":""},{"location":"AgenticAI/framework/#6-state-memory-layer","title":"<code>6. State &amp; Memory Layer</code>","text":""},{"location":"AgenticAI/framework/#7-policy-enforcement-guardrails-layer","title":"<code>7. Policy Enforcement &amp; Guardrails Layer</code>","text":""},{"location":"AgenticAI/framework/#8-human-in-the-loop-layer","title":"<code>8. Human in the Loop Layer</code>","text":""},{"location":"AgenticAI/framework/#9-tools-integration-layer","title":"<code>9. Tools &amp; Integration Layer</code>","text":""},{"location":"AgenticAI/framework/#10-llm-model-abstraction-layer","title":"<code>10. LLM &amp; Model Abstraction Layer</code>","text":""},{"location":"AgenticAI/framework/#11-registry-discovery-layer","title":"<code>11. Registry &amp; Discovery Layer</code>","text":""},{"location":"AgenticAI/framework/#12-security-layer","title":"<code>12. Security Layer</code>","text":""},{"location":"AgenticAI/framework/#13-governance-policy-layer","title":"<code>13. Governance &amp; Policy Layer</code>","text":""},{"location":"AgenticAI/framework/#14-compliance-layer","title":"<code>14. Compliance Layer</code>","text":""},{"location":"AgenticAI/framework/#15-audit-traceability-layer","title":"<code>15. Audit &amp; Traceability Layer</code>","text":""},{"location":"AgenticAI/framework/#16-monitoring-observability-layer","title":"<code>16. Monitoring &amp; Observability Layer</code>","text":""},{"location":"AgenticAI/framework/#17-cost-finops-layer","title":"<code>17. Cost &amp; FinOps Layer</code>","text":""},{"location":"AgenticAI/framework/#18-deployment-lifecycle-layer","title":"<code>18. Deployment &amp; Lifecycle Layer</code>","text":""},{"location":"AgenticAI/framework/#agentic-ai-16-layers","title":"Agentic AI - 16 Layers","text":"# Layer Name Purpose Key Components Patterns Used CoT / ToT / Reflection use 1 Input &amp; Access Layer Ingest alerts, telemetry, tickets, chatops APIs, OTel collectors, event bus Event-driven, normalization \u274c No 2 Presentation / Consumption Layer UI, dashboards, ChatOps, reports Web UI, Slack/Teams bot, APIs Human-in-the-loop, outcome views \u274c No 3 Orchestration &amp; Control Plane Workflow routing, state graph, task lifecycle LangGraph, workflow engine, scheduler Workflow graph, saga, routing \u274c No 4 Agent Runtime Layer Reasoning, planning, decision making LangChain/CrewAI agents CoT, ReAct, Plan-Act-Reflect, ToT \u2705 Primary 5 State &amp; Memory Layer Persist workflow state and context Redis, vector DB, graph DB Working memory, episodic memory \u26a0\ufe0f Stores outputs only 6 Policy Enforcement &amp; Guardrails Validate actions before execution Policy engine, safety checks Guardrails, risk-tier autonomy \u274c No 7 Human-in-the-Loop Layer Approval, override, feedback Approval UI, escalation logic Approval gate, feedback loop \u274c No 8 Tools &amp; Integration Layer Execute actions and fetch data Cloud APIs, K8s, ITSM, monitoring Tool chaining, parallel tools \u274c No 9 LLM &amp; Model Abstraction Layer Model routing and prompt control GPT, Claude, Gemini, local LLM Model routing, token budgeting \u26a0\ufe0f Hosts CoT execution 10 Registry &amp; Discovery Layer Catalog agents, tools, capabilities Agent registry, tool metadata Capability discovery \u274c No 11 Security Layer Identity, secrets, access control IAM, RBAC/ABAC, vault Permission scoping \u274c No 12 Governance &amp; Policy Layer Compliance rules and policies Policy-as-code, risk matrix Policy routing \u274c No 13 Audit &amp; Traceability Layer Full decision and action logs Audit DB, reasoning logs Traceable reasoning \u26a0\ufe0f Stores CoT summary 14 Monitoring &amp; Observability Layer Track agent performance and health Metrics, logs, traces Agent telemetry, drift detection \u274c No 15 Cost &amp; FinOps Layer Track and optimize LLM/tool cost Cost analyzer, budget policies Model routing, caching \u274c No 16 Deployment &amp; Lifecycle Layer Versioning, rollout, testing CI/CD, canary agents, feature flags Blue-green, versioned agents \u274c No"},{"location":"AgenticAI/framework/#pattern-layer-matrix","title":"Pattern \u2192 Layer Matrix","text":"Pattern L1 Input L2 UI L3 Orchestrator L4 Agent Runtime L5 Memory L6 Guardrails L7 HITL L8 Tools L9 LLM Abstraction L10 Registry L11 Security L12 Governance L13 Audit L14 Observability L15 FinOps L16 Deploy Chain-of-Thought (CoT) \u274c \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u26a0 \u274c Tree-of-Thoughts (ToT) \u274c \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u26a0 \u274c ReAct (Reason+Act) \u274c \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c Plan\u2013Act\u2013Reflect \u274c \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c Hypothesis Testing \u274c \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c Goal Decomposition \u274c \u274c \u26a0 \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c Orchestrator\u2013Worker \u274c \u274c \u2714 \u26a0 \u274c \u274c \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u274c \u274c \u274c Planner\u2013Executor \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c Workflow Graph \u274c \u274c \u2714 \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u2714 Saga / Rollback \u274c \u274c \u2714 \u274c \u2714 \u26a0 \u274c \u2714 \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c \u2714 Checkpoint / Resume \u274c \u274c \u2714 \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u2714 Policy-Based Routing \u274c \u274c \u2714 \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u2714 \u274c Circuit Breaker \u274c \u274c \u2714 \u274c \u274c \u2714 \u274c \u2714 \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c \u2714 Guardrails \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u26a0 \u274c \u2714 \u2714 \u2714 \u2714 \u274c \u274c Risk-Tiered Autonomy \u274c \u274c \u2714 \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u274c Human Approval Gate \u274c \u2714 \u2714 \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c Tool Chaining \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Parallel Tool Execution \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Fallback Tools \u274c \u274c \u2714 \u274c \u274c \u2714 \u274c \u2714 \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c \u2714 Function Calling \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Model Routing \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u274c Token Budgeting \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u274c Caching / Memoization \u274c \u274c \u274c \u26a0 \u2714 \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c Selective Reasoning \u274c \u274c \u2714 \u2714 \u274c \u2714 \u274c \u274c \u2714 \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u274c RAG \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Knowledge Graph Reasoning \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Semantic Memory \u274c \u274c \u274c \u26a0 \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u274c Episodic Memory \u274c \u274c \u274c \u26a0 \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u274c \u274c Traceable Reasoning Logs \u274c \u274c \u274c \u26a0 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c \u274c Agent Telemetry \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 \u274c Drift Detection \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 FinOps Cost Tracking \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u274c Registry &amp; Discovery \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u274c \u2714 \u274c \u274c \u2714 RBAC / ABAC \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u2714 \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u274c \u274c Policy-as-Code \u274c \u274c \u2714 \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u2714 Versioned Agents \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u2714 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 Canary Agents \u274c \u274c \u274c \u2714 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u2714 Blue/Green Rollout \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2714 \u2714 \u2714 \u2714 \u2714"},{"location":"AgenticAI/general/","title":"general","text":""},{"location":"AgenticAI/general/#what-is-agentic-ai-and-how-will-it-change-work","title":"what is agentic ai and how will it change work?","text":"<p>Agentic AI refers to AI systems capable of autonomous action and decision-making to achieve specific goals, often with minimal human supervision. It's a step beyond traditional AI and generative AI, enabling AI agents to go beyond content creation and perform complex tasks by integrating with various systems and tools.</p>"},{"location":"AgenticAI/general/#what-is-agentic-ai","title":"What is Agentic AI?","text":"<p>At its core, Agentic AI is a type of AI that\u2019s all about autonomy. This means that it can make decisions, take actions, and even learn on its own to achieve specific goals. It\u2019s kind of like having a virtual assistant that can think, reason, and adapt to changing circumstances without needing constant direction.</p> <p>Agentic AI operates in four key stages:</p> <ol> <li>Perception: It gathers data from the world around it.</li> <li>Reasoning: It processes this data to understand what\u2019s going on.</li> <li>Action: It decides what to do based on its understanding.</li> <li>Learning: It improves and adapts over time, learning from feedback and experience.</li> </ol> <p></p> <ol> <li> <p>Autonomous Agents:Agentic AI systems utilize AI agents \u2013 individual software entities that perform specific tasks with a degree of autonomy. </p> </li> <li> <p>Goal-Oriented:These agents are designed to understand user goals and autonomously take actions to achieve them, rather than just responding to specific instructions. </p> </li> <li> <p>Adaptive and Learning:Agentic AI systems are adaptive and can learn from interactions, refining their strategies and improving performance over time. </p> </li> <li> <p>Beyond Generative AI:While generative AI focuses on creating content, agentic AI extends this capability by using generated content to complete tasks, integrate with other systems, and achieve broader goals. </p> </li> </ol>"},{"location":"AgenticAI/general/#how-agentic-ai-will-change-work","title":"How Agentic AI Will Change Work:","text":"<ol> <li> <p>Increased Automation:Agentic AI can automate complex, multi-step workflows and business processes that were previously difficult or impossible to automate with traditional systems. </p> </li> <li> <p>Improved Productivity:By taking over routine and repetitive tasks, agentic AI can free up human workers to focus on more strategic, creative, and high-value work.</p> </li> <li> <p>Enhanced Collaboration:Agentic AI can facilitate seamless collaboration between humans and AI, with AI acting as a partner, assistant, or even a coach. </p> </li> <li> <p>Data-Driven Decisions:Agentic AI can analyze large amounts of data and provide insights that can help organizations make more informed decisions. </p> </li> </ol>"},{"location":"AgenticAI/general/#what-is-an-ai-agent","title":"What is an AI Agent?","text":"<p>On the other hand, AI Agents are typically built to do specific tasks. They\u2019re designed to help you with something \u2014 like answering questions, organizing your calendar, or even managing your email inbox. AI Agents are great at automating simple, repetitive tasks but don\u2019t have the autonomy or decision-making abilities that Agentic AI does. Think of them as virtual helpers that do exactly what you tell them to do, without thinking for themselves.</p> <p></p>"},{"location":"AgenticAI/general/#whats-the-difference","title":"What\u2019s the Difference?","text":"Aspect Agentic AI AI Agent Autonomy Level Highly autonomous, can act independently Limited autonomy, needs human input Goal-Orientation Goal-driven, solves problems on its own Task-specific, follows set instructions Learning Capabilities Continuously learns and improves May not learn or only learns within set rules Complexity Handles complex, dynamic environments Handles simpler, more structured tasks Decision-Making Process Makes decisions based on reasoning and analysis Pre-programmed responses to inputs Interaction with Environment Actively adapts to surroundings and changes Reacts to set inputs but doesn\u2019t adapt Responsiveness to Change Changes its goals and methods autonomously Limited ability to adapt to new situations"},{"location":"AgenticAI/general/#where-do-we-see-these-in-the-real-world","title":"Where Do We See These in the Real World?","text":"<p>Both Agentic AI and AI Agents have started popping up in various industries, and their applications are growing fast.</p>"},{"location":"AgenticAI/general/#agentic-ai-in-action","title":"Agentic AI in Action","text":"<ol> <li> <p>Self-Driving Cars: One of the most exciting uses of Agentic AI is in autonomous vehicles. These AI systems perceive their surroundings, make driving decisions, and learn from every trip. Over time, they get better at navigating and handling new challenges on the road. For example, Tesla\u2019s Full Self-Driving system is an example of Agentic AI that continuously learns from the driving environment and adjusts its behavior to improve safety and efficiency.</p> </li> <li> <p>Supply Chain Management: Agentic AI is also helping companies optimize their supply chains. By autonomously managing inventory, predicting demand, and adjusting delivery routes in real-time, AI can ensure smoother, more efficient operations. Amazon\u2019s Warehouse Robots, powered by AI, are an example \u2014 these robots navigate complex environments, adapt to different conditions, and autonomously move goods around warehouses.</p> </li> <li> <p>Cybersecurity: In the world of cybersecurity, Agentic AI can detect threats and vulnerabilities by analyzing network activity and automatically responding to potential breaches. Darktrace, an AI cybersecurity company, uses Agentic AI to autonomously detect, respond to, and learn from potential cyber threats in real-time.</p> </li> <li> <p>Healthcare: AI is playing a big role in healthcare, too. Agentic AI can assist with diagnostics, treatment recommendations, and patient care management. It analyzes medical data, identifies patterns, and helps doctors make more informed decisions. For instance, IBM\u2019s Watson Health uses AI to analyze massive amounts of healthcare data, learning from new information to offer insights that help doctors and healthcare professionals.</p> </li> </ol>"},{"location":"AgenticAI/general/#ai-agents-in-action","title":"AI Agents in Action","text":"<ol> <li> <p>Customer Support: One of the most common uses of AI Agents is in customer service. Chatbots can answer questions, resolve issues, and guide customers through processes \u2014 all without needing human intervention. Zendesk\u2019s AI-powered chatbot helps businesses respond to customer queries quickly and efficiently, acting as an AI Agent that handles common issues and frees up human agents for more complex tasks.</p> </li> <li> <p>Personal Assistants: You probably already interact with an AI Agent every day if you use voice assistants like Siri or Google Assistant. They can help you set reminders, check the weather, or play your favorite music \u2014 tasks that are useful but don\u2019t require much decision-making. These AI Agents rely on predefined commands and are great at handling simple, repetitive tasks.</p> </li> <li> <p>Email Management: AI Agents are also great for managing your inbox. They can sort emails, flag important ones, and even provide smart replies to save you time.Google\u2019s Gmail Smart Compose feature is an excellent example of an AI Agent at work, helping users respond to emails faster by suggesting phrases based on context.</p> </li> <li> <p>Productivity Tools: Tools like GitHub Copilot are AI Agents that help software developers by suggesting code and helping with debugging. They\u2019re like having a second set of eyes that\u2019s always there to help. By offering code suggestions in real-time, this AI Agent enhances developer productivity, allowing them to focus on more creative aspects of their work.</p> </li> </ol>"},{"location":"AgenticAI/overview/","title":"Overview","text":"\u2705 What is Agentic AI? <p>The field of artificial intelligence is rapidly evolving beyond reactive models to embrace a new paradigm: Agentic AI.This transformative approach empowers AI systems to act autonomously, make decisions, and proactively work towards goals with minimal human intervention. Unlike traditional AI that responds to specific commands, agentic AI systems are dynamic problem-solvers, capable of planning, executing multi-step tasks, and adapting to changing environments.</p> <p>At its core, an agentic AI system, or an \"AI agent,\" is a software entity that perceives its environment, reasons about its observations, and takes actions to achieve a specific objective. This process involves a sophisticated interplay of several key components:</p> <ul> <li> <p>Perception: The agent gathers information from its digital or physical environment through various inputs like data feeds, APIs, or sensor readings.</p> </li> <li> <p>Reasoning and Planning: Leveraging large language models (LLMs) and other advanced algorithms, the agent analyzes the perceived information, breaks down complex goals into smaller, manageable steps, and formulates a plan of action.</p> </li> <li> <p>Action: The agent executes its plan by interacting with its environment. This could involve anything from sending an email and updating a database to controlling a robotic arm.</p> </li> <li> <p>Learning and Adaptation: Through feedback loops and experience, the agent continuously learns and refines its strategies to improve its performance over time. This adaptability is a hallmark of agentic AI.</p> </li> <li> <p>Orchestration: In more complex scenarios, multiple AI agents can be orchestrated to work together, each with specialized skills, to tackle multifaceted problems.</p> </li> </ul> \ud83d\udccc Types of Agents in AI: <p>There are several types of agents in AI, each differing in how they perceive the environment, process information, and take actions.</p> <p>Artificial Intelligence (AI) agents are the foundation of many intelligent systems which helps them to understand their environment, make decisions and take actions to achieve specific goals. These agents vary in complexity from simple reflex-based systems to advanced models that learn and adapt over time.</p> 1. Simple Reflex Agents: <p>A Simple Reflex Agent is the most basic type of intelligent agent in Artificial Intelligence (AI). It works on the principle of condition\u2013action rules (also called if\u2013then rules). That means it looks at the current percept (the environment input it senses) and chooses an action without considering history or future consequences.</p> <p>Note: <code>Simple Reflex Agents (SRA)</code> are not typically implemented with frameworks like CrewAI, LangChain, LangGraph, or AutoGen.</p> <ul> <li> <p>Simple Reflex Agent = Based only on if\u2013then rules (condition \u2192 action).</p> </li> <li> <p>It does not require memory, reasoning, planning, or multi-agent collaboration.</p> </li> <li> <p>Frameworks like CrewAI, LangChain, AutoGen are designed for complex, reasoning-based, multi-agent workflows with LLMs \u2014 which is much more than a reflex agent needs.</p> </li> </ul> <p>\u2705 Suitable Frameworks for Simple Reflex Agent</p> <p>If you want to implement a Simple Reflex Agent, you\u2019d use:</p> <ul> <li> <p>Finite State Machines (FSMs)</p> </li> <li> <p>Rule-based systems (Drools, CLIPS, PyKnow in Python)</p> </li> <li> <p>Custom Python code with condition\u2013action rules</p> </li> </ul> <p>Note: This is a true Simple Reflex Agent \u2014 <code>no LLM, no memory, no reasoning</code>.</p> <p>\u274c Not Ideal (But Possible)</p> <ul> <li> <p>CrewAI, LangChain/LangGraph, AutoGen \u2192 These are too heavy for simple reflex use cases. They\u2019re for reasoning, planning, tool use, and autonomous workflows.</p> </li> <li> <p>You can technically implement reflex behavior in them, but it would just be LLM-wrapped if-else rules, which is inefficient.</p> </li> </ul> \u2705 Key Characteristics of a Simple Reflex Agent: <ol> <li>Action is based only on the current percept.</li> </ol> <p>It ignores past percepts (no memory) and does not plan for the future.</p> <ol> <li> <p>Uses condition\u2013action rules.</p> </li> <li> <p>Example:</p> <ul> <li> <p><code>If traffic light is red \u2192 stop</code></p> </li> <li> <p><code>If traffic light is green \u2192 go</code></p> </li> </ul> </li> <li> <p>No internal state.</p> </li> </ol> <p>It doesn\u2019t keep track of what happened before.</p> <ol> <li>Efficient but limited.</li> </ol> <p>Works only in fully observable environments where the current percept gives enough information to make the right decision.</p> <p>\u2696\ufe0f Advantages</p> <ul> <li> <p>Very fast and simple to design.</p> </li> <li> <p>Works well in simple, predictable environments.</p> </li> </ul> <p>\u274c Limitations</p> <ul> <li> <p>Fails in partially observable or complex environments.</p> </li> <li> <p>Cannot learn or adapt to changes.</p> </li> <li> <p>No memory or long-term strategy.</p> </li> </ul> <p>\ud83c\udf0d Domain-Specific Real-Time Examples of Simple Reflex Agents</p> Domain Example Sensors (Percepts) Rules (Condition \u2192 Action) Actuators \ud83c\udfe5 Healthcare Automatic Insulin Pump Glucose sensor If Glucose &gt; threshold \u2192 Inject insulin; Else \u2192 Do nothing Insulin injector \ud83d\ude97 Automotive Automatic Wipers Rain sensor If Rain detected \u2192 Turn wipers ON; Else \u2192 OFF Wiper motor \ud83d\ude97 Automotive Automatic Headlights Light sensor If Brightness &lt; threshold \u2192 Lights ON; Else \u2192 Lights OFF Headlight switch \ud83c\udfed Manufacturing Furnace Temperature Control Thermocouple If Temp &lt; 1200\u00b0C \u2192 Increase gas; If Temp \u2265 1200\u00b0C \u2192 Cut gas Gas valve \ud83c\udfed Manufacturing Conveyor Belt QC Size/weight sensor If Item defective \u2192 Push off belt; Else \u2192 Let pass Robotic arm / ejector \ud83c\udfe6 Banking ATM Security (PIN check) Card reader &amp; PIN input If 3 wrong PINs \u2192 Block card; Else \u2192 Continue ATM system controller \ud83c\udfe0 Smart Home Smart Lights Motion sensor If Motion detected \u2192 Light ON; If No motion 5 min \u2192 Light OFF Smart light switch \ud83c\udfe0 Smart Home Smart Sprinkler Soil moisture sensor If Moisture &lt; 30% \u2192 Turn sprinkler ON; Else OFF Water pump \u2708\ufe0f Aviation Landing Gear Warning System Altitude sensor + Gear status If Altitude &lt; 1000 ft AND Gear not down \u2192 Alarm ON; Else \u2192 Do nothing Alarm system \ud83e\udde0 Agentic AI Frameworks \u2013 Comparison for Simple Reflex Agent: Framework Agent Architecture Agent Types Supported Orchestration / Flow Tooling / Integrations Best Use Cases CrewAI Multi-agent orchestration with Crew (team of agents) Research Agent, Code Agent, Reviewer Agent, QA Agent, Custom agents Task + Role-based orchestration (assign roles, tasks, dependencies) Python functions, APIs, DB connectors, GitHub, Slack, Jira, etc. Collaborative multi-agent workflows (code review, GitHub automation, research &amp; summarization, DevOps pipelines) LangChain / LangGraph Modular chain + graph-based orchestration Conversational agent, Retrieval agent, Tool-using agent, Memory-enabled agent Sequential Chains &amp; Graph DAG execution (LangGraph) 100+ integrations (LLMs, vector DBs, APIs, RAG pipelines, memory) RAG, chatbots, document Q\\&amp;A, knowledge workers, enterprise apps AutoGen (Microsoft) Agent \u2194 Agent conversation paradigm Assistant Agent, User Proxy Agent, Custom Multi-agent teams Conversation-based orchestration (agents talk to each other) Python code execution, Web search, APIs, Data pipelines AI coding assistants, self-correcting agents, reasoning loops Google AI Agent Builder (ADK) Pre-built + customizable agents Chatbots, RAG Agents, Workflow agents Declarative agent builder, UI + API GCP services (Vertex AI, BigQuery, Cloud Functions, Pub/Sub, Drive) Customer support, search, enterprise knowledge assistants AWS Bedrock Serverless orchestration for LLM apps Reasoning agent, Knowledge agent, Orchestration agent Managed orchestration (Amazon Agents for Bedrock) AWS ecosystem (S3, DynamoDB, SageMaker, API Gateway, Lambda) Enterprise AI apps, RAG, financial &amp; retail AI copilots, compliance-heavy use cases <p>\ud83c\udfd7\ufe0f Simple Reflex Agent \u2013 Architecture</p> <p></p> <pre><code>+-----------------+\n|   Environment   |\n+-----------------+\n        ^\n        |   (Changes Environment)\n        |\n+-----------------+      +--------------------------------+\n|    Actuators    | &lt;--- |Condition-Action Rules (if-then)|\n+-----------------+      |                                |\n        ^                |  \"What action should I take?\"  |\n        |                +--------------------------------+\n        |   (Action)             ^\n        |                        |\n        |                        |\n+-----------------+      +--------------------------------+\n|     Sensors     | ---&gt; |    \"What is the world like?\"   |\n+-----------------+      +--------------------------------+\n        ^\n        |   (Percepts)\n        |\n+-----------------+\n|   Environment   |\n+-----------------+\n</code></pre> 2. Model-Based Reflex Agents: <p>These agents improve over simple reflex agents by maintaining an internal state (model of the world) that helps them handle partially observable environments.</p> Comparison Model-Based Reflex Agent vs Simple Reflex Agent: Feature Simple Reflex Agent Model-Based Reflex Agent Memory No memory Maintains internal state World Understanding Current percept only Current percept + history Action Immediate reaction Informed decision Example Car reacts only to \u201clane empty now\u201d Car predicts \u201clane empty now and in next 3 sec\u201d \ud83e\udde0 Agentic AI Frameworks \u2013 Comparison: Framework Simple Reflex Agent Example Use Case (Simple Reflex) Model-Based Reflex Agent Example Use Case (Model-Based Reflex) CrewAI \u274c Not ideal (CrewAI is task/role-driven, not simple rules) \u2013 \u2705 Can maintain memory/state across tasks Customer support agent that remembers last user complaint while solving next LangChain / LangGraph \u26a0\ufe0f Possible with <code>Rule-based chains</code> but overkill Rule-based chatbot that always responds \u201cYes\u201d if keyword found \u2705 Strong fit \u2013 uses memory + state graphs Conversational bot that uses memory to track user preferences AutoGen \u274c Not suitable (designed for multi-agent conversations, not reflex rules) \u2013 \u26a0\ufe0f Possible with stateful multi-agent design Two agents coordinate: one tracks state (model) while other acts Google AI Agent Development Kit (ADK) \u2705 Can build simple reflex with event triggers IoT agent that turns off light if sensor &gt; threshold \u2705 Natively supports state via environment modeling Smart thermostat that remembers past temperatures to adjust next AWS Bedrock \u274c Not for reflex (LLM-focused, no direct reflex handling) \u2013 \u26a0\ufe0f Limited, can add external memory layer (DynamoDB, Lambda) Chatbot that remembers last query using external DB \ud83c\udf10 Model-Based Reflex Agents \u2013 Domain Examples: Domain Example Use Case Percepts (Inputs) Internal Model (State Tracking) Actions (Outputs) Smart Home (IoT) Energy-efficient lighting Motion sensor data, time of day Tracks last detected motion + daylight state Turn lights on/off intelligently Healthcare Remote patient monitoring Vitals (HR, BP, glucose), patient activity Maintains patient\u2019s baseline health model Alert doctor if abnormal pattern persists Retail Smart shelf inventory Camera feed, weight sensors Tracks stock changes + delivery schedule Trigger restock request to supplier Finance Fraud detection in transactions Transaction amount, location, device Maintains customer\u2019s transaction history model Flag suspicious activity, request OTP Cybersecurity Intrusion detection Network traffic logs, system events Tracks baseline network behavior Block IP, trigger alert if anomaly persists Autonomous Vehicles Lane keeping Camera, lidar, GPS signals Tracks vehicle\u2019s position + road map Adjust steering to remain in lane Manufacturing Predictive maintenance Sensor readings from machines Tracks wear-and-tear progression Schedule maintenance before failure Customer Service (Chatbot) Context-aware support User query, past chat history Maintains conversation context state Respond with relevant solution instead of repeating questions <p>Key Characteristics:</p> <ul> <li> <p>Internal State: By maintaining an internal model of the environment, these agents can handle scenarios where some aspects are not directly observable thus it provides more flexible decision-making.</p> </li> <li> <p>Adaptive: They update their internal model based on new information which allows them to adapt to changes in the environment.</p> </li> <li> <p>Better Decision-Making: The ability to refer to the internal model helps agents make more informed decisions which reduces the risk of making impulsive or suboptimal choices.</p> </li> <li> <p>Increased Complexity: Maintaining an internal model increases computational demands which requires more memory and processing power to track changes in the environment.</p> </li> </ul> <p>\ud83d\udd11 Key Takeaways:</p> <ul> <li> <p>Simple Reflex Agents \u2192 Best suited for rule-based systems (Google ADK fits well, LangChain can do it but heavy).</p> </li> <li> <p>Model-Based Reflex Agents \u2192 Need state/memory \u2192 CrewAI, LangChain/LangGraph, and Google ADK are strongest.</p> </li> <li> <p>AutoGen &amp; Bedrock \u2192 More aligned with conversational or LLM-driven agents, not pure reflex.</p> </li> </ul> Model-Based Reflex Agents \u2013 Architecture: <p></p> <pre><code>+----------------------+      +---------------------------+\n| How the world evolves| ---&gt; |                           |\n+----------------------+      |      Internal State       |      +---------------------+\n                            |         (Model)           | ---&gt;   | Condition-Action    |\n+----------------------+      |                           |      |   Rules (if-then)   |\n| How my actions affect| ---&gt; | \"How the world is now\"    |      |                     |\n| the world            |      +---------------------------+      | \"What action should |\n+----------------------+               ^                         |  I take now?\"       |\n                                       |                         +---------------------+\n                                       |                                    | (Action)\n                                       |                                    V\n+-----------------+      +---------------------------+             +-----------------+\n|   Environment   | ---&gt; |          Sensors          | ----------&gt; |    Actuators    |\n|                 |      |                           |             |                 |\n| (Percepts)      |      | \"What is the world like?\" |             +-----------------+\n+-----------------+      +---------------------------+                      |\n        ^                                                                   | (Changes \n        |                                                                   | Environment)\n        +-------------------------------------------------------------------+\n</code></pre> 3. Goal-based AI Agents: <p>Goal-based AI agents represent a sophisticated approach in artificial intelligence (AI), where agents are programmed to achieve specific objectives. These agents are designed to plan, execute, and adjust their actions dynamically to meet predefined goals. This approach is particularly useful in complex environments where flexibility and adaptability are crucial.</p>"},{"location":"AgenticAI/overview/#key-concepts-of-goal-based-ai-agents","title":"Key Concepts of Goal-Based AI Agents","text":""},{"location":"AgenticAI/overview/#1-goals","title":"1. Goals","text":"<ul> <li>Planning </li> <li>Execution </li> <li>Adaptation </li> </ul>"},{"location":"AgenticAI/overview/#2-components-of-goal-based-ai-agents","title":"2. Components of Goal-Based AI Agents","text":"<ul> <li>Perception Module </li> <li>Knowledge Base </li> <li>Decision-Making Module </li> <li>Planning Module </li> <li>Execution Module </li> </ul>"},{"location":"AgenticAI/overview/#3-types-of-goal-based-agents","title":"3. Types of Goal-Based Agents","text":"<ul> <li>Reactive Agents </li> <li>Deliberative Agents </li> <li>Hybrid Agents </li> <li>Learning Agents </li> </ul>"},{"location":"AgenticAI/overview/#4-applications-of-goal-based-agents","title":"4. Applications of Goal-Based Agents","text":"<ul> <li>Robotics </li> <li>Game AI </li> <li>Autonomous Vehicles </li> <li>Resource Management </li> <li>Healthcare </li> </ul>"},{"location":"AgenticAI/overview/#5-challenges-and-future-directions","title":"5. Challenges and Future Directions","text":"<ul> <li>Complexity and Computation </li> <li>Uncertainty and Adaptation </li> <li>Ethical and Safety Concerns </li> <li>Future Directions </li> </ul>"},{"location":"AgenticAI/overview/#key-concepts-of-goal-based-ai-agents_1","title":"Key Concepts of Goal-Based AI Agents","text":"<p>Goals</p> <p>Goals are the specific objectives that the agent aims to achieve. These can range from simple tasks, such as sorting objects, to complex missions, such as navigating a robot through a maze, solving a puzzle, or managing resources in a simulated environment. Goals provide a clear direction for the agent's actions and decisions.</p> <p>Planning</p> <p>Planning involves determining the sequence of actions required to achieve the goal. This process can be complex, involving predictive models, heuristics, and algorithms to evaluate possible future states and actions. Effective planning allows agents to anticipate potential obstacles and devise strategies to overcome them.</p> <p>Execution</p> <p>Execution is the phase where the agent carries out the planned actions. This involves interacting with the environment and performing tasks that bring the agent closer to its goal. Successful execution requires precise coordination of actions and real-time responsiveness to changes in the environment.</p> <p>Adaptation</p> <p>Adaptation is essential as the agent interacts with its environment. It may encounter unexpected obstacles or changes, and adaptation involves modifying plans and actions in response to new information, ensuring the agent remains on track to achieve its goal. This ability to adapt makes goal-based agents robust and flexible.</p> <p>Components of Goal-Based AI Agents</p> <p>Perception Module</p> <p>The perception module is responsible for collecting data from the environment using sensors or input mechanisms and processing this data to form a coherent understanding of the current state. This information is crucial for informed decision-making and planning.</p> <p>Knowledge Base</p> <p>The knowledge base includes the world model, which is a representation of the environment and the agent\u2019s understanding of it, as well as the rules and facts about how the world operates and the rules governing the agent\u2019s actions. This structured knowledge helps the agent to interpret sensory data and make logical decisions.</p> <p>Decision-Making Module</p> <p>The decision-making module involves goal formulation, where the goals are defined and updated based on the current state and objectives, and action selection, where actions are chosen based on the current state, goals, and predicted outcomes. This module ensures that the agent's actions are aligned with its objectives.</p> <p>Planning Module</p> <p>The planning module handles path planning, determining the optimal sequence of actions to achieve the goal, and contingency planning, developing alternative plans in case of unexpected changes or failures. Effective planning minimizes the risk of failure and enhances the agent's efficiency.</p> <p>Execution Module</p> <p>The execution module is responsible for carrying out the planned actions in the environment, and for monitoring and feedback, continuously monitoring the results of actions and adjusting plans as needed. This module ensures that the agent remains responsive to real-time changes and maintains progress towards its goal.</p> <p>Types of Goal-Based Agents</p> <p>Reactive Agents</p> <p>Reactive agents operate based on immediate perceptions and pre-defined rules. They quickly respond to changes in the environment without long-term planning. These agents are suitable for simple tasks where rapid response is more important than complex decision-making.</p> <p>Deliberative Agents</p> <p>Deliberative agents involve a higher level of planning and reasoning. They create detailed plans and execute them, adjusting their actions based on feedback and changes in the environment. These agents are suitable for complex tasks that require strategic thinking and adaptability.</p> <p>Hybrid Agents</p> <p>Hybrid agents combine reactive and deliberative approaches. They can respond quickly to changes while also engaging in higher-level planning when necessary. This combination allows them to handle a wide range of tasks with varying complexity and urgency.</p> <p>Learning Agents</p> <p>Learning agents can adapt their strategies and improve performance over time by learning from their interactions with the environment. They use techniques like reinforcement learning to enhance their ability to achieve goals. Learning agents are particularly useful in dynamic environments where conditions and requirements change frequently.</p> \ud83e\udde0 Goal-Based Agents \u2013 Structured View Aspect Details Suitable Frameworks - LangChain / LangGraph (goal-directed planning with chains &amp; memory) - AutoGen (multi-agent collaboration to achieve goals) - CrewAI (team of agents aligned to a goal, role-based execution) - Haystack (goal-based retrieval-augmented generation) - BabyAGI / Task-driven frameworks (goal-driven task decomposition &amp; execution) Key Characteristics - Decisions depend on goals, not just current state - Requires planning &amp; search to achieve goals - Can evaluate multiple actions and select the one aligned with the final objective - Uses reasoning, knowledge base, and inference - More flexible and adaptive than reflex agents Domain-Specific Real-Time Examples - Healthcare: AI treatment planner deciding best therapy for a patient (goal = patient recovery) - Finance: Robo-advisor optimizing portfolio allocation for target ROI - Autonomous Driving: Car plans optimal route considering traffic &amp; safety (goal = reach destination safely) - Customer Service: AI assistant aiming to resolve customer queries with minimal interactions - DevOps: Automated incident resolution agent targeting service uptime &amp; SLA compliance Agentic AI Frameworks \u2013 Comparison LangChain / LangGraph \u2192 Best for building structured, goal-oriented workflows with memory &amp; reasoning AutoGen \u2192 Useful for multi-agent collaboration toward a shared goal (e.g., one agent plans, another executes) CrewAI \u2192 Best when a team of agents with specialized roles must collectively achieve a business goal BabyAGI \u2192 Great for iterative task decomposition to achieve open-ended goals Haystack \u2192 Best when goals rely on retrieval &amp; knowledge-based reasoning Architecture - Goal Module \u2192 Defines target state - Knowledge Base \u2192 Stores domain/world model - Inference Engine \u2192 Plans sequence of actions - Decision-Making Unit \u2192 Chooses action based on goal satisfaction - Execution Module \u2192 Acts in environment - Feedback Loop \u2192 Evaluates progress toward goal Key Takeaways \u2705 Goal-based agents provide flexibility &amp; reasoning beyond reflex agents. \u2705 They require planning, memory, and decision evaluation. \u2705 Agentic AI frameworks like LangChain, AutoGen, CrewAI are well-suited. \u2705 Real-world use cases include healthcare, finance, autonomous driving, DevOps, and customer service. \u2705 Architecture emphasizes goal setting, planning, execution, and feedback. Model-Based Reflex Agents \u2013 Architecture: <p></p> <p></p> <pre><code>+----------------------+      +---------------------------+\n| How the world evolves| ---&gt; |                           |\n+----------------------+      |      Internal State       |\n                              |         (Model)           |\n+----------------------+      |                           |      +---------------------+\n| How my actions affect| ---&gt; | \"How the world is now\"    | ---&gt; |                     |\n| the world            |      +---------------------------+      |   Planning/Search   |\n+----------------------+               ^                         |                     |\n                                       |                         | \"What if I do A?\"   |\n                                       |                         | \"How do I reach my  |      +----------+\n+-----------------+      +---------------------------+      |  goal?\"             | &lt;--- |   Goal   |\n|   Environment   | ---&gt; |          Sensors          | ----&gt; +---------------------+      +----------+\n|                 |      |                           |                  | (Chosen Action)\n| (Perceiving)    |      | \"What is the world like?\" |                  V\n+-----------------+      +---------------------------+      +-----------------+\n        ^                                                   |    Actuators    |\n        | (Acting on Environment)                           +-----------------+\n        +-----------------------------------------------------------+\n</code></pre> 4. Utility-Based Agents: <p>Artificial Intelligence has boomed in growth in recent years. Various types of intelligent agents are being developed to solve complex problems. Utility-based agents hold a strong position due to their ability to make rational decisions based on a utility function. These agents are designed to optimize their performance by maximizing utility measures.</p> <p>Utility-based agents extend goal-based reasoning by considering not only whether a goal is met but also how valuable or desirable a particular outcome is. They use a utility function to quantify preferences and make trade-offs between competing objectives, enabling nuanced decision-making in uncertain or resource-limited situations. Designing an appropriate utility function is crucial for their effectiveness.</p> <p>What is Utility Theory?</p> <p>Utility theory is a fundamental concept in economics and decision theory. This theory provides a framework for understanding how individuals make choices under uncertainty. The aim of this agent is not only to achieve the goal but the best possible way to reach the goal. This idea suggests that people give a value to each possible result of a choice showing how much they like or are happy with that result. The aim is to get the highest expected value, which is the average of the values of all possible results taking into account how likely each one is to happen.</p> <p>Rational decision making</p> <p>Rational Decision making means picking the option that maximizes an agent's expected utility. i.e. give the best outcome. When it comes to AI, a rational agent always goes for the action that leads to the best results, given its current knowledge and the possible future states of the environment. To do this, the agent needs a utility function, which is a way to measure how good each option is. This helps the agent figure out which action will likely give the best results.</p> <p>h3 style=\"color:blue;\"&gt;\ud83e\udde0 Utility-Based Agents</p> Aspect Details Suitable Frameworks - LangChain / LangGraph \u2192 define utility functions for selecting best plan  - CrewAI \u2192 multi-agent setup where agents vote on best option  - AutoGen \u2192 negotiation-based decision-making  - Ray RLlib / Stable Baselines \u2192 when reinforcement learning utility optimization is needed Key Characteristics - Choose actions not only to achieve goals but also to maximize performance/utility  - Introduces preference ranking among states (e.g., \u201cbetter\u201d vs. \u201cworse\u201d outcomes)  - Balances trade-offs (speed vs. cost, accuracy vs. computation)  - Often leverages reinforcement learning or optimization models Domain-Specific Real-Time Examples - Autonomous Vehicles \u2192 deciding safest + fastest route balancing time, fuel, and risk  - Healthcare AI \u2192 recommending treatment with best success probability &amp; least side effects  - Retail Dynamic Pricing \u2192 adjusting prices based on demand elasticity to maximize revenue  - Cloud Resource Allocation \u2192 balancing cost vs. performance in real time Agentic AI Frameworks \u2013 Comparison - LangChain / LangGraph \u2192 utility functions as evaluators for agent outputs  - CrewAI \u2192 multiple agents propose, then utility scoring selects optimal  - AutoGen \u2192 conversational negotiation, pick action with highest expected utility  - RL frameworks (Ray RLlib, SB3) \u2192 explicitly optimize reward signals for utility Utility-Based Agents \u2013 Architecture 1. Perception Module \u2192 observes environment (inputs)  2. Knowledge Base / Model \u2192 state representations  3. Utility Function \u2192 assigns score (preference measure) to outcomes  4. Decision Module \u2192 selects action maximizing expected utility  5. Action Execution \u2192 performs chosen action  6. Feedback Loop \u2192 updates utility preferences from outcomes Key Takeaways - Utility-based agents go beyond goals \u2192 they optimize preferences  - Essential when multiple conflicting goals exist (e.g., speed vs. safety)  - Best for complex, uncertain, real-world environments  - Require well-defined utility/reward functions to work effectively Utility-Based Agents \u2013 Architecture: <p>alt text</p> <pre><code>+----------------------+      +---------------------------+\n| How the world evolves| ---&gt; |                           |\n+----------------------+      |      Internal State       |\n                              |         (Model)           |\n+----------------------+      |                           |      +---------------------+\n| How my actions affect| ---&gt; | \"What will my state be    | ---&gt; |                     |\n| the world            |      |  if I do action A?\"       |      |   Utility Function  |\n+----------------------+      +---------------------------+      |                     |\n        ^                              |                         |  \"How happy will I  |\n        |                              | (Possible Outcomes)     |   be in this state?\"|\n        |                              V                         +---------------------+\n+-----------------+      +---------------------------+                      | (Utility Score)\n|   Environment   | ---&gt; |          Sensors          |                      V\n|                 |      |                           |      +------------------------------+\n| (Perceiving)    |      | \"What is the world like?\" | ---&gt; |Choose Action with Max Utility|\n+-----------------+      +---------------------------+      +------------------------------+\n        ^                                                               | (Optimal Action)\n        | (Acting on Environment)                                       V\n        |                                                     +-----------------+\n        +---------------------------------------------------&gt; |    Actuators    |\n                                                              +-----------------+\n</code></pre> \ud83e\udde0 Utility-Based Agents \u2013 Structured View: Aspect Details Suitable Frameworks - LangChain / LangGraph \u2192 define utility functions for selecting best plan  - CrewAI \u2192 multi-agent setup where agents vote on best option  - AutoGen \u2192 negotiation-based decision-making  - Ray RLlib / Stable Baselines \u2192 when reinforcement learning utility optimization is needed Key Characteristics - Choose actions not only to achieve goals but also to maximize performance/utility  - Introduces preference ranking among states (e.g., \u201cbetter\u201d vs. \u201cworse\u201d outcomes)  - Balances trade-offs (speed vs. cost, accuracy vs. computation)  - Often leverages reinforcement learning or optimization models Domain-Specific Real-Time Examples - Autonomous Vehicles \u2192 deciding safest + fastest route balancing time, fuel, and risk  - Healthcare AI \u2192 recommending treatment with best success probability &amp; least side effects  - Retail Dynamic Pricing \u2192 adjusting prices based on demand elasticity to maximize revenue  - Cloud Resource Allocation \u2192 balancing cost vs. performance in real time Agentic AI Frameworks \u2013 Comparison - LangChain / LangGraph \u2192 utility functions as evaluators for agent outputs  - CrewAI \u2192 multiple agents propose, then utility scoring selects optimal  - AutoGen \u2192 conversational negotiation, pick action with highest expected utility  - RL frameworks (Ray RLlib, SB3) \u2192 explicitly optimize reward signals for utility Utility-Based Agents \u2013 Architecture 1. Perception Module \u2192 observes environment (inputs)  2. Knowledge Base / Model \u2192 state representations  3. Utility Function \u2192 assigns score (preference measure) to outcomes  4. Decision Module \u2192 selects action maximizing expected utility  5. Action Execution \u2192 performs chosen action  6. Feedback Loop \u2192 updates utility preferences from outcomes Key Takeaways - Utility-based agents go beyond goals \u2192 they optimize preferences  - Essential when multiple conflicting goals exist (e.g., speed vs. safety)  - Best for complex, uncertain, real-world environments  - Require well-defined utility/reward functions to work effectively 5. Learning Agents: <p>Learning agents improve their performance over time by learning from experience and updating their internal models, strategies or policies. They can adapt to changes in the environment and often outperform static agents in dynamic contexts. Learning may involve supervised, unsupervised or reinforcement learning techniques and these agents typically contain both a performance element (for acting) and a learning element (for improving future actions).</p> <p>Key Characteristics:</p> <ul> <li> <p>Adaptive Learning: It improve their decision-making through continuous feedback from their actions.</p> </li> <li> <p>Exploration vs. Exploitation: These agents balance exploring new actions that may lead to better outcomes with exploiting known successful strategies.</p> </li> <li> <p>Flexibility: They can adapt to a wide variety of tasks or environments by modifying their behavior based on new data.</p> </li> <li> <p>Generalization: It can apply lessons learned in one context to new, similar situations enhancing their versatility.</p> </li> </ul> <p>When to Use: They are well-suited for dynamic environments that change over time such as recommendation systems, fraud detection and personalized healthcare management.</p> <p>Example: Customer service chatbots can improve response accuracy over time by learning from previous interactions and adapting to user needs.</p> Learning Agents \u2013 Architecture: <pre><code>+---------------------------------------------------------------------------------+\n|                                  Environment                                    |\n+---------------------------------------------------------------------------------+\n        ^                                      |\n        | (Actions)                            | (Percepts)\n        |                                      V\n+-----------------+                      +-----------------+\n|    Actuators    |                      |     Sensors     |\n+-----------------+                      +-----------------+\n        ^                                      |\n        |                                      |\n+---------------------------------------------------------------------------------+\n|                                     AGENT                                       |\n|                                                                                 |\n|       +---------------------+        (Changes)       +----------------------+   |\n|       | Performance Element | &lt;---------------------- |   Learning Element   |  |\n|       |                     |                         |                      |  |\n|       | (Agent's \"Brain\")   |         (Feedback)      +----------------------+  |\n|       +---------------------+ ----------------------&gt; |        Critic        |  |\n|         ^         |                                   |                      |  |\n|         |         | (Action)                          +----------------------+  |\n|         |         +-------------------------------------------^                 |\n|         |                                                     |(How am I doing?)|\n|         | (New Problems)                                      |                 |\n|       +----------------------+                                |                 |\n|       |  Problem Generator   | -------------------------------+                 |\n|       +----------------------+                                                  |\n|                                                                                 |\n+---------------------------------------------------------------------------------+\n</code></pre> <p></p> \ud83e\udde0 Learning Agents \u2013 Structured View: Aspect Details Suitable Frameworks - LangChain / LangGraph \u2192 memory, feedback loops, continuous improvement- CrewAI \u2192 autonomous teams with adaptive behavior- AutoGen \u2192 agents that learn from multi-agent conversations and feedback- Ray RLlib / TensorFlow / PyTorch \u2192 reinforcement learning &amp; deep learning for adaptive agents Key Characteristics - Improves performance over time via feedback- Has a learning element (e.g., ML model) to adapt- Has a performance element (executes tasks)- Has a critic (evaluates performance)- Has a problem generator (explores new strategies)- Capable of self-correction and knowledge accumulation Domain-Specific Real-Time Examples Healthcare \u2192 AI agent learns from past patient interactions to improve diagnosis accuracyFinance \u2192 Fraud detection agents adapt to new fraud patterns over timeRetail \u2192 Personalized recommendation engine that learns user preferencesDevOps \u2192 CI/CD optimization agent that learns which pipeline configurations minimize failuresAutonomous Vehicles \u2192 Self-driving cars learn from real-world driving scenarios Agentic AI Frameworks \u2013 Comparison LangChain / LangGraph \u2192 LLM + Memory + Feedback (reinforcement loops)CrewAI \u2192 Role-based adaptive agents that collaborate and improve with iterationAutoGen \u2192 Agents improve by analyzing conversation history and refining reasoningRay RLlib \u2192 Reinforcement learning backbone for agents that require simulation and policy optimization Architecture 1. Learning Element \u2192 Improves agent\u2019s knowledge/strategy (ML/RL models)2. Performance Element \u2192 Executes based on learned policy3. Critic \u2192 Provides feedback on actions4. Problem Generator \u2192 Suggests exploratory actions for learning5. Environment \u2192 Provides input data and feedback signals Key Takeaways - Learning Agents continuously evolve unlike reflex or goal-based agents- Suitable for dynamic, uncertain, and data-rich environments- Combine reasoning (LLM frameworks) + adaptation (ML/RL frameworks)- Examples: Autonomous trading systems, adaptive chatbots, self-driving cars, personalized assistants \ud83d\udccc Fraud Detection in Finance \u2013 Framework Mapping: Framework How it Fits Fraud Detection Agentic AI Role Strengths Limitations LangChain Build a pipeline where incoming financial transactions are passed through an LLM with access to a vector DB (historical fraud patterns) and external APIs (AML, KYC). The LLM queries knowledge, classifies, and flags anomalies. Acts as an Orchestrator: routes queries \u2192 retrieves patterns \u2192 calls APIs \u2192 flags suspicious activity. Strong at retrieval-augmented generation (RAG), integrating structured + unstructured data, and explainability. Needs custom logic for decision-making &amp; multi-agent coordination. CrewAI Define multiple agents: e.g., Transaction Analyzer Agent (checks real-time payments), Pattern Detection Agent (compares with fraud signatures), Escalation Agent (notifies compliance team). CrewAI coordinates them. Multi-Agent Collaboration: Specialized agents working together for detection, decision, escalation. Clear separation of responsibilities, human-in-the-loop, transparency. Can be complex to design &amp; optimize for real-time latency-sensitive use cases. AutoGen Create conversational agents where one agent monitors transactions, another challenges suspicious cases (\u201cExplain why this looks fraudulent\u201d), and a supervisor agent decides the final action. Agents communicate iteratively. Dialogue-driven multi-agent workflow for fraud classification, justification, and escalation. Strong at LLM-to-LLM communication, scenario simulation, and iterative reasoning. Might add overhead if immediate, low-latency decisions are needed (e.g., payments approval). RLlib (Reinforcement Learning) Train an RL agent that learns fraud detection policies over time by maximizing reward (catching fraud, minimizing false positives). It continuously improves from transaction feedback. Learning Agent: adapts fraud detection policies dynamically. Handles real-time decision-making under uncertainty, improves over time. Needs large-scale data &amp; training; interpretability can be challenging compared to symbolic/logical reasoning. 6. Multi-Agent Systems (MAS): <p>Multi-agent systems operate in environments shared with other agents, either cooperating or competing to achieve individual or group goals. These systems are decentralized, often requiring communication, negotiation or coordination protocols. They are well-suited to distributed problem solving but can be complex to design due to emergent and unpredictable behaviors. Types of multi-agent systems:</p> <ul> <li> <p>Cooperative MAS: Agents work together toward shared objectives.</p> </li> <li> <p>Competitive MAS: Agents pursue individual goals that may conflict.</p> </li> <li> <p>Mixed MAS: Agents cooperate in some scenarios and compete in others.</p> </li> </ul> <p>Key Characteristics:</p> <ul> <li> <p>Autonomous Agents: Each agent acts on its own based on its goals and knowledge.</p> </li> <li> <p>Interactions: Agents communicate, cooperate or compete to achieve individual or shared objectives.</p> </li> <li> <p>Distributed Problem Solving: Agents work together to solve complex problems more efficiently than they could alone.</p> </li> <li> <p>Decentralization: No central control, agents make decisions independently.</p> </li> </ul> \ud83e\udde9 Multi-Agent Systems (MAS) \u2013 Structured View: Dimension Description Examples Definition A system composed of multiple interacting agents, each with autonomy, situatedness, and ability to collaborate/compete. AI trading bots, swarm robotics, distributed simulations. Types of Agents - Reactive Agents (reflexive, no memory) - Deliberative Agents (reasoning, planning) - Hybrid Agents (mix of reactive + deliberative) - Learning Agents (improve over time) Chatbots, warehouse robots, autonomous vehicles, fraud detection bots. Interaction Types - Cooperative (work toward shared goals) - Competitive (adversarial, game-theoretic) - Mixed-Motive (partial cooperation + competition) Cooperative: swarm drones; Competitive: auction bidding bots; Mixed: supply chain negotiations. Coordination Mechanisms - Communication protocols (messages, APIs) - Contract net protocol (task allocation) - Consensus algorithms (agreement) - Market-based approaches Blockchain consensus, auction-based resource allocation, task delegation in teams. Architectures - Centralized MAS (coordinator agent) - Decentralized MAS (peer-to-peer) - Hierarchical MAS (leader-follower) Centralized: traffic control system; Decentralized: swarm robotics; Hierarchical: military drones. Key Capabilities - Autonomy - Social ability (communication) - Reactivity (respond to environment) - Proactiveness (goal-driven) - Adaptability (learn from experience) Self-driving fleets, financial trading systems, cyber defense MAS. Applications - Finance (fraud detection, algorithmic trading) - Robotics (multi-robot coordination) - Smart Grids (energy distribution) - Healthcare (distributed diagnostics) - Logistics (supply chain, fleet management) Amazon delivery drones, stock trading agents, patient monitoring systems. Challenges - Scalability - Inter-agent trust &amp; coordination - Conflict resolution - Robustness in dynamic environments - Ethical considerations Multi-agent negotiations, cybersecurity MAS resilience. Key Frameworks - LangChain (Agent orchestration, memory, tools) - CrewAI (Multi-agent collaboration with roles) - AutoGen (Conversational &amp; task-based multi-agent LLM systems) - RLlib (Reinforcement learning for MAS simulation &amp; optimization) LangChain agents + tools, CrewAI role-based teamwork, AutoGen negotiation agents, RLlib multi-agent RL. Multi-Agent Systems (MAS) \u2013 Architecture: <p></p> <pre><code>+-----------------------------------------------------------------------------+\n|                                                                             |\n|                           SHARED ENVIRONMENT                                |\n|                                                                             |\n|    +----------+         (Perception/Action)          +----------+           |\n|    | Agent 1  | &lt;----------------------------------&gt; | Agent 2  |           |\n|    | (e.g.,   |                                      | (e.g.,   |           |\n|    | Goal-    | &lt;----------------------------------&gt; | Utility- |           |\n|    | Based)   |       (Communication/Interaction)    | Based)   |           |\n|    +----------+ &lt;----------------------------------&gt; +----------+           |\n|         ^                        |                      ^                   |\n|         |                        |                      |                   |\n|         | (Perception/Action)    | (Communication)      |(Perception/Action)|\n|         |                        |                      |                   |\n|         |                        V                      |                   |\n|    +----------+         (Perception/Action)          +----------+           |\n|    | Agent 3  | &lt;----------------------------------&gt; | Agent 4  |           |\n|    | (e.g.,   |                                      | (e.g.,   |           |\n|    | Model-   | -------------------------------------&gt; | Simple |           |\n|    | Based)   |                                      | Reflex)  |           |\n|    +----------+                                      +----------+           |\n|                                                                             |\n+-----------------------------------------------------------------------------+\n</code></pre> \ud83e\udde9 Multi-Agent Systems (MAS) \u2013 Real-Time Use Case Mapping: Aspect LangChain CrewAI AutoGen RLlib Use Case Supply Chain Optimization \u2013 AI agents coordinating inventory, demand forecasting, logistics, and supplier management Role of Agents Agents handle reasoning chains (e.g., forecast demand \u2192 plan procurement \u2192 adjust logistics) Specialized agents (Procurement Agent, Logistics Agent, Demand Forecasting Agent) collaborating Conversation-driven task decomposition between agents (Planner Agent \u2194 Solver Agent \u2194 Validator Agent) RL agents optimize policies for inventory, pricing, logistics, minimizing costs and delays Architecture Fit Modular chains + tools for data access, reasoning, and workflow orchestration Multi-agent orchestration with clear role assignment and crew collaboration Multi-agent dialogue framework for negotiation and plan execution Multi-agent reinforcement learning (MARL) for dynamic decision-making Strengths Easy to integrate with APIs (ERP, Supplier APIs), reasoning workflows Best for team-based coordination and structured agent roles Great for agent-to-agent dialogue and automated collaboration Handles complex adaptive optimization under uncertainty (e.g., demand fluctuations) Weaknesses Limited to reasoning workflows, not designed for autonomous decision optimization Overhead in defining roles &amp; communication protocols Dialogue-heavy approach may be verbose for real-time systems Requires high compute &amp; careful reward design Domain Example Chain for demand forecasting using historical data, ERP integration, and pricing APIs Crew with Procurement Agent (supplier negotiation), Logistics Agent (route planning), Inventory Agent (stock tracking) Agents simulate negotiation between suppliers and logistics for cost/delivery optimization RL agents learn optimal reorder policies, logistics routing under dynamic constraints Best Fit Decision support &amp; ERP/CRM integration Cross-team multi-agent workflows in enterprise AI negotiation &amp; planning bots Autonomous optimization &amp; control under uncertainty 7. Hierarchical agents <p>Hierarchical agents organize behavior into multiple layers such as strategic, tactical and operational. Higher levels make abstract decisions that break down into more specific subgoals for lower levels to execute. This structure improves scalability, reusability of skills and management of complex tasks, but requires designing effective interfaces between layers.</p> <p>Key Characteristics:</p> <ul> <li> <p>Structured Decision-Making: Decision-making is divided into different levels for more efficient task handling.</p> </li> <li> <p>Task Division: Complex tasks are broken down into simpler subtasks.</p> </li> <li> <p>Control and Guidance: Higher levels direct lower levels for coordinated action.</p> </li> </ul> \ud83e\udde9 Hierarchical Agents \u2013 Structured View: Aspect Details Definition Hierarchical Agents are agents that decompose complex problems into multiple layers (high-level \u2192 mid-level \u2192 low-level tasks). Each level manages sub-tasks and coordinates with others to achieve the overall objective. Key Characteristics - Task decomposition into levels  - Abstraction between layers  - Coordination &amp; communication across layers  - Top-down control (high-level sets goals, low-level executes)  - Reusability of sub-modules Architecture - High-Level Layer: Defines goals, strategies, and overall planning  - Mid-Level Layer: Translates goals into sub-goals &amp; manages workflows  - Low-Level Layer: Executes primitive actions and handles environment interaction Advantages - Simplifies complex decision-making  - Improves scalability and modularity  - Allows specialization at different layers  - Easier debugging &amp; monitoring Challenges - Coordination overhead  - Failure in lower layer impacts higher-level goals  - Requires robust communication mechanisms  - Complexity in dynamic/adaptive environments Domain-Specific Example Autonomous Vehicles  - High-Level: Plan route from City A \u2192 City B  - Mid-Level: Break route into road segments, traffic lights, lane selection  - Low-Level: Execute steering, braking, acceleration in real-time Agentic AI Framework Mapping - LangChain: High-level reasoning &amp; task orchestration using chains/agents  - CrewAI: Multi-agent collaboration with clear role-based hierarchy  - AutoGen: Conversation-driven sub-agent management for sub-tasks  - RLlib (Hierarchical RL): Multi-level reinforcement learning with policies at each level Key Takeaways - Hierarchical Agents = structured problem-solving  - Useful in domains with layered decision-making (e.g., robotics, supply chain, healthcare, finance)  - MAS + Hierarchical design improves coordination and modularity \ud83e\udde9 Real-Time Use Case Mapping \u2013 Hierarchical Agents Aspect LangChain CrewAI AutoGen RLlib Agent Role High-level Planner Agent breaks query \u2192 Low-level Executor Agents (retrieval, summarization, sentiment analysis) Manager Agent delegates tasks to Specialist Agents (FAQ resolver, escalation handler, ticket generator) Controller Agent coordinates Sub-Agents (NLP agent, reasoning agent, action agent) Hierarchical RL: High-level policy (decides escalation vs. resolution) \u2192 Low-level policy (specific reply actions) Framework Support Supports AgentExecutor + Chains (Planner \u2192 Tools \u2192 Sub-agents) Native hierarchical crew design \u2013 Manager defines sub-task structure Built-in multi-agent orchestration with hierarchical control flows Hierarchical RL environments (HRL), e.g., options framework for sub-policies Coordination Mechanism Chains, memory, and tool routing Task \u2192 Subtask decomposition Message passing between parent and child agents High-level rewards guide low-level policies Real-Time Example Customer asks complex billing query \u2192 Planner splits into: retrieve policy, check DB, summarize \u2192 Compose response User query \u2192 Manager delegates: FAQ agent resolves or escalates \u2192 Ticketing agent creates case if unresolved User query \u2192 Controller agent: Sentiment analysis \u2192 If negative, escalation sub-agent activates \u2192 Generate empathetic response High-level agent decides: Resolve or escalate \u2192 Low-level agent executes generate response / create escalation Strengths Modular design, easy to integrate tools Explicit role &amp; responsibility hierarchy Flexible orchestration, plug-and-play sub-agents Optimized learning of multi-level policies over time Limitations Not natively hierarchical \u2013 must design chains Complexity increases with agent hierarchy Debugging multi-layered flows harder Training hierarchical RL agents is compute-heavy Aspect Autonomous Vehicles \ud83d\ude97 Healthcare Diagnosis \ud83c\udfe5 Domain Goal Safe, efficient driving Accurate and timely disease diagnosis Top-Level Agent Route Planner Agent \u2013 decides optimal path based on destination, traffic, road conditions Medical Supervisor Agent \u2013 manages patient evaluation process, aligns with hospital protocols Mid-Level Agents - Perception Agent (interprets camera, LIDAR, radar)- Decision Agent (lane change, overtaking, stopping)- Safety Agent (obstacle avoidance) - Symptom Analyzer Agent (parses patient data)- Medical Knowledge Agent (queries guidelines, medical literature)- Risk Assessment Agent (prioritizes severity) Low-Level Agents - Steering Control Agent- Speed Control Agent- Brake Control Agent - Lab Test Interpreter Agent (blood tests, scans)- Treatment Recommendation Agent (suggests medications, therapies)- Follow-up Monitoring Agent Hierarchical Flow Goal \u2192 Route \u2192 Perception \u2192 Decision \u2192 Control Actions Goal \u2192 Patient Assessment \u2192 Symptom Analysis \u2192 Risk Prioritization \u2192 Diagnosis &amp; Treatment Feedback Loop Sensors (LIDAR, radar, GPS) provide feedback to higher-level planners Continuous patient vitals, lab results, treatment outcomes feed back into diagnosis Real-Time Challenge Handling unpredictable events (pedestrians, sudden braking, weather) Handling uncertainty in symptoms, overlapping conditions, missing data Benefit of Hierarchy Scalability, modular fault isolation (if braking agent fails, others still operate) Structured reasoning, specialization of agents by expertise (lab vs treatment vs risk) MAS Advantage Faster coordination between agents for real-time response Collaborative diagnosis combining multiple medical perspectives Hierarchical agents \u2013 Architecture: <pre><code>                +---------------------+\n                |  Coordinator Agent  | (Top-Level Goal)\n                |       (Layer 1)     |\n                +---------------------+\n                     |             |\n(Sub-Goals)          |             | (Sub-Goals)\n                     V             V\n        +------------------+     +------------------+\n        |  Manager Agent A |     |  Manager Agent B |\n        |     (Layer 2)    |     |     (Layer 2)    |\n        +------------------+     +------------------+\n          |         |              |         |\n(Actions) |         | (Actions)    |         | (Actions)\n          V         V              V         V\n      +--------+ +--------+    +--------+ +--------+\n      | Worker | | Worker |    | Worker | | Worker |\n      | Agent  | | Agent  |    | Agent  | | Agent  |\n      | (L3-A1)| | (L3-A2)|    | (L3-B1)| | (L3-B2)|\n      +--------+ +--------+    +--------+ +--------+\n          ^         ^              ^         ^\n          | (Status)|              | (Status)|\n          +---------+              +---------+\n               ^                        ^\n(Feedback)     |                        | (Feedback)\n               +------------------------+\n\n\n+------------------------------------------------------+\n|                   SHARED ENVIRONMENT                 |\n|                                                      |\n| &lt;---- (Perception/Action) ----&gt; [All Worker Agents]  |\n|                                                      |\n+------------------------------------------------------+\n</code></pre> <p></p> \ud83e\udde9 Agentic AI Types vs Frameworks \u2013 Structured View Agent Type Definition LangChain CrewAI AutoGen (MSFT) RLlib (Ray) Google ADK (Agent Developer Kit) AWS Bedrock Simple Reflex Agents Act on condition-action rules (<code>if-then</code>). Tool-Calling Chains Simple Task Agent Rule-based Dialogue Agents N/A ADK Rule-Oriented Agents Bedrock Prompt + Guardrails Model-Based Reflex Agents Maintain internal state &amp; act accordingly. ConversationalRetrievalChain Stateful Agents State Tracking Agents N/A Context-Aware ADK Agents Bedrock w/ Memory Store Goal-Based Agents Choose actions to achieve defined goals. LCEL (LangChain Expression Language) Crew Goals &amp; Roles Goal-Oriented Planning Agents N/A ADK Goal-Oriented APIs Bedrock + Knowledge Graph Utility-Based Agents Optimize outcomes via utility function (max reward). ReAct Agent w/ scoring Crew Scoring Agents Reward-based AutoAgents RLlib Policy Optimizers ADK Utility Function APIs Bedrock w/ LLM Ranking Learning Agents Improve performance via feedback &amp; learning. Fine-tuned Chains + RAG Adaptive Crew Self-Improving AutoAgents RLlib RL Learners ADK Adaptive Learning Agents Bedrock Continuous Training Hierarchical Agents Organize into sub-agents (manager/worker). Multi-Chain Agents Manager-Agent \u2192 Worker-Agent Multi-Agent Orchestrator RLlib Hierarchical RL ADK Orchestrated Agents Bedrock w/ Multi-Agent Workflow Multi-Agent Systems (MAS) Multiple agents coordinate/compete. Agent Executor w/ multiple tools Multi-Agent Crew AutoGen Multi-Agent Chat RLlib Multi-Agent Env ADK MAS Deployment Bedrock Orchestration &amp; Guardrails \ud83e\udde9 Agentic AI \u2013 Framework &amp; Domain Mapping (Finance vs Healthcare) Agent Type Frameworks (CrewAI, AutoGen, LangChain, LlamaIndex, Google ADK, AWS Bedrock) Finance (Fraud Detection) Healthcare (Diagnosis &amp; Treatment) Reactive Agents CrewAI, LangChain Detect anomalies in real-time transactions \u2192 block suspicious credit card use instantly Monitor patient vitals (e.g., heartbeat, oxygen levels) \u2192 trigger alerts for anomalies Deliberative Agents (Model-based) AutoGen, LlamaIndex Use reasoning with historical spending patterns + user profiles to flag fraud risk Analyze symptom history, lab reports \u2192 reason about possible diseases Utility-Based Agents CrewAI, AWS Bedrock Score transactions based on fraud probability &amp; business impact \u2192 prioritize investigation Rank treatment options by success rate, risk, and patient health utility Learning Agents LangChain, Google ADK, AutoGen Continuously learn from new fraud tactics \u2192 adapt fraud detection models Learn from diagnosis outcomes \u2192 refine disease prediction &amp; treatment models Hierarchical Agents CrewAI (multi-agent), LangChain Orchestrator Layered fraud defense: (1) Data collector agent \u2192 (2) Pattern analyzer agent \u2192 (3) Decision agent Layered diagnosis: (1) Symptom collector agent \u2192 (2) Diagnosis agent \u2192 (3) Treatment recommender Multi-Agent Systems (MAS) AutoGen, CrewAI, LlamaIndex Collaborative agents: one agent monitors banking apps, another monitors cards, another checks KYC \u2192 combine signals Collaborative care agents: radiology agent reads scans, lab agent processes tests, doctor agent synthesizes diagnosis Agentic AI Orchestrators Google ADK, AWS Bedrock, LangChain Bedrock \u2192 integrate fraud detection ML models + transaction APIs; Google ADK \u2192 orchestrate fraud alert workflow Bedrock \u2192 integrate diagnosis LLM + EHR APIs; Google ADK \u2192 orchestrate care pathway (triage \u2192 diagnosis \u2192 treatment) \ud83d\udccc Side-by-side comparison of AI Agents vs Agentic AI: Feature AI Agents Agentic AI Definition Software entities that perceive, decide, and act toward predefined goals Next-generation AI that proactively reasons, plans, adapts, and self-directs tasks Initiative Mostly reactive \u2013 acts after receiving input or trigger Proactive \u2013 can initiate actions without explicit prompts Planning Executes single or predefined sequences of steps Performs multi-step reasoning and can dynamically reorder or change plans Adaptability Limited to programmed rules or ML model predictions Adaptive \u2013 learns from feedback, changes strategies, and can adjust goals Tool Usage Uses specific tools or APIs assigned at design time Can select, combine, and orchestrate multiple tools or other agents on the fly Goal Handling Works toward fixed, clearly defined goals Can refine, prioritize, or even redefine goals based on changing context Learning &amp; Reflection Usually relies on offline training; no self-reflection loop Includes reflection loops \u2013 evaluates past performance, learns, and improves Example Use Case Chatbot answering FAQs, automated email sorter, robotic process automation Research assistant that autonomously investigates a problem, gathers data, writes reports, and follows up Complexity Level Lower \u2013 simpler rules or single-model logic Higher \u2013 may involve multi-agent orchestration, reasoning frameworks, and planning modules Human Oversight Frequent \u2013 requires task-by-task instructions Minimal \u2013 can work on broad objectives for extended periods \ud83d\udccc Use Case Breakdown Customer inquiry <p>Scenario:</p> <p>Customer inquiry received by email about a product (e.g., \u201cleft side footrest \u2013 what does it cost?\u201d). Steps involved:</p> <ol> <li> <p>Input processing: Receive email from website\u2019s contact form.</p> </li> <li> <p>Information retrieval: Search exploded views (Drive/T-drive) for product category \u2192 product \u2192 part number.</p> </li> <li> <p>Database lookup: Get price from DC.</p> </li> <li> <p>Contextual check: Verify if the part is under a customer contract (affects price).</p> </li> <li> <p>Decision-making: Choose correct price based on contract.</p> </li> <li> <p>Action: Respond via email/phone.</p> </li> </ol> \ud83e\udde0 Agent Type Classification Step Function Agent Type Parse email (NLP, intent extraction) Language Understanding Reactive Agent (responds to current input, doesn\u2019t learn history) Identify product &amp; map exploded views Information Retrieval Reasoning Agent (infers correct mapping using structured knowledge) Fetch pricing &amp; contract rules Knowledge/Context Query Knowledge-based Agent (uses domain knowledge like DC, contracts) Decide contract applicability Rule-based Decision Utility-based Agent (chooses outcome based on maximizing correct pricing rule) Respond (email/phone) Action Execution Action Agent (task-performing agent) \ud83d\udd17 Comparison Across Agentic AI Frameworks Framework How this Use Case Maps Agent Types Involved LangChain (LCEL + Tools + Agents) - Use an <code>EmailTool</code> for input/output.  - Use <code>RetrievalQA</code> for exploded views &amp; contracts.  - Use <code>Rule-based Agent</code> for contract vs non-contract pricing. Reactive + Reasoning + Knowledge-based + Utility-based CrewAI - Multiple specialized agents:    \u2022 Email Agent \u2192 handle incoming msg.    \u2022 Product Info Agent \u2192 query exploded views.    \u2022 Contract Agent \u2192 check contracts.    \u2022 Response Agent \u2192 send final reply.  Crew coordinates flow. Multi-Agent Orchestration AutoGen - Chat-driven agents collaborate:    \u2022 UserProxyAgent (customer input)    \u2022 ProductDBAgent (product lookup)    \u2022 ContractAgent (pricing rules)    \u2022 EmailAgent (reply). Conversational + Collaborative Agents Haystack - Use Pipelines:  Email \u2192 Text Preprocessor \u2192 Retriever (Drive/DC) \u2192 Ranker \u2192 Decision Node (contract?) \u2192 Generator (email). Knowledge + Reasoning Agents Microsoft Semantic Kernel - Skills/Plugins:    \u2022 Email Skill    \u2022 Contract Skill    \u2022 Product Lookup Skill    \u2022 Planner decides order. Planning Agent + Utility-based Google AIDK (AI Development Kit) - Agents as Vertex AI Functions:    \u2022 Retrieval function \u2192 exploded views.    \u2022 Pricing check function.    \u2022 Contract function.    \u2022 Response function. Tool-using Agents AWS Bedrock Agents - Bedrock Agent Orchestration:    \u2022 Parse email with Claude/LLM.    \u2022 Query DynamoDB (pricing, contract).    \u2022 Business logic (Lambda).    \u2022 Respond via SES/SNS. Orchestrator + Knowledge Agents <p>Note: use case is a Hybrid Multi-Agent System (Reactive + Reasoning + Knowledge-based + Utility-based + Action Agents).</p> <pre><code>                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Input Channel   \u2502\n                \u2502 (Email / Phone)   \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502   Ingestion Agent  \u2502\n               \u2502 (Parse Email Text, \u2502\n               \u2502 Extract Entities:  \u2502\n               \u2502 Product, Contact)  \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   Reasoning Agent   \u2502\n              \u2502  (Exploded View DB  \u2502\n              \u2502   Lookup \u2192 Identify \u2502\n              \u2502   Product + Part #) \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502   Knowledge Agent        \u2502\n           \u2502 (DC System / Contracts)  \u2502\n           \u2502 - Fetch price from DC    \u2502\n           \u2502 - Check customer contract\u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2518\n                       \u2502\n                       \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502   Decision Orchestrator Agent\u2502\n        \u2502 (Applies Pricing Logic:      \u2502\n        \u2502 Contract Price vs Standard)  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502   Response Generation Agent\u2502\n      \u2502 (Email/Text Generator,     \u2502\n      \u2502 Multilingual if needed)    \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502 Output Channel              \u2502\n      \u2502  - Email Reply if Email ID  \u2502\n      \u2502  - Phone Call if only Phone \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> \ud83d\udccc Documents Links <p>https://python.langchain.com/docs/modules/agents/</p> <p>https://github.com/microsoft/autogen/tree/main/notebook</p> <p>https://github.com/joaomdmoura/crewAI</p> <p>https://python.langchain.com/docs/modules/agents/get_started</p> <p>https://aws.amazon.com/blogs/machine-learning/the-next-wave-of-ai-agentic-ai/</p> <p>https://www.capgemini.com/insights/research-library/the-rise-of-agentic-ai/</p> <p>https://microsoft.github.io/autogen/</p> <p>https://github.com/joaomdmoura/crewAI</p> <p>https://www.langchain.com/</p>"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/","title":"Azure OpenAI in Microsoft Foundry models","text":"<p>Azure OpenAI is powered by a diverse set of models with different capabilities and price points.Model availability varies by region and cloud.</p> Model Category Description GPT-5.1 series NEW gpt-5.1, gpt-5.1-chat, gpt-5.1-codex, gpt-5.1-codex-mini Sora NEW sora-2 GPT-5 series gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-chat gpt-oss Open-weight reasoning models codex-mini Fine-tuned version of o4-mini GPT-4.1 series gpt-4.1, gpt-4.1-mini, gpt-4.1-nano computer-use-preview Experimental model for the Responses API computer-use tool o-series models Reasoning models with advanced problem solving and increased focus and capability GPT-4o, GPT-4o mini, GPT-4 Turbo Azure OpenAI multimodal models (text + images) GPT-4 Improved over GPT-3.5; understands and generates natural language and code GPT-3.5 Improved over GPT-3; understands and generates natural language and code Embeddings Models that convert text into vector representations for similarity tasks Image generation Models that generate original images from text Video generation Models that generate original video scenes from text Audio Speech-to-text, translation, and text-to-speech models; GPT-4o supports low-latency voice interactions"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/#simplified-architecture-friendly-domain-view-recommended-for-agent-design","title":"\ud83c\udfaf Simplified Architecture-Friendly Domain View (recommended for Agent Design)","text":"Azure Domain What Agents You Would Create Identity IdentityAgent, RBACAgent Network NetworkAgent, DNSAgent, FirewallAgent Compute ComputeAgent, VMAgent, AppServiceAgent Containers AKSAgent, ContainerRegistryAgent Storage StorageAgent Database SQLAgent, CosmosAgent, PostgresAgent Security KeyVaultAgent, PolicyAgent, DefenderAgent Governance/Management ResourceGroupAgent, TaggingAgent, CostAgent Monitoring MonitorAgent, LogAnalyticsAgent DevOps TerraformAgent, PipelineAgent Integration APIMAgent, ServiceBusAgent AI/Analytics OpenAIAgent, AMLAgent BCDR BackupAgent, RecoveryAgent"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/#master-list-azure-domains-top-level-enterprise-architecture-view","title":"\u2705 MASTER LIST \u2014 Azure Domains (Top-Level Enterprise Architecture View)","text":"<p>These are the canonical Azure domains recognized across Microsoft Cloud Adoption Framework (CAF), Landing Zones, Enterprise-Scale Architecture, and Well-Architected Framework.</p>"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/#1-identity-access-management-iam","title":"1. Identity &amp; Access Management (IAM)","text":"<ul> <li>Azure Active Directory / Entra ID</li> <li>Conditional Access</li> <li>Privileged Identity Management (PIM)</li> <li>Managed Identities</li> <li>RBAC, Custom Roles</li> </ul>"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/#2-network-connectivity","title":"2. Network &amp; Connectivity","text":"<ul> <li>Virtual Networks (VNet)</li> <li>Subnets / NSGs</li> <li>Azure Firewall / WAF</li> <li>Application Gateway</li> <li>VPN Gateway</li> <li>ExpressRoute</li> <li>Load Balancers</li> <li>Private Link / Private Endpoints</li> <li>DNS / Private DNS</li> </ul>"},{"location":"AgenticAI/AZURE/azure-openai-foundry-modelsure/#3-compute","title":"3. Compute","text":"<ul> <li>Azure Virtual Machines</li> <li>VM Scale Sets (VMSS)</li> <li>Azure App Service</li> <li>Azure Functions</li> <li>Azure Batch</li> <li>Azure Container Instances (ACI)</li> <li>Azure Container Apps</li> </ul>"},{"location":"AgenticAI/AZURE/azure/","title":"Overview","text":""},{"location":"AgenticAI/AZURE/azure/#here-is-the-azure-ai-services","title":"Here is the Azure AI services","text":"Service Description Service Type SaaS / Shelf-Managed Use Case Example Azure AI Agent Service Combine generative AI with real-world data tools for agentic applications. AI Platform Agent SaaS Build AI-powered virtual assistants that interact with databases and APIs Azure AI Model Inference Run inference on pre-trained flagship models in the Azure AI catalog. Model Serving SaaS Perform real-time text generation or image classification Azure AI Search Cloud-based search-as-a-service with AI capabilities. Cognitive Search SaaS Add AI-powered search to websites or applications Azure OpenAI Access large language models like GPT from OpenAI via API. AI Model API SaaS Build chatbots, content generation tools, or summarization services Bot Service Build and deploy conversational bots connected to multiple channels. Bot Framework SaaS Create customer service bots for web, Teams, and mobile platforms Content Safety Detects harmful or unwanted content in text and images. Safety AI Service SaaS Moderate user-generated content on social platforms Custom Vision Train custom image classification models specific to your use case. Computer Vision API SaaS Identify product defects in manufacturing Document Intelligence Extract structured data from documents like invoices, forms, etc. Form Recognizer SaaS Automate invoice processing by extracting key fields Face Facial recognition API to detect and analyze human faces. Vision API SaaS Implement identity verification or attendance tracking Immersive Reader Tool that helps users improve reading comprehension. Reading Tool SaaS Provide accessible reading support in education apps Language Offers NLP services like entity recognition, sentiment analysis, etc. NLP Platform SaaS Analyze customer feedback for sentiment and key topics Speech Speech-to-text, text-to-speech, translation, and speaker recognition services. Speech AI API SaaS Transcribe meetings or generate natural-sounding audio narration Translator Translate text into 100+ languages and dialects using AI. Translation API SaaS Provide multilingual support for global audiences Video Indexer Extract insights (speech, objects, people, sentiment) from video. Media AI Platform SaaS Tag and index video content for easy retrieval in media libraries Vision Analyze visual content (images, videos) using pre-trained models. Computer Vision API SaaS Detect objects and classify images in retail or security apps"},{"location":"AgenticAI/GCP/Agents/","title":"Agents","text":"\u2705 Agents \ud83d\udccc What is Agents? <p>In the Agent Development Kit (ADK), an Agent is a self-contained execution unit designed to act autonomously to achieve specific goals. Agents can perform tasks, interact with users, utilize external tools, and coordinate with other agents.</p> <p>The foundation for all agents in ADK is the BaseAgent class. It serves as the fundamental blueprint. To create functional agents, you typically extend BaseAgent in one of three main ways, catering to different needs \u2013 from intelligent reasoning to structured process control.</p> <p></p> \ud83d\udccc Core Agent Categories <p>ADK provides distinct agent categories to build sophisticated applications:</p> \u2705 LLM Agents (LlmAgent, Agent): <p>These agents utilize Large Language Models (LLMs) as their core engine to understand natural language, reason, plan, generate responses, and dynamically decide how to proceed or which tools to use, making them ideal for flexible, language-centric tasks.</p> \u2705 Workflow Agents (SequentialAgent, ParallelAgent, LoopAgent): <p>These specialized agents control the execution flow of other agents in predefined, deterministic patterns (sequence, parallel, or loop) without using an LLM for the flow control itself, perfect for structured processes needing predictable execution.</p> \u2705 Custom Agents: <p>Created by extending BaseAgent directly, these agents allow you to implement unique operational logic, specific control flows, or specialized integrations not covered by the standard types, catering to highly tailored application requirements.</p> \ud83d\udccc Choosing the Right Agent Type Feature LLM Agent (<code>LlmAgent</code>) Workflow Agent Custom Agent (<code>BaseAgent</code> subclass) Primary Function Reasoning, Generation, Tool Use Controlling Agent Execution Flow Implementing Unique Logic/Integrations Core Engine Large Language Model (LLM) Predefined Logic (Sequence, Parallel, Loop) Custom Code Determinism Non-deterministic (Flexible) Deterministic (Predictable) Can be either, based on implementation Primary Use Language tasks, Dynamic decisions Structured processes, Orchestration Tailored requirements, Specific workflows \ud83d\udccc Agents Working Together: Multi-Agent Systems <p>While each agent type serves a distinct purpose, the true power often comes from combining them. Complex applications frequently employ multi-agent architectures where:</p> <ul> <li> <p>LLM Agents handle intelligent, language-based task execution.</p> </li> <li> <p>Workflow Agents manage the overall process flow using standard patterns.</p> </li> <li> <p>Custom Agents provide specialized capabilities or rules needed for unique integrations.</p> </li> </ul> \ud83d\udccc LLM Agents: <p>The <code>LlmAgent</code> (often aliased simply as Agent) is a core component in ADK, acting as the \"thinking\" part of your application. It leverages the power of a Large Language Model (LLM) for reasoning, understanding natural language, making decisions, generating responses, and interacting with tools.</p> <p>Unlike deterministic <code>Workflow Agents</code> that follow predefined execution paths, LlmAgent behavior is non-deterministic. It uses the LLM to interpret instructions and context, deciding dynamically how to proceed, which tools to use (if any), or whether to transfer control to another agent.</p> <p>Building an effective <code>LlmAgent</code> involves defining its identity, clearly guiding its behavior through instructions, and equipping it with the necessary tools and capabilities.</p> \ud83d\udccc Defining the Agent's Identity and Purpose: <p>First, you need to establish what the agent is and what it's for.</p> <ul> <li> <p>name <code>(Required):</code> Every agent needs a unique string identifier. This name is crucial for internal operations, especially in multi-agent systems where agents need to refer to or delegate tasks to each other. Choose a descriptive name that reflects the agent's function (e.g., <code>customer_support_router</code>, <code>billing_inquiry_agent</code>). Avoid reserved names like <code>user</code>.</p> </li> <li> <p>description <code>(Optional, Recommended for Multi-Agent):</code> Provide a concise summary of the agent's capabilities. This description is primarily used by other LLM agents to determine if they should route a task to this agent. Make it specific enough to differentiate it from peers (e.g., \"Handles inquiries about current billing statements,\" not just \"Billing agent\").</p> </li> <li> <p>model <code>(Required):</code> Specify the underlying LLM that will power this agent's reasoning. This is a string identifier like <code>\"gemini-2.0-flash\"</code>. The choice of model impacts the agent's capabilities, cost, and performance. See the Models page for available options and considerations.</p> </li> </ul> <pre><code># Example: Defining the basic identity\ncapital_agent = LlmAgent(\n    model=\"gemini-2.0-flash\",\n    name=\"capital_agent\",\n    description=\"Answers user questions about the capital city of a given country.\"\n    # instruction and tools will be added next\n)\n</code></pre> \ud83d\udccc Guiding the Agent: Instructions (instruction) <p>The <code>instruction</code> parameter is arguably the most critical for shaping an <code>LlmAgent's</code> behavior. It's a string (or a function returning a string) that tells the agent:</p> <ul> <li> <p>Its core task or goal.</p> </li> <li> <p>Its personality or persona (e.g., \"You are a helpful assistant,\" \"You are a witty pirate\").</p> </li> <li> <p>Constraints on its behavior (e.g., \"Only answer questions about X,\" \"Never reveal Y\").</p> </li> <li> <p>How and when to use its <code>tools</code>. You should explain the purpose of each tool and the circumstances under which it should be called, supplementing any descriptions within the tool itself.</p> </li> <li> <p>The desired format for its output (e.g., \"Respond in JSON,\" \"Provide a bulleted list\").</p> </li> </ul> \ud83d\udccc Tips for Effective Instructions: <ul> <li> <p>Be Clear and Specific: Avoid ambiguity. Clearly state the desired actions and outcomes.</p> </li> <li> <p>Use Markdown: Improve readability for complex instructions using headings, lists, etc.</p> </li> <li> <p>Provide Examples (Few-Shot): For complex tasks or specific output formats, include examples directly in the instruction.</p> </li> <li> <p>Guide Tool Use: Don't just list tools; explain when and why the agent should use them.</p> </li> </ul> \ud83d\udccc State: <ul> <li> <p>The instruction is a string template, you can use the {var} syntax to insert dynamic values into the instruction.</p> </li> <li> <p><code>{var}</code> is used to insert the value of the state variable named var.</p> </li> <li> <p><code>{artifact.var}</code> is used to insert the text content of the artifact named var.</p> </li> <li> <p>If the state variable or artifact does not exist, the agent will raise an error. If you want to ignore the error, you can append a <code>?</code> to the variable name as in <code>{var?}</code>.</p> </li> </ul> <pre><code># Example: Adding instructions\ncapital_agent = LlmAgent(\n    model=\"gemini-2.0-flash\",\n    name=\"capital_agent\",\n    description=\"Answers user questions about the capital city of a given country.\",\n    instruction=\"\"\"You are an agent that provides the capital city of a country.\nWhen a user asks for the capital of a country:\n1. Identify the country name from the user's query.\n2. Use the `get_capital_city` tool to find the capital.\n3. Respond clearly to the user, stating the capital city.\nExample Query: \"What's the capital of {country}?\"\nExample Response: \"The capital of France is Paris.\"\n\"\"\",\n    # tools will be added next\n)\n</code></pre> <p>Note: For instructions that apply to all agents in a system, consider using global_instruction on the root agent.</p> \ud83d\udccc Equipping the Agent: Tools (tools) <p>Tools give your <code>LlmAgent</code> capabilities beyond the LLM's built-in knowledge or reasoning. They allow the agent to interact with the outside world, perform calculations, fetch real-time data, or execute specific actions.</p> <ul> <li> <p>tools <code>(Optional)</code>: Provide a list of tools the agent can use. Each item in the list can be:</p> <ul> <li> <p>A native function or method (wrapped as a FunctionTool). Python ADK automatically wraps the native function into a FuntionTool whereas, you must explicitly wrap your Java methods using FunctionTool.create(...)</p> </li> <li> <p>An instance of a class inheriting from <code>BaseTool</code>.</p> </li> <li> <p>An instance of another agent AgentTool, enabling agent-to-agent delegation.</p> </li> </ul> </li> </ul> <p>The LLM uses the function/tool names, descriptions (from docstrings or the description field), and parameter schemas to decide which tool to call based on the conversation and its instructions.</p> <pre><code># Define a tool function\ndef get_capital_city(country: str) -&gt; str:\n  \"\"\"Retrieves the capital city for a given country.\"\"\"\n  # Replace with actual logic (e.g., API call, database lookup)\n  capitals = {\"france\": \"Paris\", \"japan\": \"Tokyo\", \"canada\": \"Ottawa\"}\n  return capitals.get(country.lower(), f\"Sorry, I don't know the capital of {country}.\")\n\n# Add the tool to the agent\ncapital_agent = LlmAgent(\n    model=\"gemini-2.0-flash\",\n    name=\"capital_agent\",\n    description=\"Answers user questions about the capital city of a given country.\",\n    instruction=\"\"\"You are an agent that provides the capital city of a country... (previous instruction text)\"\"\",\n    tools=[get_capital_city] # Provide the function directly\n)\n</code></pre> \ud83d\udccc Advanced Configuration &amp; Control <p>Beyond the core parameters, LlmAgent offers several options for finer control:</p> <p>Configuring LLM Generation (generate_content_config)</p> <p>You can adjust how the underlying LLM generates responses using <code>generate_content_config</code>.</p> <ul> <li>generate_content_config <code>(Optional)</code>: Pass an instance of <code>google.genai.types.GenerateContentConfig</code> to control parameters like <code>temperature</code> (randomness), <code>max_output_tokens</code> (response length), <code>top_p, top_k,</code> and safety settings.</li> </ul> <pre><code>from google.genai import types\n\nagent = LlmAgent(\n    # ... other params\n    generate_content_config=types.GenerateContentConfig(\n        temperature=0.2, # More deterministic output\n        max_output_tokens=250,\n        safety_settings=[\n            types.SafetySetting(\n                category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n            )\n        ]\n    )\n)\n</code></pre> <p>Structuring Data <code>(input_schema, output_schema, output_key)</code></p> <p>For scenarios requiring structured data exchange with an LLM Agent, the ADK provides mechanisms to define expected input and desired output formats using schema definitions.</p> <ul> <li> <p>input_schema (Optional): Define a schema representing the expected input structure. If set, the user message content passed to this agent must be a JSON string conforming to this schema. Your instructions should guide the user or preceding agent accordingly.</p> </li> <li> <p>output_schema (Optional): Define a schema representing the desired output structure. If set, the agent's final response must be a JSON string conforming to this schema.</p> </li> <li> <p>output_key (Optional): Provide a string key. If set, the text content of the agent's final response will be automatically saved to the session's state dictionary under this key. This is useful for passing results between agents or steps in a workflow.</p> <ul> <li>In Python, this might look like: <code>session.state[output_key] = agent_response_text</code></li> </ul> </li> </ul> <pre><code>from pydantic import BaseModel, Field\n\nclass CapitalOutput(BaseModel):\n    capital: str = Field(description=\"The capital of the country.\")\n\nstructured_capital_agent = LlmAgent(\n    # ... name, model, description\n    instruction=\"\"\"You are a Capital Information Agent. Given a country, respond ONLY with a JSON object containing the capital. Format: {\"capital\": \"capital_name\"}\"\"\",\n    output_schema=CapitalOutput, # Enforce JSON output\n    output_key=\"found_capital\"  # Store result in state['found_capital']\n    # Cannot use tools=[get_capital_city] effectively here\n)\n</code></pre> \ud83d\udccc Managing Context (include_contents) <p>Control whether the agent receives the prior conversation history.</p> <ul> <li> <p>include_contents (Optional, Default: 'default'): Determines if the contents (history) are sent to the LLM.</p> <ul> <li> <p>'default': The agent receives the relevant conversation history.</p> </li> <li> <p>'none': The agent receives no prior contents. It operates based solely on its current instruction and any input provided in the current turn (useful for stateless tasks or enforcing specific contexts).</p> </li> </ul> </li> </ul> <pre><code>stateless_agent = LlmAgent(\n    # ... other params\n    include_contents='none'\n)\n</code></pre> \ud83d\udccc Planner <p>planner (Optional): Assign a <code>BasePlanner</code> instance to enable multi-step reasoning and planning before execution. There are two main planners:</p> <ul> <li>BuiltInPlanner: Leverages the model's built-in planning capabilities (e.g., Gemini's thinking feature)      Here, the thinking_budget parameter guides the model on the number of thinking tokens to use when generating a response. The include_thoughts parameter controls whether the model should include its raw thoughts and internal reasoning process in the response.</li> </ul> <pre><code>from google.adk import Agent\nfrom google.adk.planners import BuiltInPlanner\nfrom google.genai import types\n\nmy_agent = Agent(\n    model=\"gemini-2.5-flash\",\n    planner=BuiltInPlanner(\n        thinking_config=types.ThinkingConfig(\n            include_thoughts=True,\n            thinking_budget=1024,\n        )\n    ),\n    # ... your tools here\n)\n</code></pre> <ul> <li>PlanReActPlanner: This planner instructs the model to follow a specific structure in its output: first create a plan, then execute actions (like calling tools), and provide reasoning for its steps. It's particularly useful for models that don't have a built-in \"thinking\" feature.</li> </ul> <pre><code>from google.adk import Agent\nfrom google.adk.planners import PlanReActPlanner\n\nmy_agent = Agent(\n    model=\"gemini-2.0-flash\",\n    planner=PlanReActPlanner(),\n    # ... your tools here\n)\n</code></pre> <p>The agent's response will follow a structured format:</p> <pre><code>[user]: ai news\n[google_search_agent]: /*PLANNING*/\n1. Perform a Google search for \"latest AI news\" to get current updates and headlines related to artificial intelligence.\n2. Synthesize the information from the search results to provide a summary of recent AI news.\n\n/*ACTION*/\n/*REASONING*/\nThe search results provide a comprehensive overview of recent AI news, covering various aspects like company developments, research breakthroughs, and applications. I have enough information to answer the user's request.\n\n/*FINAL_ANSWER*/\nHere's a summary of recent AI news:\n....\n</code></pre> \ud83d\udccc Code Execution <ul> <li>code_executor (Optional): Provide a BaseCodeExecutor instance to allow the agent to execute code blocks found in the LLM's response.</li> </ul> <p>Example for using built-in-planner:</p> <pre><code>from dotenv import load_dotenv\n\n\nimport asyncio\nimport os\n\nfrom google.genai import types\nfrom google.adk.agents.llm_agent import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService # Optional\nfrom google.adk.planners import BasePlanner, BuiltInPlanner, PlanReActPlanner\nfrom google.adk.models import LlmRequest\n\nfrom google.genai.types import ThinkingConfig\nfrom google.genai.types import GenerateContentConfig\n\nimport datetime\nfrom zoneinfo import ZoneInfo\n\nAPP_NAME = \"weather_app\"\nUSER_ID = \"1234\"\nSESSION_ID = \"session1234\"\n\ndef get_weather(city: str) -&gt; dict:\n    \"\"\"Retrieves the current weather report for a specified city.\n\n    Args:\n        city (str): The name of the city for which to retrieve the weather report.\n\n    Returns:\n        dict: status and result or error msg.\n    \"\"\"\n    if city.lower() == \"new york\":\n        return {\n            \"status\": \"success\",\n            \"report\": (\n                \"The weather in New York is sunny with a temperature of 25 degrees\"\n                \" Celsius (77 degrees Fahrenheit).\"\n            ),\n        }\n    else:\n        return {\n            \"status\": \"error\",\n            \"error_message\": f\"Weather information for '{city}' is not available.\",\n        }\n\n\ndef get_current_time(city: str) -&gt; dict:\n    \"\"\"Returns the current time in a specified city.\n\n    Args:\n        city (str): The name of the city for which to retrieve the current time.\n\n    Returns:\n        dict: status and result or error msg.\n    \"\"\"\n\n    if city.lower() == \"new york\":\n        tz_identifier = \"America/New_York\"\n    else:\n        return {\n            \"status\": \"error\",\n            \"error_message\": (\n                f\"Sorry, I don't have timezone information for {city}.\"\n            ),\n        }\n\n    tz = ZoneInfo(tz_identifier)\n    now = datetime.datetime.now(tz)\n    report = (\n        f'The current time in {city} is {now.strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")}'\n    )\n    return {\"status\": \"success\", \"report\": report}\n\n# Step 1: Create a ThinkingConfig\nthinking_config = ThinkingConfig(\n    include_thoughts=True,   # Ask the model to include its thoughts in the response\n    thinking_budget=256      # Limit the 'thinking' to 256 tokens (adjust as needed)\n)\nprint(\"ThinkingConfig:\", thinking_config)\n\n# Step 2: Instantiate BuiltInPlanner\nplanner = BuiltInPlanner(\n    thinking_config=thinking_config\n)\nprint(\"BuiltInPlanner created.\")\n\n# Step 3: Wrap the planner in an LlmAgent\nagent = LlmAgent(\n    model=\"gemini-2.5-pro-preview-03-25\",  # Set your model name\n    name=\"weather_and_time_agent\",\n    instruction=\"You are an agent that returns time and weather\",\n    planner=planner,\n    tools=[get_weather, get_current_time]\n)\n\n# Session and Runner\nsession_service = InMemorySessionService()\nsession = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\nrunner = Runner(agent=agent, app_name=APP_NAME, session_service=session_service)\n\n# Agent Interaction\ndef call_agent(query):\n    content = types.Content(role='user', parts=[types.Part(text=query)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n    for event in events:\n        print(f\"\\nDEBUG EVENT: {event}\\n\")\n        if event.is_final_response() and event.content:\n            final_answer = event.content.parts[0].text.strip()\n            print(\"\\n\ud83d\udfe2 FINAL ANSWER\\n\", final_answer, \"\\n\")\n\ncall_agent(\"If it's raining in New York right now, what is the current temperature?\")\n</code></pre>"},{"location":"AgenticAI/GCP/Agents/#workflow-agents","title":"Workflow Agents","text":"\ud83d\udccc workflow agents <p>Workflow agents - specialized agents that control the execution flow of its sub-agents.</p> <p>Workflow agents are specialized components in ADK designed purely for orchestrating the execution flow of sub-agents. Their primary role is to manage how and when other agents run, defining the control flow of a process.</p> <p>Unlike LLM Agents, which use Large Language Models for dynamic reasoning and decision-making, Workflow Agents operate based on predefined logic. They determine the execution sequence according to their type (e.g., sequential, parallel, loop) without consulting an LLM for the orchestration itself. This results in deterministic and predictable execution patterns.</p> <p>ADK provides three core workflow agent types, each implementing a distinct execution pattern:</p> <ol> <li> <p>Sequential Agents</p> </li> <li> <p>Loop Agents</p> </li> <li> <p>Parallel Agents</p> </li> </ol> \ud83d\udccc Why Use Workflow Agents? <p>Workflow agents are essential when you need explicit control over how a series of tasks or agents are executed. They provide:</p> <ul> <li> <p>Predictability: The flow of execution is guaranteed based on the agent type and configuration.</p> </li> <li> <p>Reliability: Ensures tasks run in the required order or pattern consistently.</p> </li> <li> <p>Structure: Allows you to build complex processes by composing agents within clear control structures.</p> </li> </ul> \ud83d\udccc Sequential agents <p>The SequentialAgent is a workflow agent that executes its sub-agents in the order they are specified in the list.</p> <p>Example:</p> <ul> <li>You want to build an agent that can summarize any webpage, using two tools: Get Page Contents and Summarize Page. Because the agent must always call Get Page Contents before calling Summarize Page (you can't summarize from nothing!), you should build your agent using a SequentialAgent.</li> </ul> <p>As with other workflow agents, the SequentialAgent is not powered by an LLM, and is thus deterministic in how it executes. That being said, workflow agents are concerned only with their execution (i.e. in sequence), and not their internal logic; the tools or sub-agents of a workflow agent may or may not utilize LLMs.</p> <p>How it works</p> <p>When the <code>SequentialAgent</code>'s <code>Run Async</code> method is called, it performs the following actions:</p> <ol> <li> <p>Iteration: It iterates through the sub agents list in the order they were provided.</p> </li> <li> <p>Sub-Agent Execution: For each sub-agent in the list, it calls the sub-agent's <code>Run Async</code> method.</p> </li> </ol> <p></p> <p>Full Example: Code Development Pipeline</p> <p>Consider a simplified code development pipeline:</p> <ul> <li> <p>Code Writer Agent: An LLM Agent that generates initial code based on a specification.</p> </li> <li> <p>Code Reviewer Agent: An LLM Agent that reviews the generated code for errors, style issues, and adherence to best practices. It receives the output of the Code Writer Agent.</p> </li> <li> <p>Code Refactorer Agent: An LLM Agent that takes the reviewed code (and the reviewer's comments) and refactors it to improve quality and address issues.</p> </li> </ul> <p>A <code>SequentialAgent</code> is perfect for this:</p> <pre><code>SequentialAgent(sub_agents=[CodeWriterAgent, CodeReviewerAgent, CodeRefactorerAgent])\n</code></pre> <p>This ensures the code is written, then reviewed, and finally refactored, in a strict, dependable order. The output from each sub-agent is passed to the next by storing them in state via <code>Output Key</code>.</p> <p>Shared Invocation Context</p> <p>The <code>SequentialAgent</code> passes the same <code>InvocationContext</code> to each of its sub-agents. This means they all share the same session state, including the temporary (temp:) namespace, making it easy to pass data between steps within a single turn.</p> <pre><code># Part of agent.py --&gt; Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\n\n# --- 1. Define Sub-Agents for Each Pipeline Stage ---\n\n# Code Writer Agent\n# Takes the initial specification (from user query) and writes code.\ncode_writer_agent = LlmAgent(\n    name=\"CodeWriterAgent\",\n    model=GEMINI_MODEL,\n    # Change 3: Improved instruction\n    instruction=\"\"\"You are a Python Code Generator.\nBased *only* on the user's request, write Python code that fulfills the requirement.\nOutput *only* the complete Python code block, enclosed in triple backticks (```python ... ```). \nDo not add any other text before or after the code block.\n\"\"\",\n    description=\"Writes initial Python code based on a specification.\",\n    output_key=\"generated_code\" # Stores output in state['generated_code']\n)\n\n# Code Reviewer Agent\n# Takes the code generated by the previous agent (read from state) and provides feedback.\ncode_reviewer_agent = LlmAgent(\n    name=\"CodeReviewerAgent\",\n    model=GEMINI_MODEL,\n    # Change 3: Improved instruction, correctly using state key injection\n    instruction=\"\"\"You are an expert Python Code Reviewer. \n    Your task is to provide constructive feedback on the provided code.\n\n    **Code to Review:**\n    ```python\n    {generated_code}\n    ```\n\n**Review Criteria:**\n1.  **Correctness:** Does the code work as intended? Are there logic errors?\n2.  **Readability:** Is the code clear and easy to understand? Follows PEP 8 style guidelines?\n3.  **Efficiency:** Is the code reasonably efficient? Any obvious performance bottlenecks?\n4.  **Edge Cases:** Does the code handle potential edge cases or invalid inputs gracefully?\n5.  **Best Practices:** Does the code follow common Python best practices?\n\n**Output:**\nProvide your feedback as a concise, bulleted list. Focus on the most important points for improvement.\nIf the code is excellent and requires no changes, simply state: \"No major issues found.\"\nOutput *only* the review comments or the \"No major issues\" statement.\n\"\"\",\n    description=\"Reviews code and provides feedback.\",\n    output_key=\"review_comments\", # Stores output in state['review_comments']\n)\n\n\n# Code Refactorer Agent\n# Takes the original code and the review comments (read from state) and refactors the code.\ncode_refactorer_agent = LlmAgent(\n    name=\"CodeRefactorerAgent\",\n    model=GEMINI_MODEL,\n    # Change 3: Improved instruction, correctly using state key injection\n    instruction=\"\"\"You are a Python Code Refactoring AI.\nYour goal is to improve the given Python code based on the provided review comments.\n\n  **Original Code:**\n  python\n  {generated_code}\n\n\n  **Review Comments:**\n  {review_comments}\n\n**Task:**\nCarefully apply the suggestions from the review comments to refactor the original code.\nIf the review comments state \"No major issues found,\" return the original code unchanged.\nEnsure the final code is complete, functional, and includes necessary imports and docstrings.\n\n**Output:**\nOutput *only* the final, refactored Python code block, enclosed in triple backticks (```python ... ```). \nDo not add any other text before or after the code block.\n\"\"\",\n    description=\"Refactors code based on review comments.\",\n    output_key=\"refactored_code\", # Stores output in state['refactored_code']\n)\n\n\n# --- 2. Create the SequentialAgent ---\n# This agent orchestrates the pipeline by running the sub_agents in order.\ncode_pipeline_agent = SequentialAgent(\n    name=\"CodePipelineAgent\",\n    sub_agents=[code_writer_agent, code_reviewer_agent, code_refactorer_agent],\n    description=\"Executes a sequence of code writing, reviewing, and refactoring.\",\n    # The agents will run in the order provided: Writer -&gt; Reviewer -&gt; Refactorer\n)\n\n# For ADK tools compatibility, the root agent must be named `root_agent`\nroot_agent = code_pipeline_agent\n</code></pre>"},{"location":"AgenticAI/GCP/Agents/#loop-agents","title":"Loop agents","text":"\ud83d\udccc LoopAgent <p>The <code>LoopAgent</code> is a workflow agent that executes its sub-agents in a loop (i.e. iteratively). It repeatedly runs a sequence of agents for a specified number of iterations or until a termination condition is met.</p> <p>Use the <code>LoopAgent</code> when your workflow involves repetition or iterative refinement, such as revising code.</p> <p>Example:</p> <p>You want to build an agent that can generate images of food, but sometimes when you want to generate a specific number of items (e.g. 5 bananas), it generates a different number of those items in the image (e.g. an image of 7 bananas). You have two tools: <code>Generate Image</code>, <code>Count Food Items</code>. Because you want to keep generating images until it either correctly generates the specified number of items, or after a certain number of iterations, you should build your agent using a LoopAgent.</p> <p>As with other workflow agents, the LoopAgent is not powered by an LLM, and is thus deterministic in how it executes. That being said, workflow agents are only concerned only with their execution (i.e. in a loop), and not their internal logic; the tools or sub-agents of a workflow agent may or may not utilize LLMs.</p> <p>How it Works</p> <p>When the <code>LoopAgent's Run Async</code> method is called, it performs the following actions:</p> <ol> <li> <p>Sub-Agent Execution: It iterates through the Sub Agents list in order. For each sub-agent, it calls the agent's <code>Run Async</code> method.</p> </li> <li> <p>Termination Check:</p> </li> </ol> <p>Crucially, the LoopAgent itself does not inherently decide when to stop looping. You must implement a termination mechanism to prevent infinite loops. Common strategies include:</p> <ul> <li> <p>Max Iterations: Set a maximum number of iterations in the <code>LoopAgent</code>. The loop will terminate after that many iterations.</p> </li> <li> <p>Escalation from sub-agent: Design one or more sub-agents to evaluate a condition (e.g., \"Is the document quality good enough?\", \"Has a consensus been reached?\"). If the condition is met, the sub-agent can signal termination (e.g., by raising a custom event, setting a flag in a shared context, or returning a specific value).</p> </li> </ul> <p></p> <pre><code># Part of agent.py --&gt; Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\n\nimport asyncio\nimport os\nfrom google.adk.agents import LoopAgent, LlmAgent, BaseAgent, SequentialAgent\nfrom google.genai import types\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom google.adk.tools.tool_context import ToolContext\nfrom typing import AsyncGenerator, Optional\nfrom google.adk.events import Event, EventActions\n\n# --- Constants ---\nAPP_NAME = \"doc_writing_app_v3\" # New App Name\nUSER_ID = \"dev_user_01\"\nSESSION_ID_BASE = \"loop_exit_tool_session\" # New Base Session ID\nGEMINI_MODEL = \"gemini-2.0-flash\"\nSTATE_INITIAL_TOPIC = \"initial_topic\"\n\n# --- State Keys ---\nSTATE_CURRENT_DOC = \"current_document\"\nSTATE_CRITICISM = \"criticism\"\n# Define the exact phrase the Critic should use to signal completion\nCOMPLETION_PHRASE = \"No major issues found.\"\n\n# --- Tool Definition ---\ndef exit_loop(tool_context: ToolContext):\n  \"\"\"Call this function ONLY when the critique indicates no further changes are needed, signaling the iterative process should end.\"\"\"\n  print(f\"  [Tool Call] exit_loop triggered by {tool_context.agent_name}\")\n  tool_context.actions.escalate = True\n  # Return empty dict as tools should typically return JSON-serializable output\n  return {}\n\n# --- Agent Definitions ---\n\n# STEP 1: Initial Writer Agent (Runs ONCE at the beginning)\ninitial_writer_agent = LlmAgent(\n    name=\"InitialWriterAgent\",\n    model=GEMINI_MODEL,\n    include_contents='none',\n    # MODIFIED Instruction: Ask for a slightly more developed start\n    instruction=f\"\"\"You are a Creative Writing Assistant tasked with starting a story.\n    Write the *first draft* of a short story (aim for 2-4 sentences).\n    Base the content *only* on the topic provided below. Try to introduce a specific element (like a character, a setting detail, or a starting action) to make it engaging.\n    Topic: {{initial_topic}}\n\n    Output *only* the story/document text. Do not add introductions or explanations.\n\"\"\",\n    description=\"Writes the initial document draft based on the topic, aiming for some initial substance.\",\n    output_key=STATE_CURRENT_DOC\n)\n\n# STEP 2a: Critic Agent (Inside the Refinement Loop)\ncritic_agent_in_loop = LlmAgent(\n    name=\"CriticAgent\",\n    model=GEMINI_MODEL,\n    include_contents='none',\n    # MODIFIED Instruction: More nuanced completion criteria, look for clear improvement paths.\n    instruction=f\"\"\"You are a Constructive Critic AI reviewing a short document draft (typically 2-6 sentences). Your goal is balanced feedback.\n\n    **Document to Review:**\n    ```\n    {{current_document}}\n    ```\n\n    **Task:**\n    Review the document for clarity, engagement, and basic coherence according to the initial topic (if known).\n\n    IF you identify 1-2 *clear and actionable* ways the document could be improved to better capture the topic or enhance reader engagement (e.g., \"Needs a stronger opening sentence\", \"Clarify the character's goal\"):\n    Provide these specific suggestions concisely. Output *only* the critique text.\n\n    ELSE IF the document is coherent, addresses the topic adequately for its length, and has no glaring errors or obvious omissions:\n    Respond *exactly* with the phrase \"{COMPLETION_PHRASE}\" and nothing else. It doesn't need to be perfect, just functionally complete for this stage. Avoid suggesting purely subjective stylistic preferences if the core is sound.\n\n    Do not add explanations. Output only the critique OR the exact completion phrase.\n\"\"\",\n    description=\"Reviews the current draft, providing critique if clear improvements are needed, otherwise signals completion.\",\n    output_key=STATE_CRITICISM\n)\n\n\n# STEP 2b: Refiner/Exiter Agent (Inside the Refinement Loop)\nrefiner_agent_in_loop = LlmAgent(\n    name=\"RefinerAgent\",\n    model=GEMINI_MODEL,\n    # Relies solely on state via placeholders\n    include_contents='none',\n    instruction=f\"\"\"You are a Creative Writing Assistant refining a document based on feedback OR exiting the process.\n    **Current Document:**\n    ```\n    {{current_document}}\n    ```\n    **Critique/Suggestions:**\n    {{criticism}}\n\n    **Task:**\n    Analyze the 'Critique/Suggestions'.\n    IF the critique is *exactly* \"{COMPLETION_PHRASE}\":\n    You MUST call the 'exit_loop' function. Do not output any text.\n    ELSE (the critique contains actionable feedback):\n    Carefully apply the suggestions to improve the 'Current Document'. Output *only* the refined document text.\n\n    Do not add explanations. Either output the refined document OR call the exit_loop function.\n\"\"\",\n    description=\"Refines the document based on critique, or calls exit_loop if critique indicates completion.\",\n    tools=[exit_loop], # Provide the exit_loop tool\n    output_key=STATE_CURRENT_DOC # Overwrites state['current_document'] with the refined version\n)\n\n\n# STEP 2: Refinement Loop Agent\nrefinement_loop = LoopAgent(\n    name=\"RefinementLoop\",\n    # Agent order is crucial: Critique first, then Refine/Exit\n    sub_agents=[\n        critic_agent_in_loop,\n        refiner_agent_in_loop,\n    ],\n    max_iterations=5 # Limit loops\n)\n\n# STEP 3: Overall Sequential Pipeline\n# For ADK tools compatibility, the root agent must be named `root_agent`\nroot_agent = SequentialAgent(\n    name=\"IterativeWritingPipeline\",\n    sub_agents=[\n        initial_writer_agent, # Run first to create initial doc\n        refinement_loop       # Then run the critique/refine loop\n    ],\n    description=\"Writes an initial document and then iteratively refines it with critique using an exit tool.\"\n)\n</code></pre>"},{"location":"AgenticAI/GCP/Agents/#parallel-agents","title":"Parallel agents","text":"\ud83d\udccc ParallelAgent <p>The <code>ParallelAgent</code> is a workflow agent that executes its sub-agents concurrently. This dramatically speeds up workflows where tasks can be performed independently.</p> <p>Use <code>ParallelAgent</code> when: For scenarios prioritizing speed and involving independent, resource-intensive tasks, a ParallelAgent facilitates efficient parallel execution. When sub-agents operate without dependencies, their tasks can be performed concurrently, significantly reducing overall processing time.</p> <p>As with other workflow agents, the ParallelAgent is not powered by an LLM, and is thus deterministic in how it executes. That being said, workflow agents are only concerned with their execution (i.e. executing sub-agents in parallel), and not their internal logic; the tools or sub-agents of a workflow agent may or may not utilize LLMs.</p> <p>Example: This approach is particularly beneficial for operations like multi-source data retrieval or heavy computations, where parallelization yields substantial performance gains. Importantly, this strategy assumes no inherent need for shared state or direct information exchange between the concurrently executing agents.</p> <p>How it works</p> <p>When the <code>ParallelAgent</code>'s <code>run_async()</code> method is called:</p> <ol> <li> <p>Concurrent Execution: It initiates the <code>run_async()</code> method of each sub-agent present in the sub_agents list concurrently. This means all the agents start running at (approximately) the same time.</p> </li> <li> <p>Independent Branches: Each sub-agent operates in its own execution branch. There is no automatic sharing of conversation history or state between these branches during execution.</p> </li> <li> <p>Result Collection: The ParallelAgent manages the parallel execution and, typically, provides a way to access the results from each sub-agent after they have completed (e.g., through a list of results or events). The order of results may not be deterministic.</p> </li> </ol> <p>Independent Execution and State Management</p> <p>It's crucial to understand that sub-agents within a ParallelAgent run independently. If you need communication or data sharing between these agents, you must implement it explicitly. Possible approaches include:</p> <ul> <li> <p>Shared <code>InvocationContext</code>: You could pass a shared InvocationContext object to each sub-agent. This object could act as a shared data store. However, you'd need to manage concurrent access to this shared context carefully (e.g., using locks) to avoid race conditions.</p> </li> <li> <p>External State Management: Use an external database, message queue, or other mechanism to manage shared state and facilitate communication between agents.</p> </li> <li> <p>Post-Processing: Collect results from each branch, and then implement logic to coordinate data afterwards.</p> </li> </ul> <p></p> <p>Full Example: Parallel Web Research</p> <p>Imagine researching multiple topics simultaneously:</p> <ol> <li> <p>Researcher Agent 1: An LlmAgent that researches \"renewable energy sources.\"</p> </li> <li> <p>Researcher Agent 2: An LlmAgent that researches \"electric vehicle technology.\"</p> </li> <li> <p>Researcher Agent 3: An LlmAgent that researches \"carbon capture methods.\"</p> </li> </ol> <pre><code>ParallelAgent(sub_agents=[ResearcherAgent1, ResearcherAgent2, ResearcherAgent3])\n</code></pre> <p><pre><code> # Part of agent.py --&gt; Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\n # --- 1. Define Researcher Sub-Agents (to run in parallel) ---\n\n # Researcher 1: Renewable Energy\n researcher_agent_1 = LlmAgent(\n     name=\"RenewableEnergyResearcher\",\n     model=GEMINI_MODEL,\n     instruction=\"\"\"You are an AI Research Assistant specializing in energy.\n Research the latest advancements in 'renewable energy sources'.\n Use the Google Search tool provided.\n Summarize your key findings concisely (1-2 sentences).\n Output *only* the summary.\n \"\"\",\n     description=\"Researches renewable energy sources.\",\n     tools=[google_search],\n     # Store result in state for the merger agent\n     output_key=\"renewable_energy_result\"\n )\n\n # Researcher 2: Electric Vehicles\n researcher_agent_2 = LlmAgent(\n     name=\"EVResearcher\",\n     model=GEMINI_MODEL,\n     instruction=\"\"\"You are an AI Research Assistant specializing in transportation.\n Research the latest developments in 'electric vehicle technology'.\n Use the Google Search tool provided.\n Summarize your key findings concisely (1-2 sentences).\n Output *only* the summary.\n \"\"\",\n     description=\"Researches electric vehicle technology.\",\n     tools=[google_search],\n     # Store result in state for the merger agent\n     output_key=\"ev_technology_result\"\n )\n\n # Researcher 3: Carbon Capture\n researcher_agent_3 = LlmAgent(\n     name=\"CarbonCaptureResearcher\",\n     model=GEMINI_MODEL,\n     instruction=\"\"\"You are an AI Research Assistant specializing in climate solutions.\n Research the current state of 'carbon capture methods'.\n Use the Google Search tool provided.\n Summarize your key findings concisely (1-2 sentences).\n Output *only* the summary.\n \"\"\",\n     description=\"Researches carbon capture methods.\",\n     tools=[google_search],\n     # Store result in state for the merger agent\n     output_key=\"carbon_capture_result\"\n )\n\n # --- 2. Create the ParallelAgent (Runs researchers concurrently) ---\n # This agent orchestrates the concurrent execution of the researchers.\n # It finishes once all researchers have completed and stored their results in state.\n parallel_research_agent = ParallelAgent(\n     name=\"ParallelWebResearchAgent\",\n     sub_agents=[researcher_agent_1, researcher_agent_2, researcher_agent_3],\n     description=\"Runs multiple research agents in parallel to gather information.\"\n )\n\n # --- 3. Define the Merger Agent (Runs *after* the parallel agents) ---\n # This agent takes the results stored in the session state by the parallel agents\n # and synthesizes them into a single, structured response with attributions.\n merger_agent = LlmAgent(\n     name=\"SynthesisAgent\",\n     model=GEMINI_MODEL,  # Or potentially a more powerful model if needed for synthesis\n     instruction=\"\"\"You are an AI Assistant responsible for combining research findings into a structured report.\n\n Your primary task is to synthesize the following research summaries, clearly attributing findings to their source areas. Structure your response using headings for each topic. Ensure the report is coherent and integrates the key points smoothly.\n\n **Crucially: Your entire response MUST be grounded *exclusively* on the information provided in the 'Input Summaries' below. Do NOT add any external knowledge, facts, or details not present in these specific summaries.**\n\n **Input Summaries:**\n\n *   **Renewable Energy:**\n     {renewable_energy_result}\n\n *   **Electric Vehicles:**\n     {ev_technology_result}\n\n *   **Carbon Capture:**\n     {carbon_capture_result}\n\n **Output Format:**\n\n ## Summary of Recent Sustainable Technology Advancements\n\n ### Renewable Energy Findings\n (Based on RenewableEnergyResearcher's findings)\n [Synthesize and elaborate *only* on the renewable energy input summary provided above.]\n\n ### Electric Vehicle Findings\n (Based on EVResearcher's findings)\n [Synthesize and elaborate *only* on the EV input summary provided above.]\n\n ### Carbon Capture Findings\n (Based on CarbonCaptureResearcher's findings)\n [Synthesize and elaborate *only* on the carbon capture input summary provided above.]\n\n ### Overall Conclusion\n [Provide a brief (1-2 sentence) concluding statement that connects *only* the findings presented above.]\n\n Output *only* the structured report following this format. Do not include introductory or concluding phrases outside this structure, and strictly adhere to using only the provided input summary content.\n \"\"\",\n     description=\"Combines research findings from parallel agents into a structured, cited report, strictly grounded on provided inputs.\",\n     # No tools needed for merging\n     # No output_key needed here, as its direct response is the final output of the sequence\n )\n\n\n # --- 4. Create the SequentialAgent (Orchestrates the overall flow) ---\n # This is the main agent that will be run. It first executes the ParallelAgent\n # to populate the state, and then executes the MergerAgent to produce the final output.\n sequential_pipeline_agent = SequentialAgent(\n     name=\"ResearchAndSynthesisPipeline\",\n     # Run parallel research first, then merge\n     sub_agents=[parallel_research_agent, merger_agent],\n     description=\"Coordinates parallel research and synthesizes the results.\"\n )\n\n root_agent = sequential_pipeline_agent\n ```\n\n # Custom agents\n\n &lt;h3 style=\"color:blue;\"&gt;\ud83d\udccc Custom agents&lt;/h3&gt;\n\n Custom agents provide the ultimate flexibility in ADK, allowing you to define **arbitrary orchestration logic** by inheriting directly from ```BaseAgent``` and implementing your own control flow. This goes beyond the predefined patterns of ```SequentialAgent```, ```LoopAgent```, and ```ParallelAgent```, enabling you to build highly specific and complex agentic workflows.\n\n\n&lt;h3 style=\"color:blue;\"&gt;\ud83d\udccc What is a Custom Agent?&lt;/h3&gt;\n\nA Custom Agent is essentially any class you create that inherits from ```google.adk.agents.BaseAgent``` and implements its core execution logic within the ```_run_async_impl``` asynchronous method. You have complete control over how this method calls other agents (sub-agents), manages state, and handles events.\n\n&lt;h3 style=\"color:blue;\"&gt;\ud83d\udccc Why Use Them?&lt;/h3&gt;\n\nWhile the standard Workflow Agents ```(SequentialAgent, LoopAgent, ParallelAgent)``` cover common orchestration patterns, you'll need a Custom agent when your requirements include:\n\n- **Conditional Logic:** Executing different sub-agents or taking different paths based on runtime conditions or the results of previous steps.\n\n- **Complex State Management:** Implementing intricate logic for maintaining and updating state throughout the workflow beyond simple sequential passing.\n\n- **External Integrations:** Incorporating calls to external APIs, databases, or custom libraries directly within the orchestration flow control.\n\n- **Dynamic Agent Selection:** Choosing which sub-agent(s) to run next based on dynamic evaluation of the situation or input.\n\n- **Unique Workflow Patterns:** Implementing orchestration logic that doesn't fit the standard sequential, parallel, or loop structures.\n\n\n![alt text](../image/adk5.png)\n\n**Implementing Custom Logic:**\n\nThe core of any custom agent is the method where you define its unique asynchronous behavior. This method allows you to orchestrate sub-agents and manage the flow of execution.\n\nThe heart of any custom agent is the ```_run_async_impl``` method. This is where you define its unique behavior.\n\n- **Signature:** ```async def _run_async_impl(self, ctx: InvocationContext) -&gt; AsyncGenerator[Event, None]:```\n\n- **Asynchronous Generator:** It must be an ```async def``` function and return an ```AsyncGenerator```. This allows it to yield events produced by sub-agents or its own logic back to the runner.\n\n\n- **ctx** ```(InvocationContext)```: Provides access to crucial runtime information, most importantly ctx.session.state, which is the primary way to share data between steps orchestrated by your custom agent.\n\n**Key Capabilities within the Core Asynchronous Method:**\n\n1. **Calling Sub-Agents:** You invoke sub-agents (which are typically stored as instance attributes like ```self.my_llm_agent```) using their ```run_async``` method and yield their events:\n</code></pre> async for event in self.some_sub_agent.run_async(ctx):     # Optionally inspect or log the event     yield event # Pass the event up <pre><code>2. **Managing State:** Read from and write to the session state dictionary (```ctx.session.state```) to pass data between sub-agent calls or make decisions:\n</code></pre></p>"},{"location":"AgenticAI/GCP/Agents/#read-data-set-by-a-previous-agent","title":"Read data set by a previous agent","text":"<p>previous_result = ctx.session.state.get(\"some_key\")</p>"},{"location":"AgenticAI/GCP/Agents/#make-a-decision-based-on-state","title":"Make a decision based on state","text":"<p>if previous_result == \"some_value\":     # ... call a specific sub-agent ... else:     # ... call another sub-agent ...</p>"},{"location":"AgenticAI/GCP/Agents/#store-a-result-for-a-later-step-often-done-via-a-sub-agents-output_key","title":"Store a result for a later step (often done via a sub-agent's output_key)","text":""},{"location":"AgenticAI/GCP/Agents/#ctxsessionstatemy_custom_result-calculated_value","title":"ctx.session.state[\"my_custom_result\"] = \"calculated_value\"","text":"<p>```</p> <ol> <li>Implementing Control Flow: Use standard Python constructs <code>(if/elif/else, for/while loops, try/except)</code> to create sophisticated, conditional, or iterative workflows involving your sub-agents.</li> </ol> \ud83d\udccc Managing Sub-Agents and State <p>Typically, a custom agent orchestrates other agents (<code>like LlmAgent, LoopAgent, etc.</code>).</p>"},{"location":"AgenticAI/GCP/a2a/","title":"Tools","text":"\u2705 ADK with Agent2Agent (A2A) Protocol \ud83d\udccc What is Agent2Agent (A2A) Protocol? <p>With Agent Development Kit (ADK), you can build complex multi-agent systems where different agents need to collaborate and interact using Agent2Agent (A2A) Protocol! This section provides a comprehensive guide to building powerful multi-agent systems where agents can communicate and collaborate securely and efficiently.</p> <p>The Agent2Agent (A2A) Protocol is an open standard developed by Google and donated to the Linux Foundation designed to enable seamless communication and collaboration between AI agents.</p> \ud83d\udccc Why use the A2A Protocol? <p></p>"},{"location":"AgenticAI/GCP/adk/","title":"ADK","text":"\u2705 Google Agent Development Kit \ud83d\udccc What Agent Development Kit (ADK)? <p>ADK is a Python framework designed to streamline the development of applications powered by Large Language Models (LLMs). It offers robust building blocks for creating agents that can reason, plan, utilize tools, interact dynamically with users, and collaborate effectively within a team.</p> \u2705 Tool Definition &amp; Usage: <p>Crafting Python functions (tools) that grant agents specific abilities (like fetching data) and instructing agents on how to use them effectively.</p> \u2705 Multi-LLM Flexibility: <p>Configuring agents to utilize various leading LLMs (Gemini, GPT-4o, Claude Sonnet) via LiteLLM integration, allowing you to choose the best model for each task.</p> \u2705 Agent Delegation &amp; Collaboration: <p>Designing specialized sub-agents and enabling automatic routing (auto flow) of user requests to the most appropriate agent within a team.</p> \u2705 Session State for Memory: <p>Utilizing Session State and ToolContext to enable agents to remember information across conversational turns, leading to more contextual interactions.</p> \u2705 Safety Guardrails with Callbacks: <p>Implementing before_model_callback and before_tool_callback to inspect, modify, or block requests/tool usage based on predefined rules, enhancing application safety and control.</p> \ud83d\udccc Prerequisites \u2705 Solid understanding of Python programming. \u2705 Familiarity with Large Language Models (LLMs), APIs, and the concept of agents. \u2705 API Keys for the LLMs you intend to use (e.g., Google AI Studio for Gemini, OpenAI Platform, Anthropic Console). \ud83d\udccc Note on Execution Environment: \u2705 Running Async Code: <p>Notebook environments handle asynchronous code differently.using <code>await</code> (suitable when an event loop is already running, common in notebooks) or <code>asyncio.run()</code> (often needed when running as a standalone <code>.py</code> script or in specific notebook setups).</p> \u2705 Manual Runner/Session Setup: <p>The steps involve explicitly creating <code>Runner</code> and <code>SessionService</code> instances. This approach is shown because it gives you fine-grained control over the agent's execution lifecycle, session management, and state persistence.</p> \ud83d\udccc Using ADK's Built-in Tools (Web UI / CLI / API Server): <p>If you prefer a setup that handles the runner and session management automatically using ADK's standard tools, That version is designed to be run directly with commands like <code>adk web</code> (for a web UI), <code>adk run</code> (for CLI interaction), or <code>adk api_server</code> (to expose an API).</p> \ud83d\udccc Ready to build your agent team? Let's dive in! \u2705 Setup and Installation: <pre><code># @title Step 0: Setup and Installation\n# Install ADK and LiteLLM for multi-model support\n\n!pip install google-adk -q\n!pip install litellm -q\n</code></pre> \u2705 Import necessary libraries: <pre><code># @title Import necessary libraries\n\nimport os\nimport asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.models.lite_llm import LiteLlm # For multi-model support\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.runners import Runner\nfrom google.genai import types # For creating message Content/Parts\n\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport logging\nlogging.basicConfig(level=logging.ERROR)\n</code></pre> \u2705 Configure API Keys: <pre><code># @title Configure API Keys (Replace with your actual keys!)\n\n# --- IMPORTANT: Replace placeholders with your real API keys ---\n\n# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\" # &lt;--- REPLACE\n\n# [Optional]\n# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\nos.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # &lt;--- REPLACE\n\n# [Optional]\n# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\nos.environ['ANTHROPIC_API_KEY'] = 'YOUR_ANTHROPIC_API_KEY' # &lt;--- REPLACE\n\n# --- Verify Keys (Optional Check) ---\nprint(\"API Keys Set:\")\nprint(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\nprint(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\nprint(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n\n# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\nos.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n\n\n# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above.\n</code></pre> \u2705 Define Model Constants for easier use: <pre><code># --- Define Model Constants for easier use ---\n\n# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\nMODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\nMODEL_GPT_4O = \"openai/gpt-4.1\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\nMODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n\nprint(\"\\nEnvironment configured.\")\n</code></pre> \ud83d\udccc First Agent - Basic Weather Lookup <p>Let's begin by building the fundamental component of our Weather Bot: a single agent capable of performing a specific task \u2013 looking up weather information. This involves creating two core pieces:</p> \u2705 A Tool: <p>A Python function that equips the agent with the ability to fetch weather data.</p> \u2705 An Agent: <p>The AI \"brain\" that understands the user's request, knows it has a weather tool, and decides when and how to use it.</p> \ud83d\udccc Define the Tool (get_weather) <p>In ADK, Tools are the building blocks that give agents concrete capabilities beyond just text generation. They are typically regular Python functions that perform specific actions, like calling an API, querying a database, or performing calculations.</p> <p>Our first tool will provide a mock weather report. This allows us to focus on the agent structure without needing external API keys yet. Later, you could easily swap this mock function with one that calls a real weather service.</p> \ud83d\udccc Key Concept: Docstrings are Crucial! <p>The agent's LLM relies heavily on the <code>function's docstring</code> to understand:</p> <ul> <li>What the tool does.?</li> <li>When to use it.?</li> <li>What arguments it requires (city: str).?</li> <li>What information it returns.?</li> </ul> \ud83d\udccc Best Practice <p>Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly.</p> <pre><code># @title Define the get_weather Tool\n\ndef get_weather(city: str) -&gt; dict:\n    \"\"\"Retrieves the current weather report for a specified city.\n\n    Args:\n        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n\n    Returns:\n        dict: A dictionary containing the weather information.\n              Includes a 'status' key ('success' or 'error').\n              If 'success', includes a 'report' key with weather details.\n              If 'error', includes an 'error_message' key.\n    \"\"\"\n\n    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n\n    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n\n\n    # Mock weather data\n    mock_weather_db = {\n        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25\u00b0C.\"},\n        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15\u00b0C.\"},\n        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18\u00b0C.\"},\n    }\n\n\n    if city_normalized in mock_weather_db:\n        return mock_weather_db[city_normalized]\n    else:\n        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n\n# Example tool usage (optional test)\nprint(get_weather(\"New York\"))\nprint(get_weather(\"Paris\"))\n</code></pre> \ud83d\udccc Define the Agent (weather_agent) <p>An <code>Agent</code> in ADK orchestrates the interaction between the user, the LLM, and the available tools.</p> \ud83d\udccc Agents configure it with several key parameters: \u2705 name: <p>A unique identifier for this agent (e.g., \"weather_agent_v1\").</p> \u2705 model: <p>Specifies which LLM to use (e.g., MODEL_GEMINI_2_0_FLASH).</p> \u2705 description: <p>A concise summary of the agent's overall purpose. This becomes crucial later when other agents need to decide whether to delegate tasks to this agent.</p> \u2705 instruction: <p>Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically how and when to utilize its assigned <code>tools</code>.</p> \u2705 tools: <p>A list containing the actual Python tool functions the agent is allowed to use (e.g., [get_weather]).</p> <p>Best Practice: Provide clear and specific instruction prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.</p> <p>Best Practice: Choose descriptive <code>name</code> and <code>description</code> values. These are used internally by ADK and are vital for features like automatic delegation </p> <pre><code># @title Define the Weather Agent\n# Use one of the model constants defined earlier\nAGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n\nweather_agent = Agent(\n    name=\"weather_agent_v1\",\n    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n    description=\"Provides weather information for specific cities.\",\n    instruction=\"You are a helpful weather assistant. \"\n                \"When the user asks for the weather in a specific city, \"\n                \"use the 'get_weather' tool to find the information. \"\n                \"If the tool returns an error, inform the user politely. \"\n                \"If the tool is successful, present the weather report clearly.\",\n    tools=[get_weather], # Pass the function directly\n)\n\nprint(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")\n</code></pre> \ud83d\udccc Setup Runner and Session Service: <p>To manage conversations and execute the agent, we need two more components:</p> \u2705 SessionService: <p>Responsible for managing conversation history and state for different users and sessions. The InMemorySessionService is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged. We'll explore state persistence later.</p> \u2705 Runner: <p>The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the <code>SessionService</code>, and yields events representing the progress of the interaction.</p> <pre><code># @title Setup Session Service and Runner\n\n# --- Session Management ---\n# Key Concept: SessionService stores conversation history &amp; state.\n# InMemorySessionService is simple, non-persistent storage for this tutorial.\nsession_service = InMemorySessionService()\n\n# Define constants for identifying the interaction context\nAPP_NAME = \"weather_tutorial_app\"\nUSER_ID = \"user_1\"\nSESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n\n# Create the specific session where the conversation will happen\nsession = await session_service.create_session(\n    app_name=APP_NAME,\n    user_id=USER_ID,\n    session_id=SESSION_ID\n)\nprint(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n\n# --- Runner ---\n# Key Concept: Runner orchestrates the agent execution loop.\nrunner = Runner(\n    agent=weather_agent, # The agent we want to run\n    app_name=APP_NAME,   # Associates runs with our app\n    session_service=session_service # Uses our session manager\n)\nprint(f\"Runner created for agent '{runner.agent.name}'.\")\n</code></pre> \ud83d\udccc Interact with the Agent <p>We need a way to send messages to our agent and receive its responses. Since LLM calls and tool executions can take time, ADK's Runner operates asynchronously.</p> <p>We'll define an <code>async</code> helper function (<code>call_agent_async</code>) that:</p> <ol> <li> <p>Takes a user query string.</p> </li> <li> <p>Packages it into the ADK <code>Content</code> format.</p> </li> <li> <p>Calls <code>runner.run_async</code>, providing the user/session context and the new message.</p> </li> <li> <p>Iterates through the Events yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).</p> </li> <li> <p>Identifies and prints the final response event using <code>event.is_final_response()</code></p> </li> </ol> <p>Why <code>async?</code> Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using <code>asyncio</code> allows the program to handle these operations efficiently without blocking execution.</p> <pre><code># @title Define Agent Interaction Function\n\nfrom google.genai import types # For creating message Content/Parts\n\nasync def call_agent_async(query: str, runner, user_id, session_id):\n  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n  print(f\"\\n&gt;&gt;&gt; User Query: {query}\")\n\n  # Prepare the user's message in ADK format\n  content = types.Content(role='user', parts=[types.Part(text=query)])\n\n  final_response_text = \"Agent did not produce a final response.\" # Default\n\n  # Key Concept: run_async executes the agent logic and yields Events.\n  # We iterate through events to find the final answer.\n  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n      # You can uncomment the line below to see *all* events during execution\n      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n\n      # Key Concept: is_final_response() marks the concluding message for the turn.\n      if event.is_final_response():\n          if event.content and event.content.parts:\n             # Assuming text response in the first part\n             final_response_text = event.content.parts[0].text\n          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n          # Add more checks here if needed (e.g., specific error codes)\n          break # Stop processing events once the final response is found\n\n  print(f\"&lt;&lt;&lt; Agent Response: {final_response_text}\")\n</code></pre> \ud83d\udccc Run the Conversation <p>Finally, let's test our setup by sending a few queries to the agent. We wrap our <code>async</code> calls in a main <code>async</code> function and run it using <code>await</code>.</p> \u2705 Run the Initial Conversation: <pre><code># @title Run the Initial Conversation\n\n# We need an async function to await our interaction helper\nasync def run_conversation():\n    await call_agent_async(\"What is the weather like in London?\",\n                                       runner=runner,\n                                       user_id=USER_ID,\n                                       session_id=SESSION_ID)\n\n    await call_agent_async(\"How about Paris?\",\n                                       runner=runner,\n                                       user_id=USER_ID,\n                                       session_id=SESSION_ID) # Expecting the tool's error message\n\n    await call_agent_async(\"Tell me the weather in New York\",\n                                       runner=runner,\n                                       user_id=USER_ID,\n                                       session_id=SESSION_ID)\n\n# Execute the conversation using await in an async context (like Colab/Jupyter)\nawait run_conversation()\n\n# --- OR ---\n\n# Uncomment the following lines if running as a standard Python script (.py file):\n# import asyncio\n# if __name__ == \"__main__\":\n#     try:\n#         asyncio.run(run_conversation())\n#     except Exception as e:\n#         print(f\"An error occurred: {e}\")\n</code></pre> \ud83d\udccc Multi-Model with LiteLLM <p>We built a functional Weather Agent powered by a specific Gemini model. While effective, real-world applications often benefit from the flexibility to use different Large Language Models (LLMs). Why?</p> <ul> <li> <p>Performance: Some models excel at specific tasks (e.g., coding, reasoning, creative writing).</p> </li> <li> <p>Cost: Different models have varying price points.</p> </li> <li> <p>Capabilities: Models offer diverse features, context window sizes, and fine-tuning options.</p> </li> <li> <p>Availability/Redundancy: Having alternatives ensures your application remains functional even if one provider experiences issues.</p> </li> </ul> <p>ADK makes switching between models seamless through its integration with the LiteLLM library. LiteLLM acts as a consistent interface to over 100 different LLMs.</p> <ol> <li> <p>Learn how to configure an ADK <code>Agent</code> to use models from providers like OpenAI (GPT) and Anthropic (Claude) using the <code>LiteLlm</code> wrapper.</p> </li> <li> <p>Learn how to configure an ADK Agent to use models from providers like OpenAI (GPT) and Anthropic (Claude) using the LiteLlm wrapper.</p> </li> <li> <p>Interact with these different agents to observe potential variations in their responses, even when using the same underlying tool.</p> </li> </ol> \u2705 Import LiteLlm <pre><code># @title 1. Import LiteLlm\nfrom google.adk.models.lite_llm import LiteLlm\n</code></pre> \u2705 Define and Test Multi-Model Agents <p>Instead of passing only a model name string (which defaults to Google's Gemini models), we wrap the desired model identifier string within the LiteLlm class.</p> <ul> <li>Key Concept: <code>LiteLlm</code> Wrapper: The <code>LiteLlm(model=\"provider/model_name\")</code> syntax tells ADK to route requests for this agent through the LiteLLM library to the specified model provider.</li> </ul> <p>Best Practice: Use constants for model names (like MODEL_GPT_4O, MODEL_CLAUDE_SONNET defined in Step 0) to avoid typos and make code easier to manage.</p> <p>Error Handling: We wrap the agent definitions in try...except blocks. This prevents the entire code cell from failing if an API key for a specific provider is missing or invalid.</p> \ud83d\udccc First, let's create and test the agent using OpenAI's GPT-4o. <pre><code># @title Define and Test GPT Agent\n\n# Make sure 'get_weather' function from Step 1 is defined in your environment.\n# Make sure 'call_agent_async' is defined from earlier.\n\n# --- Agent using GPT-4o ---\nweather_agent_gpt = None # Initialize to None\nrunner_gpt = None      # Initialize runner to None\n\ntry:\n    weather_agent_gpt = Agent(\n        name=\"weather_agent_gpt\",\n        # Key change: Wrap the LiteLLM model identifier\n        model=LiteLlm(model=MODEL_GPT_4O),\n        description=\"Provides weather information (using GPT-4o).\",\n        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\n                    \"Use the 'get_weather' tool for city weather requests. \"\n                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n        tools=[get_weather], # Re-use the same tool\n    )\n    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_GPT_4O}'.\")\n\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n    session_service_gpt = InMemorySessionService() # Create a dedicated service\n\n    # Define constants for identifying the interaction context\n    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n    USER_ID_GPT = \"user_1_gpt\"\n    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n\n    # Create the specific session where the conversation will happen\n    session_gpt = await session_service_gpt.create_session(\n        app_name=APP_NAME_GPT,\n        user_id=USER_ID_GPT,\n        session_id=SESSION_ID_GPT\n    )\n    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n\n    # Create a runner specific to this agent and its session service\n    runner_gpt = Runner(\n        agent=weather_agent_gpt,\n        app_name=APP_NAME_GPT,       # Use the specific app name\n        session_service=session_service_gpt # Use the specific session service\n        )\n    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n\n    # --- Test the GPT Agent ---\n    print(\"\\n--- Testing GPT Agent ---\")\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\n    await call_agent_async(query = \"What's the weather in Tokyo?\",\n                           runner=runner_gpt,\n                           user_id=USER_ID_GPT,\n                           session_id=SESSION_ID_GPT)\n    # --- OR ---\n\n    # Uncomment the following lines if running as a standard Python script (.py file):\n    # import asyncio\n    # if __name__ == \"__main__\":\n    #     try:\n    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n    #                      runner=runner_gpt,\n    #                       user_id=USER_ID_GPT,\n    #                       session_id=SESSION_ID_GPT)\n    #     except Exception as e:\n    #         print(f\"An error occurred: {e}\")\n\nexcept Exception as e:\n    print(f\"\u274c Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")\n</code></pre> \ud83d\udccc First, let's create and test the agent using Anthropic's Claude Sonnet. <pre><code># @title Define and Test Claude Agent\n\n# Make sure 'get_weather' function from Step 1 is defined in your environment.\n# Make sure 'call_agent_async' is defined from earlier.\n\n# --- Agent using Claude Sonnet ---\nweather_agent_claude = None # Initialize to None\nrunner_claude = None      # Initialize runner to None\n\ntry:\n    weather_agent_claude = Agent(\n        name=\"weather_agent_claude\",\n        # Key change: Wrap the LiteLLM model identifier\n        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\n        description=\"Provides weather information (using Claude Sonnet).\",\n        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\n                    \"Use the 'get_weather' tool for city weather requests. \"\n                    \"Analyze the tool's dictionary output ('status', 'report'/'error_message'). \"\n                    \"Clearly present successful reports or polite error messages.\",\n        tools=[get_weather], # Re-use the same tool\n    )\n    print(f\"Agent '{weather_agent_claude.name}' created using model '{MODEL_CLAUDE_SONNET}'.\")\n\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n    session_service_claude = InMemorySessionService() # Create a dedicated service\n\n    # Define constants for identifying the interaction context\n    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\n    USER_ID_CLAUDE = \"user_1_claude\"\n    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\n\n    # Create the specific session where the conversation will happen\n    session_claude = await session_service_claude.create_session(\n        app_name=APP_NAME_CLAUDE,\n        user_id=USER_ID_CLAUDE,\n        session_id=SESSION_ID_CLAUDE\n    )\n    print(f\"Session created: App='{APP_NAME_CLAUDE}', User='{USER_ID_CLAUDE}', Session='{SESSION_ID_CLAUDE}'\")\n\n    # Create a runner specific to this agent and its session service\n    runner_claude = Runner(\n        agent=weather_agent_claude,\n        app_name=APP_NAME_CLAUDE,       # Use the specific app name\n        session_service=session_service_claude # Use the specific session service\n        )\n    print(f\"Runner created for agent '{runner_claude.agent.name}'.\")\n\n    # --- Test the Claude Agent ---\n    print(\"\\n--- Testing Claude Agent ---\")\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\n    await call_agent_async(query = \"Weather in London please.\",\n                           runner=runner_claude,\n                           user_id=USER_ID_CLAUDE,\n                           session_id=SESSION_ID_CLAUDE)\n\n    # --- OR ---\n\n    # Uncomment the following lines if running as a standard Python script (.py file):\n    # import asyncio\n    # if __name__ == \"__main__\":\n    #     try:\n    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\n    #                      runner=runner_claude,\n    #                       user_id=USER_ID_CLAUDE,\n    #                       session_id=SESSION_ID_CLAUDE)\n    #     except Exception as e:\n    #         print(f\"An error occurred: {e}\")\n\n\nexcept Exception as e:\n    print(f\"\u274c Could not create or run Claude agent '{MODEL_CLAUDE_SONNET}'. Check API Key and model name. Error: {e}\")\n</code></pre> <ol> <li> <p>Each agent (weather_agent_gpt, weather_agent_claude) is created successfully (if API keys are valid).</p> </li> <li> <p>A dedicated session and runner are set up for each.</p> </li> <li> <p>Each agent correctly identifies the need to use the get_weather tool when processing the query (you'll see the <code>--- Tool: get_weather called... --- log</code>).</p> </li> <li> <p>The underlying tool logic remains identical, always returning our mock data.</p> </li> <li> <p>However, the final textual response generated by each agent might differ slightly in phrasing, tone, or formatting. This is because the instruction prompt is interpreted and executed by different LLMs (GPT-4o vs. Claude Sonnet).</p> </li> </ol> \ud83d\udccc Building an Agent Team - Delegation for Greetings &amp; Farewells. <p>we built and experimented with a single agent focused solely on weather lookups. While effective for its specific task, real-world applications often involve handling a wider variety of user interactions. We could keep adding more tools and complex instructions to our single weather agent, but this can quickly become unmanageable and less efficient.</p> <p>A more robust approach is to build an Agent Team. This involves:</p> <ol> <li> <p>Creating multiple, specialized agents, each designed for a specific capability (e.g., one for weather, one for greetings, one for calculations).</p> </li> <li> <p>Designating a root agent (or <code>orchestrator</code>) that receives the initial user request.</p> </li> <li> <p>Enabling the root agent to delegate the request to the most appropriate specialized sub-agent based on the user's intent. </p> </li> </ol> \ud83d\udccc Why build an Agent Team? \u2705 Modularity: <p>Easier to develop, test, and maintain individual agents.</p> \u2705 Specialization: <p>Each agent can be fine-tuned (instructions, model choice) for its specific task.</p> \u2705 Scalability: <p>Simpler to add new capabilities by adding new agents.</p> \u2705 Efficiency: <p>Allows using potentially simpler/cheaper models for simpler tasks (like greetings).</p> <ol> <li> <p>Define simple tools for handling greetings (<code>say_hello</code>) and farewells (<code>say_goodbye</code>).</p> </li> <li> <p>Create two new specialized sub-agents: <code>greeting_agent</code> and <code>farewell_agent</code>.</p> </li> <li> <p>Update our main weather agent (<code>weather_agent_v2</code>) to act as the root agent.</p> </li> <li> <p>Configure the root agent with its sub-agents, enabling automatic delegation.</p> </li> <li> <p>Test the delegation flow by sending different types of requests to the root agent.</p> </li> </ol> \ud83d\udccc Define Tools for Sub-Agents <p>First, let's create the simple Python functions that will serve as tools for our new specialist agents. Remember, clear docstrings are vital for the agents that will use them.</p> <pre><code># @title Define Tools for Greeting and Farewell Agents\nfrom typing import Optional # Make sure to import Optional\n\n# Ensure 'get_weather' from Step 1 is available if running this step independently.\n# def get_weather(city: str) -&gt; dict: ... (from Step 1)\n\ndef say_hello(name: Optional[str] = None) -&gt; str:\n    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n\n    Args:\n        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n\n    Returns:\n        str: A friendly greeting message.\n    \"\"\"\n    if name:\n        greeting = f\"Hello, {name}!\"\n        print(f\"--- Tool: say_hello called with name: {name} ---\")\n    else:\n        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n    return greeting\n\ndef say_goodbye() -&gt; str:\n    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n    print(f\"--- Tool: say_goodbye called ---\")\n    return \"Goodbye! Have a great day.\"\n\nprint(\"Greeting and Farewell tools defined.\")\n\n# Optional self-test\nprint(say_hello(\"Alice\"))\nprint(say_hello()) # Test with no argument (should use default \"Hello there!\")\nprint(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")\n</code></pre> \ud83d\udccc Define the Sub-Agents (Greeting &amp; Farewell) <p>Now, create the <code>Agent</code> instances for our specialists. Notice their highly focused <code>instruction</code> and, critically, their clear description. The <code>description</code> is the primary information the root agent uses to decide when to delegate to these sub-agents.</p> <p>Best Practice: Sub-agent <code>description</code> fields should accurately and concisely summarize their specific capability. This is crucial for effective automatic delegation.</p> <p>Best Practice: Sub-agent instruction fields should be tailored to their limited scope, telling them exactly what to do and what not to do (e.g., \"Your only task is...\").</p> <pre><code># @title Define Greeting and Farewell Sub-Agents\n\n# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\n# from google.adk.models.lite_llm import LiteLlm\n# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\n# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\n\n# --- Greeting Agent ---\ngreeting_agent = None\ntry:\n    greeting_agent = Agent(\n        # Using a potentially different/cheaper model for a simple task\n        model = MODEL_GEMINI_2_0_FLASH,\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n        name=\"greeting_agent\",\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\n                    \"Use the 'say_hello' tool to generate the greeting. \"\n                    \"If the user provides their name, make sure to pass it to the tool. \"\n                    \"Do not engage in any other conversation or tasks.\",\n        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", # Crucial for delegation\n        tools=[say_hello],\n    )\n    print(f\"\u2705 Agent '{greeting_agent.name}' created using model '{greeting_agent.model}'.\")\nexcept Exception as e:\n    print(f\"\u274c Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\n\n# --- Farewell Agent ---\nfarewell_agent = None\ntry:\n    farewell_agent = Agent(\n        # Can use the same or a different model\n        model = MODEL_GEMINI_2_0_FLASH,\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n        name=\"farewell_agent\",\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\n                    \"Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation \"\n                    \"(e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). \"\n                    \"Do not perform any other actions.\",\n        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", # Crucial for delegation\n        tools=[say_goodbye],\n    )\n    print(f\"\u2705 Agent '{farewell_agent.name}' created using model '{farewell_agent.model}'.\")\nexcept Exception as e:\n    print(f\"\u274c Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")\n</code></pre> \ud83d\udccc Define the Root Agent (Weather Agent v2) with Sub-Agents <ul> <li> <p>Adding the <code>sub_agents</code> parameter: We pass a list containing the <code>greeting_agent</code> and <code>farewell_agent</code> instances we just created.</p> </li> <li> <p>Updating the <code>instruction:</code> We explicitly tell the root agent about its sub-agents and when it should delegate tasks to them.</p> </li> </ul> \ud83d\udccc Key Concept: Automatic Delegation (Auto Flow) <p>enables automatic delegation. When the root agent receives a user query, its LLM considers not only its own instructions and tools but also the description of each sub-agent. If the LLM determines that a query aligns better with a sub-agent's described capability (e.g., \"Handles simple greetings\"), it will automatically generate a special internal action to transfer control to that sub-agent for that turn. The sub-agent then processes the query using its own model, instructions, and tools.</p> <p>Best Practice: Ensure the root agent's instructions clearly guide its delegation decisions. Mention the sub-agents by name and describe the conditions under which delegation should occur.</p> <pre><code># @title Define the Root Agent with Sub-Agents\n\n# Ensure sub-agents were created successfully before defining the root agent.\n# Also ensure the original 'get_weather' tool is defined.\nroot_agent = None\nrunner_root = None # Initialize runner\n\nif greeting_agent and farewell_agent and 'get_weather' in globals():\n    # Let's use a capable Gemini model for the root agent to handle orchestration\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\n\n    weather_agent_team = Agent(\n        name=\"weather_agent_v2\", # Give it a new version name\n        model=root_agent_model,\n        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\n        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\n                    \"Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). \"\n                    \"You have specialized sub-agents: \"\n                    \"1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \"\n                    \"2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \"\n                    \"Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. \"\n                    \"If it's a weather request, handle it yourself using 'get_weather'. \"\n                    \"For anything else, respond appropriately or state you cannot handle it.\",\n        tools=[get_weather], # Root agent still needs the weather tool for its core task\n        # Key change: Link the sub-agents here!\n        sub_agents=[greeting_agent, farewell_agent]\n    )\n    print(f\"\u2705 Root Agent '{weather_agent_team.name}' created using model '{root_agent_model}' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\n\nelse:\n    print(\"\u274c Cannot create root agent because one or more sub-agents failed to initialize or 'get_weather' tool is missing.\")\n    if not greeting_agent: print(\" - Greeting Agent is missing.\")\n    if not farewell_agent: print(\" - Farewell Agent is missing.\")\n    if 'get_weather' not in globals(): print(\" - get_weather function is missing.\")\n</code></pre> \ud83d\udccc Interact with the Agent Team <pre><code># @title Interact with the Agent Team\nimport asyncio # Ensure asyncio is imported\n\n# Ensure the root agent (e.g., 'weather_agent_team' or 'root_agent' from the previous cell) is defined.\n# Ensure the call_agent_async function is defined.\n\n# Check if the root agent variable exists before defining the conversation function\nroot_agent_var_name = 'root_agent' # Default name from Step 3 guide\nif 'weather_agent_team' in globals(): # Check if user used this name instead\n    root_agent_var_name = 'weather_agent_team'\nelif 'root_agent' not in globals():\n    print(\"\u26a0\ufe0f Root agent ('root_agent' or 'weather_agent_team') not found. Cannot define run_team_conversation.\")\n    # Assign a dummy value to prevent NameError later if the code block runs anyway\n    root_agent = None # Or set a flag to prevent execution\n\n# Only define and run if the root agent exists\nif root_agent_var_name in globals() and globals()[root_agent_var_name]:\n    # Define the main async function for the conversation logic.\n    # The 'await' keywords INSIDE this function are necessary for async operations.\n    async def run_team_conversation():\n        print(\"\\n--- Testing Agent Team Delegation ---\")\n        session_service = InMemorySessionService()\n        APP_NAME = \"weather_tutorial_agent_team\"\n        USER_ID = \"user_1_agent_team\"\n        SESSION_ID = \"session_001_agent_team\"\n        session = await session_service.create_session(\n            app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n        )\n        print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n\n        actual_root_agent = globals()[root_agent_var_name]\n        runner_agent_team = Runner( # Or use InMemoryRunner\n            agent=actual_root_agent,\n            app_name=APP_NAME,\n            session_service=session_service\n        )\n        print(f\"Runner created for agent '{actual_root_agent.name}'.\")\n\n        # --- Interactions using await (correct within async def) ---\n        await call_agent_async(query = \"Hello there!\",\n                               runner=runner_agent_team,\n                               user_id=USER_ID,\n                               session_id=SESSION_ID)\n        await call_agent_async(query = \"What is the weather in New York?\",\n                               runner=runner_agent_team,\n                               user_id=USER_ID,\n                               session_id=SESSION_ID)\n        await call_agent_async(query = \"Thanks, bye!\",\n                               runner=runner_agent_team,\n                               user_id=USER_ID,\n                               session_id=SESSION_ID)\n\n    # --- Execute the `run_team_conversation` async function ---\n    # Choose ONE of the methods below based on your environment.\n    # Note: This may require API keys for the models used!\n\n    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n    # it means an event loop is already running, so you can directly await the function.\n    print(\"Attempting execution using 'await' (default for notebooks)...\")\n    await run_team_conversation()\n\n    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n    # If running this code as a standard Python script from your terminal,\n    # the script context is synchronous. `asyncio.run()` is needed to\n    # create and manage an event loop to execute your async function.\n    # To use this method:\n    # 1. Comment out the `await run_team_conversation()` line above.\n    # 2. Uncomment the following block:\n    \"\"\"\n    import asyncio\n    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n        try:\n            # This creates an event loop, runs your async function, and closes the loop.\n            asyncio.run(run_team_conversation())\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n    \"\"\"\n\nelse:\n    # This message prints if the root agent variable wasn't found earlier\n    print(\"\\n\u26a0\ufe0f Skipping agent team conversation execution as the root agent was not successfully defined in a previous step.\")\n</code></pre> \ud83d\udccc Adding Memory and Personalization with Session State <p>So far, our agent team can handle different tasks through delegation, but each interaction starts fresh \u2013 the agents have no memory of past conversations or user preferences within a session. To create more sophisticated and context-aware experiences, agents need memory. ADK provides this through Session State.</p> \ud83d\udccc What is Session State? <ul> <li> <p>It's a Python dictionary (<code>session.state</code>) tied to a specific user session (identified by APP_NAME, USER_ID, SESSION_ID).</p> </li> <li> <p>It persists information across multiple conversational turns within that session.</p> </li> <li> <p>Agents and Tools can read from and write to this state, allowing them to remember details, adapt behavior, and personalize responses.</p> </li> </ul> \ud83d\udccc How Agents Interact with State: <ol> <li> <p>ToolContext (Primary Method): Tools can accept a <code>ToolContext</code> object (automatically provided by ADK if declared as the last argument). This object gives direct access to the session state via <code>tool_context.state</code>, allowing tools to read preferences or save results during execution.</p> </li> <li> <p>output_key (Auto-Save Agent Response): An Agent can be configured with an <code>output_key=\"your_key\"</code>. ADK will then automatically save the agent's final textual response for a turn into <code>session.state[\"your_key\"]</code>.</p> </li> </ol> \ud83d\udccc Initialize New Session Service and State <p>To clearly demonstrate state management without interference from prior steps, we'll instantiate a new InMemorySessionService. We'll also create a session with an initial state defining the user's preferred temperature unit.</p> <pre><code># @title 1. Initialize New Session Service and State\n\n# Import necessary session components\nfrom google.adk.sessions import InMemorySessionService\n\n# Create a NEW session service instance for this state demonstration\nsession_service_stateful = InMemorySessionService()\nprint(\"\u2705 New InMemorySessionService created for state demonstration.\")\n\n# Define a NEW session ID for this part of the tutorial\nSESSION_ID_STATEFUL = \"session_state_demo_001\"\nUSER_ID_STATEFUL = \"user_state_demo\"\n\n# Define initial state data - user prefers Celsius initially\ninitial_state = {\n    \"user_preference_temperature_unit\": \"Celsius\"\n}\n\n# Create the session, providing the initial state\nsession_stateful = await session_service_stateful.create_session(\n    app_name=APP_NAME, # Use the consistent app name\n    user_id=USER_ID_STATEFUL,\n    session_id=SESSION_ID_STATEFUL,\n    state=initial_state # &lt;&lt;&lt; Initialize state during creation\n)\nprint(f\"\u2705 Session '{SESSION_ID_STATEFUL}' created for user '{USER_ID_STATEFUL}'.\")\n\n# Verify the initial state was set correctly\nretrieved_session = await session_service_stateful.get_session(app_name=APP_NAME,\n                                                         user_id=USER_ID_STATEFUL,\n                                                         session_id = SESSION_ID_STATEFUL)\nprint(\"\\n--- Initial Session State ---\")\nif retrieved_session:\n    print(retrieved_session.state)\nelse:\n    print(\"Error: Could not retrieve session.\")\n</code></pre> \ud83d\udccc Create State-Aware Weather Tool (get_weather_stateful) <p>Now, we create a new version of the weather tool. Its key feature is accepting <code>tool_context: ToolContext</code> which allows it to access <code>tool_context.state</code>. It will read the user_preference_temperature_unit and format the temperature accordingly.</p> <ul> <li> <p>Key Concept: ToolContext This object is the bridge allowing your tool logic to interact with the session's context, including reading and writing state variables. ADK injects it automatically if defined as the last parameter of your tool function.</p> </li> <li> <p>Best Practice: When reading from state, use dictionary.get('key', default_value) to handle cases where the key might not exist yet, ensuring your tool doesn't crash.</p> </li> </ul> <pre><code>from google.adk.tools.tool_context import ToolContext\n\ndef get_weather_stateful(city: str, tool_context: ToolContext) -&gt; dict:\n    \"\"\"Retrieves weather, converts temp unit based on session state.\"\"\"\n    print(f\"--- Tool: get_weather_stateful called for {city} ---\")\n\n    # --- Read preference from state ---\n    preferred_unit = tool_context.state.get(\"user_preference_temperature_unit\", \"Celsius\") # Default to Celsius\n    print(f\"--- Tool: Reading state 'user_preference_temperature_unit': {preferred_unit} ---\")\n\n    city_normalized = city.lower().replace(\" \", \"\")\n\n    # Mock weather data (always stored in Celsius internally)\n    mock_weather_db = {\n        \"newyork\": {\"temp_c\": 25, \"condition\": \"sunny\"},\n        \"london\": {\"temp_c\": 15, \"condition\": \"cloudy\"},\n        \"tokyo\": {\"temp_c\": 18, \"condition\": \"light rain\"},\n    }\n\n    if city_normalized in mock_weather_db:\n        data = mock_weather_db[city_normalized]\n        temp_c = data[\"temp_c\"]\n        condition = data[\"condition\"]\n\n        # Format temperature based on state preference\n        if preferred_unit == \"Fahrenheit\":\n            temp_value = (temp_c * 9/5) + 32 # Calculate Fahrenheit\n            temp_unit = \"\u00b0F\"\n        else: # Default to Celsius\n            temp_value = temp_c\n            temp_unit = \"\u00b0C\"\n\n        report = f\"The weather in {city.capitalize()} is {condition} with a temperature of {temp_value:.0f}{temp_unit}.\"\n        result = {\"status\": \"success\", \"report\": report}\n        print(f\"--- Tool: Generated report in {preferred_unit}. Result: {result} ---\")\n\n        # Example of writing back to state (optional for this tool)\n        tool_context.state[\"last_city_checked_stateful\"] = city\n        print(f\"--- Tool: Updated state 'last_city_checked_stateful': {city} ---\")\n\n        return result\n    else:\n        # Handle city not found\n        error_msg = f\"Sorry, I don't have weather information for '{city}'.\"\n        print(f\"--- Tool: City '{city}' not found. ---\")\n        return {\"status\": \"error\", \"error_message\": error_msg}\n\nprint(\"\u2705 State-aware 'get_weather_stateful' tool defined.\")\n</code></pre> \ud83d\udccc Redefine Sub-Agents and Update Root Agent <p>To ensure this step is self-contained and builds correctly, we first redefine the greeting_agent and farewell_agent exactly as they were in Step 3. Then, we define our new root agent (weather_agent_v4_stateful):</p> <ul> <li> <p>It uses the new get_weather_stateful tool.</p> </li> <li> <p>It includes the greeting and farewell sub-agents for delegation.</p> </li> <li> <p>Crucially, it sets output_key=\"last_weather_report\" which automatically saves its final weather response to the session state.</p> </li> </ul> <pre><code># @title 3. Redefine Sub-Agents and Update Root Agent with output_key\n\n# Ensure necessary imports: Agent, LiteLlm, Runner\nfrom google.adk.agents import Agent\nfrom google.adk.models.lite_llm import LiteLlm\nfrom google.adk.runners import Runner\n# Ensure tools 'say_hello', 'say_goodbye' are defined (from Step 3)\n# Ensure model constants MODEL_GPT_4O, MODEL_GEMINI_2_0_FLASH etc. are defined\n\n# --- Redefine Greeting Agent (from Step 3) ---\ngreeting_agent = None\ntry:\n    greeting_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"greeting_agent\",\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n        tools=[say_hello],\n    )\n    print(f\"\u2705 Agent '{greeting_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Greeting agent. Error: {e}\")\n\n# --- Redefine Farewell Agent (from Step 3) ---\nfarewell_agent = None\ntry:\n    farewell_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"farewell_agent\",\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n        tools=[say_goodbye],\n    )\n    print(f\"\u2705 Agent '{farewell_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Farewell agent. Error: {e}\")\n\n# --- Define the Updated Root Agent ---\nroot_agent_stateful = None\nrunner_root_stateful = None # Initialize runner\n\n# Check prerequisites before creating the root agent\nif greeting_agent and farewell_agent and 'get_weather_stateful' in globals():\n\n    root_agent_model = MODEL_GEMINI_2_0_FLASH # Choose orchestration model\n\n    root_agent_stateful = Agent(\n        name=\"weather_agent_v4_stateful\", # New version name\n        model=root_agent_model,\n        description=\"Main agent: Provides weather (state-aware unit), delegates greetings/farewells, saves report to state.\",\n        instruction=\"You are the main Weather Agent. Your job is to provide weather using 'get_weather_stateful'. \"\n                    \"The tool will format the temperature based on user preference stored in state. \"\n                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n                    \"Handle only weather requests, greetings, and farewells.\",\n        tools=[get_weather_stateful], # Use the state-aware tool\n        sub_agents=[greeting_agent, farewell_agent], # Include sub-agents\n        output_key=\"last_weather_report\" # &lt;&lt;&lt; Auto-save agent's final weather response\n    )\n    print(f\"\u2705 Root Agent '{root_agent_stateful.name}' created using stateful tool and output_key.\")\n\n    # --- Create Runner for this Root Agent &amp; NEW Session Service ---\n    runner_root_stateful = Runner(\n        agent=root_agent_stateful,\n        app_name=APP_NAME,\n        session_service=session_service_stateful # Use the NEW stateful session service\n    )\n    print(f\"\u2705 Runner created for stateful root agent '{runner_root_stateful.agent.name}' using stateful session service.\")\n\nelse:\n    print(\"\u274c Cannot create stateful root agent. Prerequisites missing.\")\n    if not greeting_agent: print(\" - greeting_agent definition missing.\")\n    if not farewell_agent: print(\" - farewell_agent definition missing.\")\n    if 'get_weather_stateful' not in globals(): print(\" - get_weather_stateful tool missing.\")\n</code></pre> \ud83d\udccc Interact and Test State Flow <p>Now, let's execute a conversation designed to test the state interactions using the runner_root_stateful (associated with our stateful agent and the session_service_stateful). We'll use the call_agent_async function defined earlier, ensuring we pass the correct runner, user ID (USER_ID_STATEFUL), and session ID (SESSION_ID_STATEFUL).</p> <p>The conversation flow will be:</p> <ol> <li> <p>Check weather (London) response (the weather report in Celsius) should get saved to state['last_weather_report'] via the output_key configuration.</p> </li> <li> <p>Manually update state: We will directly modify the state stored within the <code>InMemorySessionService</code> instance (<code>session_service_stateful</code>).</p> <p>Why direct modification? The session_service.get_session() method returns a copy of the session. Modifying that copy wouldn't affect the state used in subsequent agent runs. For this testing scenario with InMemorySessionService, we access the internal sessions dictionary to change the actual stored state value for user_preference_temperature_unit to \"Fahrenheit\". Note: In real applications, state changes are typically triggered by tools or agent logic returning EventActions(state_delta=...), not direct manual updates.</p> </li> <li> <p>Check weather again (New York): The get_weather_stateful tool should now read the updated \"Fahrenheit\" preference from the state and convert the temperature accordingly. The root agent's new response (weather in Fahrenheit) will overwrite the previous value in state['last_weather_report'] due to the output_key.</p> </li> <li> <p>Greet the agent: Verify that delegation to the greeting_agent still works correctly alongside the stateful operations. This interaction will become the last response saved by output_key in this specific sequence.</p> </li> <li> <p>Inspect final state: After the conversation, we retrieve the session one last time (getting a copy) and print its state to confirm the user_preference_temperature_unit is indeed \"Fahrenheit\", observe the final value saved by output_key (which will be the greeting in this run), and see the last_city_checked_stateful value written by the tool.</p> </li> </ol> <pre><code># @title 4. Interact to Test State Flow and output_key\nimport asyncio # Ensure asyncio is imported\n\n# Ensure the stateful runner (runner_root_stateful) is available from the previous cell\n# Ensure call_agent_async, USER_ID_STATEFUL, SESSION_ID_STATEFUL, APP_NAME are defined\n\nif 'runner_root_stateful' in globals() and runner_root_stateful:\n    # Define the main async function for the stateful conversation logic.\n    # The 'await' keywords INSIDE this function are necessary for async operations.\n    async def run_stateful_conversation():\n        print(\"\\n--- Testing State: Temp Unit Conversion &amp; output_key ---\")\n\n        # 1. Check weather (Uses initial state: Celsius)\n        print(\"--- Turn 1: Requesting weather in London (expect Celsius) ---\")\n        await call_agent_async(query= \"What's the weather in London?\",\n                               runner=runner_root_stateful,\n                               user_id=USER_ID_STATEFUL,\n                               session_id=SESSION_ID_STATEFUL\n                              )\n\n        # 2. Manually update state preference to Fahrenheit - DIRECTLY MODIFY STORAGE\n        print(\"\\n--- Manually Updating State: Setting unit to Fahrenheit ---\")\n        try:\n            # Access the internal storage directly - THIS IS SPECIFIC TO InMemorySessionService for testing\n            # NOTE: In production with persistent services (Database, VertexAI), you would\n            # typically update state via agent actions or specific service APIs if available,\n            # not by direct manipulation of internal storage.\n            stored_session = session_service_stateful.sessions[APP_NAME][USER_ID_STATEFUL][SESSION_ID_STATEFUL]\n            stored_session.state[\"user_preference_temperature_unit\"] = \"Fahrenheit\"\n            # Optional: You might want to update the timestamp as well if any logic depends on it\n            # import time\n            # stored_session.last_update_time = time.time()\n            print(f\"--- Stored session state updated. Current 'user_preference_temperature_unit': {stored_session.state.get('user_preference_temperature_unit', 'Not Set')} ---\") # Added .get for safety\n        except KeyError:\n            print(f\"--- Error: Could not retrieve session '{SESSION_ID_STATEFUL}' from internal storage for user '{USER_ID_STATEFUL}' in app '{APP_NAME}' to update state. Check IDs and if session was created. ---\")\n        except Exception as e:\n             print(f\"--- Error updating internal session state: {e} ---\")\n\n        # 3. Check weather again (Tool should now use Fahrenheit)\n        # This will also update 'last_weather_report' via output_key\n        print(\"\\n--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\")\n        await call_agent_async(query= \"Tell me the weather in New York.\",\n                               runner=runner_root_stateful,\n                               user_id=USER_ID_STATEFUL,\n                               session_id=SESSION_ID_STATEFUL\n                              )\n\n        # 4. Test basic delegation (should still work)\n        # This will update 'last_weather_report' again, overwriting the NY weather report\n        print(\"\\n--- Turn 3: Sending a greeting ---\")\n        await call_agent_async(query= \"Hi!\",\n                               runner=runner_root_stateful,\n                               user_id=USER_ID_STATEFUL,\n                               session_id=SESSION_ID_STATEFUL\n                              )\n\n    # --- Execute the `run_stateful_conversation` async function ---\n    # Choose ONE of the methods below based on your environment.\n\n    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n    # it means an event loop is already running, so you can directly await the function.\n    print(\"Attempting execution using 'await' (default for notebooks)...\")\n    await run_stateful_conversation()\n\n    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n    # If running this code as a standard Python script from your terminal,\n    # the script context is synchronous. `asyncio.run()` is needed to\n    # create and manage an event loop to execute your async function.\n    # To use this method:\n    # 1. Comment out the `await run_stateful_conversation()` line above.\n    # 2. Uncomment the following block:\n    \"\"\"\n    import asyncio\n    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n        try:\n            # This creates an event loop, runs your async function, and closes the loop.\n            asyncio.run(run_stateful_conversation())\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n    \"\"\"\n\n    # --- Inspect final session state after the conversation ---\n    # This block runs after either execution method completes.\n    print(\"\\n--- Inspecting Final Session State ---\")\n    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n                                                         user_id= USER_ID_STATEFUL,\n                                                         session_id=SESSION_ID_STATEFUL)\n    if final_session:\n        # Use .get() for safer access to potentially missing keys\n        print(f\"Final Preference: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\")\n        print(f\"Final Last Weather Report (from output_key): {final_session.state.get('last_weather_report', 'Not Set')}\")\n        print(f\"Final Last City Checked (by tool): {final_session.state.get('last_city_checked_stateful', 'Not Set')}\")\n        # Print full state for detailed view\n        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n    else:\n        print(\"\\n\u274c Error: Could not retrieve final session state.\")\n\nelse:\n    print(\"\\n\u26a0\ufe0f Skipping state test conversation. Stateful root agent runner ('runner_root_stateful') is not available.\")\n</code></pre> <ul> <li> <p>State Read: The weather tool (<code>get_weather_stateful</code>) correctly read <code>user_preference_temperature_unit</code> from state, initially using \"Celsius\" for London.</p> </li> <li> <p>State Update: The direct modification successfully changed the stored preference to \"Fahrenheit\".</p> </li> <li> <p>State Read (Updated): The tool subsequently read \"Fahrenheit\" when asked for New York's weather and performed the conversion.</p> </li> <li> <p>Tool State Write: The tool successfully wrote the last_city_checked_stateful (\"New York\" after the second weather check) into the state via tool_context.state.</p> </li> <li> <p>Delegation: The delegation to the greeting_agent for \"Hi!\" functioned correctly even after state modifications.</p> </li> <li> <p>output_key: The <code>output_key=\"last_weather_report\"</code> successfully saved the root agent's final response for each turn where the root agent was the one ultimately responding. In this sequence, the last response was the greeting (\"Hello, there!\"), so that overwrote the weather report in the state key.</p> </li> <li> <p>Final State: The final check confirms the preference persisted as \"Fahrenheit\".</p> </li> </ul> <p>You've now successfully integrated session state to personalize agent behavior using ToolContext, manually manipulated state for testing InMemorySessionService, and observed how output_key provides a simple mechanism for saving the agent's last response to state. This foundational understanding of state management is key as we proceed to implement safety guardrails using callbacks in the next steps.</p> \ud83d\udccc Adding Safety - Input Guardrail with before_model_callback <p>Our agent team is becoming more capable, remembering preferences and using tools effectively. However, in real-world scenarios, we often need safety mechanisms to control the agent's behavior before potentially problematic requests even reach the core Large Language Model (LLM).</p> <p>ADK provides Callbacks \u2013 functions that allow you to hook into specific points in the agent's execution lifecycle. The before_model_callback is particularly useful for input safety.</p> \ud83d\udccc What is before_model_callback? <ul> <li> <p>It's a Python function you define that ADK executes just before an agent sends its compiled request (including conversation history, instructions, and the latest user message) to the underlying LLM.</p> </li> <li> <p>Purpose: Inspect the request, modify it if necessary, or block it entirely based on predefined rules.</p> </li> </ul> \ud83d\udccc Common Use Cases: <ul> <li> <p>Input Validation/Filtering: Check if user input meets criteria or contains disallowed content (like PII or keywords).</p> </li> <li> <p>Guardrails: Prevent harmful, off-topic, or policy-violating requests from being processed by the LLM.</p> </li> <li> <p>Dynamic Prompt Modification: Add timely information (e.g., from session state) to the LLM request context just before sending.</p> </li> </ul> \ud83d\udccc How it Works: <ol> <li> <p>Define a function accepting <code>callback_context: CallbackContext</code> and <code>llm_request: LlmRequest</code>.</p> <ul> <li> <p><code>callback_context:</code> Provides access to agent info, session state <code>(callback_context.state)</code>, etc.</p> </li> <li> <p><code>llm_request:</code> Contains the full payload intended for the LLM <code>(contents, config)</code>.</p> </li> </ul> </li> <li> <p>Inside the function:</p> <ul> <li> <p>Inspect: Examine llm_request.contents (especially the last user message).</p> </li> <li> <p>Modify (Use Caution): You can change parts of llm_request.</p> </li> <li> <p>Block (Guardrail): Return an <code>LlmResponse</code> object. ADK will send this response back immediately, skipping the LLM call for that turn.</p> </li> <li> <p>Allow: Return <code>None</code>. ADK proceeds to call the LLM with the (potentially modified) request.</p> </li> </ul> </li> </ol> <p>In this step, we will:</p> <ol> <li> <p>Define a <code>before_model_callback</code> function (<code>block_keyword_guardrail</code>) that checks the user's input for a specific keyword (\"BLOCK\").</p> </li> <li> <p>Update our stateful root agent (<code>weather_agent_v4_stateful</code> from Step 4) to use this callback.</p> </li> <li> <p>Create a new runner associated with this updated agent but using the same stateful session service to maintain state continuity.</p> </li> <li> <p>Test the guardrail by sending both normal and keyword-containing requests.</p> </li> </ol> \ud83d\udccc Define the Guardrail Callback Function: <p>This function will inspect the last user message within the <code>llm_request</code> content. If it finds \"BLOCK\" (case-insensitive), it constructs and returns an <code>LlmResponse</code> to block the flow; otherwise, it returns None.</p> <pre><code># @title 1. Define the before_model_callback Guardrail\n\n# Ensure necessary imports are available\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.adk.models.llm_request import LlmRequest\nfrom google.adk.models.llm_response import LlmResponse\nfrom google.genai import types # For creating response content\nfrom typing import Optional\n\ndef block_keyword_guardrail(\n    callback_context: CallbackContext, llm_request: LlmRequest\n) -&gt; Optional[LlmResponse]:\n    \"\"\"\n    Inspects the latest user message for 'BLOCK'. If found, blocks the LLM call\n    and returns a predefined LlmResponse. Otherwise, returns None to proceed.\n    \"\"\"\n    agent_name = callback_context.agent_name # Get the name of the agent whose model call is being intercepted\n    print(f\"--- Callback: block_keyword_guardrail running for agent: {agent_name} ---\")\n\n    # Extract the text from the latest user message in the request history\n    last_user_message_text = \"\"\n    if llm_request.contents:\n        # Find the most recent message with role 'user'\n        for content in reversed(llm_request.contents):\n            if content.role == 'user' and content.parts:\n                # Assuming text is in the first part for simplicity\n                if content.parts[0].text:\n                    last_user_message_text = content.parts[0].text\n                    break # Found the last user message text\n\n    print(f\"--- Callback: Inspecting last user message: '{last_user_message_text[:100]}...' ---\") # Log first 100 chars\n\n    # --- Guardrail Logic ---\n    keyword_to_block = \"BLOCK\"\n    if keyword_to_block in last_user_message_text.upper(): # Case-insensitive check\n        print(f\"--- Callback: Found '{keyword_to_block}'. Blocking LLM call! ---\")\n        # Optionally, set a flag in state to record the block event\n        callback_context.state[\"guardrail_block_keyword_triggered\"] = True\n        print(f\"--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\")\n\n        # Construct and return an LlmResponse to stop the flow and send this back instead\n        return LlmResponse(\n            content=types.Content(\n                role=\"model\", # Mimic a response from the agent's perspective\n                parts=[types.Part(text=f\"I cannot process this request because it contains the blocked keyword '{keyword_to_block}'.\")],\n            )\n            # Note: You could also set an error_message field here if needed\n        )\n    else:\n        # Keyword not found, allow the request to proceed to the LLM\n        print(f\"--- Callback: Keyword not found. Allowing LLM call for {agent_name}. ---\")\n        return None # Returning None signals ADK to continue normally\n\nprint(\"\u2705 block_keyword_guardrail function defined.\")\n</code></pre> \ud83d\udccc Update Root Agent to Use the Callback: <p>We redefine the root agent, adding the <code>before_model_callback</code> parameter and pointing it to our new guardrail function. We'll give it a new version name for clarity.</p> <p>Important: We need to redefine the sub-agents (<code>greeting_agent, farewell_agent</code>) and the stateful tool (<code>get_weather_stateful</code>) within this context if they are not already available from previous steps, ensuring the root agent definition has access to all its components.</p> <pre><code># @title 2. Update Root Agent with before_model_callback\n\n\n# --- Redefine Sub-Agents (Ensures they exist in this context) ---\ngreeting_agent = None\ntry:\n    # Use a defined model constant\n    greeting_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"greeting_agent\", # Keep original name for consistency\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n        tools=[say_hello],\n    )\n    print(f\"\u2705 Sub-Agent '{greeting_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n\nfarewell_agent = None\ntry:\n    # Use a defined model constant\n    farewell_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"farewell_agent\", # Keep original name\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n        tools=[say_goodbye],\n    )\n    print(f\"\u2705 Sub-Agent '{farewell_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n\n\n# --- Define the Root Agent with the Callback ---\nroot_agent_model_guardrail = None\nrunner_root_model_guardrail = None\n\n# Check all components before proceeding\nif greeting_agent and farewell_agent and 'get_weather_stateful' in globals() and 'block_keyword_guardrail' in globals():\n\n    # Use a defined model constant\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\n\n    root_agent_model_guardrail = Agent(\n        name=\"weather_agent_v5_model_guardrail\", # New version name for clarity\n        model=root_agent_model,\n        description=\"Main agent: Handles weather, delegates greetings/farewells, includes input keyword guardrail.\",\n        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n                    \"Handle only weather requests, greetings, and farewells.\",\n        tools=[get_weather_stateful],\n        sub_agents=[greeting_agent, farewell_agent], # Reference the redefined sub-agents\n        output_key=\"last_weather_report\", # Keep output_key from Step 4\n        before_model_callback=block_keyword_guardrail # &lt;&lt;&lt; Assign the guardrail callback\n    )\n    print(f\"\u2705 Root Agent '{root_agent_model_guardrail.name}' created with before_model_callback.\")\n\n    # --- Create Runner for this Agent, Using SAME Stateful Session Service ---\n    # Ensure session_service_stateful exists from Step 4\n    if 'session_service_stateful' in globals():\n        runner_root_model_guardrail = Runner(\n            agent=root_agent_model_guardrail,\n            app_name=APP_NAME, # Use consistent APP_NAME\n            session_service=session_service_stateful # &lt;&lt;&lt; Use the service from Step 4\n        )\n        print(f\"\u2705 Runner created for guardrail agent '{runner_root_model_guardrail.agent.name}', using stateful session service.\")\n    else:\n        print(\"\u274c Cannot create runner. 'session_service_stateful' from Step 4 is missing.\")\n\nelse:\n    print(\"\u274c Cannot create root agent with model guardrail. One or more prerequisites are missing or failed initialization:\")\n    if not greeting_agent: print(\"   - Greeting Agent\")\n    if not farewell_agent: print(\"   - Farewell Agent\")\n    if 'get_weather_stateful' not in globals(): print(\"   - 'get_weather_stateful' tool\")\n    if 'block_keyword_guardrail' not in globals(): print(\"   - 'block_keyword_guardrail' callback\")\n</code></pre> \ud83d\udccc Interact to Test the Guardrail: <p>Let's test the guardrail's behavior. We'll use the same session (SESSION_ID_STATEFUL).</p> <ol> <li> <p>Send a normal weather request (should pass the guardrail and execute).</p> </li> <li> <p>Send a request containing \"BLOCK\" (should be intercepted by the callback).</p> </li> <li> <p>Send a greeting (should pass the root agent's guardrail, be delegated, and execute normally).</p> </li> </ol> <pre><code># @title 3. Interact to Test the Model Input Guardrail\nimport asyncio # Ensure asyncio is imported\n\n# Ensure the runner for the guardrail agent is available\nif 'runner_root_model_guardrail' in globals() and runner_root_model_guardrail:\n    # Define the main async function for the guardrail test conversation.\n    # The 'await' keywords INSIDE this function are necessary for async operations.\n    async def run_guardrail_test_conversation():\n        print(\"\\n--- Testing Model Input Guardrail ---\")\n\n        # Use the runner for the agent with the callback and the existing stateful session ID\n        # Define a helper lambda for cleaner interaction calls\n        interaction_func = lambda query: call_agent_async(query,\n                                                         runner_root_model_guardrail,\n                                                         USER_ID_STATEFUL, # Use existing user ID\n                                                         SESSION_ID_STATEFUL # Use existing session ID\n                                                        )\n        # 1. Normal request (Callback allows, should use Fahrenheit from previous state change)\n        print(\"--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\")\n        await interaction_func(\"What is the weather in London?\")\n\n        # 2. Request containing the blocked keyword (Callback intercepts)\n        print(\"\\n--- Turn 2: Requesting with blocked keyword (expect blocked) ---\")\n        await interaction_func(\"BLOCK the request for weather in Tokyo\") # Callback should catch \"BLOCK\"\n\n        # 3. Normal greeting (Callback allows root agent, delegation happens)\n        print(\"\\n--- Turn 3: Sending a greeting (expect allowed) ---\")\n        await interaction_func(\"Hello again\")\n\n    # --- Execute the `run_guardrail_test_conversation` async function ---\n    # Choose ONE of the methods below based on your environment.\n\n    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n    # it means an event loop is already running, so you can directly await the function.\n    print(\"Attempting execution using 'await' (default for notebooks)...\")\n    await run_guardrail_test_conversation()\n\n    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n    # If running this code as a standard Python script from your terminal,\n    # the script context is synchronous. `asyncio.run()` is needed to\n    # create and manage an event loop to execute your async function.\n    # To use this method:\n    # 1. Comment out the `await run_guardrail_test_conversation()` line above.\n    # 2. Uncomment the following block:\n    \"\"\"\n    import asyncio\n    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n        try:\n            # This creates an event loop, runs your async function, and closes the loop.\n            asyncio.run(run_guardrail_test_conversation())\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n    \"\"\"\n\n    # --- Inspect final session state after the conversation ---\n    # This block runs after either execution method completes.\n    # Optional: Check state for the trigger flag set by the callback\n    print(\"\\n--- Inspecting Final Session State (After Guardrail Test) ---\")\n    # Use the session service instance associated with this stateful session\n    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n                                                         user_id=USER_ID_STATEFUL,\n                                                         session_id=SESSION_ID_STATEFUL)\n    if final_session:\n        # Use .get() for safer access\n        print(f\"Guardrail Triggered Flag: {final_session.state.get('guardrail_block_keyword_triggered', 'Not Set (or False)')}\")\n        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n    else:\n        print(\"\\n\u274c Error: Could not retrieve final session state.\")\n\nelse:\n    print(\"\\n\u26a0\ufe0f Skipping model guardrail test. Runner ('runner_root_model_guardrail') is not available.\")\n</code></pre> \ud83d\udccc Observe the execution flow: <ol> <li> <p>London Weather: The callback runs for <code>weather_agent_v5_model_guardrail</code>, inspects the message, prints \"Keyword not found. Allowing LLM call.\", and returns <code>None</code>. The agent proceeds, calls the <code>get_weather_stateful</code> tool (which uses the \"Fahrenheit\" preference from Step 4's state change), and returns the weather. This response updates <code>last_weather_report via output_key</code>.</p> </li> <li> <p>BLOCK Request: The callback runs again for <code>weather_agent_v5_model_guardrail</code>, inspects the message, finds \"BLOCK\", prints \"Blocking LLM call!\", sets the state flag, and returns the predefined LlmResponse. The agent's underlying LLM is never called for this turn. The user sees the callback's blocking message.</p> </li> <li> <p>Hello Again: The callback runs for weather_agent_v5_model_guardrail, allows the request. The root agent then delegates to greeting_agent. Note: The before_model_callback defined on the root agent does NOT automatically apply to sub-agents. The greeting_agent proceeds normally, calls its say_hello tool, and returns the greeting.</p> </li> </ol> <p>You have successfully implemented an input safety layer! The before_model_callback provides a powerful mechanism to enforce rules and control agent behavior before expensive or potentially risky LLM calls are made. Next, we'll apply a similar concept to add guardrails around tool usage itself.</p> \ud83d\udccc Adding Safety - Tool Argument Guardrail (before_tool_callback): <p>In Step 5, we added a guardrail to inspect and potentially block user input before it reached the LLM. Now, we'll add another layer of control after the LLM has decided to use a tool but before that tool actually executes. This is useful for validating the arguments the LLM wants to pass to the tool.</p> <p>ADK provides the <code>before_tool_callback</code> for this precise purpose.</p> \ud83d\udccc What is before_tool_callback? <ul> <li> <p>It's a Python function executed just before a specific tool function runs, after the LLM has requested its use and decided on the arguments.</p> </li> <li> <p>Purpose: Validate tool arguments, prevent tool execution based on specific inputs, modify arguments dynamically, or enforce resource usage policies.</p> </li> </ul> \ud83d\udccc Common Use Cases: <ul> <li> <p>Argument Validation: Check if arguments provided by the LLM are valid, within allowed ranges, or conform to expected formats.</p> </li> <li> <p>Resource Protection: Prevent tools from being called with inputs that might be costly, access restricted data, or cause unwanted side effects (e.g., blocking API calls for certain parameters).</p> </li> <li> <p>Dynamic Argument Modification: Adjust arguments based on session state or other contextual information before the tool runs.</p> </li> </ul> <p>How it Works:</p> <ol> <li> <p>Define a function accepting <code>tool: BaseTool</code>, <code>args: Dict[str, Any]</code>, and <code>tool_context: ToolContext</code>.</p> <ul> <li> <p>tool: The tool object about to be called (inspect <code>tool.name</code>).</p> </li> <li> <p>args: The dictionary of arguments the LLM generated for the tool.</p> </li> <li> <p>tool_context: Provides access to session state (tool_context.state), agent info, etc.</p> </li> </ul> </li> <li> <p>Inside the function:</p> <ul> <li> <p>Inspect: Examine the <code>tool.name</code> and the args dictionary.</p> </li> <li> <p>Modify: Change values within the <code>args</code> dictionary directly. If you return <code>None</code>, the tool runs with these modified args.</p> </li> <li> <p>Block/Override (Guardrail): Return a dictionary. ADK treats this dictionary as the result of the tool call, completely skipping the execution of the original tool function. The dictionary should ideally match the expected return format of the tool it's blocking.</p> </li> <li> <p>Allow: Return <code>None</code>. ADK proceeds to execute the actual tool function with the (potentially modified) arguments.</p> </li> </ul> </li> </ol> <p>In this step, we will:</p> <ol> <li> <p>Define a <code>before_tool_callback</code> function (<code>block_paris_tool_guardrail</code>) that specifically checks if the <code>get_weather_stateful</code> tool is called with the city \"Paris\".</p> </li> <li> <p>If \"Paris\" is detected, the callback will block the tool and return a custom error dictionary.</p> </li> <li> <p>Update our root agent (<code>weather_agent_v6_tool_guardrail</code>) to include both the <code>before_model_callback</code> and this new <code>before_tool_callback</code>.</p> </li> <li> <p>Create a new runner for this agent, using the same stateful session service.</p> </li> <li> <p>Test the flow by requesting weather for allowed cities and the blocked city (\"Paris\").</p> </li> </ol> \ud83d\udccc Define the Tool Guardrail Callback Function <p>This function targets the <code>get_weather_stateful</code> tool. It checks the city argument. If it's \"Paris\", it returns an error dictionary that looks like the tool's own error response. Otherwise, it allows the tool to run by returning None.</p> <pre><code># @title 1. Define the before_tool_callback Guardrail\n\n# Ensure necessary imports are available\nfrom google.adk.tools.base_tool import BaseTool\nfrom google.adk.tools.tool_context import ToolContext\nfrom typing import Optional, Dict, Any # For type hints\n\ndef block_paris_tool_guardrail(\n    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Checks if 'get_weather_stateful' is called for 'Paris'.\n    If so, blocks the tool execution and returns a specific error dictionary.\n    Otherwise, allows the tool call to proceed by returning None.\n    \"\"\"\n    tool_name = tool.name\n    agent_name = tool_context.agent_name # Agent attempting the tool call\n    print(f\"--- Callback: block_paris_tool_guardrail running for tool '{tool_name}' in agent '{agent_name}' ---\")\n    print(f\"--- Callback: Inspecting args: {args} ---\")\n\n    # --- Guardrail Logic ---\n    target_tool_name = \"get_weather_stateful\" # Match the function name used by FunctionTool\n    blocked_city = \"paris\"\n\n    # Check if it's the correct tool and the city argument matches the blocked city\n    if tool_name == target_tool_name:\n        city_argument = args.get(\"city\", \"\") # Safely get the 'city' argument\n        if city_argument and city_argument.lower() == blocked_city:\n            print(f\"--- Callback: Detected blocked city '{city_argument}'. Blocking tool execution! ---\")\n            # Optionally update state\n            tool_context.state[\"guardrail_tool_block_triggered\"] = True\n            print(f\"--- Callback: Set state 'guardrail_tool_block_triggered': True ---\")\n\n            # Return a dictionary matching the tool's expected output format for errors\n            # This dictionary becomes the tool's result, skipping the actual tool run.\n            return {\n                \"status\": \"error\",\n                \"error_message\": f\"Policy restriction: Weather checks for '{city_argument.capitalize()}' are currently disabled by a tool guardrail.\"\n            }\n        else:\n             print(f\"--- Callback: City '{city_argument}' is allowed for tool '{tool_name}'. ---\")\n    else:\n        print(f\"--- Callback: Tool '{tool_name}' is not the target tool. Allowing. ---\")\n\n\n    # If the checks above didn't return a dictionary, allow the tool to execute\n    print(f\"--- Callback: Allowing tool '{tool_name}' to proceed. ---\")\n    return None # Returning None allows the actual tool function to run\n\nprint(\"\u2705 block_paris_tool_guardrail function defined.\")\n</code></pre> \ud83d\udccc Update Root Agent to Use Both Callbacks <p>We redefine the root agent again (<code>weather_agent_v6_tool_guardrail</code>), this time adding the <code>before_tool_callback</code>parameter alongside the <code>before_model_callback</code></p> <p>Self-Contained Execution Note: Ensure all prerequisites (sub-agents, tools, <code>before_model_callback</code>) are defined or available in the execution context before defining this agent.</p> <pre><code># @title 2. Update Root Agent with BOTH Callbacks (Self-Contained)\n\n# --- Ensure Prerequisites are Defined ---\n# (Include or ensure execution of definitions for: Agent, LiteLlm, Runner, ToolContext,\n#  MODEL constants, say_hello, say_goodbye, greeting_agent, farewell_agent,\n#  get_weather_stateful, block_keyword_guardrail, block_paris_tool_guardrail)\n\n# --- Redefine Sub-Agents (Ensures they exist in this context) ---\ngreeting_agent = None\ntry:\n    # Use a defined model constant\n    greeting_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"greeting_agent\", # Keep original name for consistency\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n        tools=[say_hello],\n    )\n    print(f\"\u2705 Sub-Agent '{greeting_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n\nfarewell_agent = None\ntry:\n    # Use a defined model constant\n    farewell_agent = Agent(\n        model=MODEL_GEMINI_2_0_FLASH,\n        name=\"farewell_agent\", # Keep original name\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n        tools=[say_goodbye],\n    )\n    print(f\"\u2705 Sub-Agent '{farewell_agent.name}' redefined.\")\nexcept Exception as e:\n    print(f\"\u274c Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n\n# --- Define the Root Agent with Both Callbacks ---\nroot_agent_tool_guardrail = None\nrunner_root_tool_guardrail = None\n\nif ('greeting_agent' in globals() and greeting_agent and\n    'farewell_agent' in globals() and farewell_agent and\n    'get_weather_stateful' in globals() and\n    'block_keyword_guardrail' in globals() and\n    'block_paris_tool_guardrail' in globals()):\n\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\n\n    root_agent_tool_guardrail = Agent(\n        name=\"weather_agent_v6_tool_guardrail\", # New version name\n        model=root_agent_model,\n        description=\"Main agent: Handles weather, delegates, includes input AND tool guardrails.\",\n        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n                    \"Delegate greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n                    \"Handle only weather, greetings, and farewells.\",\n        tools=[get_weather_stateful],\n        sub_agents=[greeting_agent, farewell_agent],\n        output_key=\"last_weather_report\",\n        before_model_callback=block_keyword_guardrail, # Keep model guardrail\n        before_tool_callback=block_paris_tool_guardrail # &lt;&lt;&lt; Add tool guardrail\n    )\n    print(f\"\u2705 Root Agent '{root_agent_tool_guardrail.name}' created with BOTH callbacks.\")\n\n    # --- Create Runner, Using SAME Stateful Session Service ---\n    if 'session_service_stateful' in globals():\n        runner_root_tool_guardrail = Runner(\n            agent=root_agent_tool_guardrail,\n            app_name=APP_NAME,\n            session_service=session_service_stateful # &lt;&lt;&lt; Use the service from Step 4/5\n        )\n        print(f\"\u2705 Runner created for tool guardrail agent '{runner_root_tool_guardrail.agent.name}', using stateful session service.\")\n    else:\n        print(\"\u274c Cannot create runner. 'session_service_stateful' from Step 4/5 is missing.\")\n\nelse:\n    print(\"\u274c Cannot create root agent with tool guardrail. Prerequisites missing.\")\n</code></pre> \ud83d\udccc Interact to Test the Tool Guardrail <ol> <li> <p>Request weather for \"New York\": Passes both callbacks, tool executes (using Fahrenheit preference from state).</p> </li> <li> <p>Request weather for \"Paris\": Passes <code>before_model_callback</code>. LLM decides to call <code>get_weather_stateful(city='Paris'). before_tool_callback</code> intercepts, blocks the tool, and returns the error dictionary. Agent relays this error.</p> </li> <li> <p>Request weather for \"London\": Passes both callbacks, tool executes normally.</p> </li> </ol> <pre><code># @title 3. Interact to Test the Tool Argument Guardrail\nimport asyncio # Ensure asyncio is imported\n\n# Ensure the runner for the tool guardrail agent is available\nif 'runner_root_tool_guardrail' in globals() and runner_root_tool_guardrail:\n    # Define the main async function for the tool guardrail test conversation.\n    # The 'await' keywords INSIDE this function are necessary for async operations.\n    async def run_tool_guardrail_test():\n        print(\"\\n--- Testing Tool Argument Guardrail ('Paris' blocked) ---\")\n\n        # Use the runner for the agent with both callbacks and the existing stateful session\n        # Define a helper lambda for cleaner interaction calls\n        interaction_func = lambda query: call_agent_async(query,\n                                                         runner_root_tool_guardrail,\n                                                         USER_ID_STATEFUL, # Use existing user ID\n                                                         SESSION_ID_STATEFUL # Use existing session ID\n                                                        )\n        # 1. Allowed city (Should pass both callbacks, use Fahrenheit state)\n        print(\"--- Turn 1: Requesting weather in New York (expect allowed) ---\")\n        await interaction_func(\"What's the weather in New York?\")\n\n        # 2. Blocked city (Should pass model callback, but be blocked by tool callback)\n        print(\"\\n--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\")\n        await interaction_func(\"How about Paris?\") # Tool callback should intercept this\n\n        # 3. Another allowed city (Should work normally again)\n        print(\"\\n--- Turn 3: Requesting weather in London (expect allowed) ---\")\n        await interaction_func(\"Tell me the weather in London.\")\n\n    # --- Execute the `run_tool_guardrail_test` async function ---\n    # Choose ONE of the methods below based on your environment.\n\n    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n    # it means an event loop is already running, so you can directly await the function.\n    print(\"Attempting execution using 'await' (default for notebooks)...\")\n    await run_tool_guardrail_test()\n\n    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n    # If running this code as a standard Python script from your terminal,\n    # the script context is synchronous. `asyncio.run()` is needed to\n    # create and manage an event loop to execute your async function.\n    # To use this method:\n    # 1. Comment out the `await run_tool_guardrail_test()` line above.\n    # 2. Uncomment the following block:\n    \"\"\"\n    import asyncio\n    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n        try:\n            # This creates an event loop, runs your async function, and closes the loop.\n            asyncio.run(run_tool_guardrail_test())\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n    \"\"\"\n\n    # --- Inspect final session state after the conversation ---\n    # This block runs after either execution method completes.\n    # Optional: Check state for the tool block trigger flag\n    print(\"\\n--- Inspecting Final Session State (After Tool Guardrail Test) ---\")\n    # Use the session service instance associated with this stateful session\n    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n                                                         user_id=USER_ID_STATEFUL,\n                                                         session_id= SESSION_ID_STATEFUL)\n    if final_session:\n        # Use .get() for safer access\n        print(f\"Tool Guardrail Triggered Flag: {final_session.state.get('guardrail_tool_block_triggered', 'Not Set (or False)')}\")\n        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n    else:\n        print(\"\\n\u274c Error: Could not retrieve final session state.\")\n\nelse:\n    print(\"\\n\u26a0\ufe0f Skipping tool guardrail test. Runner ('runner_root_tool_guardrail') is not available.\")\n</code></pre> \ud83d\udccc Key Takeaways: <ul> <li> <p>Agents &amp; Tools: The fundamental building blocks for defining capabilities and reasoning. Clear instructions and docstrings are paramount.</p> </li> <li> <p>Runners &amp; Session Services: The engine and memory management system that orchestrate agent execution and maintain conversational context.</p> </li> <li> <p>Delegation: Designing multi-agent teams allows for specialization, modularity, and better management of complex tasks. Agent description is key for auto-flow.</p> </li> <li> <p>Session State (ToolContext, output_key): Essential for creating context-aware, personalized, and multi-turn conversational agents.</p> </li> <li> <p>Callbacks (before_model, before_tool): Powerful hooks for implementing safety, validation, policy enforcement, and dynamic modifications before critical operations (LLM calls or tool execution).</p> </li> <li> <p>Flexibility (LiteLlm): ADK empowers you to choose the best LLM for the job, balancing performance, cost, and features.</p> </li> </ul> \ud83d\udccc ADK and enhance your application: <ol> <li> <p>Real Weather API: Replace the mock_weather_db in your get_weather tool with a call to a real weather API (like OpenWeatherMap, WeatherAPI).</p> </li> <li> <p>More Complex State: Store more user preferences (e.g., preferred location, notification settings) or conversation summaries in the session state.</p> </li> <li> <p>Refine Delegation: Experiment with different root agent instructions or sub-agent descriptions to fine-tune the delegation logic. Could you add a \"forecast\" agent?</p> </li> <li> <p>Advanced Callbacks:</p> <ul> <li> <p>Use <code>after_model_callback</code> to potentially reformat or sanitize the LLM's response after it's generated.</p> </li> <li> <p>Use <code>after_tool_callback</code> to process or log the results returned by a tool.</p> </li> <li> <p>Implement <code>before_agent_callback</code> or <code>after_agent_callback</code> for agent-level entry/exit logic.</p> </li> </ul> </li> <li> <p>Error Handling: Improve how the agent handles tool errors or unexpected API responses. Maybe add retry logic within a tool.</p> </li> <li> <p>Persistent Session Storage: Explore alternatives to InMemorySessionService for storing session state persistently (e.g., using databases like Firestore or Cloud SQL \u2013 requires custom implementation or future ADK integrations).</p> </li> <li> <p>Streaming UI: Integrate your agent team with a web framework (like FastAPI, as shown in the ADK Streaming Quickstart) to create a real-time chat interface.</p> </li> </ol> Service Description Service Type SaaS / Shelf-Managed Use Case Example Vertex AI Studio Rapid prototyping and testing of generative AI models Platform SaaS Quickly experiment with LLMs for text generation Vertex AI Agent Builder No-code generative AI agents grounded in organizational data Platform SaaS Build chatbots for internal IT helpdesk without coding Generative AI Document Summarization Summarizes documents using Vertex AI and stores in BigQuery SaaS SaaS Automatically summarize lengthy legal or medical documents Gemini for Google Cloud AI-powered assistance for writing, coding, and data analysis SaaS SaaS Use AI to assist developers with code generation and debugging Veo 2 AI video model for generation and editing with cinematic features SaaS SaaS Create promotional videos with AI-based editing Imagen 3 Text-to-image model with advanced editing SaaS SaaS Generate marketing images from text descriptions Chirp 3 Synthetic speech model for realistic voice and transcription SaaS SaaS Develop realistic voice assistants and transcription services Lyria Text-to-music generation model SaaS SaaS Generate background music tracks for apps or games Vertex AI Platform Unified platform for building, deploying ML models Platform SaaS Deploy production-grade ML models for customer churn prediction Vertex AI Notebooks Environments for model development (e.g., Colab, Workbench) Platform SaaS Collaborative data science and model development AutoML Train custom ML models with minimal effort Platform SaaS Build custom image classifiers without deep ML expertise Natural Language API Sentiment analysis and entity recognition from text API SaaS Analyze customer reviews for sentiment trends Speech-to-Text API Converts speech into text API SaaS Transcribe customer service calls in real-time Text-to-Speech API Generates natural-sounding speech API SaaS Create audio narration for e-learning courses Translation API Translates text into multiple languages API SaaS Provide multilingual support on websites and apps Dialogflow API Conversational interface for chatbots and voice apps API SaaS Build customer support chatbots for websites Vision API Image labeling, face detection, OCR API SaaS Automate product tagging in e-commerce Video Intelligence API Video content analysis and object tracking API SaaS Detect and index objects and actions in surveillance videos Vertex AI Vision Build and deploy computer vision applications Platform SaaS Develop quality control systems for manufacturing Document AI API Extracts data from structured/unstructured documents API SaaS Extract invoice details automatically for accounts payable Document Warehouse API Stores and retrieves processed documents API SaaS Centralize storage of processed contract documents Dialogflow CX/ES Build advanced conversational agents Platform SaaS Create sophisticated virtual assistants with complex workflows Agent Assist Real-time agent support with AI suggestions SaaS SaaS Provide call center agents with live help and suggestions Contact Center AI AI-powered virtual agents for customer service SaaS SaaS Automate customer inquiries with virtual agents Vertex AI Search Semantic search and Q\\&amp;A experiences Platform SaaS Implement intelligent document search within enterprise systems Recommendations AI Product recommendations using ML SaaS SaaS Personalized product suggestions for e-commerce Retail Search AI-driven retail site search with personalization SaaS SaaS Enhance online retail search with personalized results Translation Hub Manage and translate large content volumes SaaS SaaS Localize marketing content for global audiences Agentspace Automation using agents and Gemini models Platform SaaS Automate workflows with AI agents interacting on your behalf <p></p> <p></p> <p></p> <p>Agentspace</p>"},{"location":"AgenticAI/LangGraph/Install/","title":"Install LangGraph - LangGraph v1.0","text":""},{"location":"AgenticAI/LangGraph/Install/#to-install-the-base-langgraph-package","title":"To install the base LangGraph package:","text":"<p>pip: <pre><code>pip install -U langgraph\n</code></pre></p> <p>uv: <pre><code>uv add langgraph\n</code></pre></p>"},{"location":"AgenticAI/LangGraph/Install/#to-use-langgraph-you-will-usually-want-to-access-llms-and-define-tools","title":"To use LangGraph you will usually want to access LLMs and define tools.","text":"<ul> <li>One way to do this (is to use <code>LangChain</code>.)</li> </ul>"},{"location":"AgenticAI/LangGraph/Install/#install-langchain-with","title":"Install LangChain with:","text":"<p>pip: <pre><code># Requires Python 3.10+\npip install -U langchain\n</code></pre></p> <p>uv: <pre><code># Requires Python 3.10+\nuv add langchain\n</code></pre></p>"},{"location":"AgenticAI/LangGraph/Install/#to-work-with-specific-llm-provider-packages-you-will-need-install-them-separately","title":"To work with specific LLM provider packages, you will need install them separately.","text":""},{"location":"AgenticAI/LangGraph/LangGraph-runtime/","title":"LangGraph runtime","text":"<p>Pregel implements LangGraph\u2019s runtime, managing the execution of LangGraph applications.</p> <p>Compiling a <code>StateGraph</code> or creating an <code>entrypoint</code> produces a <code>Pregel</code> instance that can be invoked with input.</p> <p>Note: The <code>Pregel</code> runtime is named after <code>Google\u2019s Pregel algorithm</code>, which describes an efficient method for large-scale parallel computation using graphs.</p>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#overview","title":"Overview","text":"<p>In LangGraph, Pregel combines <code>actors</code> and <code>channels</code> into a single application. <code>Actors</code> read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the <code>Pregel Algorithm/Bulk Synchronous Parallel</code> model.</p> <p>Each step consists of three phases:</p> <ul> <li> <p>Plan: Determine which <code>actors</code> to execute in this step. For example, in the first step, select the <code>actors</code> that subscribe to the special <code>input</code> channels; in subsequent steps, select the <code>actors</code> that subscribe to channels updated in the previous step.</p> </li> <li> <p>Execution: Execute all selected <code>actors</code> in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.</p> </li> <li> <p>Update: Update the channels with the values written by the actors in this step.</p> </li> </ul> <p>Repeat until no actors are selected for execution, or a maximum number of steps is reached.</p>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#actors","title":"Actors","text":"<p>An <code>actor</code> is a <code>PregelNode</code>. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes implement LangChain\u2019s Runnable interface.</p>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#channels","title":"Channels","text":"<p>Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function \u2013 which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:</p> <ul> <li> <p>LastValue: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.</p> </li> <li> <p>Topic: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.</p> </li> <li> <p>BinaryOperatorAggregate: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,<code>total = BinaryOperatorAggregate(int, operator.add)</code></p> </li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#examples","title":"Examples","text":"<p>While most users will interact with Pregel through the <code>StateGraph API</code> or the <code>entrypoint</code>decorator, it is possible to interact with Pregel directly.</p> <p>Below are a few different examples to give  a sense of the Pregel API.</p>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#single-node","title":"Single node","text":"<pre><code>import { EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\");\n\nconst app = new Pregel({\n  nodes: { node1 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"b\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#multiple-nodes","title":"Multiple nodes","text":"<pre><code>import { LastValue, EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\");\n\nconst node2 = new NodeBuilder()\n  .subscribeOnly(\"b\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"c\");\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new LastValue&lt;string&gt;(),\n    c: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"b\", \"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#topic","title":"Topic","text":"<pre><code>import { EphemeralValue, Topic } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\", \"c\");\n\nconst node2 = new NodeBuilder()\n  .subscribeTo(\"b\")\n  .do((x: { b: string }) =&gt; x.b + x.b)\n  .writeTo(\"c\");\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n    c: new Topic&lt;string&gt;({ accumulate: true }),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#binaryoperatoraggregate","title":"BinaryOperatorAggregate","text":"<p>This example demonstrates how to use the BinaryOperatorAggregate channel to implement a reducer.</p> <pre><code>import { EphemeralValue, BinaryOperatorAggregate } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\", \"c\");\n\nconst node2 = new NodeBuilder()\n  .subscribeOnly(\"b\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"c\");\n\nconst reducer = (current: string, update: string) =&gt; {\n  if (current) {\n    return current + \" | \" + update;\n  } else {\n    return update;\n  }\n};\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n    c: new BinaryOperatorAggregate&lt;string&gt;({ operator: reducer }),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph-runtime/#cycle","title":"Cycle","text":"<p>This example demonstrates how to introduce a cycle in the graph, by having a chain write to a channel it subscribes to. Execution will continue until a null value is written to the channel.</p> <pre><code>import { EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder, ChannelWriteEntry } from \"@langchain/langgraph/pregel\";\n\nconst exampleNode = new NodeBuilder()\n  .subscribeOnly(\"value\")\n  .do((x: string) =&gt; x.length &lt; 10 ? x + x : null)\n  .writeTo(new ChannelWriteEntry(\"value\", { skipNone: true }));\n\nconst app = new Pregel({\n  nodes: { exampleNode },\n  channels: {\n    value: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"value\"],\n  outputChannels: [\"value\"],\n});\n\nawait app.invoke({ value: \"a\" });\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/","title":"Overview","text":"\u2705 LangGraph \ud83d\udccc What is LangGraph? <p>LangGraph is built for developers who want to build powerful, adaptable AI agents.Developers choose LangGraph for:</p> <ul> <li> <p>Reliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.</p> </li> <li> <p>Low-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.</p> </li> <li> <p>First-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.</p> </li> </ul> \ud83d\udccc Learn LangGraph basics <ol> <li>Build a basic chatbot</li> <li>Add tools</li> <li>Add memory</li> <li>Add human-in-the-loop controls</li> <li>Customize state</li> <li>Time travel</li> </ol>"},{"location":"AgenticAI/LangGraph/LangGraph/#agent-architectures","title":"Agent architectures","text":"<p>Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context.</p> <p>Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:</p> <ul> <li> <p>An LLM can route between two potential paths</p> </li> <li> <p>An LLM can decide which of many tools to call</p> </li> <li> <p>An LLM can decide whether the generated answer is sufficient or more work is needed</p> </li> </ul> <p>Router</p> <p>A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.</p> <p>Structured Output</p> <p>Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:</p> <ol> <li> <p>Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.</p> </li> <li> <p>Output parsers: Using post-processing to extract structured data from LLM responses.</p> </li> <li> <p>Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.</p> </li> </ol> <p>Tool-calling agent</p> <p>cWhile a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:</p> <ol> <li> <p>Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.</p> </li> <li> <p>Tool access:The LLM can choose from and use a variety of tools to accomplish tasks.</p> </li> </ol> \ud83d\udccc Install LangGraph: <pre><code>pip install -U langgraph\n</code></pre> \ud83d\udccc create an agent using prebuilt components: <pre><code>import os\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain.chat_models import init_chat_model\n\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-*****\"\nllm = init_chat_model(\"openai:gpt-4.1\")\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_react_agent(\n    model=llm,\n    tools=[get_weather],\n    prompt=\"You are a helpful assistant\"\n)\n\n# Run the agent\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n\n# Print the output\nprint(result['messages'][-1].content)\n</code></pre> <p>Output:</p> <pre><code>python langgraph_test.py\nThe weather in San Francisco is reported to be always sunny! If you need a more detailed or up-to-date weather report, please let me know.\n</code></pre> \ud83d\udccc Core benefits <p>LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:</p> <ul> <li> <p>Durable execution: Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.</p> </li> <li> <p>Human-in-the-loop: Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.</p> </li> <li> <p>Comprehensive memory: Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.</p> </li> <li> <p>Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.</p> </li> <li> <p>Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.</p> </li> </ul> \ud83d\udccc LangGraph\u2019s ecosystem <p>While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:</p> <ul> <li> <p>LangSmith \u2014 Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.</p> </li> <li> <p>LangGraph Platform \u2014 Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in LangGraph Studio.</p> </li> <li> <p>LangChain \u2013 Provides integrations and composable components to streamline LLM application development.</p> </li> </ul> \ud83d\udccc Models <p>LangGraph provides built-in support for LLMs (language models) via the LangChain library. This makes it easy to integrate various LLMs into your agents and workflows.</p> <p>Initialize a model</p> <p>Use <code>init_chat_model</code> to initialize models:</p> \ud83d\udccc OpenAI: <pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre> \ud83d\udccc Anthropic: <pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre> \ud83d\udccc Azure: <pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre> \ud83d\udccc Google Gemini: <pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre> \ud83d\udccc AWS Bedrock: <pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre> \ud83d\udccc Instantiate a model directly <p>If a model provider is not available via init_chat_model, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling:</p> <pre><code># Anthropic is already supported by `init_chat_model`,\n# but you can also instantiate it directly.\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(\n  model=\"claude-3-7-sonnet-latest\",\n  temperature=0,\n  max_tokens=2048\n)\n</code></pre> \ud83d\udccc Dynamic model selection <p>Pass a callable function to create_react_agent to dynamically select the model at runtime. This is useful for scenarios where you want to choose a model based on user input, configuration settings, or other runtime conditions.</p> <p>The selector function must return a chat model. If you're using tools, you must bind the tools to the model within the selector function.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Literal\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.runtime import Runtime\n\n@tool\ndef weather() -&gt; str:\n    \"\"\"Returns the current weather conditions.\"\"\"\n    return \"It's nice and sunny.\"\n\n\n# Define the runtime context\n@dataclass\nclass CustomContext:\n    provider: Literal[\"anthropic\", \"openai\"]\n\n# Initialize models\nopenai_model = init_chat_model(\"openai:gpt-4o\")\nanthropic_model = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")\n\n\n# Selector function for model choice\ndef select_model(state: AgentState, runtime: Runtime[CustomContext]) -&gt; BaseChatModel:\n    if runtime.context.provider == \"anthropic\":\n        model = anthropic_model\n    elif runtime.context.provider == \"openai\":\n        model = openai_model\n    else:\n        raise ValueError(f\"Unsupported provider: {runtime.context.provider}\")\n\n    # With dynamic model selection, you must bind tools explicitly\n    return model.bind_tools([weather])\n\n\n# Create agent with dynamic model selection\nagent = create_react_agent(select_model, tools=[weather])\n\n# Invoke with context to select model\noutput = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Which model is handling this?\",\n            }\n        ]\n    },\n    context=CustomContext(provider=\"openai\"),\n)\n\nprint(output[\"messages\"][-1].text())\n</code></pre> \ud83d\udccc Advanced model configuration <p>Disable streaming</p> <p>To disable streaming of the individual LLM tokens, set disable_streaming=True when initializing the model:</p> <p>init_chat_model</p> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    disable_streaming=True\n)\n</code></pre> <p>ChatModel</p> <pre><code>from langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(\n    model=\"claude-3-7-sonnet-latest\",\n    disable_streaming=True\n)\n</code></pre> \ud83d\udccc Add model fallbacks <p>You can add a fallback to a different model or a different LLM provider using <code>model.with_fallbacks([...])</code>:</p> <p>init_chat_model</p> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel_with_fallbacks = (\n    init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n    .with_fallbacks([\n        init_chat_model(\"openai:gpt-4.1-mini\"),\n    ])\n)\n</code></pre> <p>ChatModel</p> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel_with_fallbacks = (\n    init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n    .with_fallbacks([\n        init_chat_model(\"openai:gpt-4.1-mini\"),\n    ])\n)\n</code></pre> \ud83d\udccc Use the built-in rate limiter <p>Langchain includes a built-in in-memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.</p> <pre><code>from langchain_core.rate_limiters import InMemoryRateLimiter\nfrom langchain_anthropic import ChatAnthropic\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=0.1,  # &lt;-- Super slow! We can only make a request once every 10 seconds!!\n    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n    max_bucket_size=10,  # Controls the maximum burst size.\n)\n\nmodel = ChatAnthropic(\n   model_name=\"claude-3-opus-20240229\",\n   rate_limiter=rate_limiter\n)\n</code></pre> \ud83d\udccc Bring your own model <p>If your desired LLM isn't officially supported by LangChain, consider these options:</p> <ol> <li> <p>Implement a custom LangChain chat model: Create a model conforming to the LangChain chat model interface. This enables full compatibility with LangGraph's agents and workflows but requires understanding of the LangChain framework.</p> </li> <li> <p>Direct invocation with custom streaming: Use your model directly by adding custom streaming logic with StreamWriter.</p> </li> </ol>"},{"location":"AgenticAI/LangGraph/LangGraph/#tools","title":"Tools","text":"<p>Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems\u2014such as APIs, databases, or file systems\u2014using structured input. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.</p> <p>Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and with what arguments.</p> <p></p> <p>Tool calling is typically conditional. Based on the user input and available tools, the model may choose to issue a tool call request. This request is returned in an AIMessage object, which includes a tool_calls field that specifies the tool name and input arguments:</p> <pre><code>llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n# -&gt; AIMessage(tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, ...}])\n</code></pre> <pre><code>AIMessage(\n  tool_calls=[\n    ToolCall(name=\"multiply\", args={\"a\": 2, \"b\": 3}),\n    ...\n  ]\n)\n</code></pre> <p>If the input is unrelated to any tool, the model returns only a natural language message:</p> <pre><code>llm_with_tools.invoke(\"Hello world!\")  # -&gt; AIMessage(content=\"Hello!\")\n</code></pre> <p>Importantly, the model does not execute the tool\u2014it only generates a request. A separate executor (such as a runtime or agent) is responsible for handling the tool call and returning the result.</p>"},{"location":"AgenticAI/LangGraph/LangGraph/#custom-tools","title":"Custom tools","text":"<p>You can define custom tools using the @tool decorator or plain Python functions. For example:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#call-tools","title":"Call tools","text":"<p>Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and determine the appropriate arguments.</p> <p>You can define your own tools or use prebuilt tools</p> <p>Define a tool</p> <p>Define a basic tool with the @tool decorator:</p> <p>Run a tool</p> <p>Tools conform to the Runnable interface, which means you can run a tool using the invoke method:</p> <pre><code>multiply.invoke({\"a\": 6, \"b\": 7})  # returns 42\n</code></pre> <p>If the tool is invoked with <code>type=\"tool_call\"</code>, it will return a ToolMessage:</p> <pre><code>tool_call = {\n    \"type\": \"tool_call\",\n    \"id\": \"1\",\n    \"args\": {\"a\": 42, \"b\": 7}\n}\nmultiply.invoke(tool_call) # returns a ToolMessage object\n</code></pre> <p>Output:</p> <pre><code>ToolMessage(content='294', name='multiply', tool_call_id='1')\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#use-in-an-agent","title":"Use in an agent","text":"<p>To create a tool-calling agent, you can use the prebuilt <code>create_react_agent</code>:</p> <pre><code>from langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet\",\n    tools=[multiply]\n)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#dynamically-select-tools","title":"Dynamically select tools","text":"<p>Configure tool availability at runtime based on context:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Literal\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.tools import tool\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass CustomContext:\n    tools: list[Literal[\"weather\", \"compass\"]]\n\n\n@tool\ndef weather() -&gt; str:\n    \"\"\"Returns the current weather conditions.\"\"\"\n    return \"It's nice and sunny.\"\n\n\n@tool\ndef compass() -&gt; str:\n    \"\"\"Returns the direction the user is facing.\"\"\"\n    return \"North\"\n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")\n\ndef configure_model(state: AgentState, runtime: Runtime[CustomContext]):\n    \"\"\"Configure the model with tools based on runtime context.\"\"\"\n    selected_tools = [\n        tool\n        for tool in [weather, compass]\n        if tool.name in runtime.context.tools\n    ]\n    return model.bind_tools(selected_tools)\n\n\nagent = create_react_agent(\n    # Dynamically configure the model with tools based on runtime context\n    configure_model,\n    # Initialize with all tools available\n    tools=[weather, compass]\n)\n\noutput = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Who are you and what tools do you have access to?\",\n            }\n        ]\n    },\n    context=CustomContext(tools=[\"weather\"]),  # Only enable the weather tool\n)\n\nprint(output[\"messages\"][-1].text())\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#use-in-a-workflow","title":"Use in a workflow","text":"<p>If you are writing a custom workflow, you will need to:</p> <ol> <li> <p>register the tools with the chat model</p> </li> <li> <p>call the tool if the model decides to use it</p> </li> </ol> <p>Use <code>model.bind_tools()</code> to register the tools with the model.</p> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n\nmodel_with_tools = model.bind_tools([multiply])\n</code></pre> <p>ToolNode</p> <p>To execute tools in custom workflows, use the prebuilt ToolNode or implement your own custom node.</p> <p>ToolNode is a specialized node for executing tools in a workflow. It provides the following features:</p> <ul> <li> <p>Supports both synchronous and asynchronous tools.</p> </li> <li> <p>Executes multiple tools concurrently.</p> </li> <li> <p>Handles errors during tool execution (handle_tool_errors=True, enabled by default).</p> </li> </ul> <p>ToolNode operates on <code>MessagesState</code>:</p> <ul> <li> <p>Input: MessagesState, where the last message is an AIMessage containing the tool_calls parameter.</p> </li> <li> <p>Output: MessagesState updated with the resulting ToolMessage from executed tools.</p> </li> </ul> <pre><code>from langgraph.prebuilt import ToolNode\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ndef get_coolest_cities():\n    \"\"\"Get a list of coolest cities\"\"\"\n    return \"nyc, sf\"\n\ntool_node = ToolNode([get_weather, get_coolest_cities])\ntool_node.invoke({\"messages\": [...]})\n</code></pre> <p>Single tool call</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Define tools\n@tool\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ntool_node = ToolNode([get_weather])\n\nmessage_with_single_tool_call = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"name\": \"get_weather\",\n            \"args\": {\"location\": \"sf\"},\n            \"id\": \"tool_call_id\",\n            \"type\": \"tool_call\",\n        }\n    ],\n)\n\ntool_node.invoke({\"messages\": [message_with_single_tool_call]})\n</code></pre> <p>Multiple tool calls</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Define tools\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ndef get_coolest_cities():\n    \"\"\"Get a list of coolest cities\"\"\"\n    return \"nyc, sf\"\n\ntool_node = ToolNode([get_weather, get_coolest_cities])\n\nmessage_with_multiple_tool_calls = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"name\": \"get_coolest_cities\",\n            \"args\": {},\n            \"id\": \"tool_call_id_1\",\n            \"type\": \"tool_call\",\n        },\n        {\n            \"name\": \"get_weather\",\n            \"args\": {\"location\": \"sf\"},\n            \"id\": \"tool_call_id_2\",\n            \"type\": \"tool_call\",\n        },\n    ],\n)\n\ntool_node.invoke({\"messages\": [message_with_multiple_tool_calls]})\n</code></pre> <p>Use with a chat model</p> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import ToolNode\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ntool_node = ToolNode([get_weather])\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\nmodel_with_tools = model.bind_tools([get_weather])  \n\n\nresponse_message = model_with_tools.invoke(\"what's the weather in sf?\")\ntool_node.invoke({\"messages\": [response_message]})\n</code></pre> <p>Use in a tool-calling agent</p> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ntool_node = ToolNode([get_weather])\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\nmodel_with_tools = model.bind_tools([get_weather])\n\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model_with_tools.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_conditional_edges(\"call_model\", should_continue, [\"tools\", END])\nbuilder.add_edge(\"tools\", \"call_model\")\n\ngraph = builder.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]})\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#tool-customization","title":"Tool customization","text":"<p>For more control over tool behavior, use the @tool decorator.</p> <p>Parameter descriptions</p> <p>Auto-generate descriptions from docstrings:</p> <pre><code>from langchain_core.tools import tool\n\n@tool(\"multiply_tool\", parse_docstring=True)\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\n\n    Args:\n        a: First operand\n        b: Second operand\n    \"\"\"\n    return a * b\n</code></pre> <p>Explicit input schema</p> <p>Define schemas using args_schema:</p> <pre><code>from pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\n\nclass MultiplyInputSchema(BaseModel):\n    \"\"\"Multiply two numbers\"\"\"\n    a: int = Field(description=\"First operand\")\n    b: int = Field(description=\"Second operand\")\n\n@tool(\"multiply_tool\", args_schema=MultiplyInputSchema)\ndef multiply(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Tool name</p> <p>Override the default tool name using the first argument or name property:</p> <pre><code>from langchain_core.tools import tool\n\n@tool(\"multiply_tool\")\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#context-management","title":"Context management","text":"<p>Tools within LangGraph sometimes require context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. LangGraph provides three methods for managing such context:</p> <p>Configuration</p> <p>Use configuration when you have immutable runtime data that tools require, such as user identifiers. You pass these arguments via RunnableConfig at invocation and access them in the tool:</p> <pre><code>from langchain_core.tools import tool\nfrom langchain_core.runnables import RunnableConfig\n\n@tool\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\n# Invocation example with an agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user info\"}]},\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <p>Extended example: Access config in tools</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\ndef get_user_info(\n    config: RunnableConfig,\n) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <p>Short-term memory</p> <p>Short-term memory maintains dynamic state that changes during a single execution.</p> <p>To access (read) the graph state inside the tools, you can use a special parameter annotation \u2014 InjectedState:</p> <pre><code>from typing import Annotated, NotRequired\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import InjectedState, create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\nclass CustomState(AgentState):\n    # The user_name field in short-term state\n    user_name: NotRequired[str]\n\n@tool\ndef get_user_name(\n    state: Annotated[CustomState, InjectedState]\n) -&gt; str:\n    \"\"\"Retrieve the current user-name from state.\"\"\"\n    # Return stored name or a default if not set\n    return state.get(\"user_name\", \"Unknown user\")\n\n# Example agent setup\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_name],\n    state_schema=CustomState,\n)\n\n# Invocation: reads the name from state (initially empty)\nagent.invoke({\"messages\": \"what's my name?\"})\n</code></pre> <p>Long-term memory</p> <p>Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.</p> <p>To use long-term memory, you need to:</p> <ol> <li> <p>Configure a store to persist data across invocations.</p> </li> <li> <p>Access the store from within tools.</p> </li> </ol> <p>To access information in the store:</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\nfrom langgraph.config import get_store\n\n@tool\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n</code></pre> <p>Access long-term memory</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.config import get_store\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore() \n\nstore.put(  \n    (\"users\",),  \n    \"user_123\",  \n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    } \n)\n\n@tool\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    store = get_store() \n    user_id = config[\"configurable\"].get(\"user_id\")\n    user_info = store.get((\"users\",), user_id) \n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n    store=store \n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <p>To update information in the store:</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\nfrom langgraph.config import get_store\n\n@tool\ndef save_user_info(user_info: str, config: RunnableConfig) -&gt; str:\n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n</code></pre> <p>Update long-term memory</p> <pre><code>from typing_extensions import TypedDict\n\nfrom langchain_core.tools import tool\nfrom langgraph.config import get_store\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore() \n\nclass UserInfo(TypedDict): \n    name: str\n\n@tool\ndef save_user_info(user_info: UserInfo, config: RunnableConfig) -&gt; str: \n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    store = get_store() \n    user_id = config[\"configurable\"].get(\"user_id\")\n    store.put((\"users\",), user_id, user_info) \n    return \"Successfully saved user info.\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[save_user_info],\n    store=store\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    config={\"configurable\": {\"user_id\": \"user_123\"}} \n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#advanced-tool-features","title":"Advanced tool features","text":"<p>Immediate return</p> <p>Use <code>return_direct=True</code> to immediately return a tool's result without executing additional logic.</p> <p>This is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.</p> <pre><code>@tool(return_direct=True)\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n</code></pre> <p>Extended example: Using return_direct in a prebuilt agent</p> <pre><code>from langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\n@tool(return_direct=True)\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[add]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]}\n)\n</code></pre> <p>Force tool use</p> <p>If you need to force a specific tool to be used, you will need to configure this at the model level using the <code>tool_choice</code> parameter in the bind_tools method.</p> <p>Force specific tool usage via tool_choice:</p> <pre><code>@tool(return_direct=True)\ndef greet(user_name: str) -&gt; int:\n    \"\"\"Greet user.\"\"\"\n    return f\"Hello {user_name}!\"\n\ntools = [greet]\n\nconfigured_model = model.bind_tools(\n    tools,\n    # Force the use of the 'greet' tool\n    tool_choice={\"type\": \"tool\", \"name\": \"greet\"}\n)\n</code></pre> <p>Extended example: Force tool usage in an agent</p> <pre><code>from langchain_core.tools import tool\n\n@tool(return_direct=True)\ndef greet(user_name: str) -&gt; int:\n    \"\"\"Greet user.\"\"\"\n    return f\"Hello {user_name}!\"\n\ntools = [greet]\n\nagent = create_react_agent(\n    model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}),\n    tools=tools\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]}\n)\n</code></pre> <p><code>Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:</code></p> <ul> <li> <p>Mark the tool with <code>return_direct=True</code> to end the loop after execution.</p> </li> <li> <p>Set <code>recursion_limit</code> to restrict the number of execution steps.</p> </li> </ul> <p>Disable parallel calls</p> <p>For supported providers, you can disable parallel tool calling by setting <code>parallel_tool_calls=False</code> via the <code>model.bind_tools()</code> method:</p> <p>Extended example: disable parallel tool calls in a prebuilt agent</p> <pre><code>from langchain.chat_models import init_chat_model\n\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\ntools = [add, multiply]\nagent = create_react_agent(\n    # disable parallel tool calls\n    model=model.bind_tools(tools, parallel_tool_calls=False),\n    tools=tools\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]}\n)\n</code></pre> <p>Handle errors</p> <p>LangGraph provides built-in error handling for tool execution through the prebuilt ToolNode component, used both independently and in prebuilt agents.</p> <p>By default, ToolNode catches exceptions raised during tool execution and returns them as ToolMessage objects with a status indicating an error.</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\ndef multiply(a: int, b: int) -&gt; int:\n    if a == 42:\n        raise ValueError(\"The ultimate error\")\n    return a * b\n\n# Default error handling (enabled by default)\ntool_node = ToolNode([multiply])\n\nmessage = AIMessage(\n    content=\"\",\n    tool_calls=[{\n        \"name\": \"multiply\",\n        \"args\": {\"a\": 42, \"b\": 7},\n        \"id\": \"tool_call_id\",\n        \"type\": \"tool_call\"\n    }]\n)\n\nresult = tool_node.invoke({\"messages\": [message]})\n</code></pre> <p>Output:</p> <pre><code>{'messages': [\n    ToolMessage(\n        content=\"Error: ValueError('The ultimate error')\\n Please fix your mistakes.\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n</code></pre> <p>Disable error handling</p> <p>To propagate exceptions directly, disable error handling:</p> <pre><code>tool_node = ToolNode([multiply], handle_tool_errors=False)\n</code></pre> <p>Example output:</p> <pre><code>{'messages': [\n    ToolMessage(\n        content=\"Can't use 42 as the first operand, please switch operands!\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n</code></pre> <p>Error handling in agents</p> <p>Error handling in prebuilt agents (<code>create_react_agent</code>) leverages <code>ToolNode</code>:</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[multiply]\n)\n\n# Default error handling\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre> <p>To disable or customize error handling in prebuilt agents, explicitly pass a configured ToolNode:</p> <pre><code>custom_tool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Cannot use 42 as a first operand!\"\n)\n\nagent_custom = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=custom_tool_node\n)\n\nagent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre> <p>Handle large numbers of tools</p> <p>As the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning.</p> <p>To address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.</p>"},{"location":"AgenticAI/LangGraph/LangGraph/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>To review, edit, and approve tool calls in an agent or workflow, use LangGraph's human-in-the-loop features to enable human intervention at any point in a workflow. This is especially useful in large language model (LLM)-driven applications where model output may require validation, correction, or additional context.</p> <p></p> <p>Key capabilities</p> <ul> <li>Persistent execution state: Interrupts use LangGraph's persistence layer, which saves the graph state, to indefinitely pause graph execution until you resume. This is possible because LangGraph checkpoints the graph state after each step, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. This supports asynchronous human review or input without time constraints.</li> </ul> <p>There are two ways to pause a graph:</p> <ul> <li> <p>Dynamic interrupts: Use interrupt to pause a graph from inside a specific node, based on the current state of the graph.</p> </li> <li> <p>Static interrupts: Use <code>interrupt_before</code> and <code>interrupt_after</code> to pause the graph at pre-defined points, either before or after a node executes.</p> </li> </ul> <p></p> <ul> <li>Flexible integration points: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations.</li> </ul> <p>Patterns</p> <p>There are four typical design patterns that you can implement using <code>interrupt</code> and <code>Command</code>:</p> <ul> <li> <p>Approve or reject: Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involves routing the graph based on the human's input.</p> </li> <li> <p>Edit graph state: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.</p> </li> <li> <p>Review tool calls: Pause the graph to review and edit tool calls requested by the LLM before tool execution.</p> </li> <li> <p>Validate human input: Pause the graph to validate human input before proceeding with the next step.</p> </li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraph/#enable-human-intervention","title":"Enable human intervention","text":"<p>To review, edit, and approve tool calls in an agent or workflow, use interrupts to pause a graph and wait for human input. Interrupts use LangGraph's <code>persistence</code> layer, which saves the graph state, to indefinitely pause graph execution until you resume.</p> <p>Pause using interrupt</p> <p>Dynamic interrupts (also known as dynamic breakpoints) are triggered based on the current state of the graph. You can set dynamic interrupts by calling interrupt function in the appropriate place. The graph will pause, which allows for human intervention, and then resumes the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context.</p> <p>To use <code>interrupt</code> in your graph, you need to:</p> <ol> <li> <p>Specify a checkpointer to save the graph state after each step.</p> </li> <li> <p>Call <code>interrupt()</code> in the appropriate place. See the Common Patterns section for examples.</p> </li> <li> <p>Run the graph with a thread ID until the <code>interrupt</code> is hit.</p> </li> <li> <p>Resume execution using <code>invoke/stream</code></p> </li> </ol> <pre><code>from langgraph.types import interrupt, Command\n\ndef human_node(state: State):\n    value = interrupt( \n        {\n            \"text_to_revise\": state[\"some_text\"] \n        }\n    )\n    return {\n        \"some_text\": value \n    }\n\n\ngraph = graph_builder.compile(checkpointer=checkpointer) \n\n# Run the graph until the interrupt is hit.\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\nresult = graph.invoke({\"some_text\": \"original text\"}, config=config) \nprint(result['__interrupt__']) \n# &gt; [\n# &gt;    Interrupt(\n# &gt;       value={'text_to_revise': 'original text'},\n# &gt;       resumable=True,\n# &gt;       ns=['human_node:6ce9e64f-edef-fe5d-f7dc-511fa9526960']\n# &gt;    )\n# &gt; ]\n\nprint(graph.invoke(Command(resume=\"Edited text\"), config=config)) \n# &gt; {'some_text': 'Edited text'}\n</code></pre> <p></p> <p>When the interrupt function is used within a graph, execution pauses at that point and awaits user input.</p> <p>To resume execution, use the Command primitive, which can be supplied via the invoke or stream methods. The graph resumes execution from the beginning of the node where interrupt(...) was initially called. This time, the interrupt function will return the value provided in Command(resume=value) rather than pausing again. All code from the beginning of the node to the interrupt will be re-executed.</p> <pre><code># Resume graph execution by providing the user's input.\ngraph.invoke(Command(resume={\"age\": \"25\"}), thread_config)\n</code></pre> <p>Resume multiple interrupts with one invocation</p> <p>When nodes with interrupt conditions are run in parallel, it's possible to have multiple interrupts in the task queue. For example, the following graph has two nodes run in parallel that require human input:</p> <p></p> <p>Once your graph has been interrupted and is stalled, you can resume all the interrupts at once with Command.resume, passing a dictionary mapping of interrupt ids to resume values.</p> <pre><code>from typing import TypedDict\nimport uuid\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.constants import START\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\n\n\nclass State(TypedDict):\n    text_1: str\n    text_2: str\n\n\ndef human_node_1(state: State):\n    value = interrupt({\"text_to_revise\": state[\"text_1\"]})\n    return {\"text_1\": value}\n\n\ndef human_node_2(state: State):\n    value = interrupt({\"text_to_revise\": state[\"text_2\"]})\n    return {\"text_2\": value}\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"human_node_1\", human_node_1)\ngraph_builder.add_node(\"human_node_2\", human_node_2)\n\n# Add both nodes in parallel from START\ngraph_builder.add_edge(START, \"human_node_1\")\ngraph_builder.add_edge(START, \"human_node_2\")\n\ncheckpointer = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\nthread_id = str(uuid.uuid4())\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": thread_id}}\nresult = graph.invoke(\n    {\"text_1\": \"original text 1\", \"text_2\": \"original text 2\"}, config=config\n)\n\n# Resume with mapping of interrupt IDs to values\nresume_map = {\n    i.id: f\"edited text for {i.value['text_to_revise']}\"\n    for i in graph.get_state(config).interrupts\n}\nprint(graph.invoke(Command(resume=resume_map), config=config))\n# &gt; {'text_1': 'edited text for original text 1', 'text_2': 'edited text for original text 2'}\n</code></pre> <p>Common patterns</p> <p>Below we show different design patterns that can be implemented using interrupt and Command.</p> <p></p> <p>Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action.</p> <pre><code>from typing import Literal\nfrom langgraph.types import interrupt, Command\n\ndef human_approval(state: State) -&gt; Command[Literal[\"some_node\", \"another_node\"]]:\n    is_approved = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            # Surface the output that should be\n            # reviewed and approved by the human.\n            \"llm_output\": state[\"llm_output\"]\n        }\n    )\n\n    if is_approved:\n        return Command(goto=\"some_node\")\n    else:\n        return Command(goto=\"another_node\")\n\n# Add the node to the graph in an appropriate location\n# and connect it to the relevant nodes.\ngraph_builder.add_node(\"human_approval\", human_approval)\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# After running the graph and hitting the interrupt, the graph will pause.\n# Resume it with either an approval or rejection.\nthread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\ngraph.invoke(Command(resume=True), config=thread_config)\n</code></pre> <p></p> <pre><code>from langgraph.types import interrupt\n\ndef human_editing(state: State):\n    ...\n    result = interrupt(\n        # Interrupt information to surface to the client.\n        # Can be any JSON serializable value.\n        {\n            \"task\": \"Review the output from the LLM and make any necessary edits.\",\n            \"llm_generated_summary\": state[\"llm_generated_summary\"]\n        }\n    )\n\n    # Update the state with the edited text\n    return {\n        \"llm_generated_summary\": result[\"edited_text\"]\n    }\n\n# Add the node to the graph in an appropriate location\n# and connect it to the relevant nodes.\ngraph_builder.add_node(\"human_editing\", human_editing)\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n...\n\n# After running the graph and hitting the interrupt, the graph will pause.\n# Resume it with the edited text.\nthread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\ngraph.invoke(\n    Command(resume={\"edited_text\": \"The edited text\"}),\n    config=thread_config\n)\n</code></pre> <p></p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import interrupt\nfrom langgraph.prebuilt import create_react_agent\n\n# An example of a sensitive tool that requires human review / approval\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    response = interrupt(  \n        f\"Trying to call `book_hotel` with args {{'hotel_name': {hotel_name}}}. \"\n        \"Please approve or suggest edits.\"\n    )\n    if response[\"type\"] == \"accept\":\n        pass\n    elif response[\"type\"] == \"edit\":\n        hotel_name = response[\"args\"][\"hotel_name\"]\n    else:\n        raise ValueError(f\"Unknown response type: {response['type']}\")\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ncheckpointer = InMemorySaver() \n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_hotel],\n    checkpointer=checkpointer, \n)\n</code></pre> <p>Run the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations.</p> <pre><code>config = {\n   \"configurable\": {\n      \"thread_id\": \"1\"\n   }\n}\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p><code>You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input.</code></p> <p>Resume the agent with a Command to continue based on human input.</p> <pre><code>from langgraph.types import Command\n\nfor chunk in agent.stream(\n    Command(resume={\"type\": \"accept\"}),  \n    # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}),\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>Add interrupts to any tool</p> <p>You can create a wrapper to add interrupts to any tool. The example below provides a reference implementation compatible with Agent Inbox UI and Agent Chat UI.</p> <p>Wrapper that adds human-in-the-loop to any tool</p> <pre><code>from typing import Callable\nfrom langchain_core.tools import BaseTool, tool as create_tool\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.types import interrupt\nfrom langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt\n\ndef add_human_in_the_loop(\n    tool: Callable | BaseTool,\n    *,\n    interrupt_config: HumanInterruptConfig = None,\n) -&gt; BaseTool:\n    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n    if not isinstance(tool, BaseTool):\n        tool = create_tool(tool)\n\n    if interrupt_config is None:\n        interrupt_config = {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        }\n\n    @create_tool(  \n        tool.name,\n        description=tool.description,\n        args_schema=tool.args_schema\n    )\n    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n        request: HumanInterrupt = {\n            \"action_request\": {\n                \"action\": tool.name,\n                \"args\": tool_input\n            },\n            \"config\": interrupt_config,\n            \"description\": \"Please review the tool call\"\n        }\n        response = interrupt([request])[0]  \n        # approve the tool call\n        if response[\"type\"] == \"accept\":\n            tool_response = tool.invoke(tool_input, config)\n        # update tool call args\n        elif response[\"type\"] == \"edit\":\n            tool_input = response[\"args\"][\"args\"]\n            tool_response = tool.invoke(tool_input, config)\n        # respond to the LLM with user feedback\n        elif response[\"type\"] == \"response\":\n            user_feedback = response[\"args\"]\n            tool_response = user_feedback\n        else:\n            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n\n        return tool_response\n\n    return call_tool_with_interrupt\n</code></pre> <p>You can use the wrapper to add interrupt() to any tool without having to add it inside the tool:</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.prebuilt import create_react_agent\n\ncheckpointer = InMemorySaver()\n\ndef book_hotel(hotel_name: str):\n   \"\"\"Book a hotel\"\"\"\n   return f\"Successfully booked a stay at {hotel_name}.\"\n\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[\n        add_human_in_the_loop(book_hotel), \n    ],\n    checkpointer=checkpointer,\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the agent\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>Validate human input</p> <p>If you need to validate the input provided by the human within the graph itself (rather than on the client side), you can achieve this by using multiple interrupt calls within a single node.</p> <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n    question = \"What is your age?\"\n\n    while True:\n        answer = interrupt(question)\n\n        # Validate answer, if the answer isn't valid ask for input again.\n        if not isinstance(answer, int) or answer &lt; 0:\n            question = f\"'{answer} is not a valid age. What is your age?\"\n            answer = None\n            continue\n        else:\n            # If the answer is valid, we can proceed.\n            break\n\n    print(f\"The human in the loop is {answer} years old.\")\n    return {\n        \"age\": answer\n    }\n</code></pre> <p>Debug with interrupts</p> <p>To debug and test a graph, use static interrupts (also known as static breakpoints) to step through the graph execution one node at a time or to pause the graph execution at specific nodes. Static interrupts are triggered at defined points either before or after a node executes. You can set static interrupts by specifying interrupt_before and interrupt_after at compile time or run time.</p> <p><code>Static interrupts are not recommended for human-in-the-loop workflows. Use dynamic interrupts instead.</code></p> <p>compile time</p> <pre><code>graph = graph_builder.compile( \n    interrupt_before=[\"node_a\"], \n    interrupt_after=[\"node_b\", \"node_c\"], \n    checkpointer=checkpointer, \n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=thread_config) \n\n# Resume the graph\ngraph.invoke(None, config=thread_config)\n</code></pre> <p>Run time</p> <pre><code>graph.invoke( \n    inputs,\n    interrupt_before=[\"node_a\"], \n    interrupt_after=[\"node_b\", \"node_c\"] \n    config={\n        \"configurable\": {\"thread_id\": \"some_thread\"}\n    },\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=config) \n\n# Resume the graph\ngraph.invoke(None, config=config)\n</code></pre> <p><code>You cannot set static breakpoints at runtime for sub-graphs. If you have a sub-graph, you must set the breakpoints at compilation time.</code></p> <p>Use static interrupts in LangGraph Studio</p> <p>You can use LangGraph Studio to debug your graph. You can set static breakpoints in the UI and then run the graph. You can also use the UI to inspect the graph state at any point in the execution.</p> <p></p> <p><code>LangGraph Studio is free with locally deployed applications using langgraph dev.</code></p> <p>Considerations</p> <p>When using human-in-the-loop, there are some considerations to keep in mind.</p> <p>Using with code with side-effects</p> <p>Place code with side effects, such as API calls, after the interrupt or in a separate node to avoid duplication, as these are re-triggered every time the node is resumed.</p> <p>Side effects after interrupt</p> <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n\n    answer = interrupt(question)\n\n    api_call(answer) # OK as it's after the interrupt\n</code></pre> <p>Side effects in a separate node</p> <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n\n    answer = interrupt(question)\n\n    return {\n        \"answer\": answer\n    }\n\ndef api_call_node(state: State):\n    api_call(...) # OK as it's in a separate node\n</code></pre> <p>Using with subgraphs called as functions</p> <p>When invoking a subgraph as a function, the parent graph will resume execution from the beginning of the node where the subgraph was invoked where the interrupt was triggered. Similarly, the subgraph will resume from the beginning of the node where the interrupt() function was called.</p> <pre><code>def node_in_parent_graph(state: State):\n    some_code()  # &lt;-- This will re-execute when the subgraph is resumed.\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)\n    ...\n</code></pre> <p>Using multiple interrupts in a single node</p> <p>Using multiple interrupts within a single node can be helpful for patterns like validating human input. However, using multiple interrupts in the same node can lead to unexpected behavior if not handled carefully.</p> <p>When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is strictly index-based, so the order of interrupt calls within the node is critical.</p> <p>To avoid issues, refrain from dynamically changing the node's structure between executions. This includes adding, removing, or reordering interrupt calls, as such changes can result in mismatched indices. These problems often arise from unconventional patterns, such as mutating state via Command(resume=..., update=SOME_STATE_MUTATION) or relying on global variables to modify the node's structure dynamically.</p>"},{"location":"AgenticAI/LangGraph/LangGraph/#time-travel","title":"Time Travel","text":"<p>When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:</p> <ol> <li> <p>\ud83e\udd14 Understand reasoning: Analyze the steps that led to a successful result.</p> </li> <li> <p>\ud83d\udc1e Debug mistakes: Identify where and why errors occurred.</p> </li> <li> <p>\ud83d\udd0d Explore alternatives: Test different paths to uncover better solutions.</p> </li> </ol> <p>LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint \u2014 either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.</p> <p>Use time-travel</p> <p>To use time-travel in LangGraph:</p> <ol> <li> <p>Run the graph with initial inputs using invoke or stream methods.</p> </li> <li> <p>Identify a checkpoint in an existing thread: Use the <code>get_state_history()</code> method to retrieve the execution history for a specific thread_id and locate the desired checkpoint_id. Alternatively, set an interrupt before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.</p> </li> <li> <p>**Update the graph state (optional): Use the update_state method to modify the graph's state at the checkpoint and resume execution from alternative state.</p> </li> <li> <p>Resume execution from the checkpoint: Use the invoke or stream methods with an input of None and a configuration containing the appropriate thread_id and checkpoint_id.</p> </li> </ol> <p>In a workflow</p> <p>This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution</p> <p>checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.</p> <p>Setup</p> <p>First we need to install the packages required</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_anthropic\n</code></pre> <p>Next, we need to set API keys for Anthropic (the LLM we will use)</p> <pre><code>import uuid\n\nfrom typing_extensions import TypedDict, NotRequired\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    topic: NotRequired[str]\n    joke: NotRequired[str]\n\n\nllm = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    temperature=0,\n)\n\n\ndef generate_topic(state: State):\n    \"\"\"LLM call to generate a topic for the joke\"\"\"\n    msg = llm.invoke(\"Give me a funny topic for a joke\")\n    return {\"topic\": msg.content}\n\n\ndef write_joke(state: State):\n    \"\"\"LLM call to write a joke based on the topic\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_topic\", generate_topic)\nworkflow.add_node(\"write_joke\", write_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_topic\")\nworkflow.add_edge(\"generate_topic\", \"write_joke\")\nworkflow.add_edge(\"write_joke\", END)\n\n# Compile\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\ngraph\n</code></pre> <pre><code>import uuid\n\nfrom typing_extensions import TypedDict, NotRequired\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    topic: NotRequired[str]\n    joke: NotRequired[str]\n\n\nllm = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    temperature=0,\n)\n\n\ndef generate_topic(state: State):\n    \"\"\"LLM call to generate a topic for the joke\"\"\"\n    msg = llm.invoke(\"Give me a funny topic for a joke\")\n    return {\"topic\": msg.content}\n\n\ndef write_joke(state: State):\n    \"\"\"LLM call to write a joke based on the topic\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_topic\", generate_topic)\nworkflow.add_node(\"write_joke\", write_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_topic\")\nworkflow.add_edge(\"generate_topic\", \"write_joke\")\nworkflow.add_edge(\"write_joke\", END)\n\n# Compile\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\ngraph\n</code></pre> <p>Output:</p> <pre><code>How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!\n\n# The Secret Life of Socks in the Dryer\n\nI finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all\u2014they've just eloped with someone else's socks from the laundromat to start new lives together.\n\nMy blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.\n</code></pre> <p>** Identify a checkpoint**</p> <pre><code># The states are returned in reverse chronological order.\nstates = list(graph.get_state_history(config))\n\nfor state in states:\n    print(state.next)\n    print(state.config[\"configurable\"][\"checkpoint_id\"])\n    print()\n</code></pre> <p>Output:</p> <pre><code>()\n1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n\n('write_joke',)\n1f02ac4a-ce2a-6494-8001-cb2e2d651227\n\n('generate_topic',)\n1f02ac4a-a4e0-630d-8000-b73c254ba748\n\n('__start__',)\n1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\n</code></pre> <pre><code># This is the state before last (states are listed in chronological order)\nselected_state = states[1]\nprint(selected_state.next)\nprint(selected_state.values)\n</code></pre> <p>Output:</p> <pre><code>('write_joke',)\n{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n</code></pre> <p>Update the state (optional)</p> <p>update_state will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.</p> <pre><code>new_config = graph.update_state(selected_state.config, values={\"topic\": \"chickens\"})\nprint(new_config)\n</code></pre> <p>Output:</p> <pre><code>{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n</code></pre> <p>Resume execution from the checkpoint</p> <pre><code>graph.invoke(None, new_config)\n</code></pre> <p>Output:</p> <pre><code>{'topic': 'chickens',\n 'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'}\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#subgraphs","title":"Subgraphs","text":"<p>A subgraph is a graph that is used as a node in another graph \u2014 this is the concept of encapsulation applied to LangGraph. Subgraphs allow you to build complex systems with multiple components that are themselves graphs.</p> <p></p> <p>Some reasons for using subgraphs are:</p> <ul> <li> <p>building multi-agent systems</p> </li> <li> <p>when you want to reuse a set of nodes in multiple graphs</p> </li> <li> <p>when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph</p> </li> </ul> <p>The main question when adding subgraphs is how the parent graph and subgraph communicate, i.e. how they pass the state between each other during the graph execution. There are two scenarios:</p> <ul> <li>parent and subgraph have shared state keys in their state schemas. In this case, you can include the subgraph as a node in the parent graph</li> </ul> <pre><code>from langgraph.graph import StateGraph, MessagesState, START\n\n# Subgraph\n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(call_model)\n...\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"subgraph_node\", subgraph)\nbuilder.add_edge(START, \"subgraph_node\")\ngraph = builder.compile()\n...\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n</code></pre> <ul> <li>parent graph and subgraph have different schemas (no shared state keys in their state schemas). In this case, you have to call the subgraph from inside a node in the parent graph: this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph</li> </ul> <pre><code>from typing_extensions import TypedDict, Annotated\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.graph.message import add_messages\n\nclass SubgraphMessagesState(TypedDict):\n    subgraph_messages: Annotated[list[AnyMessage], add_messages]\n\n# Subgraph\n\ndef call_model(state: SubgraphMessagesState):\n    response = model.invoke(state[\"subgraph_messages\"])\n    return {\"subgraph_messages\": response}\n\nsubgraph_builder = StateGraph(SubgraphMessagesState)\nsubgraph_builder.add_node(\"call_model_from_subgraph\", call_model)\nsubgraph_builder.add_edge(START, \"call_model_from_subgraph\")\n...\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\ndef call_subgraph(state: MessagesState):\n    response = subgraph.invoke({\"subgraph_messages\": state[\"messages\"]})\n    return {\"messages\": response[\"subgraph_messages\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"subgraph_node\", call_subgraph)\nbuilder.add_edge(START, \"subgraph_node\")\ngraph = builder.compile()\n...\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#use-subgraphs","title":"Use subgraphs","text":"<p>This guide explains the mechanics of using subgraphs. A common application of subgraphs is to build multi-agent systems.</p> <p>When adding subgraphs, you need to define how the parent graph and the subgraph communicate:</p> <ul> <li> <p>Shared state schemas \u2014 parent and subgraph have shared state keys in their state schemas</p> </li> <li> <p>Different state schemas \u2014 no shared state keys in parent and subgraph schemas</p> </li> </ul> <p>Setup</p> <pre><code>pip install -U langgraph\n</code></pre> <p>Shared state schemas</p> <p>A common case is for the parent graph and subgraph to communicate over a shared state key (channel) in the schema. For example, in multi-agent systems, the agents often communicate over a shared messages key.</p> <p>If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:</p> <ol> <li> <p>Define the subgraph workflow (<code>subgraph_builder</code> in the example below) and compile it</p> </li> <li> <p>Pass compiled subgraph to the <code>.add_node</code> method when defining the parent graph workflow</p> </li> </ol> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n</code></pre> <p>Full example: shared state schemas</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  \n    bar: str  \n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}):\n    print(chunk)\n</code></pre> <p>Different state schemas</p> <p>For more complex systems you might want to define subgraphs that have a completely different schema from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.</p> <p>If that's the case for your application, you need to define a node function that invokes the subgraph. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass SubgraphState(TypedDict):\n    bar: str\n\n# Subgraph\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"hi! \" + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nclass State(TypedDict):\n    foo: str\n\ndef call_subgraph(state: State):\n    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  \n    return {\"foo\": subgraph_output[\"bar\"]}  \n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", call_subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n</code></pre> <p>Full example: different state schemas</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"baz\": \"baz\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\ndef node_2(state: ParentState):\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})  \n    return {\"foo\": response[\"bar\"]}  \n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n    print(chunk)\n</code></pre> <p>Full example: different state schemas (two levels of subgraphs)</p> <pre><code># Grandchild graph\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START, END\n\nclass GrandChildState(TypedDict):\n    my_grandchild_key: str\n\ndef grandchild_1(state: GrandChildState) -&gt; GrandChildState:\n    # NOTE: child or parent keys will not be accessible here\n    return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n\n\ngrandchild = StateGraph(GrandChildState)\ngrandchild.add_node(\"grandchild_1\", grandchild_1)\n\ngrandchild.add_edge(START, \"grandchild_1\")\ngrandchild.add_edge(\"grandchild_1\", END)\n\ngrandchild_graph = grandchild.compile()\n\n# Child graph\nclass ChildState(TypedDict):\n    my_child_key: str\n\ndef call_grandchild_graph(state: ChildState) -&gt; ChildState:\n    # NOTE: parent or grandchild keys won't be accessible here\n    grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}  \n    grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n    return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}  \n\nchild = StateGraph(ChildState)\nchild.add_node(\"child_1\", call_grandchild_graph)  \nchild.add_edge(START, \"child_1\")\nchild.add_edge(\"child_1\", END)\nchild_graph = child.compile()\n\n# Parent graph\nclass ParentState(TypedDict):\n    my_key: str\n\ndef parent_1(state: ParentState) -&gt; ParentState:\n    # NOTE: child or grandchild keys won't be accessible here\n    return {\"my_key\": \"hi \" + state[\"my_key\"]}\n\ndef parent_2(state: ParentState) -&gt; ParentState:\n    return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n\ndef call_child_graph(state: ParentState) -&gt; ParentState:\n    child_graph_input = {\"my_child_key\": state[\"my_key\"]}  \n    child_graph_output = child_graph.invoke(child_graph_input)\n    return {\"my_key\": child_graph_output[\"my_child_key\"]}  \n\nparent = StateGraph(ParentState)\nparent.add_node(\"parent_1\", parent_1)\nparent.add_node(\"child\", call_child_graph)  \nparent.add_node(\"parent_2\", parent_2)\n\nparent.add_edge(START, \"parent_1\")\nparent.add_edge(\"parent_1\", \"child\")\nparent.add_edge(\"child\", \"parent_2\")\nparent.add_edge(\"parent_2\", END)\n\nparent_graph = parent.compile()\n\nfor chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n    print(chunk)\n</code></pre> <p>Add persistence</p> <p>You only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n</code></pre> <p>If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:</p> <pre><code>subgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n</code></pre> <p>View subgraph state</p> <p>When you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.</p> <p>You can inspect the graph state via <code>graph.get_state(config)</code>. To view the subgraph state, you can use <code>graph.get_state(config, subgraphs=True)</code>.</p> <p><code>Subgraph state can only be viewed when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.</code></p> <p>View interrupted subgraph state</p> <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import interrupt, Command\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    value = interrupt(\"Provide value:\")\n    return {\"foo\": state[\"foo\"] + value}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\n\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ngraph.invoke({\"foo\": \"\"}, config)\nparent_state = graph.get_state(config)\nsubgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state  \n\n# resume the subgraph\ngraph.invoke(Command(resume=\"bar\"), config)\n</code></pre> <p>Stream subgraph outputs</p> <p>To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.</p> <pre><code>for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, \n    stream_mode=\"updates\",\n):\n    print(chunk)\n</code></pre> <p>Stream from subgraphs</p> <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    subgraphs=True, \n):\n    print(chunk)\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#multi-agent-systems","title":"Multi-agent systems","text":"<p>An agent is a system that uses an LLM to decide the control flow of an application. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:</p> <ul> <li> <p>agent has too many tools at its disposal and makes poor decisions about which tool to call next</p> </li> <li> <p>context grows too complex for a single agent to keep track of</p> </li> <li> <p>there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)</p> </li> </ul> <p>To tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a multi-agent system.These independent agents can be as simple as a prompt and an LLM call, or as complex as a ReAct agent (and more!).</p> <p>The primary benefits of using multi-agent systems are:</p> <ul> <li> <p>Modularity: Separate agents make it easier to develop, test, and maintain agentic systems.</p> </li> <li> <p>Specialization: You can create expert agents focused on specific domains, which helps with the overall system performance.</p> </li> <li> <p>Control: You can explicitly control how agents communicate (as opposed to relying on function calling).</p> </li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraph/#multi-agent-architectures","title":"Multi-agent architectures","text":"<p>There are several ways to connect agents in a multi-agent system:</p> <ul> <li> <p>Network: each agent can communicate with every other agent. Any agent can decide which other agent to call next.</p> </li> <li> <p>Supervisor: each agent communicates with a single supervisor agent. Supervisor agent makes decisions on which agent should be called next.</p> </li> <li> <p>Supervisor (tool-calling): this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.</p> </li> <li> <p>Hierarchical: you can define a multi-agent system with a supervisor of supervisors. This is a generalization of the supervisor architecture and allows for more complex control flows.</p> </li> <li> <p>Custom multi-agent workflow: each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.</p> </li> </ul> <p>Handoffs</p> <p>In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li> <p>destination: target agent to navigate to (e.g., name of the node to go to)</p> </li> <li> <p>payload: information to pass to that agent (e.g., state update)</p> </li> </ul> <p>To implement handoffs in LangGraph, agent nodes can return Command object that allows you to combine both control flow and state updates:</p> <pre><code>def agent(state) -&gt; Command[Literal[\"agent\", \"another_agent\"]]:\n    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    return Command(\n        # Specify which agent to call next\n        goto=goto,\n        # Update the graph state\n        update={\"my_state_key\": \"my_state_value\"}\n    )\n</code></pre> <p>In a more complex scenario where each agent node is itself a graph (i.e., a subgraph), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, alice and bob (subgraph nodes in a parent graph), and alice needs to navigate to bob, you can set <code>graph=Command.PARENT</code> in the Command object:</p> <pre><code>def some_node_inside_alice(state):\n    return Command(\n        goto=\"bob\",\n        update={\"my_state_key\": \"my_state_value\"},\n        # specify which graph to navigate to (defaults to the current graph)\n        graph=Command.PARENT,\n    )\n</code></pre> <p>If you need to support visualization for subgraphs communicating using Command(graph=Command.PARENT) you would need to wrap them in a node function with Command annotation: Instead of this:</p> <pre><code>builder.add_node(alice)\n</code></pre> <p>you would need to do this:</p> <pre><code>def call_alice(state) -&gt; Command[Literal[\"bob\"]]:\n    return alice.invoke(state)\n\nbuilder.add_node(\"alice\", call_alice)\n</code></pre> <p>Handoffs as tools</p> <p>One of the most common agent types is a tool-calling agent. For those types of agents, a common pattern is wrapping a handoff in a tool call:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef transfer_to_bob():\n    \"\"\"Transfer to bob.\"\"\"\n    return Command(\n        # name of the agent (node) to go to\n        goto=\"bob\",\n        # data to send to the agent\n        update={\"my_state_key\": \"my_state_value\"},\n        # indicate to LangGraph that we need to navigate to\n        # agent node in a parent graph\n        graph=Command.PARENT,\n    )\n</code></pre> <p>This is a special case of updating the graph state from tools where, in addition to the state update, the control flow is included as well.</p> <p>If you want to use tools that return Command, you can use the prebuilt create_react_agent / ToolNode components, or else implement your own logic:</p> <pre><code>def call_tools(state):\n    ...\n    commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n    return commands\n</code></pre> <p>Network</p> <p>In this architecture, agents are defined as graph nodes. Each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState) -&gt; Command[Literal[\"agent_2\", \"agent_3\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the LLM's decision\n    # if the LLM returns \"__end__\", the graph will finish execution\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_2(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_3\", END]]:\n    response = model.invoke(...)\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_3(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    ...\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\nbuilder.add_node(agent_3)\n\nbuilder.add_edge(START, \"agent_1\")\nnetwork = builder.compile()\n</code></pre> <p>Supervisor</p> <p>In this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use Command to route execution to the appropriate agent node based on supervisor's decision. This architecture also lends itself well to running multiple agents in parallel or using <code>map-reduce pattern</code>.</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef supervisor(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_agent\"])\n\ndef agent_1(state: MessagesState) -&gt; Command[Literal[\"supervisor\"]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\ndef agent_2(state: MessagesState) -&gt; Command[Literal[\"supervisor\"]]:\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(supervisor)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n\nbuilder.add_edge(START, \"supervisor\")\n\nsupervisor = builder.compile()\n</code></pre> <p>Supervisor (tool-calling)</p> <p>In this variant of the supervisor architecture, we define a supervisor agent which is responsible for calling sub-agents. The sub-agents are exposed to the supervisor as tools, and the supervisor agent decides which tool to call next. The supervisor agent follows a standard implementation as an LLM running in a while loop calling tools until it decides to stop.</p> <pre><code>from typing import Annotated\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import InjectedState, create_react_agent\n\nmodel = ChatOpenAI()\n\n# this is the agent function that will be called as tool\n# notice that you can pass the state to the tool via InjectedState annotation\ndef agent_1(state: Annotated[dict, InjectedState]):\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    # return the LLM response as a string (expected tool response format)\n    # this will be automatically turned to ToolMessage\n    # by the prebuilt create_react_agent (supervisor)\n    return response.content\n\ndef agent_2(state: Annotated[dict, InjectedState]):\n    response = model.invoke(...)\n    return response.content\n\ntools = [agent_1, agent_2]\n# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\n# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\nsupervisor = create_react_agent(model, tools)\n</code></pre> <p>Hierarchical</p> <p>As you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, or the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.</p> <p>To address this, you can design your system hierarchically. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\nmodel = ChatOpenAI()\n\n# define team 1 (same as the single supervisor example above)\n\ndef team_1_supervisor(state: MessagesState) -&gt; Command[Literal[\"team_1_agent_1\", \"team_1_agent_2\", END]]:\n    response = model.invoke(...)\n    return Command(goto=response[\"next_agent\"])\n\ndef team_1_agent_1(state: MessagesState) -&gt; Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\ndef team_1_agent_2(state: MessagesState) -&gt; Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\nteam_1_builder = StateGraph(Team1State)\nteam_1_builder.add_node(team_1_supervisor)\nteam_1_builder.add_node(team_1_agent_1)\nteam_1_builder.add_node(team_1_agent_2)\nteam_1_builder.add_edge(START, \"team_1_supervisor\")\nteam_1_graph = team_1_builder.compile()\n\n# define team 2 (same as the single supervisor example above)\nclass Team2State(MessagesState):\n    next: Literal[\"team_2_agent_1\", \"team_2_agent_2\", \"__end__\"]\n\ndef team_2_supervisor(state: Team2State):\n    ...\n\ndef team_2_agent_1(state: Team2State):\n    ...\n\ndef team_2_agent_2(state: Team2State):\n    ...\n\nteam_2_builder = StateGraph(Team2State)\n...\nteam_2_graph = team_2_builder.compile()\n\n\n# define top-level supervisor\n\nbuilder = StateGraph(MessagesState)\ndef top_level_supervisor(state: MessagesState) -&gt; Command[Literal[\"team_1_graph\", \"team_2_graph\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which team to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_team\" field)\n    response = model.invoke(...)\n    # route to one of the teams or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_team\"])\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(top_level_supervisor)\nbuilder.add_node(\"team_1_graph\", team_1_graph)\nbuilder.add_node(\"team_2_graph\", team_2_graph)\nbuilder.add_edge(START, \"top_level_supervisor\")\nbuilder.add_edge(\"team_1_graph\", \"top_level_supervisor\")\nbuilder.add_edge(\"team_2_graph\", \"top_level_supervisor\")\ngraph = builder.compile()\n</code></pre> <p>Custom multi-agent workflow</p> <p>In this architecture we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:</p> <ul> <li> <p>Explicit control flow (normal edges): LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via normal graph edges. This is the most deterministic variant of this architecture above \u2014 we always know which agent will be called next ahead of time.</p> </li> <li> <p>Dynamic control flow (Command): in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using Command. A special case of this is a supervisor tool-calling architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.</p> </li> </ul> <pre><code>from langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\ndef agent_2(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n# define the flow explicitly\nbuilder.add_edge(START, \"agent_1\")\nbuilder.add_edge(\"agent_1\", \"agent_2\")\n</code></pre> <p>Communication and state management</p> <p>The most important thing when building multi-agent systems is figuring out how the agents communicate.</p> <p>Handoffs vs tool calls</p> <p>What is the \"payload\" that is being passed around between agents? In most of the architectures discussed above, the agents communicate via handoffs and pass the graph state as part of the handoff payload. Specifically, agents pass around lists of messages as part of the graph state. In the case of the supervisor with tool-calling, the payloads are tool call arguments.</p> <p></p> <p>Message passing between agents</p> <p>The most common way for agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents (e.g., messages). When communicating via a shared message list, there is an additional consideration: should the agents share the full history of their thought process or only the final result?</p> <p></p> <p>Sharing full thought process</p> <p>Agents can share the full history of their thought process (i.e., \"scratchpad\") with all other agents. This \"scratchpad\" would typically look like a list of messages. The benefit of sharing the full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the \"scratchpad\" will grow quickly and might require additional strategies for memory management.</p> <p>Sharing only final results</p> <p>Agents can have their own private \"scratchpad\" and only share the final result with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with different state schemas.</p> <p>For agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows passing state to individual tools at runtime, so subordinate agents can access parent state, if needed.</p> <p>Indicating agent name in messages</p> <p>It can be helpful to indicate which agent a particular AI message is from, especially for long message histories. Some LLM providers (like OpenAI) support adding a name parameter to messages \u2014 you can use that to attach the agent name to the message. If that is not supported, you can consider manually injecting the agent name into the message content, e.g., alicemessage from alice.</p>"},{"location":"AgenticAI/LangGraph/LangGraph/#multi-agent","title":"Multi-agent","text":"<p>A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and compose them into a multi-agent system.</p> <p>In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent.</p> <p>Two of the most popular multi-agent architectures are:</p> <ul> <li> <p>supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</p> </li> <li> <p>swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent.</p> </li> </ul> <p></p> <p>Use <code>langgraph-supervisor</code> library to create a supervisor multi-agent system:</p> <pre><code>pip install langgraph-supervisor\n</code></pre> <pre><code>from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph_supervisor import create_supervisor\n\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\nflight_assistant = create_react_agent(\n    model=\"openai:gpt-4o\",\n    tools=[book_flight],\n    prompt=\"You are a flight booking assistant\",\n    name=\"flight_assistant\"\n)\n\nhotel_assistant = create_react_agent(\n    model=\"openai:gpt-4o\",\n    tools=[book_hotel],\n    prompt=\"You are a hotel booking assistant\",\n    name=\"hotel_assistant\"\n)\n\nsupervisor = create_supervisor(\n    agents=[flight_assistant, hotel_assistant],\n    model=ChatOpenAI(model=\"gpt-4o\"),\n    prompt=(\n        \"You manage a hotel booking assistant and a\"\n        \"flight booking assistant. Assign work to them.\"\n    )\n).compile()\n\nfor chunk in supervisor.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p></p> <p>Use <code>langgraph-swarm</code> library to create a swarm multi-agent system:</p> <pre><code>pip install langgraph-swarm\n</code></pre> <pre><code>from langgraph.prebuilt import create_react_agent\nfrom langgraph_swarm import create_swarm, create_handoff_tool\n\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    name=\"hotel_assistant\"\n)\n\nswarm = create_swarm(\n    agents=[flight_assistant, hotel_assistant],\n    default_active_agent=\"flight_assistant\"\n).compile()\n\nfor chunk in swarm.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>Handoffs</p> <p>A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li> <p>destination: target agent to navigate to</p> </li> <li> <p>payload: information to pass to that agent</p> </li> </ul> <p>This is used both by <code>langgraph-supervisor</code>(supervisor hands off to individual agents) and <code>langgraph-swarm</code> (an individual agent can hand off to other agents).</p> <p>To implement handoffs with <code>create_react_agent</code>, you need to:</p> <ol> <li>Create a special tool that can transfer control to a different agent</li> </ol> <pre><code>def transfer_to_bob():\n    \"\"\"Transfer to bob.\"\"\"\n    return Command(\n        # name of the agent (node) to go to\n        goto=\"bob\",\n        # data to send to the agent\n        update={\"messages\": [...]},\n        # indicate to LangGraph that we need to navigate to\n        # agent node in a parent graph\n        graph=Command.PARENT,\n    )\n</code></pre> <ol> <li>Create individual agents that have access to handoff tools:</li> </ol> <pre><code>flight_assistant = create_react_agent(\n    ..., tools=[book_flight, transfer_to_hotel_assistant]\n)\nhotel_assistant = create_react_agent(\n    ..., tools=[book_hotel, transfer_to_flight_assistant]\n)\n</code></pre> <ol> <li>Define a parent graph that contains individual agents as nodes:</li> </ol> <pre><code>from langgraph.graph import StateGraph, MessagesState\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    ...\n)\n</code></pre> <p>Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant:</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        state: Annotated[MessagesState, InjectedState], \n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  \n            goto=agent_name,  \n            update={\"messages\": state[\"messages\"] + [tool_message]},  \n            graph=Command.PARENT,  \n        )\n    return handoff_tool\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\n# Simple agent tools\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n\n# Run the multi-agent graph\nfor chunk in multi_agent_graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#build-multi-agent-systems","title":"Build multi-agent systems","text":"<p>Handoffs</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        state: Annotated[MessagesState, InjectedState], \n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  \n            goto=agent_name,  \n            update={\"messages\": state[\"messages\"] + [tool_message]},  \n            graph=Command.PARENT,  \n        )\n    return handoff_tool\n</code></pre> <p>Control agent inputs</p> <p>You can use the Send() primitive to directly send data to the worker agents during the handoff. For example, you can request that the calling agent populate a task description for the next agent:</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command, Send\n\ndef create_task_description_handoff_tool(\n    *, agent_name: str, description: str | None = None\n):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Ask {agent_name} for help.\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # this is populated by the calling agent\n        task_description: Annotated[\n            str,\n            \"Description of what the next agent should do, including all of the relevant context.\",\n        ],\n        # these parameters are ignored by the LLM\n        state: Annotated[MessagesState, InjectedState],\n    ) -&gt; Command:\n        task_description_message = {\"role\": \"user\", \"content\": task_description}\n        agent_input = {**state, \"messages\": [task_description_message]}\n        return Command(\n            goto=[Send(agent_name, agent_input)],\n            graph=Command.PARENT,\n        )\n\n    return handoff_tool\n</code></pre> <p>Build a multi-agent system</p> <p>You can use handoffs in any agents built with LangGraph. We recommend using the prebuilt agent or ToolNode, as they natively support handoffs tools returning Command. Below is an example of how you can implement a multi-agent system for booking travel using handoffs:</p> <pre><code>from langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import StateGraph, START, MessagesState\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    # same implementation as above\n    ...\n    return Command(...)\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(agent_name=\"hotel_assistant\")\ntransfer_to_flight_assistant = create_handoff_tool(agent_name=\"flight_assistant\")\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[..., transfer_to_hotel_assistant],\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[..., transfer_to_flight_assistant],\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n</code></pre> <p>Full example: Multi-agent system for booking travel</p> <pre><code>from typing import Annotated\nfrom langchain_core.messages import convert_to_messages\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\n# We'll use `pretty_print_messages` helper to render the streamed agent outputs nicely\n\ndef pretty_print_message(message, indent=False):\n    pretty_message = message.pretty_repr(html=True)\n    if not indent:\n        print(pretty_message)\n        return\n\n    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n    print(indented)\n\n\ndef pretty_print_messages(update, last_message=False):\n    is_subgraph = False\n    if isinstance(update, tuple):\n        ns, update = update\n        # skip parent graph updates in the printouts\n        if len(ns) == 0:\n            return\n\n        graph_id = ns[-1].split(\":\")[0]\n        print(f\"Update from subgraph {graph_id}:\")\n        print(\"\\n\")\n        is_subgraph = True\n\n    for node_name, node_update in update.items():\n        update_label = f\"Update from node {node_name}:\"\n        if is_subgraph:\n            update_label = \"\\t\" + update_label\n\n        print(update_label)\n        print(\"\\n\")\n\n        messages = convert_to_messages(node_update[\"messages\"])\n        if last_message:\n            messages = messages[-1:]\n\n        for m in messages:\n            pretty_print_message(m, indent=is_subgraph)\n        print(\"\\n\")\n\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        state: Annotated[MessagesState, InjectedState], \n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  \n            goto=agent_name,  \n            update={\"messages\": state[\"messages\"] + [tool_message]},  \n            graph=Command.PARENT,  \n        )\n    return handoff_tool\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\n# Simple agent tools\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n\n# Run the multi-agent graph\nfor chunk in multi_agent_graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    },\n    subgraphs=True\n):\n    pretty_print_messages(chunk)\n</code></pre> <p>Multi-turn conversation</p> <p>Users might want to engage in a multi-turn conversation with one or more agents. To build a system that can handle this, you can create a node that uses an interrupt to collect user input and routes back to the active agent.</p> <p>The agents can then be implemented as nodes in a graph that executes agent steps and determines the next action:</p> <ol> <li> <p>Wait for user input to continue the conversation, or</p> </li> <li> <p>Route to another agent (or back to itself, such as in a loop) via a handoff</p> </li> </ol> <pre><code>def human(state) -&gt; Command[Literal[\"agent\", \"another_agent\"]]:\n    \"\"\"A node for collecting user input.\"\"\"\n    user_input = interrupt(value=\"Ready for user input.\")\n\n    # Determine the active agent.\n    active_agent = ...\n\n    ...\n    return Command(\n        update={\n            \"messages\": [{\n                \"role\": \"human\",\n                \"content\": user_input,\n            }]\n        },\n        goto=active_agent\n    )\n\ndef agent(state) -&gt; Command[Literal[\"agent\", \"another_agent\", \"human\"]]:\n    # The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    if goto:\n        return Command(goto=goto, update={\"my_state_key\": \"my_state_value\"})\n    else:\n        return Command(goto=\"human\") # Go to human node\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#mcp","title":"MCP","text":"<p>Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to language models. LangGraph agents can use tools defined on MCP servers through the langchain-mcp-adapters library.</p> <p></p> <p>Install the <code>langchain-mcp-adapters</code> library to use MCP tools in LangGraph:</p> <pre><code>pip install langchain-mcp-adapters\n</code></pre> <p>Use MCP tools</p> <p>The <code>langchain-mcp-adapters</code> package enables agents to use tools defined across one or more MCP servers.</p> <p>In an agent</p> <pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\n\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"command\": \"python\",\n            # Replace with absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n            \"transport\": \"stdio\",\n        },\n        \"weather\": {\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transport\": \"streamable_http\",\n        }\n    }\n)\ntools = await client.get_tools()\nagent = create_react_agent(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    tools\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n</code></pre> <p>In a workflow</p> <pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.prebuilt import ToolNode\n\n# Initialize the model\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n# Set up MCP client\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"command\": \"python\",\n            # Make sure to update to the full absolute path to your math_server.py file\n            \"args\": [\"./examples/math_server.py\"],\n            \"transport\": \"stdio\",\n        },\n        \"weather\": {\n            # make sure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp/\",\n            \"transport\": \"streamable_http\",\n        }\n    }\n)\ntools = await client.get_tools()\n\n# Bind tools to model\nmodel_with_tools = model.bind_tools(tools)\n\n# Create ToolNode\ntool_node = ToolNode(tools)\n\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\n# Define call_model function\nasync def call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = await model_with_tools.ainvoke(messages)\n    return {\"messages\": [response]}\n\n# Build the graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_conditional_edges(\n    \"call_model\",\n    should_continue,\n)\nbuilder.add_edge(\"tools\", \"call_model\")\n\n# Compile the graph\ngraph = builder.compile()\n\n# Test the graph\nmath_response = await graph.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await graph.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n</code></pre> <p>Custom MCP servers</p> <p>To create your own MCP servers, you can use the mcp library. This library provides a simple way to define tools and run them as servers.</p> <p>Install the MCP library:</p> <pre><code>pip install mcp\n</code></pre> <p>Use the following reference implementations to test your agent with MCP tool servers.</p> <p>Example Math Server (stdio transport)</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>Example Weather Server (Streamable HTTP transport)</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#tracing","title":"Tracing","text":"<p>Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use <code>LangSmith</code> to visualize these execution steps. To use it, enable tracing for your application. This enables you to do the following:</p> <p>Enable tracing for your application</p> <pre><code>export LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=&lt;your-api-key&gt;\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#evals","title":"Evals","text":"<p>To evaluate your agent's performance you can use LangSmith evaluations. You would need to first define an evaluator function to judge the results from an agent, such as final outputs or trajectory. Depending on your evaluation technique, this may or may not involve a reference output:</p> <pre><code>def evaluator(*, outputs: dict, reference_outputs: dict):\n    # compare agent outputs against reference outputs\n    output_messages = outputs[\"messages\"]\n    reference_messages = reference_outputs[\"messages\"]\n    score = compare_messages(output_messages, reference_messages)\n    return {\"key\": \"evaluator_score\", \"score\": score}\n</code></pre> <p>To get started, you can use prebuilt evaluators from AgentEvals package:</p> <pre><code>pip install -U agentevals\n</code></pre> <p>Create evaluator</p> <p>A common way to evaluate agent performance is by comparing its trajectory (the order in which it calls its tools) against a reference trajectory:</p> <pre><code>import json\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\noutputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n            {\n                \"function\": {\n                    \"name\": \"get_directions\",\n                    \"arguments\": json.dumps({\"destination\": \"presidio\"}),\n                }\n            }\n        ],\n    }\n]\nreference_outputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n        ],\n    }\n]\n\n# Create the evaluator\nevaluator = create_trajectory_match_evaluator(\n    trajectory_match_mode=\"superset\",  \n)\n\n# Run the evaluator\nresult = evaluator(\n    outputs=outputs, reference_outputs=reference_outputs\n)\n</code></pre> <p>LLM-as-a-judge</p> <p>You can use LLM-as-a-judge evaluator that uses an LLM to compare the trajectory against the reference outputs and output a score:</p> <pre><code>import json\nfrom agentevals.trajectory.llm import (\n    create_trajectory_llm_as_judge,\n    TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\n)\n\nevaluator = create_trajectory_llm_as_judge(\n    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n    model=\"openai:o3-mini\"\n)\n</code></pre> <p>Run evaluator</p> <p>To run an evaluator, you will first need to create a LangSmith dataset. To use the prebuilt AgentEvals evaluators, you will need a dataset with the following schema:</p> <ul> <li>input: {\"messages\": [...]} input messages to call the agent with.</li> <li>output: {\"messages\": [...]} expected message history in the agent output. For trajectory evaluation, you can choose to keep only assistant messages.</li> </ul> <pre><code>from langsmith import Client\nfrom langgraph.prebuilt import create_react_agent\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\nclient = Client()\nagent = create_react_agent(...)\nevaluator = create_trajectory_match_evaluator(...)\n\nexperiment_results = client.evaluate(\n    lambda inputs: agent.invoke(inputs),\n    # replace with your dataset name\n    data=\"&lt;Name of your dataset&gt;\",\n    evaluators=[evaluator]\n)\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraph/#reference","title":"Reference","text":"<p>!LangGraph</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/","title":"Graphs","text":"<p>At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:</p> <ol> <li>Tools</li> <li>LLM</li> <li>State</li> <li>Memory</li> <li>Nodes</li> <li>Edges</li> <li>Compile</li> <li>Invoke</li> <li>ZOD</li> <li>TOON</li> <li>Annotation</li> <li>Messages</li> <li>Reducers</li> <li>Serialization</li> <li>START Node</li> <li>END Node</li> <li>Node Caching</li> <li>Normal Edges</li> <li>Conditional Edges</li> </ol> Layer LangGraph Feature / Function Purpose Mandatory in Prod Notes / Best Practice Graph Definition <code>StateGraph</code> Defines multi-agent workflow \u2705 Yes Always use typed state <code>add_node()</code> Register agent / tool nodes \u2705 Yes One responsibility per node <code>add_edge()</code> Static routing between agents \u2705 Yes Avoid hard-coded branching in agents <code>add_conditional_edges()</code> Dynamic routing \u2705 Yes Use rule-first, LLM-second <code>set_entry_point()</code> Define start node \u2705 Yes Single entry point only <code>set_finish_point()</code> Define completion node \u2705 Yes Required for safe termination State Management Typed State (Pydantic / TypedDict) Shared agent state \u2705 Yes Never use raw dict Partial State Updates Update only owned fields \u2705 Yes Prevent state corruption State History Track decisions \u26a0\ufe0f Recommended Required for audit Agent Execution Agent Node (LLM) Reasoning &amp; planning \u2705 Yes Separate planner vs executor Tool Node External actions \u2705 Yes All side-effects via tools Subgraph Reusable agent flows \u26a0\ufe0f Recommended Use for SOPs / runbooks Control Flow Conditional Routing Decision-based flow \u2705 Yes Deterministic &gt; LLM Loop Control Iterative reasoning \u26a0\ufe0f Recommended Always cap iterations <code>langgraph_step</code> Proactive recursion guard \u2705 Yes Prevent infinite loops Error Handling Failure Edges Graceful failure routing \u2705 Yes No uncaught exceptions Retry Logic Controlled retries \u26a0\ufe0f Recommended Prefer Temporal for retries <code>GraphRecursionError</code> Loop overflow detection \u26a0\ufe0f Backup only Never primary control Human-in-the-Loop Interrupt / Pause Node Approval gating \u26a0\ufe0f Recommended Mandatory for prod changes Resume Execution Continue after approval \u26a0\ufe0f Recommended Preserve state Memory &amp; Knowledge Vector Store Access Semantic KB \u26a0\ufe0f Recommended Read-only by default Graph DB Access Relationship reasoning \u26a0\ufe0f Recommended SOP \u2192 action mapping SQL / Audit DB Execution logs \u2705 Yes Compliance requirement Observability Node-level Tracing Debugging \u2705 Yes Required for RCA Execution Metadata Time, tokens, agent \u26a0\ufe0f Recommended Cost &amp; performance Security &amp; Safety Tool Whitelisting Prevent misuse \u2705 Yes Explicit allowlist Input Validation (Zod/Pydantic) Safe execution \u2705 Yes LLM output never trusted RBAC per Agent Least privilege \u26a0\ufe0f Recommended Prod requirement Scalability Async Execution Parallel agents \u26a0\ufe0f Recommended Fan-out/fan-in patterns Idempotent Tools Safe retries \u2705 Yes Mandatory with Temporal Termination Completion Node Clean exit \u2705 Yes No dangling execution Fallback Node Safe failure end \u2705 Yes Never crash silently Workflow Durability Temporal Integration Long-running workflows \u26a0\ufe0f Recommended Mandatory for prod ops Testing &amp; Validation Dry-run Mode No side-effects \u26a0\ufe0f Recommended Mandatory before prod Golden Path Tests Known flows \u2705 Yes Regression safety"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#state","title":"State","text":"<p>A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.</p> <ul> <li>The first thing you do when you define a graph is define the State of the graph.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#schema","title":"Schema","text":"<ul> <li>The main documented way to specify the schema of a graph is by using Zod schemas.</li> <li>By default, the graph will have the same input and output schemas.</li> <li>If you want to change this, you can also specify explicit input and output schemas directly.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#multiple-schemas","title":"Multiple schemas","text":"<ul> <li>Typically, all graph nodes communicate with a single schema.</li> <li>This means that they will read and write to the same state channels.</li> <li> <p>But, there are cases where we want more control over this:</p> <ul> <li>Internal nodes can pass information that is not required in the graph\u2019s input / output.</li> <li>We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.</li> </ul> </li> <li> <p>It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.</p> </li> <li> <p>It is also possible to define explicit input and output schemas for a graph. In these cases, we define an \u201cinternal\u201d schema that contains all keys relevant to graph operations.</p> </li> </ul> <p>example:</p> <pre><code>const InputState = z.object({\n  userInput: z.string(),\n});\n\nconst OutputState = z.object({\n  graphOutput: z.string(),\n});\n\nconst OverallState = z.object({\n  foo: z.string(),\n  userInput: z.string(),\n  graphOutput: z.string(),\n});\n\nconst PrivateState = z.object({\n  bar: z.string(),\n});\n\nconst graph = new StateGraph({\n  state: OverallState,\n  input: InputState,\n  output: OutputState,\n})\n  .addNode(\"node1\", (state) =&gt; {\n    // Write to OverallState\n    return { foo: state.userInput + \" name\" };\n  })\n  .addNode(\"node2\", (state) =&gt; {\n    // Read from OverallState, write to PrivateState\n    return { bar: state.foo + \" is\" };\n  })\n  .addNode(\n    \"node3\",\n    (state) =&gt; {\n      // Read from PrivateState, write to OutputState\n      return { graphOutput: state.bar + \" Lance\" };\n    },\n    { input: PrivateState }\n  )\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\")\n  .addEdge(\"node2\", \"node3\")\n  .addEdge(\"node3\", END)\n  .compile();\n\nawait graph.invoke({ userInput: \"My\" });\n// { graphOutput: 'My name is Lance' }\n</code></pre> <ol> <li>We pass state as the input schema to node1.</li> <li>But, we write out to foo, a channel in OverallState.</li> <li>How can we write out to a state channel that is not included in the input schema?</li> <li>This is because a node can write to any state channel in the graph state.</li> <li>The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.</li> <li>We initialize the graph with StateGraph({ state: OverallState, input: InputState, output: OutputState }).</li> <li>So, how can we write to PrivateState in node2?</li> <li>How does the graph gain access to this schema if it was not passed in the StateGraph initialization?</li> <li>We can do this because nodes can also declare additional state channels as long as the state schema definition exists.</li> <li>In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.</li> </ol>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#reducers","title":"Reducers","text":"<ul> <li>Reducers are key to understanding how updates from nodes are applied to the State.</li> <li>Each key in the State has its own independent reducer function.</li> <li>If no reducer function is explicitly specified then it is assumed that all updates to that key should override it.</li> <li>There are a few different types of reducers, starting with the default type of reducer:</li> </ul> <p>Default Reducer</p> <p>These two examples show how to use the default reducer:</p> <pre><code>const State = z.object({\n  foo: z.number(),\n  bar: z.array(z.string()),\n});\n</code></pre> <ul> <li>In this example, no reducer functions are specified for any key. Let\u2019s assume the input to the graph is:</li> </ul> <p><code>`{ foo: 1, bar: [\"hi\"] }</code> </p> <ul> <li>Let\u2019s then assume the first Node returns <code>{ foo: 2 }</code></li> <li>This is treated as an update to the state.</li> <li>Notice that the Node does not need to return the whole State schema - just an update.</li> <li>After applying this update, the State would then be <code>{ foo: 2, bar: [\"hi\"] }</code>.</li> <li>If the second node returns <code>{ bar: [\"bye\"] }</code> then the State would then be <code>{ foo: 2, bar: [\"bye\"] }</code>.</li> </ul> <p>Example</p> <pre><code>import * as z from \"zod\";\nimport { registry } from \"@langchain/langgraph/zod\";\n\nconst State = z.object({\n  foo: z.number(),\n  bar: z.array(z.string()).register(registry, {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [] as string[],\n  }),\n});\n</code></pre> <ul> <li>In this example, we\u2019ve used Zod 4 registries(<code>foo, bar, fn, default</code>) to specify a reducer function for the second key <code>(bar)</code>.</li> <li>Note that the first key foo remains unchanged.</li> <li>Let\u2019s assume the input to the graph is <code>{ foo: 1, bar: [\"hi\"] }</code>.</li> <li>Let\u2019s then assume the first Node returns <code>{ foo: 2 }</code>,</li> <li>This is treated as an update to the state.</li> <li>Notice that the Node does not need to return the whole State schema - just an update.</li> <li>After applying this update, the State would then be <code>{ foo: 2, bar: [\"hi\"] }</code>.</li> <li>If the second node returns <code>{ bar: [\"bye\"] }</code> then the State would then be <code>{ foo: 2, bar: [\"hi\", \"bye\"] }</code>.</li> <li>Notice here that the bar key is updated by adding the two arrays together.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#difference-between-zod-schema-and-annotation-schema","title":"Difference Between Zod Schema and Annotation Schema","text":"<ul> <li>Zod = Executable runtime validation</li> <li>Annotation schema = Declarative metadata / hints</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#zod-schema","title":"Zod Schema?","text":"<p>Zod is a runtime schema validation library (TypeScript-first).</p> <ul> <li>Validates data at runtime</li> <li>Parses &amp; sanitizes input</li> <li>Throws or returns structured errors</li> <li>Is executable code </li> </ul> <p>What Zod Does</p> <p>\u2714 Ensures data is valid \u2714 Blocks bad input \u2714 Produces structured errors \u2714 Enforces constraints</p> <p>Example (Zod)</p> <pre><code>import { z } from \"zod\";\n\nconst CreateVMRequest = z.object({\n  region: z.string(),\n  vmSize: z.enum([\"small\", \"medium\", \"large\"]),\n  costCenter: z.string().regex(/^CC-\\d+$/),\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#what-is-an-annotation-schema","title":"What is an Annotation Schema?","text":"<p>An annotation schema is descriptive metadata attached to fields.</p> <ul> <li>Documents intent</li> <li>Guides LLM behavior</li> <li>Does NOT enforce anything</li> <li>Is not executable validation</li> </ul> <p>What Annotations Do</p> <p>\u2714 Explain meaning \u2714 Improve prompts \u2714 Help LLM reasoning \u274c Do NOT block bad data</p> <p>Example (Annotation)</p> <pre><code>{\n  \"region\": {\n    \"type\": \"string\",\n    \"description\": \"Cloud region for VM provisioning\",\n    \"example\": \"eastus\"\n  },\n  \"vmSize\": {\n    \"type\": \"string\",\n    \"description\": \"Size of the virtual machine\"\n  }\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#annotation-only-dangerous","title":"\u274c Annotation-Only = Dangerous","text":"<p>If your agent relies only on annotations:</p> <p><pre><code>LLM says:\n\"vmSize\": \"super-large-quantum\"\n</code></pre> No system stops it.</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#zod-safety-gate","title":"\u2705 Zod = Safety Gate","text":"<p>With Zod:</p> <pre><code>Invalid input \u2192 Execution stops \u2192 Error handled\n</code></pre> <p>This is non-negotiable in:</p> <ul> <li>Cloud provisioning</li> <li>Auto-remediation</li> <li>Financial workflows</li> <li>Compliance systems</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#how-they-work-together-best-practice","title":"How They Work Together (Best Practice)","text":"<p>Enterprise Pattern</p> <pre><code>Annotation \u2192 LLM understands intent\nZod        \u2192 System enforces reality\n</code></pre> <p>Example: Agent Tool Definition</p> <pre><code>const CreateVMInput = z.object({\n  region: z.string().describe(\"Azure region like eastus\"),\n  vmSize: z.enum([\"B2s\", \"D4s\", \"E8s\"])\n    .describe(\"Allowed VM sizes\"),\n});\n</code></pre> <p>Here:</p> <ul> <li><code>.describe()</code> = Annotation</li> <li><code>z.object()</code> = Enforcement</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#in-langgraph-agentic-frameworks","title":"In LangGraph / Agentic Frameworks","text":"<p>Where Zod is Used?</p> <ul> <li>Tool inputs</li> <li>Agent state</li> <li>API contracts</li> <li>Human approvals</li> <li>Safety checks</li> </ul> <p>Where Annotations are Used</p> <ul> <li>Prompt construction</li> <li>Reasoning hints</li> <li>Agent instructions</li> <li>UI auto-generation</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#what-is-toon","title":"What is TOON?","text":"<p>TOON (Typed Object Oriented Notation) is a LLM output-constraining schema.</p> <ul> <li>Guides LLMs to produce structured JSON</li> <li>Reduces hallucinations</li> <li>Improves determinism</li> <li>Works inside prompts / agent reasoning</li> </ul> <p>\u26a0\ufe0f TOON does NOT execute validation logic at runtime like Zod.</p> <p>Example (TOON)</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"region\": {\n      \"type\": \"string\",\n      \"description\": \"Azure region\"\n    },\n    \"vmSize\": {\n      \"type\": \"string\",\n      \"enum\": [\"B2s\", \"D4s\", \"E8s\"]\n    }\n  },\n  \"required\": [\"region\", \"vmSize\"]\n}\n</code></pre> <p>What TOON Does</p> <p>\u2714 Forces structured output \u2714 Limits LLM choices \u2714 Improves reliability \u274c Does NOT stop invalid runtime values</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#what-is-json","title":"What is JSON?","text":"<p>JSON (JavaScript Object Notation) is a data interchange format.</p> <ul> <li>Represents structured data</li> <li>Is language-agnostic</li> <li>Has NO rules, NO validation, NO meaning by default</li> </ul> <p>Example (Pure JSON)</p> <pre><code>{\n  \"region\": \"eastus\",\n  \"vmSize\": \"D4s\"\n}\n</code></pre> <p>What JSON Does</p> <p>\u2714 Stores data \u2714 Transfers data \u274c No validation \u274c No constraints \u274c No semantics</p> <p>JSON is just a container, not a guardrail.</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#what-is-temporal","title":"What is Temporal?","text":"<p>Temporal is a distributed workflow orchestration engine designed for:</p> <ul> <li>Long-running workflows</li> <li>Exactly-once execution</li> <li>Automatic retries</li> <li>State durability</li> <li>Fault tolerance</li> </ul> <p>Temporal guarantees that:</p> <p><code>Your workflow will complete correctly, even if everything crashes.</code></p> <p>What Temporal Does</p> <p>\u2714 Persists workflow state \u2714 Handles crashes &amp; restarts \u2714 Supports long waits (days/months) \u2714 Handles retries &amp; backoff \u2714 Supports signals &amp; timers \u2714 Provides full audit history</p> <p>Temporal Example (Simplified)</p> <pre><code>@workflow.defn\nclass ProvisionVMWorkflow:\n\n    @workflow.run\n    async def run(self, request):\n        await workflow.execute_activity(validate_input, request)\n        await workflow.execute_activity(create_vm, request)\n</code></pre> <p>Temporal ensures:</p> <p>\u2714 No double execution \u2714 Resume after crash \u2714 Full traceability</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#what-is-pydantic","title":"What is Pydantic?","text":"<p>Pydantic is a Python runtime schema validation library.</p> <ul> <li>Validates data at runtime</li> <li>Enforces types</li> <li>Converts raw JSON into safe Python objects</li> <li>Raises structured validation errors</li> </ul> <p>What Pydantic Does</p> <p>\u2714 Validates LLM output \u2714 Blocks invalid input \u2714 Enforces constraints \u2714 Provides type-safe objects \u2714 Essential for AI agents</p> <p>Pydantic Example</p> <pre><code>from pydantic import BaseModel, Field\n\nclass VMRequest(BaseModel):\n    region: str = Field(description=\"Azure region\")\n    vm_size: str = Field(pattern=\"^(B2s|D4s|E8s)$\")\n</code></pre> <p>If invalid data arrives:</p> <pre><code>ValidationError \u2192 Execution stops\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#without-pydantic","title":"\u274c Without Pydantic","text":"<ul> <li>LLM outputs garbage</li> <li>Tools crash</li> <li>Security risk</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#without-temporal","title":"\u274c Without Temporal","text":"<ul> <li>Workflow dies on crash</li> <li>Manual retries</li> <li>No audit trail</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#ultimate-comparison-matrix-executive-view","title":"Ultimate Comparison Matrix (Executive View)","text":"Layer Preferred Choice Data format JSON Documentation Annotation LLM output shaping TOON Runtime safety Zod / Pydantic Agent state LangGraph State Workflow durability Temporal Knowledge Vector + Graph DB"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#llm-prompt-schema-vs-hard-schema","title":"LLM Prompt Schema vs Hard Schema","text":"Aspect Prompt Schema Zod/Pydantic Enforced by code \u274c \u2705 Reliable \u274c \u2705 Explainable \u26a0\ufe0f \u2705 Safe for prod \u274c \u2705"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#vector-db-vs-graph-db-vs-sql-knowledge-layer","title":"Vector DB vs Graph DB vs SQL (Knowledge Layer)","text":"Use Case Vector DB Graph DB SQL Semantic search \u2705 \u274c \u274c Relationships \u274c \u2705 \u26a0\ufe0f Transactions \u274c \u274c \u2705 Agent memory \u2705 \u2705 \u26a0\ufe0f"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#temporal-vs-cron-vs-celery-workflow-safety","title":"Temporal vs Cron vs Celery (Workflow Safety)","text":"Feature Temporal Cron Celery Durable workflows \u2705 \u274c \u26a0\ufe0f Long-running \u2705 \u274c \u26a0\ufe0f Retry control \u2705 \u26a0\ufe0f \u26a0\ufe0f Audit &amp; replay \u2705 \u274c \u274c Agentic AI fit \u2705 \u274c \u26a0\ufe0f"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#langgraph-state-vs-plain-json-state","title":"LangGraph State vs Plain JSON State","text":"Aspect LangGraph State Plain JSON Typed \u2705 \u274c Enforced \u2705 \u274c State transitions Controlled Free-form Audit-ready \u2705 \u274c Production-safe \u2705 \u274c"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#zod-vs-pydantic-vs-marshmallow-runtime-validation","title":"Zod vs Pydantic vs Marshmallow (Runtime Validation)","text":"Feature Zod (TS) Pydantic (Py) Marshmallow Runtime validation \u2705 \u2705 \u2705 LLM friendly \u274c \u274c \u274c LangGraph fit \u2705 \u2705 \u26a0\ufe0f Typed outputs \u2705 \u2705 \u26a0\ufe0f Best for agents \u2705 \u2705 \u274c"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#toon-vs-json-schema-vs-openai-function-calling","title":"TOON vs JSON Schema vs OpenAI Function Calling","text":"Feature TOON JSON Schema Function Calling LLM structured output \u2705 \u2705 \u2705 Enum constraints \u2705 \u2705 \u2705 Runtime enforcement \u274c \u274c \u274c Prompt-native \u2705 \u26a0\ufe0f \u274c Vendor lock-in \u274c \u274c \u26a0\ufe0f"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#annotation-vs-openapi-vs-swagger","title":"Annotation vs OpenAPI vs Swagger","text":"Aspect Annotation OpenAPI Swagger Purpose Explain fields API contract UI + docs Runtime validation \u274c \u26a0\ufe0f \u274c LLM usability \u2705 \u26a0\ufe0f \u274c Tool generation \u274c \u2705 \u2705 Enterprise APIs \u274c \u2705 \u2705"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#full-comparison-table","title":"Full Comparison Table","text":"Category JSON Annotation TOON JSON Schema OpenAI Function Calling Zod Pydantic LangGraph State Temporal Primary Role Data format Meaning / Docs LLM constraint Structure spec LLM tool binding Runtime validation (TS) Runtime validation (Py) Agent state control Durable workflow Executable \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u2705 \u2705 Validates at runtime \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u26a0\ufe0f \u274c Blocks bad execution \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u26a0\ufe0f \u274c Guides LLM output \u274c \u2705 \u2705 \u26a0\ufe0f \u2705 \u274c \u274c \u274c \u274c Prevents hallucination \u274c \u274c \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 \u26a0\ufe0f \u274c LLM-native \u274c \u26a0\ufe0f \u2705 \u26a0\ufe0f \u2705 \u274c \u274c \u274c \u274c Type-safe objects \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u2705 \u274c Agent state safety \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u2705 \u274c Tool input safety \u274c \u274c \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 \u274c \u274c Long-running workflows \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u26a0\ufe0f \u2705 Crash recovery \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Retries &amp; backoff \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Human-in-loop support \u274c \u274c \u274c \u274c \u26a0\ufe0f \u274c \u274c \u2705 \u2705 Audit &amp; replay \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u26a0\ufe0f \u2705 Security-critical safe \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u26a0\ufe0f \u2705 Enterprise production-ready \u274c \u274c \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 \u2705 \u2705"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#messages-in-graph-state","title":"Messages in Graph State","text":"<p>Why use messages?</p> <ul> <li>Most modern LLM providers have a chat model interface that accepts a list of messages as input.</li> <li>LangChain\u2019s chat model interface in particular accepts a list of message objects as inputs.</li> <li>These messages come in a variety of forms such as HumanMessage <code>(user input)</code> or AIMessage <code>(LLM response)</code>.</li> </ul> <p>Using Messages in your Graph</p> <ul> <li>In many cases, it is helpful to <code>store prior conversation history</code> as a <code>list of messages</code> in your <code>graph state</code>.</li> <li>To do so, we can add a <code>key (channel)</code> to the <code>graph state</code> that stores a <code>list of Message objects and annotate</code> it with a <code>reducer</code> function.</li> <li>The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update).</li> <li>If you don\u2019t specify a reducer, every state update will overwrite the list of messages with the most recently provided value.</li> <li> <p>If you wanted to simply append messages to the existing list, you could use a function that concatenates arrays as a reducer.</p> </li> <li> <p>However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop).</p> </li> <li>If you were to use a simple concatenation function, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. </li> <li>To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt messagesStateReducer function or MessagesZodMeta when state schema is defined with Zod.</li> <li>For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#serialization","title":"Serialization","text":"<p>In addition to keeping track of message IDs, MessagesZodMeta will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. This allows sending graph inputs / state updates in the following format:</p> <pre><code>// this is supported\n{\n  messages: [new HumanMessage(\"message\")];\n}\n\n// and this is also supported\n{\n  messages: [{ role: \"human\", content: \"message\" }];\n}\n</code></pre> <p>Since the state updates are always deserialized into LangChain Messages when using MessagesZodMeta, you should use dot notation to access message attributes, like <code>state.messages[state.messages.length - 1].content</code>. Below is an example of a graph that uses MessagesZodMeta:</p> <pre><code>import { StateGraph, MessagesZodMeta } from \"@langchain/langgraph\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst graph = new StateGraph(MessagesZodState)\n  ...\n</code></pre> <ul> <li>MessagesZodState is defined with a single messages key which is a list of BaseMessage objects and uses the appropriate reducer.</li> <li>Typically, there is more state to track than just messages, so we see people extend this state and add more fields, like:</li> </ul> <pre><code>const State = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n  documents: z.array(z.string()),\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#nodes","title":"Nodes","text":"<p>In LangGraph, nodes are typically <code>functions (sync or async)</code> that accept the following arguments:</p> <ol> <li>state \u2013 The state of the graph</li> <li> <p>config \u2013 A RunnableConfig object that contains configuration information like thread_id and tracing information like tags</p> </li> <li> <p>You can add nodes to a graph using the addNode method.</p> </li> </ol> <pre><code>import { StateGraph } from \"@langchain/langgraph\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  input: z.string(),\n  results: z.string(),\n});\n\nconst builder = new StateGraph(State);\n  .addNode(\"myNode\", (state, config) =&gt; {\n    console.log(\"In node: \", config?.configurable?.user_id);\n    return { results: `Hello, ${state.input}!` };\n  })\n  addNode(\"otherNode\", (state) =&gt; {\n    return state;\n  })\n  ...\n</code></pre> <ul> <li>Behind the scenes, functions are converted to RunnableLambda, which add batch and async support to your function, along with native tracing and debugging.</li> <li>If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.</li> </ul> <pre><code>builder.addNode(myNode);\n// You can then create edges to/from this node by referencing it as `\"myNode\"`\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#start-node","title":"START Node","text":"<p>The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.</p> <pre><code>import { START } from \"@langchain/langgraph\";\n\ngraph.addEdge(START, \"nodeA\");\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#end-node","title":"END Node","text":"<p>The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.</p> <pre><code>import { END } from \"@langchain/langgraph\";\n\ngraph.addEdge(\"nodeA\", END);\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#node-caching","title":"Node Caching","text":"<p>LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:</p> <ul> <li>Specify a <code>cache</code> when compiling a graph (or specifying an <code>entrypoint</code>)</li> <li>Specify a cache policy for nodes. Each cache policy supports:</li> <li>keyFunc, which is used to generate a cache key based on the input to a node.</li> <li>ttl, the time to live for the cache in seconds. If not specified, the cache will never expire.</li> </ul> <pre><code>import { StateGraph, MessagesZodMeta } from \"@langchain/langgraph\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\nimport { InMemoryCache } from \"@langchain/langgraph-checkpoint\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst graph = new StateGraph(MessagesZodState)\n  .addNode(\n    \"expensive_node\",\n    async () =&gt; {\n      // Simulate an expensive operation\n      await new Promise((resolve) =&gt; setTimeout(resolve, 3000));\n      return { result: 10 };\n    },\n    { cachePolicy: { ttl: 3 } }\n  )\n  .addEdge(START, \"expensive_node\")\n  .compile({ cache: new InMemoryCache() });\n\nawait graph.invoke({ x: 5 }, { streamMode: \"updates\" });   \n// [{\"expensive_node\": {\"result\": 10}}]\nawait graph.invoke({ x: 5 }, { streamMode: \"updates\" });   \n// [{\"expensive_node\": {\"result\": 10}, \"__metadata__\": {\"cached\": true}}]\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#edges","title":"Edges","text":"<ul> <li>Edges define how the logic is routed and how the graph decides to stop. </li> <li> <p>This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:</p> </li> <li> <p>Normal Edges: Go directly from one node to the next.</p> </li> <li>Conditional Edges: Call a function to determine which node(s) to go to next.</li> <li>Entry Point: Which node to call first when user input arrives.</li> <li>Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.</li> </ul> <p>A node can have multiple outgoing edges. If a node has multiple outgoing edges, all of those destination nodes will be executed in parallel</p> <p>Normal Edges</p> <ul> <li>If you always want to go from node A to node B, you can use the addEdge method directly.</li> </ul> <p><code>graph.addEdge(\"nodeA\", \"nodeB\");</code></p> <p>Conditional Edges</p> <p>If you want to optionally route to one or more edges (or optionally terminate), you can use the addConditionalEdges method. This method accepts the name of a node and a \u201crouting function\u201d to call after that node is executed:</p> <p><code>graph.addConditionalEdges(\"nodeA\", routingFunction);</code></p> <p>Similar to nodes, the routingFunction accepts the current state of the graph and returns a value.</p> <p>By default, the return value routingFunction is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.</p> <p>You can optionally provide an object that maps the routingFunction\u2019s output to the name of the next node.</p> <pre><code>graph.addConditionalEdges(\"nodeA\", routingFunction, {\n  true: \"nodeB\",\n  false: \"nodeC\",\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#entry-point","title":"Entry point","text":"<p>The entry point is the first node(s) that are run when the graph starts. You can use the addEdge method from the virtual START node to the first node to execute to specify where to enter the graph.</p> <pre><code>import { START } from \"@langchain/langgraph\";\n\ngraph.addEdge(START, \"nodeA\");\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#conditional-entry-point","title":"Conditional entry point","text":"<p>A conditional entry point lets you start at different nodes depending on custom logic. You can use addConditionalEdges from the virtual START node to accomplish this.</p> <pre><code>import { START } from \"@langchain/langgraph\";\n\ngraph.addConditionalEdges(START, routingFunction);\n</code></pre> <p>You can optionally provide an object that maps the routingFunction\u2019s output to the name of the next node.</p> <pre><code>graph.addConditionalEdges(START, routingFunction, {\n  true: \"nodeB\",\n  false: \"nodeC\",\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#send","title":"Send","text":"<p>By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).</p> <p>To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.</p> <pre><code>import { Send } from \"@langchain/langgraph\";\n\ngraph.addConditionalEdges(\"nodeA\", (state) =&gt; {\n  return state.subjects.map((subject) =&gt; new Send(\"generateJoke\", { subject }));\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#command","title":"Command","text":"<p>It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\ngraph.addNode(\"myNode\", (state) =&gt; {\n  return new Command({\n    update: { foo: \"bar\" },\n    goto: \"myOtherNode\",\n  });\n});\n</code></pre> <p>With Command you can also achieve dynamic control flow behavior (identical to conditional edges):</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\ngraph.addNode(\"myNode\", (state) =&gt; {\n  if (state.foo === \"bar\") {\n    return new Command({\n      update: { foo: \"baz\" },\n      goto: \"myOtherNode\",\n    });\n  }\n});\n</code></pre> <p>When using Command in your node functions, you must add the ends parameter when adding the node to specify which nodes it can route to:</p> <pre><code>builder.addNode(\"myNode\", myNode, {\n  ends: [\"myOtherNode\", END],\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#when-should-i-use-command-instead-of-conditional-edges","title":"When should I use Command instead of conditional edges?","text":"<ul> <li>Use Command when you need to both update the graph state and route to a different node.</li> <li>For example, when implementing multi-agent handoffs where it\u2019s important to route to a different agent and pass some information to that agent.</li> <li>Use conditional edges to route between nodes conditionally without updating the state.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#navigating-to-a-node-in-a-parent-graph","title":"Navigating to a node in a parent graph","text":"<p>If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph: Command.PARENT in Command:</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\ngraph.addNode(\"myNode\", (state) =&gt; {\n  return new Command({\n    update: { foo: \"bar\" },\n    goto: \"otherSubgraph\", // where `otherSubgraph` is a node in the parent graph\n    graph: Command.PARENT,\n  });\n});\n</code></pre> <p>Setting graph to <code>Command.PARENT</code> will navigate to the closest parent graph. When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph <code>state schemas</code>, you must define a <code>reducer</code> for the key you\u2019re updating in the parent graph state.</p> <ul> <li>This is particularly useful when implementing multi-agent handoffs.</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#using-inside-tools","title":"Using inside tools","text":"<p>A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>Command is an important part of human-in-the-loop workflows: when using <code>interrupt()</code> to collect user input, Command is then used to supply the input and resume execution via new <code>Command({ resume: \"User input\" })</code></p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#graph-migrations","title":"Graph migrations","text":"<p>LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#runtime-context","title":"Runtime context","text":"<ul> <li>When creating a graph, you can specify a contextSchema for runtime context passed to nodes. </li> <li>This is useful for passing information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.</li> </ul> <pre><code>import * as z from \"zod\";\n\nconst ContextSchema = z.object({\n  llm: z.union([z.literal(\"openai\"), z.literal(\"anthropic\")]),\n});\n\nconst graph = new StateGraph(State, ContextSchema);\n</code></pre> <p>You can then pass this configuration into the graph using the context property.</p> <pre><code>const config = { context: { llm: \"anthropic\" } };\n\nawait graph.invoke(inputs, config);\n</code></pre> <p>You can then access and use this context inside a node or conditional edge:</p> <pre><code>import { Runtime } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst nodeA = (\n  state: z.infer&lt;typeof State&gt;,\n  runtime: Runtime&lt;z.infer&lt;typeof ContextSchema&gt;&gt;,\n) =&gt; {\n  const llm = getLLM(runtime.context?.llm);\n  // ...\n};\n</code></pre> <pre><code>graph.addNode(\"myNode\", (state, runtime) =&gt; {\n  const llmType = runtime.context?.llm || \"openai\";\n  const llm = getLLM(llmType);\n  return { results: `Hello, ${state.input}!` };\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#recursion-limit","title":"Recursion limit","text":"<ul> <li>The recursion limit sets the maximum number of super-steps the graph can execute during a single execution.</li> <li>Once the limit is reached, LangGraph will raise GraphRecursionError.</li> <li>By default this value is set to 25 steps.</li> <li>The recursion limit can be set on any graph at runtime, and is passed to <code>invoke/stream</code> via the config object. </li> <li>Importantly, <code>recursionLimit</code> is a standalone <code>config</code> key and should not be passed inside the <code>configurable</code> key as all other user-defined configuration. See the example below:</li> </ul> <pre><code>await graph.invoke(inputs, {\n  recursionLimit: 5,\n  context: { llm: \"anthropic\" },\n});\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#accessing-and-handling-the-recursion-counter","title":"Accessing and handling the recursion counter","text":"<p>The current step counter is accessible in config.metadata.langgraph_step within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.</p> <p>How it works</p> <p>The step counter is stored in <code>config.metadata.langgraph_step</code>. The recursion limit check follows the logic: <code>step &gt; stop</code> where <code>stop = step + recursionLimit + 1</code>. When the limit is exceeded, LangGraph raises a <code>GraphRecursionError</code>.</p>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#accessing-the-current-step-counter","title":"Accessing the current step counter","text":"<p>You can access the current step counter within any node to monitor execution progress.</p> <pre><code>import { RunnableConfig } from \"@langchain/core/runnables\";\nimport { StateGraph } from \"@langchain/langgraph\";\n\nasync function myNode(state: any, config: RunnableConfig): Promise&lt;any&gt; {\n  const currentStep = config.metadata?.langgraph_step;\n  console.log(`Currently on step: ${currentStep}`);\n  return state;\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#proactive-recursion-handling","title":"Proactive recursion handling","text":"<p>You can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.</p> <pre><code>import { RunnableConfig } from \"@langchain/core/runnables\";\nimport { StateGraph, END } from \"@langchain/langgraph\";\n\ninterface State {\n  messages: string[];\n  route_to?: string;\n  reason?: string;\n  done?: boolean;\n}\n\nasync function reasoningNode(\n  state: State,\n  config: RunnableConfig\n): Promise&lt;Partial&lt;State&gt;&gt; {\n  const currentStep = config.metadata?.langgraph_step ?? 0;\n  const recursionLimit = config.recursionLimit!; // always present, defaults to 25\n\n  // Check if we're approaching the limit (e.g., 80% threshold)\n  if (currentStep &gt;= recursionLimit * 0.8) {\n    return {\n      ...state,\n      route_to: \"fallback\",\n      reason: \"Approaching recursion limit\"\n    };\n  }\n\n  // Normal processing\n  return {\n    messages: [...state.messages, \"thinking...\"]\n  };\n}\n\nasync function fallbackNode(\n  state: State,\n  config: RunnableConfig\n): Promise&lt;Partial&lt;State&gt;&gt; {\n  return {\n    ...state,\n    messages: [\n      ...state.messages,\n      \"Reached complexity limit, providing best effort answer\"\n    ]\n  };\n}\n\nfunction routeBasedOnState(state: State): string {\n  if (state.route_to === \"fallback\") {\n    return \"fallback\";\n  } else if (state.done) {\n    return END;\n  }\n  return \"reasoning\";\n}\n\n// Build graph\nconst graph = new StateGraph&lt;State&gt;({ channels: {} })\n  .addNode(\"reasoning\", reasoningNode)\n  .addNode(\"fallback\", fallbackNode)\n  .addConditionalEdges(\"reasoning\", routeBasedOnState)\n  .addEdge(\"fallback\", END);\n\nconst app = graph.compile();\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#proactive-vs-reactive-approaches","title":"Proactive vs reactive approaches","text":"<p>There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).</p> <pre><code>import { RunnableConfig } from \"@langchain/core/runnables\";\nimport { StateGraph, END } from \"@langchain/langgraph\";\nimport { GraphRecursionError } from \"@langchain/langgraph\";\n\ninterface State {\n  messages: string[];\n  status?: string;\n  final_answer?: string;\n}\n\n// Proactive Approach (recommended)\nasync function agentWithMonitoring(\n  state: State,\n  config: RunnableConfig\n): Promise&lt;Partial&lt;State&gt;&gt; {\n  const currentStep = config.metadata?.langgraph_step ?? 0;\n  const recursionLimit = config.recursionLimit!;\n\n  // Early detection - route to internal handling\n  if (currentStep &gt;= recursionLimit - 2) { // 2 steps before limit\n    return {\n      ...state,\n      status: \"recursion_limit_approaching\",\n      final_answer: \"Reached iteration limit, returning partial result\"\n    };\n  }\n\n  // Normal processing\n  return {\n    messages: [...state.messages, `Step ${currentStep}`]\n  };\n}\n\n// Reactive Approach (fallback)\ntry {\n  const result = await graph.invoke(initialState, { recursionLimit: 10 });\n} catch (error) {\n  if (error instanceof GraphRecursionError) {\n    // Handle externally after graph execution fails\n    const result = await fallbackHandler(initialState);\n  }\n}\n</code></pre> <p>The key differences between these approaches are:</p> Approach Detection Handling Control Flow Proactive (using <code>langgraph_step</code>) Before limit reached Inside graph via conditional routing Graph continues to completion node Reactive (catching <code>GraphRecursionError</code>) After limit exceeded Outside graph in <code>try/catch</code> Graph execution terminated <p>Proactive advantages:</p> <ul> <li>Graceful degradation within the graph</li> <li>Can save intermediate state in checkpoints</li> <li>Better user experience with partial results</li> <li>Graph completes normally (no exception)</li> </ul> <p>Reactive advantages:</p> <ul> <li>Simpler implementation</li> <li>No need to modify graph logic</li> <li>Centralized error handling</li> </ul>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#other-available-metadata","title":"Other available metadata","text":"<p>Along with <code>langgraph_step</code>, the following metadata is also available in <code>config.metadata</code>:</p> <pre><code>async function inspectMetadata(\n  state: any,\n  config: RunnableConfig\n): Promise&lt;any&gt; {\n  const metadata = config.metadata;\n\n  console.log(`Step: ${metadata?.langgraph_step}`);\n  console.log(`Node: ${metadata?.langgraph_node}`);\n  console.log(`Triggers: ${metadata?.langgraph_triggers}`);\n  console.log(`Path: ${metadata?.langgraph_path}`);\n  console.log(`Checkpoint NS: ${metadata?.langgraph_checkpoint_ns}`);\n\n  return state;\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#visualization","title":"Visualization","text":"<p>It\u2019s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs.</p> <p>Here we demonstrate how to visualize the graphs you create.</p> <p>You can visualize any arbitrary Graph, including StateGraph.</p> <p>Let\u2019s create a simple example graph to demonstrate visualization.</p> <pre><code>import { StateGraph, START, END, MessagesZodMeta } from \"@langchain/langgraph\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n  value: z.number().register(registry, {\n    reducer: {\n      fn: (x, y) =&gt; x + y,\n    },\n  }),\n});\n\nconst app = new StateGraph(State)\n  .addNode(\"node1\", (state) =&gt; {\n    return { value: state.value + 1 };\n  })\n  .addNode(\"node2\", (state) =&gt; {\n    return { value: state.value * 2 };\n  })\n  .addEdge(START, \"node1\")\n  .addConditionalEdges(\"node1\", (state) =&gt; {\n    if (state.value &lt; 10) {\n      return \"node2\";\n    }\n    return END;\n  })\n  .addEdge(\"node2\", \"node1\")\n  .compile();\n  ```\n\n  ## Mermaid\n  We can also convert a graph class into Mermaid syntax.\n\n  ```\n  const drawableGraph = await app.getGraphAsync();\nconsole.log(drawableGraph.drawMermaid());\n</code></pre> <pre><code>%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n    tart__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n    e1(node1)\n    e2(node2)\n    nd__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n    tart__ --&gt; node1;\n    e1 -.-&gt; node2;\n    e1 -.-&gt; __end__;\n    e2 --&gt; node1;\n    ssDef default fill:#f2f0ff,line-height:1.2\n    ssDef first fill-opacity:0\n    ssDef last fill:#bfb6fc\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#png","title":"PNG","text":"<p>If preferred, we could render the Graph into a .png. This uses the Mermaid.ink API to generate the diagram.</p> <pre><code>import * as fs from \"node:fs/promises\";\n\nconst drawableGraph = await app.getGraphAsync();\nconst image = await drawableGraph.drawMermaidPng();\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\n\nawait fs.writeFile(\"graph.png\", imageBuffer);\n</code></pre>"},{"location":"AgenticAI/LangGraph/LangGraphDetails/#functional-api-vs-graph-api","title":"Functional API vs Graph API","text":"Aspect Functional API Graph API Programming Model Linear function calls Node\u2013edge graph execution Control Flow Hard-coded in call order Explicit, visualized flow Execution Style Sequential Conditional, looping, parallel State Management Passed manually between functions Shared, managed graph state Dynamic Routing Difficult (if/else in code) Native via conditional edges Loops / Recursion Manual, error-prone First-class support Parallelism Limited, manual Native (fan-out / fan-in) Error Handling try/except per function Dedicated failure edges Human-in-Loop Hard to pause/resume Native pause, resume, approve Observability Logs scattered Node-level tracing Reusability Function-level reuse Subgraphs &amp; reusable patterns Scalability Code complexity grows fast Graph scales cleanly Determinism Implicit Explicit and auditable Visualization \u274c Not visible \u2705 Graph is inspectable Agentic AI Fit \u26a0\ufe0f Simple agents only \u2705 Multi-agent workflows Best Use Cases Simple pipelines, utilities Agent orchestration, workflows"},{"location":"AgenticAI/LangGraph/Subgraphs/","title":"Subgraphs","text":"<p>A subgraph is a <code>graph</code> that is used as a <code>node</code> in another graph.</p> <p>Subgraphs are useful for:</p> <ul> <li>Building <code>multi-agent systems</code></li> <li>Re-using a set of nodes in multiple graphs</li> <li>Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph</li> </ul> <p>When adding subgraphs, you need to define how the parent graph and the subgraph communicate:</p> <ul> <li><code>Invoke a graph from a node</code> \u2014 subgraphs are called from inside a node in the parent graph</li> <li><code>Add a graph as a node</code> \u2014 a subgraph is added directly as a node in the parent and <code>shares state keys</code> with the parent</li> </ul>"},{"location":"AgenticAI/LangGraph/Subgraphs/#setup","title":"Setup","text":"<pre><code>npm install @langchain/langgraph\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#invoke-a-graph-from-a-node","title":"Invoke a graph from a node","text":"<p>A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have <code>completely different schemas</code> from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a <code>multi-agent</code> system.</p> <p>If that\u2019s the case for your application, you need to define a node <code>function that invokes the subgraph</code>. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.</p> <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst SubgraphState = z.object({\n  bar: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"hi! \" + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Transform the state to the subgraph state and back\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", async (state) =&gt; {\n    const subgraphOutput = await subgraph.invoke({ bar: state.foo });\n    return { foo: subgraphOutput.bar };\n  })\n  .addEdge(START, \"node1\");\n\nconst graph = builder.compile();\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#full-example-different-state-schemas","title":"Full example: different state schemas","text":"<pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  // note that none of these keys are shared with the parent graph state\n  bar: z.string(),\n  baz: z.string(),\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { baz: \"baz\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    return { bar: state.bar + state.baz };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", async (state) =&gt; {\n    const response = await subgraph.invoke({ bar: state.foo });   \n    return { foo: response.bar };   \n  })\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  { subgraphs: true }\n)) {\n  console.log(chunk);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#full-example-different-state-schemas-two-levels-of-subgraphs","title":"Full example: different state schemas (two levels of subgraphs)","text":"<ul> <li>This is an example with two levels of subgraphs: <code>parent</code> -&gt; <code>child</code> -&gt; <code>grandchild</code>.</li> </ul> <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Grandchild graph\nconst GrandChildState = z.object({\n  myGrandchildKey: z.string(),\n});\n\nconst grandchild = new StateGraph(GrandChildState)\n  .addNode(\"grandchild1\", (state) =&gt; {\n    // NOTE: child or parent keys will not be accessible here\n    return { myGrandchildKey: state.myGrandchildKey + \", how are you\" };\n  })\n  .addEdge(START, \"grandchild1\")\n  .addEdge(\"grandchild1\", END);\n\nconst grandchildGraph = grandchild.compile();\n\n// Child graph\nconst ChildState = z.object({\n  myChildKey: z.string(),\n});\n\nconst child = new StateGraph(ChildState)\n  .addNode(\"child1\", async (state) =&gt; {\n    // NOTE: parent or grandchild keys won't be accessible here\n    const grandchildGraphInput = { myGrandchildKey: state.myChildKey };   \n    const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput);\n    return { myChildKey: grandchildGraphOutput.myGrandchildKey + \" today?\" };   \n  })   \n  .addEdge(START, \"child1\")\n  .addEdge(\"child1\", END);\n\nconst childGraph = child.compile();\n\n// Parent graph\nconst ParentState = z.object({\n  myKey: z.string(),\n});\n\nconst parent = new StateGraph(ParentState)\n  .addNode(\"parent1\", (state) =&gt; {\n    // NOTE: child or grandchild keys won't be accessible here\n    return { myKey: \"hi \" + state.myKey };\n  })\n  .addNode(\"child\", async (state) =&gt; {\n    const childGraphInput = { myChildKey: state.myKey };   \n    const childGraphOutput = await childGraph.invoke(childGraphInput);\n    return { myKey: childGraphOutput.myChildKey };   \n  })   \n  .addNode(\"parent2\", (state) =&gt; {\n    return { myKey: state.myKey + \" bye!\" };\n  })\n  .addEdge(START, \"parent1\")\n  .addEdge(\"parent1\", \"child\")\n  .addEdge(\"child\", \"parent2\")\n  .addEdge(\"parent2\", END);\n\nconst parentGraph = parent.compile();\n\nfor await (const chunk of await parentGraph.stream(\n  { myKey: \"Bob\" },\n  { subgraphs: true }\n)) {\n  console.log(chunk);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#add-a-graph-as-a-node","title":"Add a graph as a node","text":"<p>When the parent graph and subgraph can communicate over a shared state key (channel) in the <code>schema</code>, you can add a graph as a <code>node</code> in another graph. For example, in <code>multi-agent</code> systems, the agents often communicate over a shared <code>messages</code> key.</p> <p></p> <p>If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:</p> <ol> <li>Define the subgraph workflow (<code>subgraphBuilder</code> in the example below) and compile it</li> <li>Pass compiled subgraph to the <code>.addNode</code> method when defining the parent graph workflow</li> </ol> <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst graph = builder.compile();\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#ull-example-shared-state-schemas","title":"ull example: shared state schemas","text":"<pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  foo: z.string(),    \n  bar: z.string(),    \n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"bar\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    // note that this node is using a state key ('bar') that is only available in the subgraph\n    // and is sending update on the shared state key ('foo')\n    return { foo: state.foo + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", subgraph)\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream({ foo: \"foo\" })) {\n  console.log(chunk);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#add-persistence","title":"Add persistence","text":"<p>You only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <pre><code>import { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { foo: state.foo + \"bar\" };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n</code></pre> <p>If you want the subgraph to <code>have its own memory</code>, you can compile it with the appropriate checkpointer option. This is useful in <code>multi-agent</code> systems, if you want agents to keep track of their internal message histories:</p> <pre><code>const subgraphBuilder = new StateGraph(...)\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#view-subgraph-state","title":"View subgraph state","text":"<p>When you enable <code>persistence</code>, you can <code>inspect the graph state</code> (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.</p> <p>You can inspect the graph state via <code>graph.getState(config)</code>. To view the subgraph state, you can use <code>graph.getState(config, { subgraphs: true })</code>.</p>"},{"location":"AgenticAI/LangGraph/Subgraphs/#view-interrupted-subgraph-state","title":"View interrupted subgraph state","text":"<pre><code>import { StateGraph, START, MemorySaver, interrupt, Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    const value = interrupt(\"Provide value:\");\n    return { foo: state.foo + value };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nawait graph.invoke({ foo: \"\" }, config);\nconst parentState = await graph.getState(config);\nconst subgraphState = (await graph.getState(config, { subgraphs: true })).tasks[0].state;   \n\n// resume the subgraph\nawait graph.invoke(new Command({ resume: \"bar\" }), config);\n</code></pre>"},{"location":"AgenticAI/LangGraph/Subgraphs/#stream-subgraph-outputs","title":"Stream subgraph outputs","text":"<p>To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.</p> <pre><code>for await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    subgraphs: true,   \n    streamMode: \"updates\",\n  }\n)) {\n  console.log(chunk);\n}\n</code></pre> <ol> <li>Set subgraphs: <code>true</code> to stream outputs from subgraphs.</li> </ol>"},{"location":"AgenticAI/LangGraph/Subgraphs/#stream-from-subgraphs","title":"Stream from subgraphs","text":"<pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  foo: z.string(),\n  bar: z.string(),\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"bar\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    // note that this node is using a state key ('bar') that is only available in the subgraph\n    // and is sending update on the shared state key ('foo')\n    return { foo: state.foo + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", subgraph)\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    streamMode: \"updates\",\n    subgraphs: true,   \n  }\n)) {\n  console.log(chunk);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/","title":"Edges","text":"<p>Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:</p> <ul> <li>Normal Edges: Go directly from one node to the next.</li> <li>Conditional Edges: Call a function to determine which node(s) to go to next.</li> <li>Entry Point: Which node to call first when user input arrives.</li> </ul>"},{"location":"AgenticAI/LangGraph/edge/#normal-edges","title":"Normal Edges","text":"<p>If you always want to go from node A to node B, you can use the add_edge method directly.</p> <pre><code>graph.add_edge(\"node_a\", \"node_b\")\n````\ngraph.add_edge(\"node_a\", \"node_b\")\n</code></pre> <p>If you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a \u201crouting function\u201d to call after that node is executed:</p> <pre><code>graph.add_conditional_edges(\"node_a\", routing_function)\n</code></pre> <p>Similar to nodes, the routing_function accepts the current state of the graph and returns a value. By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function\u2019s output to the name of the next node.</p> <pre><code>graph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n</code></pre> <ul> <li>Use Command instead of conditional edges if you want to combine state updates and routing in a single function. \u200b</li> </ul>"},{"location":"AgenticAI/LangGraph/edge/#entry-point","title":"Entry point","text":"<p>The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.</p> <pre><code>from langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")````\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#conditional-entry-point","title":"Conditional entry point","text":"<p>A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.</p> <pre><code>from langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n</code></pre> <p>You can optionally provide a dictionary that maps the routing_function\u2019s output to the name of the next node. <code>graph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})</code></p>"},{"location":"AgenticAI/LangGraph/edge/#send","title":"Send","text":"<p>By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).</p> <p>To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.</p> <pre><code>def continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#command","title":"Command","text":"<p>It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:</p> <p><pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n</code></pre> With Command you can also achieve dynamic control flow behavior (identical to conditional edges):</p> <p>With Command you can also achieve dynamic control flow behavior (identical to conditional edges):</p> <pre><code>Ask AI\ndef my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#when-should-i-use-command-instead-of-conditional-edges","title":"When should I use Command instead of conditional edges?","text":"<ul> <li>Use Command when you need to both update the graph state and route to a different node.</li> <li>For example, when implementing multi-agent handoffs where it\u2019s important to route to a different agent and pass some information to that agent.</li> <li>Use conditional edges to route between nodes conditionally without updating the state.</li> </ul>"},{"location":"AgenticAI/LangGraph/edge/#navigating-to-a-node-in-a-parent-graph","title":"Navigating to a node in a parent graph","text":"<p>If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify <code>graph=Command.PARENT</code> in <code>Command</code>:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n</code></pre> <p>Note: Setting graph to Command.PARENT will navigate to the closest parent graph.</p> <p>When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state. </p>"},{"location":"AgenticAI/LangGraph/edge/#using-inside-tools","title":"Using inside tools","text":"<p>A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.</p>"},{"location":"AgenticAI/LangGraph/edge/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\")</p>"},{"location":"AgenticAI/LangGraph/edge/#graph-migrations","title":"Graph migrations","text":"<p>LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.</p>"},{"location":"AgenticAI/LangGraph/edge/#runtime-context","title":"Runtime context","text":"<p>When creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.</p> <pre><code>@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n</code></pre> <p>You can then pass this context into the graph using the context parameter of the invoke method.</p> <pre><code>graph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n</code></pre> <p>You can then access and use this context inside a node or conditional edge:</p> <pre><code>from langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    # ...\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#recursion-limit","title":"Recursion limit","text":"<p>The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:</p> <pre><code>graph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#accessing-and-handling-the-recursion-counter","title":"Accessing and handling the recursion counter","text":"<p>The current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic. \u200b How it works The step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step &gt; stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.</p>"},{"location":"AgenticAI/LangGraph/edge/#accessing-the-current-step-counter","title":"Accessing the current step counter","text":"<p>You can access the current step counter within any node to monitor execution progress.</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\n\ndef my_node(state: dict, config: RunnableConfig) -&gt; dict:\n    current_step = config[\"metadata\"][\"langgraph_step\"]\n    print(f\"Currently on step: {current_step}\")\n    return state\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#proactive-recursion-handling","title":"Proactive recursion handling","text":"<p>You can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, END\n\ndef reasoning_node(state: dict, config: RunnableConfig) -&gt; dict:\n    current_step = config[\"metadata\"][\"langgraph_step\"]\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\n\n    # Check if we're approaching the limit (e.g., 80% threshold)\n    if current_step &gt;= recursion_limit * 0.8:\n        return {\n            **state,\n            \"route_to\": \"fallback\",\n            \"reason\": \"Approaching recursion limit\"\n        }\n\n    # Normal processing\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\n\ndef fallback_node(state: dict, config: RunnableConfig) -&gt; dict:\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\n    return {\n        **state,\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\n    }\n\ndef route_based_on_state(state: dict) -&gt; str:\n    if state.get(\"route_to\") == \"fallback\":\n        return \"fallback\"\n    elif state.get(\"done\"):\n        return END\n    return \"reasoning\"\n\n# Build graph\ngraph = StateGraph(dict)\ngraph.add_node(\"reasoning\", reasoning_node)\ngraph.add_node(\"fallback\", fallback_node)\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\ngraph.add_edge(\"fallback\", END)\ngraph.set_entry_point(\"reasoning\")\n\napp = graph.compile()\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#proactive-vs-reactive-approaches","title":"Proactive vs reactive approaches","text":"<p>There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.errors import GraphRecursionError\n\n# Proactive Approach (recommended)\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -&gt; dict:\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\n    current_step = config[\"metadata\"][\"langgraph_step\"]\n    recursion_limit = config[\"recursion_limit\"]\n\n    # Early detection - route to internal handling\n    if current_step &gt;= recursion_limit - 2:  # 2 steps before limit\n        return {\n            **state,\n            \"status\": \"recursion_limit_approaching\",\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\n        }\n\n    # Normal processing\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\n\n# Reactive Approach (fallback)\ntry:\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\nexcept GraphRecursionError as e:\n    # Handle externally after graph execution fails\n    result = fallback_handler(initial_state)\n</code></pre>"},{"location":"AgenticAI/LangGraph/edge/#visualization","title":"Visualization","text":"<p>It\u2019s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See</p>"},{"location":"AgenticAI/LangGraph/edge/#visualize-your-graph","title":"Visualize your graph","text":"<p>https://docs.langchain.com/oss/python/langgraph/use-graph-api#visualize-your-graph</p>"},{"location":"AgenticAI/LangGraph/graph-api/","title":"Graph API","text":""},{"location":"AgenticAI/LangGraph/graph-api/#what-is-graphs","title":"What is Graphs?","text":"<p>LangGraph treats any AI workflow\u2014like an agent reasoning, using tools, collaborating with other agents, or looping until a goal is reached\u2014as a graph, not a sequence.</p> <p>A graph is made up of:</p> <ul> <li>Nodes \u2192 steps that do work (LLM call, tool call, function, decision, API call, human approval, etc.)</li> <li>Edges \u2192 rules that decide where to go next based on the result or state</li> </ul>"},{"location":"AgenticAI/LangGraph/graph-api/#why-graphs","title":"\ud83d\udd0d Why graphs?","text":"<p>Typical LLM pipelines (LangChain chains, simple agents) run in a linear flow:</p> <pre><code>User \u2192 LLM \u2192 Tool \u2192 LLM \u2192 Result\n</code></pre> <p>But real agent workflows are not linear. They often need:</p> <ul> <li>loops</li> <li>conditional decisions</li> <li>retries</li> <li>multi-step task decomposition</li> <li>multiple agents collaborating</li> <li>waiting for user input</li> <li>pausing + resuming long tasks</li> </ul> <p>A graph can easily express all these patterns.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#how-langgraph-uses-graphs","title":"\ud83e\udde0 How LangGraph uses graphs","text":"<p>1. Nodes = work</p> <p>Each node performs a meaningful step:</p> <ul> <li>Call an LLM</li> <li>Call a tool</li> <li>Summarize text</li> <li>Search the web</li> <li>Parse results</li> <li>Decide the next step</li> </ul> <p>2. Edges = transitions</p> <p>Edges define:</p> <ul> <li>What happens next?</li> <li>Conditional routing (if the tool failed \u2192 go to repair node)</li> <li>Loops (repeat analysis until done)</li> <li>Branching (select which agent handles a task)</li> </ul> <p>3. State = memory</p> <p>The shared state tracks:</p> <ul> <li>conversation history</li> <li>tool results</li> <li>agent decisions</li> <li>partial outputs</li> </ul> <p>Nodes update the state and edges read it to make decisions.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#visualizing-the-idea","title":"\ud83d\udcca Visualizing the idea","text":"<p>This is a graph with:</p> <ul> <li>nodes: A, B, C</li> <li>edges: transitions based on success/failure</li> <li>loops: from C \u2192 A \u2192 B until successful</li> </ul>"},{"location":"AgenticAI/LangGraph/graph-api/#the-big-advantage","title":"\ud83d\udca1 The big advantage","text":"<p>Using a graph lets agents behave like deterministic programs, not unpredictable black boxes.</p> <ul> <li>predictable sequences</li> <li>strict control over agent reasoning</li> <li>recoverable/resumable workflows</li> <li>multi-agent orchestration</li> <li>production-grade reliability</li> </ul> <p>This is why LangGraph excels in agentic AI, workflow AI, copilots, RAG agents, and automated multi-step tasks.</p> <p>You define the behavior of your agents using <code>three key components</code>:</p> <ol> <li> <p>State: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.</p> </li> <li> <p>Nodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.</p> </li> <li> <p>Edges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.</p> </li> </ol> <p>By composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state.</p> <p>To emphasize: Nodes and Edges are nothing more than functions \u2013 they can contain an LLM or just good ol\u2019 code.</p> <p>In short: nodes do the work, edges tell what to do next.</p> <p>LangGraph\u2019s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\u2019s Pregel system, the program proceeds in discrete \u201csuper-steps.\u201d</p> <p>A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or \u201cchannels\u201d). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#stategraph","title":"StateGraph","text":"<p>The StateGraph class is the main graph class to use. This is parameterized by a user defined State object.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#compiling-your-graph","title":"Compiling your graph","text":"<p>To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?</p> <p>Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:</p> <pre><code>graph = graph_builder.compile(...)\n</code></pre> <p>Note: You MUST compile your graph before you can use it.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#state","title":"State","text":"<p>The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#schema","title":"Schema","text":"<ul> <li>The main documented way to specify the schema of a graph is by using a TypedDict.</li> <li>If you want to provide default values in your state, use a dataclass.</li> <li>We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that Pydantic is less performant than a TypedDict or dataclass).</li> <li>By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output.</li> </ul>"},{"location":"AgenticAI/LangGraph/graph-api/#multiple-schemas","title":"Multiple schemas","text":"<p>Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:</p> <ul> <li>Internal nodes can pass information that is not required in the graph\u2019s input / output.</li> <li>We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.</li> </ul> <p>It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.</p> <p>It is also possible to define explicit input and output schemas for a graph. In these cases, we define an \u201cinternal\u201d schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the \u201cinternal\u201d schema to constrain the input and output of the graph. </p> <p>Let\u2019s look at an example:</p> <pre><code>class InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -&gt; OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -&gt; PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -&gt; OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n# {'graph_output': 'My name is Lance'}\n</code></pre> <p>There are two subtle and important points to note here:</p> <ol> <li> <p>We pass state: InputState as the input schema to node_1 But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.</p> </li> <li> <p>We initialize the graph with:</p> </li> </ol> <pre><code>StateGraph(\n    OverallState,\n    input_schema=InputState,\n    output_schema=OutputState\n)\n</code></pre> <p>So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization?</p> <p>We can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.</p>"},{"location":"AgenticAI/LangGraph/graph-api/#reducers","title":"Reducers","text":"<p>Reducers are key to understanding how updates from nodes are applied to the State</p> <p>Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:</p> <p>Default Reducer</p> <p>These two examples show how to use the default reducer:</p> <p><pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n</code></pre> In this example, no reducer functions are specified for any key. Let\u2019s assume the input to the graph is:</p> <p><pre><code>\n</code></pre> from typing import Annotated from typing_extensions import TypedDict from operator import add</p> <p>class State(TypedDict):     foo: int     bar: Annotated[list[str], add] <pre><code>In this example, we\u2019ve used the ```Annotated``` type to specify a reducer function ```(operator.add)``` for the second key (bar). Note that the first key remains unchanged. Let\u2019s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let\u2019s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\n\u200b\n\n## Overwrite\nIn some cases, you may want to bypass a reducer and directly ```overwrite``` a state value. LangGraph provides the ```Overwrite``` type for this purpose.\n\n\n## Working with Messages in Graph State\n\n**Why use messages?**\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\u2019s **chat model interface** in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as **HumanMessage** (user input) or **AIMessage** (LLM response).\n\n\n## Using Messages in your Graph\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of **Message** objects and annotate it with a reducer function\n\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\n\n## Serialization\nIn addition to keeping track of message IDs, the **add_messages** function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel.\n\nSee more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\n</code></pre></p>"},{"location":"AgenticAI/LangGraph/graph-api/#this-is-supported","title":"this is supported","text":"<p>{\"messages\": [HumanMessage(content=\"message\")]}</p>"},{"location":"AgenticAI/LangGraph/graph-api/#and-this-is-also-supported","title":"and this is also supported","text":"<p>{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]} <pre><code>Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like ```state[\"messages\"][-1].content```.\n\n\nBelow is an example of a graph that uses add_messages as its reducer function.\n</code></pre> from langchain.messages import AnyMessage from langgraph.graph.message import add_messages from typing import Annotated from typing_extensions import TypedDict</p> <p>class GraphState(TypedDict):     messages: Annotated[list[AnyMessage], add_messages] ```</p>"},{"location":"AgenticAI/LangGraph/graph-api/#messagesstate","title":"MessagesState","text":"<p>Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:</p>"},{"location":"AgenticAI/LangGraph/memory/","title":"Memory","text":"<ul> <li>AI applications need memory to share context across multiple interactions.</li> <li> <p>In LangGraph, you can add two types of memory:</p> </li> <li> <p><code>Add short-term memory</code> as a part of your agent\u2019s <code>state</code> to enable multi-turn conversations.</p> </li> <li> <p><code>Add long-term memory</code> to store user-specific or application-level data across sessions.</p> </li> </ul>"},{"location":"AgenticAI/LangGraph/memory/#add-short-term-memory","title":"Add short-term memory","text":"<p><code>Short-term memory</code> (thread-level <code>persistence</code>) enables agents to track multi-turn conversations. To add short-term memory:</p> <pre><code>import { MemorySaver, StateGraph } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n\nawait graph.invoke(\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n  { configurable: { thread_id: \"1\" } }\n);\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#use-in-production","title":"Use in production","text":"<p>In production, use a checkpointer backed by a database:</p> <pre><code>import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n</code></pre> <p>Example: using Postgres checkpointer</p> <pre><code>npm install @langchain/langgraph-checkpoint-postgres\n</code></pre> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n// await checkpointer.setup();\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", async (state) =&gt; {\n    const response = await model.invoke(state.messages);\n    return { messages: [response] };\n  })\n  .addEdge(START, \"call_model\");\n\nconst graph = builder.compile({ checkpointer });\n\nconst config = {\n  configurable: {\n    thread_id: \"1\"\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#use-in-subgraphs","title":"Use in subgraphs","text":"<p>If your graph contains <code>subgraphs</code>, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <pre><code>import { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({ foo: z.string() });\n\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraph_node_1\", (state) =&gt; {\n    return { foo: state.foo + \"bar\" };\n  })\n  .addEdge(START, \"subgraph_node_1\");\nconst subgraph = subgraphBuilder.compile();\n\nconst builder = new StateGraph(State)\n  .addNode(\"node_1\", subgraph)\n  .addEdge(START, \"node_1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n</code></pre> <p>If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in <code>multi-agent</code> systems, if you want agents to keep track of their internal message histories.</p> <pre><code>const subgraphBuilder = new StateGraph(...);\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#add-long-term-memory","title":"Add long-term memory","text":"<p>Use long-term memory to store user-specific or application-specific data across conversations.</p> <pre><code>import { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\n\nconst store = new InMemoryStore();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#use-in-production_1","title":"Use in production","text":"<p>In production, use a store backed by a database:</p> <pre><code>import { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst store = PostgresStore.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n</code></pre> <p>Example: using Postgres store</p> <pre><code>npm install @langchain/langgraph-checkpoint-postgres\n</code></pre> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\nimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\n\nconst store = PostgresStore.fromConnString(DB_URI);\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n// await store.setup();\n// await checkpointer.setup();\n\nconst callModel = async (\n  state: z.infer&lt;typeof MessagesZodState&gt;,\n  config: LangGraphRunnableConfig,\n) =&gt; {\n  const userId = config.configurable?.userId;\n  const namespace = [\"memories\", userId];\n  const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\n  const info = memories?.map(d =&gt; d.value.data).join(\"\\n\") || \"\";\n  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\n\n  // Store new memories if the user asks the model to remember\n  const lastMessage = state.messages.at(-1);\n  if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\n    const memory = \"User name is Bob\";\n    await config.store?.put(namespace, uuidv4(), { data: memory });\n  }\n\n  const response = await model.invoke([\n    { role: \"system\", content: systemMsg },\n    ...state.messages\n  ]);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addEdge(START, \"call_model\");\n\nconst graph = builder.compile({\n  checkpointer,\n  store,\n});\n\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n    userId: \"1\",\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n\nconst config2 = {\n  configurable: {\n    thread_id: \"2\",\n    userId: \"1\",\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"what is my name?\" }] },\n  { ...config2, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#use-semantic-search","title":"Use semantic search","text":"<p>Enable semantic search in your graph\u2019s memory store to let graph agents search for items in the store by semantic similarity.</p> <pre><code>import { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\n// Create store with semantic search enabled\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\nconst store = new InMemoryStore({\n  index: {\n    embeddings,\n    dims: 1536,\n  },\n});\n\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\nconst items = await store.search([\"user_123\", \"memories\"], {\n  query: \"I'm hungry\",\n  limit: 1,\n});\n</code></pre> <p>Long-term memory with semantic search</p> <pre><code>import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START, MessagesZodMeta, InMemoryStore } from \"@langchain/langgraph\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst MessagesZodState = z.object({\n    messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\n// Create store with semantic search enabled\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\nconst store = new InMemoryStore({\n    index: {\n    embeddings,\n    dims: 1536,\n    }\n});\n\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\nconst chat = async (state: z.infer&lt;typeof MessagesZodState&gt;, config) =&gt; {\n    // Search based on user's last message\n    const items = await config.store.search(\n    [\"user_123\", \"memories\"],\n    { query: state.messages.at(-1)?.content, limit: 2 }\n    );\n    const memories = items.map(item =&gt; item.value.text).join(\"\\n\");\n    const memoriesText = memories ? `## Memories of user\\n${memories}` : \"\";\n\n    const response = await model.invoke([\n    { role: \"system\", content: `You are a helpful assistant.\\n${memoriesText}` },\n    ...state.messages,\n    ]);\n\n    return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n    .addNode(\"chat\", chat)\n    .addEdge(START, \"chat\");\nconst graph = builder.compile({ store });\n\nfor await (const [message, metadata] of await graph.stream(\n    { messages: [{ role: \"user\", content: \"I'm hungry\" }] },\n    { streamMode: \"messages\" }\n)) {\n    if (message.content) {\n    console.log(message.content);\n    }\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#manage-short-term-memory","title":"Manage short-term memory","text":"<p>With <code>short-term memory</code> enabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:</p> <ul> <li><code>Trim messages:</code> Remove first or last N messages (before calling LLM)</li> <li><code>Delete messages</code> from LangGraph state permanently</li> <li><code>Summarize messages:</code> Summarize earlier messages in the history and replace them with a summary</li> <li><code>Manage checkpoints</code> to store and retrieve message history</li> <li>Custom strategies (e.g., message filtering, etc.)</li> </ul> <p>This allows the agent to keep track of the conversation without exceeding the LLM\u2019s context window.</p>"},{"location":"AgenticAI/LangGraph/memory/#trim-messages","title":"Trim messages","text":"<ul> <li>Most LLMs have a maximum supported context window (denominated in tokens). </li> <li>One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit.</li> <li>If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last maxTokens) to use for handling the boundary.</li> </ul> <p>To trim message history, use the <code>trimMessages</code> function:</p> <pre><code>import { trimMessages, BaseMessage } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, START, MessagesZodMeta, MemorySaver } from \"@langchain/langgraph\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = trimMessages(state.messages, {\n    strategy: \"last\",\n    maxTokens: 128,\n    startOn: \"human\",\n    endOn: [\"human\", \"tool\"],\n    tokenCounter: model,\n  });\n  const response = await model.invoke(messages);\n  return { messages: [response] };\n};\n\nconst checkpointer = new MemorySaver();\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addEdge(START, \"call_model\");\nconst graph = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\nawait graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\nawait graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\nconst finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what's my name?\" }] }, config);\n\nconsole.log(finalResponse.messages.at(-1)?.content);\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#delete-messages","title":"Delete messages","text":"<ul> <li>You can delete messages from the graph state to manage the message history.</li> <li>This is useful when you want to remove specific messages or clear the entire message history.</li> <li>To delete messages from the graph state, you can use the <code>RemoveMessage</code>. </li> <li>For RemoveMessage to work, you need to use a state key with <code>messagesStateReducer</code> reducer, like <code>MessagesZodState</code>.</li> </ul> <p>To remove specific messages:</p> <pre><code>import { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) =&gt; {\n  const messages = state.messages;\n  if (messages.length &gt; 2) {\n    // remove the earliest two messages\n    return {\n      messages: messages\n        .slice(0, 2)\n        .map((m) =&gt; new RemoveMessage({ id: m.id })),\n    };\n  }\n};\n</code></pre> <p>Full example: delete messages</p> <pre><code>import { RemoveMessage, BaseMessage } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, START, MemorySaver, MessagesZodMeta } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\nimport { registry } from \"@langchain/langgraph/zod\";\n\nconst MessagesZodState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n});\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\nconst deleteMessages = (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = state.messages;\n  if (messages.length &gt; 2) {\n    // remove the earliest two messages\n    return { messages: messages.slice(0, 2).map(m =&gt; new RemoveMessage({ id: m.id })) };\n  }\n  return {};\n};\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(state.messages);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addNode(\"delete_messages\", deleteMessages)\n  .addEdge(START, \"call_model\")\n  .addEdge(\"call_model\", \"delete_messages\");\n\nconst checkpointer = new MemorySaver();\nconst app = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nfor await (const event of await app.stream(\n  { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(event.messages.map(message =&gt; [message.getType(), message.content]));\n}\n\nfor await (const event of await app.stream(\n  { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(event.messages.map(message =&gt; [message.getType(), message.content]));\n}\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#summarize-messages","title":"Summarize messages","text":"<p>The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.</p> <p></p> <ul> <li>Prompting and orchestration logic can be used to summarize the message history.</li> <li>For example, in LangGraph you can include a summary key in the state alongside the messages key:</li> </ul> <pre><code>import { BaseMessage } from \"@langchain/core/messages\";\nimport { MessagesZodMeta } from \"@langchain/langgraph\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n  summary: z.string().optional(),\n});\n</code></pre> <p>Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This <code>summarizeConversation</code> node can be called after some number of messages have accumulated in the <code>messages</code> state key.</p> <pre><code>import { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst summarizeConversation = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // First, we get any existing summary\n  const summary = state.summary || \"\";\n\n  // Create our summarization prompt\n  let summaryMessage: string;\n  if (summary) {\n    // A summary already exists\n    summaryMessage =\n      `This is a summary of the conversation to date: ${summary}\\n\\n` +\n      \"Extend the summary by taking into account the new messages above:\";\n  } else {\n    summaryMessage = \"Create a summary of the conversation above:\";\n  }\n\n  // Add prompt to our history\n  const messages = [\n    ...state.messages,\n    new HumanMessage({ content: summaryMessage })\n  ];\n  const response = await model.invoke(messages);\n\n  // Delete all but the 2 most recent messages\n  const deleteMessages = state.messages\n    .slice(0, -2)\n    .map(m =&gt; new RemoveMessage({ id: m.id }));\n\n  return {\n    summary: response.content,\n    messages: deleteMessages\n  };\n};\n</code></pre> <p>Full example: summarize messages</p> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport {\n  SystemMessage,\n  HumanMessage,\n  RemoveMessage,\n  type BaseMessage\n} from \"@langchain/core/messages\";\nimport {\n  MessagesZodMeta,\n  StateGraph,\n  START,\n  END,\n  MemorySaver,\n} from \"@langchain/langgraph\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst memory = new MemorySaver();\n\n// We will add a `summary` attribute (in addition to `messages` key,\n// which MessagesZodState already has)\nconst GraphState = z.object({\n  messages: z\n    .array(z.custom&lt;BaseMessage&gt;())\n    .register(registry, MessagesZodMeta),\n  summary: z.string().default(\"\"),\n});\n\n// We will use this model for both the conversation and the summarization\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\n// Define the logic to call the model\nconst callModel = async (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  // If a summary exists, we add this in as a system message\n  const { summary } = state;\n  let { messages } = state;\n  if (summary) {\n    const systemMessage = new SystemMessage({\n      id: uuidv4(),\n      content: `Summary of conversation earlier: ${summary}`,\n    });\n    messages = [systemMessage, ...messages];\n  }\n  const response = await model.invoke(messages);\n  // We return an object, because this will get added to the existing state\n  return { messages: [response] };\n};\n\n// We now define the logic for determining whether to end or summarize the conversation\nconst shouldContinue = (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  const messages = state.messages;\n  // If there are more than six messages, then we summarize the conversation\n  if (messages.length &gt; 6) {\n    return \"summarize_conversation\";\n  }\n  // Otherwise we can just end\n  return END;\n};\n\nconst summarizeConversation = async (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  // First, we summarize the conversation\n  const { summary, messages } = state;\n  let summaryMessage: string;\n  if (summary) {\n    // If a summary already exists, we use a different system prompt\n    // to summarize it than if one didn't\n    summaryMessage =\n      `This is summary of the conversation to date: ${summary}\\n\\n` +\n      \"Extend the summary by taking into account the new messages above:\";\n  } else {\n    summaryMessage = \"Create a summary of the conversation above:\";\n  }\n\n  const allMessages = [\n    ...messages,\n    new HumanMessage({ id: uuidv4(), content: summaryMessage }),\n  ];\n\n  const response = await model.invoke(allMessages);\n\n  // We now need to delete messages that we no longer want to show up\n  // I will delete all but the last two messages, but you can change this\n  const deleteMessages = messages\n    .slice(0, -2)\n    .map((m) =&gt; new RemoveMessage({ id: m.id! }));\n\n  if (typeof response.content !== \"string\") {\n    throw new Error(\"Expected a string response from the model\");\n  }\n\n  return { summary: response.content, messages: deleteMessages };\n};\n\n// Define a new graph\nconst workflow = new StateGraph(GraphState)\n  // Define the conversation node and the summarize node\n  .addNode(\"conversation\", callModel)\n  .addNode(\"summarize_conversation\", summarizeConversation)\n  // Set the entrypoint as conversation\n  .addEdge(START, \"conversation\")\n  // We now add a conditional edge\n  .addConditionalEdges(\n    // First, we define the start node. We use `conversation`.\n    // This means these are the edges taken after the `conversation` node is called.\n    \"conversation\",\n    // Next, we pass in the function that will determine which node is called next.\n    shouldContinue,\n  )\n  // We now add a normal edge from `summarize_conversation` to END.\n  // This means that after `summarize_conversation` is called, we end.\n  .addEdge(\"summarize_conversation\", END);\n\n// Finally, we compile it!\nconst app = workflow.compile({ checkpointer: memory });\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#manage-checkpoints","title":"Manage checkpoints","text":"<p>You can view and delete the information stored by the checkpointer.</p> <p>View thread state</p> <pre><code>const config = {\n  configurable: {\n    thread_id: \"1\",\n    // optionally provide an ID for a specific checkpoint,\n    // otherwise the latest checkpoint is shown\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n  },\n};\nawait graph.getState(config);\n</code></pre> <pre><code>{\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\n  next: [],\n  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n  metadata: {\n    source: 'loop',\n    writes: { call_model: { messages: AIMessage(...) } },\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  createdAt: '2025-05-05T16:01:24.680462+00:00',\n  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n  tasks: [],\n  interrupts: []\n}\n</code></pre> <p>View the history of the thread</p> <pre><code>const config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nconst history = [];\nfor await (const state of graph.getStateHistory(config)) {\n  history.push(state);\n}\n</code></pre> <p>Delete all checkpoints for a thread</p> <pre><code>const threadId = \"1\";\nawait checkpointer.deleteThread(threadId);\n</code></pre>"},{"location":"AgenticAI/LangGraph/memory/#database-management","title":"Database management","text":"<p>If you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database. By convention, most database-specific libraries define a <code>setup()</code> method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of <code>BaseCheckpointSaver</code> or <code>BaseStore</code> to confirm the exact method name and usage.</p>"},{"location":"AgenticAI/LangGraph/node/","title":"Nodes","text":"<p>In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:</p> <ol> <li> <p>state \u2013 The state of the graph</p> </li> <li> <p>config \u2013 A <code>RunnableConfig</code> object that contains configuration information like <code>thread_id</code> and tracing information like <code>tags</code></p> </li> <li> <p>runtime \u2013 A Runtime object that contains runtime context and other information like store and stream_writer</p> </li> </ol> <p>Similar to NetworkX, you add these nodes to a graph using the add_node method:</p> <pre><code>from dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n...\n</code></pre>"},{"location":"AgenticAI/LangGraph/node/#start-node","title":"START Node","text":"<p>The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.</p> <pre><code>from langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n</code></pre>"},{"location":"AgenticAI/LangGraph/node/#end-node","title":"END Node","text":"<p>The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.</p> <pre><code>from langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n</code></pre>"},{"location":"AgenticAI/LangGraph/node/#node-caching","title":"Node Caching","text":"<p>LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:</p> <ul> <li>Specify a cache when compiling a graph (or specifying an entrypoint)</li> <li>Specify a cache policy for nodes. Each cache policy supports:<ul> <li><code>key_func</code> used to generate a cache key based on the input to a node, which defaults to a <code>hash</code> of the input with pickle.</li> <li><code>ttl</code>, the time to live for the cache in seconds. If not specified, the cache will never expire.</li> </ul> </li> </ul> <pre><code>import time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -&gt; dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))    \n# [{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))    \n# [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n</code></pre> <ol> <li>First run takes two seconds to run (due to mocked expensive computation).</li> <li>Second run utilizes cache and returns quickly.</li> </ol>"},{"location":"AgenticAI/LangGraph/persistence/","title":"Persistence","text":"<p>LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph\u2019s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we\u2019ll discuss each of these concepts in more detail.</p> <p></p> <p>Note: LangGraph API handles checkpointing automatically When using the LangGraph API, you don\u2019t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.</p>"},{"location":"AgenticAI/LangGraph/persistence/#threads","title":"Threads","text":"<ul> <li>A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer.</li> <li>It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.</li> </ul> <p>When invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config.</p> <pre><code>{\"configurable\": {\"thread_id\": \"1\"}}\n</code></pre> <p>A thread\u2019s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state.</p>"},{"location":"AgenticAI/LangGraph/persistence/#checkpoints","title":"Checkpoints","text":"<p>The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by <code>StateSnapshot</code> object with the following key properties:</p> <ul> <li>config: Config associated with this checkpoint.</li> <li>metadata: Metadata associated with this checkpoint.</li> <li>values: Values of the state channels at this point in time.</li> <li>next A tuple of the node names to execute next in the graph.</li> <li>tasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.</li> </ul> <p>Checkpoints are persisted and can be used to restore the state of a thread at a later time.</p> <p>Let\u2019s see what checkpoints are saved when a simple graph is invoked as follows:</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n</code></pre> <p>After we run the graph, we expect to see exactly 4 checkpoints:</p> <ul> <li>Empty checkpoint with START as the next node to be executed</li> <li>Checkpoint with the user input <code>{'foo': '', 'bar': []}</code> and <code>node_a</code> as the next node to be executed</li> <li>Checkpoint with the outputs of <code>node_a</code> <code>{'foo': 'a', 'bar': ['a']}</code> and <code>node_b</code> as the next node to be executed</li> </ul> <p>Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel.</p>"},{"location":"AgenticAI/LangGraph/persistence/#get-state","title":"Get state","text":"<p>When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.</p> <pre><code># get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n</code></pre> <p>In our example, the output of <code>get_state</code> will look like this:</p> <pre><code>StateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n</code></pre>"},{"location":"AgenticAI/LangGraph/persistence/#get-state-history","title":"Get state history","text":"<p>You can get the full history of the graph execution for a given thread by calling <code>graph.get_state_history(config)</code>.  This will return a list of <code>StateSnapshot</code> objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / <code>StateSnapshot</code> being the first in the list.</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n</code></pre> <p>In our example, the output of <code>get_state_history</code> will look like this:</p> <pre><code>[\n    StateSnapshot(\n        values={'foo': 'b', 'bar': ['a', 'b']},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n        created_at='2024-08-29T19:19:38.821749+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        tasks=(),\n    ),\n    StateSnapshot(\n        values={'foo': 'a', 'bar': ['a']},\n        next=('node_b',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\n        created_at='2024-08-29T19:19:38.819946+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'foo': '', 'bar': []},\n        next=('node_a',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\n        created_at='2024-08-29T19:19:38.817813+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'bar': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\n        created_at='2024-08-29T19:19:38.816205+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n    )\n]\n</code></pre> <p></p>"},{"location":"AgenticAI/LangGraph/persistence/#replay","title":"Replay","text":"<p>It\u2019s also possible to play-back a prior graph execution. If we <code>invoke</code> a graph with a <code>thread_id</code> and a <code>checkpoint_id</code>, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.</p> <ul> <li>thread_id is the ID of a thread.</li> <li>checkpoint_id is an identifier that refers to a specific checkpoint within a thread.</li> </ul> <p>You must pass these when invoking the graph as part of the configurable portion of the config:</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n</code></pre> <p>Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously.</p> <p></p>"},{"location":"AgenticAI/LangGraph/persistence/#update-state","title":"Update state","text":"<p>In addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using update_state. This method accepts three different arguments:</p> <p>config The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.</p> <p>values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let\u2019s walk through an example</p> <p>Let\u2019s assume you have defined the state of your graph with the following schema (see full example above):</p> <pre><code>from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n</code></pre> <p>Let\u2019s now assume the current state of the graph is</p> <pre><code>{\"foo\": 1, \"bar\": [\"a\"]}\n</code></pre> <p>If you update the state as below:</p> <pre><code>graph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n</code></pre> <p>Then the new state of the graph will be:</p> <pre><code>{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n</code></pre> <p>The foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar.</p> <p>as_node</p> <p>The final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. </p> <p></p>"},{"location":"AgenticAI/LangGraph/persistence/#memory-store","title":"Memory Store","text":"<p>A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.</p> <p>But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!</p> <p>With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new <code>in_memory_store</code> variable.</p> <p>Note: LangGraph API handles stores automatically When using the LangGraph API, you don\u2019t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.</p>"},{"location":"AgenticAI/LangGraph/persistence/#basic-usage","title":"Basic Usage","text":"<p>First, let\u2019s showcase this in isolation without using LangGraph.</p> <pre><code>from langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n</code></pre> <p>Memories are namespaced by a tuple, which in this specific example will be <code>(&lt;user_id&gt;, \"memories\")</code>. The namespace can be any length and represent anything, does not have to be user specific.</p> <pre><code>user_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n</code></pre> <p>We use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself.</p> <pre><code>memory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n</code></pre> <p>We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.</p> <pre><code>memories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n</code></pre> <p>Each memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above.</p> <p>The attributes it has are:</p> <ul> <li>value: The value (itself a dictionary) of this memory</li> <li>key: A unique key for this memory in this namespace</li> <li>namespace: A list of strings, the namespace of this memory type</li> <li>created_at: Timestamp for when this memory was created</li> <li>updated_at: Timestamp for when this memory was updated</li> </ul>"},{"location":"AgenticAI/LangGraph/persistence/#semantic-search","title":"Semantic Search","text":"<p>Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:</p> <pre><code>from langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n</code></pre> <p>Now when searching, you can use natural language queries to find relevant memories:</p> <pre><code># Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n</code></pre> <p>You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:</p> <pre><code># Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n</code></pre>"},{"location":"AgenticAI/LangGraph/persistence/#using-in-langgraph","title":"Using in LangGraph","text":"<p>With this all in place, we use the <code>in_memory_store</code> in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows.</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n</code></pre> <p>We invoke the graph with a thread_id, as before, and also with a user_id, which we\u2019ll use to namespace our memories to this particular user as we showed above.</p> <pre><code># Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n</code></pre> <p>We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here\u2019s how we might use semantic search in a node to find relevant memories:</p> <pre><code>def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n</code></pre> <p>As we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.</p> <pre><code>memories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n</code></pre> <p>We can access the memories and use them in our model call.</p> <pre><code>def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n</code></pre> <p>If we create a new thread, we can still access the same memories so long as the user_id is the same.</p> <pre><code># Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n</code></pre> <p>When we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:</p> <pre><code>{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n</code></pre> <p>Deployment guide: https://docs.langchain.com/langsmith/semantic-search</p>"},{"location":"AgenticAI/LangGraph/persistence/#checkpointer-libraries","title":"Checkpointer libraries","text":"<p>Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:</p> <ul> <li>langgraph-checkpoint: The base interface for checkpointer savers.</li> <li>(BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol).</li> <li>Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. </li> <li> <p>LangGraph comes with langgraph-checkpoint included.</p> </li> <li> <p>langgraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.</p> </li> <li> <p>langgraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangSmith. Ideal for using in production. Needs to be installed separately.</p> </li> </ul>"},{"location":"AgenticAI/LangGraph/persistence/#checkpointer-interface","title":"Checkpointer interface","text":"<p>Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:</p> <ul> <li>.put - Store a checkpoint with its configuration and metadata.</li> <li>.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).</li> <li>.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().</li> <li>.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()</li> </ul> <p>If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via <code>.ainvoke</code>, <code>.astream</code>, .<code>abatch</code>), asynchronous versions of the above methods will be used (<code>.aput</code>, <code>.aput_writes</code>, <code>.aget_tuple</code>, <code>.alist</code>).</p>"},{"location":"AgenticAI/LangGraph/persistence/#serializer","title":"Serializer","text":"<p>When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.</p> <p>langgraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.</p>"},{"location":"AgenticAI/LangGraph/persistence/#serialization-with-pickle","title":"Serialization with pickle","text":"<p>The default serializer, JsonPlusSerializer, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.</p> <p>If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes), you can use the pickle_fallback argument of the JsonPlusSerializer:</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n</code></pre>"},{"location":"AgenticAI/LangGraph/persistence/#encryption","title":"Encryption","text":"<p>Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. </p> <p>The easiest way to create an encrypted serializer is via from_pycryptodome_aes, which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):</p> <pre><code>import sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n</code></pre> <pre><code>from langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n</code></pre> <p>When running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer.</p>"},{"location":"AgenticAI/LangGraph/persistence/#capabilities","title":"Capabilities","text":"<ul> <li> <p>Human-in-the-loop First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state.</p> </li> <li> <p>Memory Second, checkpointers allow for \u201cmemory\u201d between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.</p> </li> <li> <p>Time Travel Third, checkpointers allow for \u201ctime travel\u201d, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.</p> </li> <li> <p>Fault-tolerance Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don\u2019t re-run the successful nodes.</p> </li> <li> <p>Pending writes Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don\u2019t re-run the successful nodes.</p> </li> </ul>"},{"location":"AgenticAI/LangGraph/streaming/","title":"Streaming","text":"<p>LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.</p> <p>What\u2019s possible with LangGraph streaming:</p> <ul> <li>Stream graph state \u2014 get state updates / values with updates and values modes.</li> <li>Stream subgraph outputs \u2014 include outputs from both the parent graph and any nested subgraphs.</li> <li>Stream LLM tokens \u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools.</li> <li>Stream custom data \u2014 send custom updates or progress signals directly from tool functions.</li> <li>Use multiple streaming modes \u2014 choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).</li> </ul>"},{"location":"AgenticAI/LangGraph/streaming/#supported-stream-modes","title":"Supported stream modes","text":"<p>Pass one or more of the following stream modes as a list to the stream or astream methods:</p> Mode Description values Streams the full value of the state after each step of the graph. updates Streams the updates to the state after each step of the graph. If multiple updates occur in the same step (e.g., multiple nodes run), each update is streamed separately. custom Streams custom data emitted from inside your graph nodes. messages Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. debug Streams all available information throughout the graph execution."},{"location":"AgenticAI/LangGraph/streaming/#basic-usage-example","title":"Basic usage example","text":"<p>LangGraph graphs expose the <code>stream</code> (sync) and <code>astream</code> (async) methods to yield streamed outputs as iterators.</p> <pre><code>for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n    print(chunk)\n</code></pre> <pre><code>from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(refine_topic)\n    .add_node(generate_joke)\n    .add_edge(START, \"refine_topic\")\n    .add_edge(\"refine_topic\", \"generate_joke\")\n    .add_edge(\"generate_joke\", END)\n    .compile()\n)\n\n# The stream() method returns an iterator that yields streamed outputs\nfor chunk in graph.stream(  \n    {\"topic\": \"ice cream\"},\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n    # Other stream modes are also available. See supported stream modes for details\n    stream_mode=\"updates\",  \n):\n    print(chunk)\n</code></pre>"},{"location":"AgenticAI/LangGraph/streaming/#stream-multiple-modes","title":"Stream multiple modes","text":"<p>You can pass a list as the stream_mode parameter to stream multiple modes at once. The streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.</p> <pre><code>for mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n</code></pre>"},{"location":"AgenticAI/LangGraph/streaming/#stream-graph-state","title":"Stream graph state","text":"<p>Use the stream modes updates and values to stream the state of the graph as it executes.</p> <ul> <li>updates streams the updates to the state after each step of the graph.</li> <li>values streams the full value of the state after each step of the graph.</li> </ul> <pre><code>from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)\n</code></pre> <p>More info link: https://docs.langchain.com/oss/python/langgraph/streaming#stream-graph-state</p>"},{"location":"AgenticAI/LangGraph/time-travel/","title":"Use time-travel","text":"<p>When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:</p> <ol> <li>Understand reasoning: Analyze the steps that led to a successful result.</li> <li>Debug mistakes: Identify where and why errors occurred.</li> <li>Explore alternatives: Test different paths to uncover better solutions.</li> </ol> <p>LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint \u2014 either replaying the same state or modifying it to explore alternatives.In all cases, resuming past execution produces a new fork in the history.</p> <p>To use time-travel in LangGraph:</p> <ol> <li>Run the graph with initial inputs using <code>invoke</code> or <code>stream</code> methods.</li> <li> <p>Identify a checkpoint in an existing thread: Use the <code>getStateHistory</code> method to retrieve the execution history for a specific <code>thread_id</code> and locate the desired ```checkpoint_id. Alternatively, set a breakpoint before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.</p> </li> <li> <p>Update the graph state (optional): Use the updateState method to modify the graph\u2019s state at the checkpoint and resume execution from alternative state.</p> </li> <li>Resume execution from the checkpoint: Use the invoke or stream methods with an input of null and a configuration containing the appropriate thread_id and checkpoint_id.</li> </ol> <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport * as z from \"zod\";\nimport { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst State = z.object({\n  topic: z.string().optional(),\n  joke: z.string().optional(),\n});\n\nconst model = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n  temperature: 0,\n});\n\n// Build workflow\nconst workflow = new StateGraph(State)\n  // Add nodes\n  .addNode(\"generateTopic\", async (state) =&gt; {\n    // LLM call to generate a topic for the joke\n    const msg = await model.invoke(\"Give me a funny topic for a joke\");\n    return { topic: msg.content };\n  })\n  .addNode(\"writeJoke\", async (state) =&gt; {\n    // LLM call to write a joke based on the topic\n    const msg = await model.invoke(`Write a short joke about ${state.topic}`);\n    return { joke: msg.content };\n  })\n  // Add edges to connect nodes\n  .addEdge(START, \"generateTopic\")\n  .addEdge(\"generateTopic\", \"writeJoke\")\n  .addEdge(\"writeJoke\", END);\n\n// Compile\nconst checkpointer = new MemorySaver();\nconst graph = workflow.compile({ checkpointer });\n</code></pre> <p>1. Run the graph</p> <pre><code>const config = {\n  configurable: {\n    thread_id: uuidv4(),\n  },\n};\n\nconst state = await graph.invoke({}, config);\n\nconsole.log(state.topic);\nconsole.log();\nconsole.log(state.joke);\n</code></pre> <p>2. Identify a checkpoint</p> <pre><code>// The states are returned in reverse chronological order.\nconst states = [];\nfor await (const state of graph.getStateHistory(config)) {\n  states.push(state);\n}\n\nfor (const state of states) {\n  console.log(state.next);\n  console.log(state.config.configurable?.checkpoint_id);\n  console.log();\n}\n</code></pre> <pre><code>// This is the state before last (states are listed in chronological order)\nconst selectedState = states[1];\nconsole.log(selectedState.next);\nconsole.log(selectedState.values);\n</code></pre> <p>3. Update the state</p> <p>updateState will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.</p> <pre><code>const newConfig = await graph.updateState(selectedState.config, {\n  topic: \"chickens\",\n});\nconsole.log(newConfig);\n</code></pre> <p>4. Resume execution from the checkpoint</p> <pre><code>await graph.invoke(null, newConfig);\n</code></pre>"},{"location":"Computer-Vision/CNNs/","title":"CNN (Convolutional Neural Network)","text":"<p>A Convolutional Neural Network (CNN) is a specialized type of deep learning neural network designed mainly for processing visual data such as images and videos. CNNs automatically learn spatial features (<code>edges, textures, shapes, objects</code>) from images without manual feature engineering.</p>"},{"location":"Computer-Vision/CNNs/#why-cnn-was-invented-problem-with-traditional-neural-networks","title":"Why CNN was invented (Problem with traditional Neural Networks)","text":"<p>If you use a fully connected neural network on an image:</p> <ul> <li>A small image (<code>224\u00d7224\u00d73</code>) \u2192 <code>150,528</code> input neurons</li> <li>Too many parameters \u2192 <code>slow</code>, <code>overfitting</code></li> <li>Loses spatial information (pixels near each other matter!)</li> </ul>"},{"location":"Computer-Vision/CNNs/#cnn-solves-this-by","title":"\ud83d\udc49 CNN solves this by:","text":"<ul> <li>Using local connectivity</li> <li>Sharing weights</li> <li>Preserving spatial structure</li> </ul>"},{"location":"Computer-Vision/CNNs/#core-idea-of-cnn","title":"Core Idea of CNN","text":"<p>CNN works based on three key concepts:</p> <ol> <li>Local receptive fields</li> <li>Shared weights (filters/kernels)</li> <li>Spatial hierarchy of features</li> </ol>"},{"location":"Computer-Vision/CNNs/#cnn-architecture-high-level-flow","title":"CNN Architecture (High-Level Flow)","text":""},{"location":"Computer-Vision/CNNs/#main-building-blocks-of-cnn","title":"Main Building Blocks of CNN","text":"<ol> <li>Convolution Layer (Feature Extraction)</li> <li>Activation Function (ReLU)</li> <li>Pooling Layer (Downsampling)</li> <li>Flatten Layer</li> <li>Fully Connected (Dense) Layer</li> <li>Output Layer</li> </ol> <p>1. Convolution Layer (Feature Extraction)</p> <ul> <li> <p>Applies filters (kernels) like <code>3\u00d73</code>, <code>5\u00d75</code> on the image</p> </li> <li> <p>Each filter detects <code>specific patterns</code></p> </li> </ul> <p>Example filters:</p> <ul> <li> <p>Edge detector</p> </li> <li> <p>Corner detector</p> </li> <li> <p>Texture detector</p> </li> </ul> <p>Mathematically: <code>Feature Map = Input \u2297 Kernel + Bias</code></p> <p>\ud83d\udccc Output = <code>Feature Map</code></p> <p>2. Activation Function (ReLU)</p> <p>Introduces <code>non-linearity</code></p> <p><code>ReLU(x) = max(0, x)</code></p> <p>Why ReLU?</p> <ul> <li> <p>Faster training</p> </li> <li> <p>Prevents vanishing gradients</p> </li> </ul> <p>3. Pooling Layer (Downsampling)</p> <p>Reduces spatial size while keeping important features.</p> <p>Types:</p> <ul> <li> <p><code>Max Pooling</code> (most common)</p> </li> <li> <p><code>Average Pooling</code></p> </li> </ul> <p>Example: <code>2\u00d72 Max Pool \u2192 takes maximum value</code></p> <p>Benefits:</p> <ul> <li> <p>Reduces computation</p> </li> <li> <p>Makes model <code>translation invariant</code></p> </li> </ul> <p>4. Flatten Layer</p> <p>Converts <code>2D feature maps</code> into <code>1D vector</code></p> <p><code>[H \u00d7 W \u00d7 C] \u2192 [HWC]</code></p> <p>5. Fully Connected (Dense) Layer</p> <ul> <li> <p>Performs <code>high-level reasoning</code></p> </li> <li> <p>Same as traditional neural networks</p> </li> </ul> <p>6. Output Layer</p> <p>Depends on task:</p> <ul> <li> <p>Softmax \u2192 <code>Multi-class classification</code></p> </li> <li> <p>Sigmoid \u2192 <code>Binary classification</code></p> </li> <li> <p>Linear \u2192 <code>Regression</code></p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#convolution-layer-feature-extraction","title":"Convolution Layer (Feature Extraction)","text":"<ul> <li>Applies filters (kernels) like <code>3\u00d73</code>, <code>5\u00d75</code> on the image</li> <li>Each filter detects <code>specific patterns</code></li> </ul> <p>Example filters:</p> <ul> <li>Edge detector</li> <li>Corner detector</li> <li>Texture detector</li> </ul> <p>Mathematically:</p> <p><code>Feature Map = Input \u2297 Kernel + Bias</code></p> <p>\ud83d\udccc Output = <code>Feature Map</code></p>"},{"location":"Computer-Vision/CNNs/#what-is-a-kernel","title":"What is a Kernel?","text":"<p>A Kernel is small weight matrix used to detect patterns like edges, corners, textures. Each window like either <code>3x3</code> or <code>5x5</code> window matrix applied weight in the input indivisual pixcel value with defined weight matrix and filter the feature. </p>"},{"location":"Computer-Vision/CNNs/#example-kernel-as-weights","title":"Example: Kernel as Weights","text":"<p>Input Image (part of it)</p> <p></p> <p></p>"},{"location":"Computer-Vision/CNNs/#1-what-operation-is-this","title":"1\ufe0f\u20e3 What operation is this?","text":"<p>This is a dot product between:</p> <ul> <li>a <code>kernel (filter)</code></li> <li>and a <code>small patch of an image (feature map)</code></li> </ul> <p>This is the core math behind <code>convolution in CNNs</code>.</p>"},{"location":"Computer-Vision/CNNs/#2-how-to-see-it-as-a-cnn-operation","title":"2\ufe0f\u20e3 How to see it as a CNN operation","text":"<p>Kernel (example)</p> <pre><code>K =\n[-1  1  1\n  1  1  0\n  0 -1  0]\n</code></pre> <p>Image patch (example)</p> <pre><code>P =\n[0 0 0\n 0 2 1\n 0 1 0]\n</code></pre>"},{"location":"Computer-Vision/CNNs/#3-element-wise-multiply-sum","title":"3\ufe0f\u20e3 Element-wise multiply + sum","text":"<p>CNN does <code>element-wise multiplication and then adds everything</code>.</p> <ul> <li>Row 1:</li> </ul> <p><code>0\u00d7(-1) + 0\u00d71 + 0\u00d71 = 0</code></p> <ul> <li>Row 2:</li> </ul> <p><code>0\u00d71 + 2\u00d71 + 1\u00d70 = 2</code></p> <ul> <li>Row 3:</li> </ul> <p><code>0\u00d70 + 1\u00d7(-1) + 0\u00d70 = -1</code></p>"},{"location":"Computer-Vision/CNNs/#why-cnn-does-this","title":"Why CNN does this","text":"<p>Each kernel learns to detect <code>patterns</code>:</p> Kernel pattern Detects Vertical edges \u2502 Horizontal edges \u2500 Diagonal edges / Corners \u231f Textures dots, gradients"},{"location":"Computer-Vision/CNNs/#important-cnn-clarification","title":"Important CNN clarification","text":"<p>\u274c CNN does <code>NOT</code> check only center pixel</p> <p>\u2705 CNN looks at <code>all pixels in the window</code>, but the <code>center has higher influence</code> if its weight is larger.</p> <p></p>"},{"location":"Computer-Vision/CNNs/#what-does-32-kernels-mean-in-a-cnn","title":"What does \u201c32 kernels\u201d mean in a CNN?","text":"<p>It means the convolution layer has <code>32 different filters</code>, and <code>each filter has its own set of learnable parameters (weights + bias)</code>.</p> <p>Each kernel learns <code>one pattern</code>.</p>"},{"location":"Computer-Vision/CNNs/#kernel-parameter-list-general-formula","title":"\ud83d\udd39 Kernel parameter list (general formula)","text":"<p>For a Conv2D layer:</p> <ul> <li>Kernel size = <code>Kernel hight(KH)</code> x <code>Kernel width(KW)</code></li> <li>Input channels = <code>Cin</code></li> <li>Number of kernels = <code>Cout</code></li> </ul> <p>Parameters per kernel</p> <ul> <li>(<code>KH</code> \u00d7 <code>KW</code> \u00d7 <code>Cin</code>) + <code>1 bias</code></li> </ul> <p>Total parameters</p> <ul> <li>(<code>KH</code> \u00d7 <code>KW</code> \u00d7 <code>Cin</code> \u00d7 <code>Cout</code>) + <code>Cout</code></li> </ul>"},{"location":"Computer-Vision/CNNs/#example-1-grayscale-image-1-channel","title":"\ud83d\udd39 Example 1: Grayscale image (1 channel)","text":"<p>Configuration</p> <ul> <li>Kernel size = <code>3\u00d73</code></li> <li>Input channels = <code>1</code> (Grayscale image (1 channel))</li> <li>Number of kernels = <code>32</code></li> </ul> <p>Parameters per kernel</p> <pre><code>3 \u00d7 3 \u00d7 1 = 9 weights\n+ 1 bias\n= 10 parameters\n</code></pre>"},{"location":"Computer-Vision/CNNs/#total-parameters","title":"\ud83d\udd22 Total parameters","text":"<p><code>32 \u00d7 10 = 320 parameters</code></p>"},{"location":"Computer-Vision/CNNs/#one-kernel-parameter-list-example","title":"\ud83d\udccc One kernel parameter list (example)","text":"<p>Kernel #1:</p> <pre><code>Weights:\n-0.21   0.05   0.34\n-0.12   0.01   0.56\n-0.09  -0.18   0.42\n\nBias:\n0.03\n</code></pre> <p>Kernel #2:</p> <pre><code>Weights:\n 0.11  -0.44   0.08\n-0.36   0.29  -0.17\n 0.51  -0.06   0.22\n\nBias:\n-0.01\n</code></pre> <p>.....</p> <p><code>Up to Kernel #32</code></p>"},{"location":"Computer-Vision/CNNs/#example-2-rgb-image-3-channels","title":"\ud83d\udd39 Example 2: RGB image (3 channels) \ud83d\udd34\ud83d\udfe2\ud83d\udd35","text":"<p>Configuration</p> <pre><code>Kernel size = 3\u00d73\nInput channels = 3\nNumber of kernels = 32\n</code></pre> <p>One kernel shape</p> <pre><code>3 \u00d7 3 \u00d7 3\n</code></pre> <ul> <li>Think of it as `3 stacked 3\u00d73 matrices:</li> </ul> <pre><code>Kernel #1\nChannel R:\n[ w1 w2 w3\n  w4 w5 w6\n  w7 w8 w9 ]\n\nChannel G:\n[ w10 w11 w12\n  w13 w14 w15\n  w16 w17 w18 ]\n\nChannel B:\n[ w19 w20 w21\n  w22 w23 w24\n  w25 w26 w27 ]\n\nBias: b1\n</code></pre> <p>Parameters per kernel</p> <pre><code>3 \u00d7 3 \u00d7 3 = 27 weights\n+ 1 bias\n= 28 parameters\n</code></pre>"},{"location":"Computer-Vision/CNNs/#total-parameters_1","title":"Total parameters","text":"<pre><code>32 \u00d7 28 = 896 parameters\n</code></pre>"},{"location":"Computer-Vision/CNNs/#what-do-the-32-kernels-usually-learn","title":"What do the 32 kernels usually learn?","text":"<p>Typical first CNN layer:</p> Kernel # Learns 1\u20134 Vertical edges 5\u20138 Horizontal edges 9\u201312 Diagonal edges 13\u201318 Corners 19\u201324 Texture patterns 25\u201332 Color / intensity gradients <p>\u26a0\ufe0f This is learned automatically, not hard-coded.</p>"},{"location":"Computer-Vision/CNNs/#output-shape-with-32-kernels","title":"Output shape with 32 kernels","text":"<pre><code>5 \u00d7 5 \u00d7 1\n</code></pre>"},{"location":"Computer-Vision/CNNs/#example-image-classification-cat-vs-dog","title":"Example: Image Classification (Cat vs Dog)","text":"<ol> <li> <p>Early Conv layers \u2192 detect edges</p> </li> <li> <p>Middle layers \u2192 detect eyes, ears</p> </li> <li> <p>Deep layers \u2192 detect face</p> </li> <li> <p>FC layer \u2192 classify Cat or Dog</p> </li> </ol>"},{"location":"Computer-Vision/CNNs/#simple-cnn-example-conceptual","title":"Simple CNN Example (Conceptual)","text":"<pre><code>Input Image (32\u00d732\u00d73)\n\u2193\nConv(3\u00d73, 32 filters)\n\u2193\nReLU\n\u2193\nMaxPool(2\u00d72)\n\u2193\nConv(3\u00d73, 64 filters)\n\u2193\nReLU\n\u2193\nMaxPool(2\u00d72)\n\u2193\nFlatten\n\u2193\nDense(128)\n\u2193\nSoftmax(10 classes)\n</code></pre>"},{"location":"Computer-Vision/CNNs/#example-1","title":"Example: 1","text":"<ul> <li> <p>Input image size: <code>32 \u00d7 32 (Height \u00d7 Width)</code></p> </li> <li> <p>Kernel window: <code>3 \u00d7 3</code></p> </li> <li> <p>Number of kernels (filters): <code>32</code></p> </li> <li> <p>Stride: <code>1</code></p> </li> <li> <p>Padding: <code>same</code></p> </li> <li> <p>Image type: <code>Grayscale (1 channel)</code></p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#1-total-input-pixel-count","title":"1\ufe0f\u20e3 Total Input Pixel Count","text":"<p>Single image</p> <pre><code>32 \u00d7 32 = 1,024 pixels\n\nwidth = 32\nHight = 32\n</code></pre> <p><code>So input tensor shape is:</code></p> <pre><code>32 \u00d7 32 \u00d7 1\n\nwidth = 32\nHight = 32\nchannel = 1\n</code></pre>"},{"location":"Computer-Vision/CNNs/#2-what-does-one-33-kernel-see","title":"2\ufe0f\u20e3 What does ONE 3\u00d73 kernel see?","text":"<p>A <code>3\u00d73 kernel</code> looks at <code>9 pixels at a time</code>:</p> <pre><code>3 \u00d7 3 = 9 pixel values\n</code></pre> <ul> <li>Each pixel has a <code>weight</code>.</li> </ul>"},{"location":"Computer-Vision/CNNs/#3-parameters-in-one-kernel","title":"3\ufe0f\u20e3 Parameters in ONE kernel","text":"<p>For Grayscale (1 channel)</p> <p>Each kernel has:</p> <ul> <li> <p><code>3 \u00d7 3 \u00d7 1 = 9 weights</code></p> </li> <li> <p><code>+ 1 bias</code></p> </li> </ul> <pre><code>Parameters per kernel = 9 + 1 = 10\n</code></pre>"},{"location":"Computer-Vision/CNNs/#4-total-parameters-for-32-kernels","title":"4\ufe0f\u20e3 Total Parameters for 32 kernels","text":"<pre><code>10 parameters \u00d7 32 kernels = 320 parameters\n</code></pre>"},{"location":"Computer-Vision/CNNs/#total-trainable-parameters-320","title":"\u2705 TOTAL TRAINABLE PARAMETERS = 320","text":"<p>\ud83d\udd11 Important:</p> <ul> <li> <p><code>Parameters do NOT depend on image size</code></p> </li> <li> <p><code>Same kernel is reused everywhere (weight sharing)</code></p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#5-output-feature-map-size","title":"5\ufe0f\u20e3 Output Feature Map Size","text":"<p>Using:</p> <ul> <li> <p>Kernel = <code>3\u00d73</code></p> </li> <li> <p>Stride = <code>1</code></p> </li> <li> <p>Padding = <code>same</code></p> </li> </ul> <p>Since  <code>32 kernels</code>:</p> <pre><code>Output tensor = 32 \u00d7 32 \u00d7 32\n</code></pre>"},{"location":"Computer-Vision/CNNs/#6-total-output-pixel-count-activations","title":"6\ufe0f\u20e3 Total Output Pixel Count (Activations)","text":"<pre><code>32 \u00d7 32 \u00d7 32 = 32,768 values\n</code></pre>"},{"location":"Computer-Vision/CNNs/#7-how-many-times-does-each-kernel-slide","title":"7\ufe0f\u20e3 How many times does each kernel slide?","text":"<p>Each kernel slides to every pixel position:</p> <pre><code>32 \u00d7 32 = 1,024 positions\n</code></pre> <p>Each time it:</p> <ul> <li> <p>Reads <code>9 pixels</code></p> </li> <li> <p>Multiplies by <code>9 weights</code></p> </li> <li> <p>Adds bias</p> </li> <li> <p>Produces <code>1 output value</code></p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#8-total-pixel-multiplications-compute-intuition","title":"8\ufe0f\u20e3 Total pixel multiplications (compute intuition)","text":"<p>Per kernel:</p> <pre><code>1,024 positions \u00d7 9 pixels = 9,216 multiplications\n</code></pre> <p>For 32 kernels:</p> <pre><code>9,216 \u00d7 32 = 294,912 multiplications\n</code></pre>"},{"location":"Computer-Vision/CNNs/#9-final-summary-table","title":"9\ufe0f\u20e3 Final Summary Table","text":"Item Value Input pixels 1,024 Kernel size 3 \u00d7 3 Pixels per kernel window 9 Number of kernels 32 Parameters per kernel 10 \ud83d\udd25 Total parameters 320 Output size 32 \u00d7 32 \u00d7 32 Output values 32,768"},{"location":"Computer-Vision/CNNs/#if-image-was-rgb-3-channels","title":"\ud83d\udd01 If Image Was RGB (3 channels)","text":"<p>Just one change \ud83d\udc47</p> <p>Parameters per kernel:</p> <pre><code>3 \u00d7 3 \u00d7 3 + 1 = 28\n</code></pre> <p>Total parameters:</p> <pre><code>28 \u00d7 32 = 896\n</code></pre> <p>Everything else stays the same.</p>"},{"location":"Computer-Vision/CNNs/#key-cnn-insight-very-important","title":"\ud83e\udde0 Key CNN Insight (Very Important)","text":"<ul> <li> <p>Image size affects computation</p> </li> <li> <p>Kernel count affects learning capacity</p> </li> <li> <p>Kernel size affects local pattern detection</p> </li> <li> <p>Parameter count depends only on:</p> </li> </ul> <pre><code>(kernel height \u00d7 kernel width \u00d7 input channels \u00d7 number of kernels) + biases\n</code></pre>"},{"location":"Computer-Vision/CNNs/#input-image-9-9","title":"\ud83d\udd22 INPUT IMAGE (9 \u00d7 9)","text":"<pre><code>5  2  6  8  2  0  1  2  4\n4  3  4  5  1  9  6  3  7\n3  9  2  4  7  7  6  9  2\n1  3  4  6  8  2  2  1  0\n8  4  6  2  3  1  8  8  6\n5  8  9  0  1  0  2  3  4\n9  2  6  6  3  6  2  1  5\n9  8  8  2  6  3  4  5  7\n4  1  3  9  0  2  8  6  1\n</code></pre>"},{"location":"Computer-Vision/CNNs/#1-convolution-layer","title":"1\ufe0f\u20e3 CONVOLUTION LAYER","text":"<p>Assumptions (standard)</p> <ul> <li> <p>Kernel size: <code>3\u00d73</code></p> </li> <li> <p>Stride: <code>1</code></p> </li> <li> <p>Padding: <code>none (valid)</code></p> </li> <li> <p>1 kernel only (to keep math readable)</p> </li> <li> <p>Bias = <code>0</code></p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#kernel-simple-edge-detector","title":"Kernel (simple edge detector)","text":"<pre><code>1  0 -1\n1  0 -1\n1  0 -1\n</code></pre>"},{"location":"Computer-Vision/CNNs/#first-convolution-calculation-top-left-from-1-position-to-3rd-position","title":"First convolution calculation (top-left - from 1 position to 3rd position)","text":"<p>Image patch (3\u00d73)</p> <pre><code>5  2  6\n4  3  4\n3  9  2\n</code></pre> <p>Multiply + sum</p> <pre><code>(5\u00d71) + (2\u00d70) + (6\u00d7-1)\n(4\u00d71) + (3\u00d70) + (4\u00d7-1)\n(3\u00d71) + (9\u00d70) + (2\u00d7-1)\n\n= (5 - 6) + (4 - 4) + (3 - 2)\n= -1 + 0 + 1\n= 0\n</code></pre> <p>\u27a1\ufe0f First output pixel = 0</p> <p>After applying ReLU</p> <p><code>ReLU(x) = max(0, x)</code></p> <p>So:</p> <p><code>0 \u2192 0</code></p>"},{"location":"Computer-Vision/CNNs/#next-position-slide-right-skip-1-position-from-2nd-position-to-4th-position","title":"Next position (slide right skip 1 position from 2nd position to 4th position)","text":"<p>Patch:</p> <pre><code>2  6  8\n3  4  5\n9  2  4\n</code></pre> <p>Calculation:</p> <pre><code>(2\u00d71) + (6\u00d70) + (8\u00d7-1)\n(3\u00d71) + (4\u00d70) + (5\u00d7-1)\n(9\u00d71) + (2\u00d70) + (4\u00d7-1)\n\n= (2 - 8) + (3 - 5) + (9 - 5)\n= -6 - 2 + 5\n= -3\n</code></pre> <p>\u27a1\ufe0f Second output pixel = -3</p> <p>After applying ReLU</p> <p><code>ReLU(x) = max(0, x)</code></p> <p>So:</p> <p><code>-3 \u2192 0</code></p>"},{"location":"Computer-Vision/CNNs/#convolution-output-size","title":"Convolution Output Size","text":"<ul> <li> <p>Input: <code>9 \u00d7 9</code></p> </li> <li> <p>Kernel: <code>3 \u00d7 3</code></p> </li> <li> <p>Stride: <code>1</code></p> </li> </ul> <p>Output size: = <code>(9 - 3 + 1) = 7 \u00d7 7</code></p>"},{"location":"Computer-Vision/CNNs/#convolution-output-after-relu-simplified","title":"Convolution Output (after ReLU) \u2013 simplified","text":"<pre><code>0  0  2  4  0  0  0\n0  0  3  0  0  0  0\n0  0  2  0  0  0  0\n0  0  0  0  5  0  0\n0  0  0  0  6  0  0\n0  0  0  0  4  0  0\n0  0  0  0  2  0  0\n</code></pre> <p>(Exact values depend on kernel \u2014 this is representative and consistent)</p>"},{"location":"Computer-Vision/CNNs/#max-pooling-layer","title":"MAX POOLING LAYER","text":"<p>Pooling settings</p> <ul> <li> <p>MaxPooling 2\u00d72</p> </li> <li> <p>Stride = 2</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#apply-pooling","title":"Apply pooling","text":"<ul> <li><code>Take max from each 2\u00d72 block:</code></li> </ul> <p>Example:</p> <pre><code>0  0\n0  0\n\u2192 max = 0\n</code></pre>"},{"location":"Computer-Vision/CNNs/#pooling-output-size","title":"Pooling Output Size","text":"<p><code>7 \u00d7 7 \u2192 3 \u00d7 3   (floor division)</code></p>"},{"location":"Computer-Vision/CNNs/#cnn-context-most-common-meaning","title":"CNN context (most common meaning)","text":"<p>This usually happens due to Pooling or Convolution with stride.</p> <p>Example: Max Pooling 2\u00d72 with stride 2</p> <p>Input: <code>7 \u00d7 7 feature map</code></p> <p>Operation:</p> <ul> <li> <p><code>Pool size = 2\u00d72</code></p> </li> <li> <p><code>Stride = 2</code></p> </li> <li> <p><code>No padding</code></p> </li> </ul> <p>Output size formula:</p> <p></p> <p>Where:</p> <ul> <li> <p><code>N=7 (input size)</code></p> </li> <li> <p><code>F=2 (filter / pool size)</code></p> </li> <li> <p><code>S=2 (stride)</code></p> </li> </ul> <p></p> <p>So: <code>7 \u00d7 7  \u2192  3 \u00d7 3</code></p> <p>\ud83d\udd11 What \u201cfloor division\u201d means here</p> <ul> <li> <p><code>5 / 2 = 2.5</code></p> </li> <li> <p><code>floor(2.5) = 2</code></p> </li> <li> <p><code>CNN drops the extra pixel at the edge</code></p> </li> </ul> <p>No partial windows allowed.</p> <p>Visual intuition</p> <pre><code>7 pixels\n\u2b1c\u2b1c \u2b1c\u2b1c \u2b1c\u2b1c \u2b1c   \u2190 last pixel ignored\n</code></pre> <p>Only complete 2\u00d72 windows are used.</p>"},{"location":"Computer-Vision/CNNs/#why-cnns-do-this","title":"Why CNNs do this","text":"<p>\u2714 Keeps output size integer</p> <p>\u2714 Faster computation</p> <p>\u2714 Avoids partial pooling windows</p> <p>\u2714 Standard behavior in most frameworks (TensorFlow / PyTorch)</p>"},{"location":"Computer-Vision/CNNs/#important-note","title":"Important note","text":"<p>If padding was added (same padding), then:</p> <pre><code>7 \u00d7 7 \u2192 4 \u00d7 4\n</code></pre> <ul> <li><code>But with no padding (valid) \u2192 floor division applies</code></li> </ul> <p>7 \u00d7 7 \u2192 3 \u00d7 3 (floor division)</p> <p>\ud83d\udc49 CNN divides the input into fixed-size blocks, drops leftovers, and keeps only full windows.</p>"},{"location":"Computer-Vision/CNNs/#maxpooling-output-33","title":"MaxPooling Output (3\u00d73)","text":"<pre><code>0  4  0\n0  3  0\n0  6  0\n</code></pre>"},{"location":"Computer-Vision/CNNs/#starting-point-after-convolution","title":"Starting Point (after Convolution)","text":"<p>Assume after 1st convolution + ReLU, we already have:</p> <pre><code>7 \u00d7 7 feature map (numeric values)\n\n2  2  1  1  0  0  0\n2  2  1  1  0  0  0\n3  3  5  5  2  2  1\n3  3  5  5  2  2  1\n0  0  2  2  4  4  3\n0  0  2  2  4  4  3\n1  1  0  0  2  2  2\n</code></pre> <p><code>(pixel values \u00d7 kernel weights) + bias \u2192 ReLU</code></p>"},{"location":"Computer-Vision/CNNs/#step-1-max-pooling-2-2-stride-2","title":"STEP 1: MAX POOLING (2 \u00d7 2, stride = 2)","text":"<p>What Max Pooling does</p> <ul> <li> <p>Takes <code>2\u00d72 block</code></p> </li> <li> <p>Picks the <code>maximum value</code></p> </li> <li> <p>Reduces spatial size</p> </li> <li> <p>Keeps strongest feature (edge/texture)</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#why-same-values-repeat-important","title":"Why same values repeat (important!)","text":"<p>Look at this block:</p> <pre><code>2  2\n2  2\n</code></pre> <p>\ud83d\udc49 This means <code>the same local pattern</code> (edge / texture) appears in nearby pixels.</p> <p>CNN kernel:</p> <ul> <li> <p>Slides over image</p> </li> <li> <p>Sees similar pixel neighborhoods</p> </li> <li> <p>Produces similar activation values</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#mapping-numbers-symbols-a-b-c-d","title":"Mapping numbers \u2192 symbols (a, b, c, d)","text":"<p>To simplify explanation, we replace repeated numbers with symbols:</p> Symbol Actual Value Meaning a 2 One type of detected feature b 1 Another feature c 0 No feature d 3 Stronger feature e 5 Very strong feature f 4 Another strong feature <p>So the SAME matrix becomes:</p> <pre><code>a a b b c c c\na a b b c c c\nd d e e b b b\nd d e e b b b\nc c b b f f d\nc c b b f f d\nb b c c b b b\n</code></pre> <p>\u2714 Nothing magical happened</p> <p>\u2714 Only renaming numeric activations</p>"},{"location":"Computer-Vision/CNNs/#max-pooling-2-2-stride-2","title":"MAX POOLING (2 \u00d7 2, stride = 2)","text":"<p>What Max Pooling does</p> <ul> <li> <p>Takes <code>2\u00d72 block</code></p> </li> <li> <p>Picks the <code>maximum value</code></p> </li> <li> <p>Reduces spatial size</p> </li> <li> <p>Keeps strongest feature (edge/texture)</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#pooling-operation-visually","title":"Pooling operation visually","text":"<pre><code>[a a] \u2192 a\n[a a]\n\n[b b] \u2192 b\n[b b]\n\n[c c] \u2192 c\n[c c]\n</code></pre> <p>Do this across the whole image.</p> <p>Size calculation (important)</p> <p></p> <p>Output after Max Pooling</p> <pre><code>3 \u00d7 3 \u00d7 1\n\na   b   c\ne   f   g\ni   j   k\n</code></pre> <p>\u2714 Spatial size reduced</p> <p>\u2714 Important features preserved</p>"},{"location":"Computer-Vision/CNNs/#flatten","title":"FLATTEN","text":"<p>What Flatten does</p> <ul> <li> <p>Converts <code>2D</code> feature maps \u2192 <code>1D</code> vector</p> </li> <li> <p>No learning, only reshaping</p> </li> </ul> <p>Before Flatten</p> <pre><code>3 \u00d7 3 \u00d7 1\n</code></pre> <p>After Flatten</p> <pre><code>[ a, b, c, e, f, g, i, j, k ]\n</code></pre> <p>Size: <code>9 neurons</code></p>"},{"location":"Computer-Vision/CNNs/#dense-fully-connected-layer","title":"DENSE (Fully Connected Layer)","text":"<p>What Dense layer does</p> <ul> <li> <p>Learns global patterns</p> </li> <li> <p>Combines all extracted features</p> </li> <li> <p>Uses <code>weights</code> + <code>bias</code> + <code>activation</code></p> </li> </ul> <p>Dense layer example</p> <p>Assume: <code>Dense(4 neurons)</code></p> <p>Computation</p> <p>For each neuron:</p> <pre><code>y=w1\u200bx1\u200b+w2\u200bx2\u200b+...+w9\u200bx9\u200b+b\n</code></pre> <p>Example:</p> <pre><code>Neuron 1 \u2192 y1\nNeuron 2 \u2192 y2\nNeuron 3 \u2192 y3\nNeuron 4 \u2192 y4\n</code></pre> <p>Output: <code>[ y1, y2, y3, y4 ]</code></p> <p>Apply ReLU:</p> <p><code>max(0, y)</code></p>"},{"location":"Computer-Vision/CNNs/#final-output-layer","title":"FINAL OUTPUT LAYER","text":"<p>Case 1: Binary Classification (e.g., Cat / Dog)</p> <p><code>Dense(1) + Sigmoid</code></p> <p></p> <p>Example: <code>0.87 \u2192 Dog</code></p> <p>Case 2: Multi-class Classification (e.g., Digits 0-9)</p> <p><code>Dense(10) + Softmax</code></p> <p>Example output: <code>[0.01, 0.02, 0.90, 0.01, ...]</code></p> <p>Highest probability \u2192 <code>Predicted class</code></p>"},{"location":"Computer-Vision/CNNs/#key-intuition","title":"Key Intuition","text":"Layer Role Convolution Detect local patterns Max Pooling Keep strongest features Flatten Prepare for decision Dense Learn relationships Output Make prediction"},{"location":"Computer-Vision/CNNs/#how-kernel-works-mathematically","title":"How Kernel Works (Mathematically)","text":"<p>For each position:</p> <pre><code>Output = \u03a3 (Input_pixel \u00d7 Kernel_weight) + bias\n</code></pre> <ul> <li> <p>Each kernel element = <code>weight</code></p> </li> <li> <p>Entire kernel = <code>collection of weights</code></p> </li> <li> <p>One <code>bias</code> per kernel</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#kernel-vs-weight-important-distinction","title":"Kernel vs Weight (Important Distinction)","text":"Term Meaning Weight A single learnable value Kernel / Filter A matrix of weights Bias One extra learnable value per kernel"},{"location":"Computer-Vision/CNNs/#kernel-dimensions-very-important","title":"Kernel Dimensions (Very Important)","text":"<p>Case 1: Grayscale Image</p> <pre><code>Input: 28 \u00d7 28 \u00d7 1\nKernel: 3 \u00d7 3 \u00d7 1\n</code></pre> <p>Case 2: Color Image (RGB)</p> <pre><code>Input: 224 \u00d7 224 \u00d7 3\nKernel: 3 \u00d7 3 \u00d7 3\n</code></pre> <p>\ud83d\udc49 <code>Each kernel spans all input channels</code></p>"},{"location":"Computer-Vision/CNNs/#number-of-kernels-number-of-feature-maps","title":"Number of Kernels = Number of Feature Maps","text":"<p>If:</p> <ul> <li> <p>You use <code>32 kernels</code></p> </li> <li> <p>You get <code>32 feature maps</code></p> </li> </ul> <p>Each kernel learns a <code>different feature</code>.</p>"},{"location":"Computer-Vision/CNNs/#are-kernel-values-fixed","title":"Are Kernel Values Fixed?","text":"<p>\u274c No \u2014 kernels are <code>learned automatically</code> during training.</p> <p>Initially: <code>Random values</code></p> <p>After training: <code>Edge detectors, shape detectors, texture detectors</code></p>"},{"location":"Computer-Vision/CNNs/#kernel-vs-neuron-comparison","title":"Kernel vs Neuron (Comparison)","text":"Fully Connected NN CNN One neuron = many weights One kernel = many weights No spatial meaning Spatial meaning Separate weights per pixel Shared weights"},{"location":"Computer-Vision/CNNs/#why-kernel-weight-sharing-is-powerful","title":"Why Kernel Weight Sharing is Powerful","text":"<ul> <li> <p>Same kernel used across entire image</p> </li> <li> <p>Detects feature <code>anywhere</code></p> </li> <li> <p>Fewer parameters \u2192 faster, better generalization</p> </li> </ul>"},{"location":"Computer-Vision/CNNs/#summary","title":"Summary","text":"<p><code>A kernel in CNN is a small matrix of trainable weights that slides over the input to extract features.</code></p>"},{"location":"Computer-Vision/CNNs/#how-strongly-a-kernel-detected-its-pattern-at-that-location","title":"How strongly a kernel detected its pattern at that location","text":"<p>Example kernel (vertical edge)</p> <pre><code>-1  0  1\n-1  0  1\n-1  0  1\n</code></pre> Output Value Meaning 0 No edge 1\u20132 Weak edge 3\u20134 Medium edge 5+ Strong edge"},{"location":"Computer-Vision/CNNs/#why-max-pooling-picks-a-b-c","title":"Why max pooling picks a, b, c","text":"<p>Take this 2\u00d72 window:</p> <pre><code>a a\na a\n</code></pre> <p>Actual numbers:</p> <pre><code>2 2\n2 2\n</code></pre> <p>MaxPooling picks:</p> <pre><code>max(2,2,2,2) = 2 \u2192 a\n</code></pre> <p>Another window:</p> <pre><code>d d\nd d\n</code></pre> <p>Actual numbers:</p> <pre><code>3 3\n3 3\n</code></pre> <p>MaxPooling picks:</p> <pre><code>3 \u2192 d\n</code></pre>"},{"location":"Computer-Vision/CNNs/#why-letters-help-understanding","title":"Why letters help understanding","text":"<p>Using letters:</p> <ul> <li> <p>Focus on patterns, not arithmetic</p> </li> <li> <p>See feature dominance</p> </li> <li> <p>Understand why pooling keeps strongest signals</p> </li> <li> <p>CNN does NOT care about pixel identity</p> </li> <li> <p>CNN cares about <code>pattern strength</code></p> </li> </ul> <p>Note: <code>a, b, c, d are symbolic names for convolution activations that represent how strongly a kernel detected a specific pattern at each location.</code></p>"},{"location":"Computer-Vision/CNNs/#complete-cnn-architectures","title":"COMPLETE CNN ARCHITECTURES","text":""},{"location":"Computer-Vision/CNNs/#foundational-classic-cnns","title":"\ud83d\udd34 Foundational / Classic CNNs","text":"Architecture Year Key Idea Why It Matters Limitations Typical Use LeNet-5 1998 Conv + Pool + FC First practical CNN Shallow Digit recognition AlexNet 2012 ReLU, Dropout, GPU Started DL revolution Huge params Image classification ZFNet 2013 Better stride &amp; visualization Improved AlexNet Still heavy Research"},{"location":"Computer-Vision/CNNs/#vgg-family","title":"\ud83d\udfe0 VGG Family","text":"Architecture Key Idea Strength Weakness Use Case VGG-16 Stacked 3\u00d73 conv Simple &amp; deep 138M params Transfer learning VGG-19 Deeper VGG High accuracy Very slow Feature extractor"},{"location":"Computer-Vision/CNNs/#inception-family","title":"\ud83d\udfe1 Inception Family","text":"Architecture Key Idea Advantage Limitation Use Case GoogLeNet (v1) Multi-scale conv Efficient Complex ImageNet Inception-v2/v3 Factorized conv Fewer FLOPs Hard to tune Classification Inception-ResNet Inception + Skip Very deep Heavy Research"},{"location":"Computer-Vision/CNNs/#residual-networks","title":"\ud83d\udfe2 Residual Networks","text":"Architecture Key Idea Why Important Use Case ResNet-18/34 Skip connections Solves vanishing gradient Small datasets ResNet-50/101/152 Bottleneck blocks Industry standard Medical, Vision ResNeXt Grouped conv Better accuracy More complex High-end CV"},{"location":"Computer-Vision/CNNs/#dense-connectivity","title":"\ud83d\udd35 Dense Connectivity","text":"Architecture Key Idea Benefit Drawback Use DenseNet All-to-all layer links Feature reuse High memory Medical imaging"},{"location":"Computer-Vision/CNNs/#efficient-mobile-cnns","title":"\ud83d\udfe3 Efficient / Mobile CNNs","text":"Architecture Key Idea Strength Target MobileNet-v1 Depthwise conv Lightweight Mobile MobileNet-v2 Inverted residual Faster Edge MobileNet-v3 NAS optimized Best mobile CNN IoT ShuffleNet Channel shuffle Ultra-light Low power EfficientNet (B0\u2013B7) Compound scaling Best FLOPs/accuracy Cloud &amp; Edge"},{"location":"Computer-Vision/CNNs/#segmentation-cnns","title":"\ud83d\udfe4 Segmentation CNNs","text":"Architecture Key Idea Specialty Use Case FCN Fully conv Pixel labeling Segmentation U-Net \u2b50 Encoder-Decoder + skips Medical king MRI, CT UNet++ Dense skips Better localization Medical DeepLab v3+ Atrous conv + ASPP Large context Autonomous"},{"location":"Computer-Vision/CNNs/#object-detection-cnns","title":"\ud83d\udd36 Object Detection CNNs","text":"Architecture Type Key Idea Speed R-CNN Two-stage Region proposals Slow Fast R-CNN Shared conv Faster Medium Faster R-CNN RPN Accurate Slower YOLO (v1\u2013v9) One-stage Real-time Very Fast SSD One-stage Multi-scale Fast RetinaNet One-stage Focal loss Balanced"},{"location":"Computer-Vision/CNNs/#modern-hybrid-cnns","title":"\ud83e\uddec Modern / Hybrid CNNs","text":"Architecture Key Idea Why Needed ConvNeXt CNN redesigned like ViT CNN revival CoAtNet CNN + Transformer Best of both"},{"location":"Computer-Vision/CNNs/#specialized-cnns","title":"\ud83e\uddea Specialized CNNs","text":"Architecture Purpose Siamese CNN Similarity / Face verification Autoencoder CNN Anomaly detection 3D CNN Video, CT, MRI Graph CNN (GCN) Graph &amp; molecule data"},{"location":"Computer-Vision/CNNs/#cnn-quick-selection-guide-all-cases","title":"CNN QUICK SELECTION GUIDE (ALL CASES)","text":""},{"location":"Computer-Vision/CNNs/#image-classification","title":"\ud83d\uddbc\ufe0f IMAGE CLASSIFICATION","text":"Scenario Best CNN Why Small dataset ResNet-18 / VGG-16 (TL) Transfer learning Medium dataset ResNet-50 Balanced depth &amp; accuracy Very large dataset EfficientNet-B4+ Best accuracy/FLOPs Simple objects AlexNet / VGG Simple patterns Fine-grained classes DenseNet Feature reuse"},{"location":"Computer-Vision/CNNs/#medical-imaging","title":"\ud83c\udfe5 MEDICAL IMAGING","text":"Task Best CNN Why X-ray / CT classification ResNet-50 Robust features MRI segmentation U-Net \u2b50 Pixel-level accuracy Tumor boundary detection UNet++ Dense skip connections Organ segmentation DeepLab v3+ Large context Low data medical cases DenseNet Efficient learning"},{"location":"Computer-Vision/CNNs/#object-detection","title":"\ud83d\ude97 OBJECT DETECTION","text":"Scenario Best CNN Why Real-time detection YOLO (v5\u2013v9) Single-pass inference High accuracy detection Faster R-CNN Region refinement Embedded camera SSD + MobileNet Lightweight Small objects RetinaNet Focal loss Autonomous driving YOLO + ResNet Speed + accuracy"},{"location":"Computer-Vision/CNNs/#video-3d-data","title":"\ud83c\udfa5 VIDEO &amp; 3D DATA","text":"Task Best CNN Why Action recognition 3D CNN (C3D, I3D) Spatiotemporal Medical CT volumes 3D U-Net Volume segmentation Surveillance video 3D ResNet Temporal learning"},{"location":"Computer-Vision/CNNs/#mobile-edge-devices","title":"\ud83d\udcf1 MOBILE / EDGE DEVICES","text":"Constraint Best CNN Why Very low compute MobileNet-v1 Smallest Balanced mobile MobileNet-v2 Inverted residual Best mobile accuracy MobileNet-v3 NAS optimized IoT / microcontrollers ShuffleNet Ultra-light"},{"location":"Computer-Vision/CNNs/#industrial-manufacturing","title":"\u2699\ufe0f INDUSTRIAL &amp; MANUFACTURING","text":"Task Best CNN Why Defect detection EfficientNet High resolution Visual inspection ResNet-50 Stable features Anomaly detection CNN Autoencoder Reconstruction error OCR / document scan VGG / ResNet Texture-based"},{"location":"Computer-Vision/CNNs/#face-biometrics-similarity","title":"\ud83d\udc64 FACE, BIOMETRICS &amp; SIMILARITY","text":"Task Best CNN Why Face recognition ResNet / Inception Deep embeddings Face verification Siamese CNN Distance learning Fingerprint matching Siamese / ResNet Pattern similarity"},{"location":"Computer-Vision/CNNs/#science-special-data","title":"\ud83e\uddec SCIENCE &amp; SPECIAL DATA","text":"Task Best CNN Why Molecule graphs Graph CNN (GCN) Graph structure Satellite imagery ResNet / EfficientNet Multi-scale Remote sensing DeepLab Pixel precision"},{"location":"Computer-Vision/CNNs/#nlp-vision-hybrid","title":"\ud83e\udde0 NLP + VISION (HYBRID)","text":"Task Best CNN Why Image captioning CNN + LSTM/Transformer Visual \u2192 text OCR + text CNN + Transformer Layout learning Multimodal AI ConvNeXt / CoAtNet CNN + ViT"},{"location":"Computer-Vision/CNNs/#research-experimentation","title":"\ud83e\uddea RESEARCH &amp; EXPERIMENTATION","text":"Goal Best CNN Why Fast prototyping ResNet-18 Simple &amp; fast Architecture research DenseNet / ConvNeXt Feature experiments CNN vs ViT ConvNeXt Fair comparison"},{"location":"Computer-Vision/CNNs/#final-decision-flow","title":"FINAL DECISION FLOW","text":"<pre><code>Need speed?        \u2192 YOLO / MobileNet\nNeed accuracy?     \u2192 ResNet / EfficientNet\nNeed segmentation? \u2192 U-Net / DeepLab\nNeed edge device?  \u2192 MobileNet / ShuffleNet\nNeed similarity?   \u2192 Siamese CNN\n</code></pre>"},{"location":"Computer-Vision/CNNs/#production-cnn-architecture","title":"PRODUCTION CNN ARCHITECTURE","text":""},{"location":"Computer-Vision/CNNs/#1-end-to-end-system-layers","title":"1\ufe0f\u20e3 END-TO-END SYSTEM LAYERS","text":"Layer Purpose Key Components Production Notes Data Sources Provide images/videos Cameras, Medical devices, Docs Raw, untrusted data Ingestion Collect data ETL, Kafka, Kinesis Batch + streaming Storage Persist data S3, GCS, Blob Versioned buckets Validation Quality control Schema, image checks Mandatory gate Labeling Ground truth Label Studio, CVAT Human-in-loop Training Learn features PyTorch / TF Distributed Registry Version control MLflow, SageMaker Rollback support Inference Serve model FastAPI, TorchServe Low latency Monitoring Observe model Prometheus, Arize Drift detection Feedback Improve model Re-label, retrain Closed loop"},{"location":"Computer-Vision/CNNs/#2-data-pipeline-details","title":"2\ufe0f\u20e3 DATA PIPELINE DETAILS","text":"Stage What Happens Tools Why It Matters Ingest Collect raw images Kafka, S3 Scale Clean Remove noise OpenCV Accuracy Validate Check format Great Expectations Prevent failures Augment Flip, crop Albumentations Generalization Split Train/Val/Test Custom scripts Fair evaluation"},{"location":"Computer-Vision/CNNs/#3-cnn-model-architecture-production","title":"3\ufe0f\u20e3 CNN MODEL ARCHITECTURE (PRODUCTION)","text":"Component Choice Reason Input Fixed resolution Batch efficiency Backbone ResNet / EfficientNet Stable &amp; proven Neck FPN / PAN Multi-scale Head Task-specific Classification / Detection Regularization Dropout, BN Prevent overfit Precision FP16 Faster training"},{"location":"Computer-Vision/CNNs/#4-backbone-selection-guide","title":"4\ufe0f\u20e3 BACKBONE SELECTION GUIDE","text":"Constraint Backbone High accuracy EfficientNet Medical DenseNet / ResNet Real-time YOLO backbone Mobile MobileNet-v3 Research ConvNeXt"},{"location":"Computer-Vision/CNNs/#5-training-pipeline","title":"5\ufe0f\u20e3 TRAINING PIPELINE","text":"Step Description Production Practice Init Load pretrained Transfer learning Train Optimize weights DDP / Horovod Validate Measure metrics Every epoch Tune Hyperparameters Optuna Save Store artifacts Registry"},{"location":"Computer-Vision/CNNs/#6-model-registry-versioning","title":"6\ufe0f\u20e3 MODEL REGISTRY &amp; VERSIONING","text":"Feature Purpose Version ID Rollback Metrics Compare models Data hash Reproducibility Config Audit Approval Governance"},{"location":"Computer-Vision/CNNs/#7-inference-architecture","title":"7\ufe0f\u20e3 INFERENCE ARCHITECTURE","text":"Mode Architecture Use Case Online REST / gRPC Real-time Batch Scheduled jobs Analytics Edge On-device IoT Streaming Kafka consumers Video"},{"location":"Computer-Vision/CNNs/#8-deployment-scaling","title":"8\ufe0f\u20e3 DEPLOYMENT &amp; SCALING","text":"Component Tool Benefit Container Docker Portability Orchestration Kubernetes Scaling Autoscaling HPA Cost control Deployment Canary Safe rollout Acceleration TensorRT Low latency"},{"location":"Computer-Vision/CNNs/#9-monitoring-observability","title":"9\ufe0f\u20e3 MONITORING &amp; OBSERVABILITY","text":"Category Metrics Model Accuracy, Drift Data Distribution shift System Latency, GPU Business SLA, ROI"},{"location":"Computer-Vision/CNNs/#10-security-governance","title":"\ud83d\udd10 1\ufe0f\u20e30\ufe0f\u20e3 SECURITY &amp; GOVERNANCE","text":"Area Controls Access OAuth2, IAM Model Encryption Explainability Grad-CAM Bias Fairness checks Audit Logs, lineage"},{"location":"Computer-Vision/CNNs/#11-feedback-retraining","title":"\ud83e\uddea 1\ufe0f\u20e31\ufe0f\u20e3 FEEDBACK &amp; RETRAINING","text":"Step Description Capture Predictions Review Human feedback Relabel Correct errors Retrain Scheduled Redeploy Versioned"},{"location":"Computer-Vision/CNNs/#production-cnn-decision-matrix","title":"\ud83c\udfaf PRODUCTION CNN DECISION MATRIX","text":"Need Solution High availability Kubernetes Low latency TensorRT Compliance Model registry Continuous learning Feedback loop Cost efficiency Autoscaling"},{"location":"Computer-Vision/CNNs/#why-kernel-counts-are-16-32-64-128","title":"WHY KERNEL COUNTS ARE 16, 32, 64, 128\u2026","text":""},{"location":"Computer-Vision/CNNs/#1-what-a-kernel-count-means","title":"1\ufe0f\u20e3 WHAT A \u201cKERNEL COUNT\u201d MEANS","text":"Term Meaning Kernel / Filter A learnable pattern detector (edge, corner, texture) Kernel count (16/32/64) Number of different patterns learned at a layer Output channels Same as number of kernels <p>\ud83d\udc49 <code>More kernels = more pattern types learned</code></p>"},{"location":"Computer-Vision/CNNs/#2-why-numbers-double-16-32-64-128","title":"2\ufe0f\u20e3 WHY NUMBERS DOUBLE (16 \u2192 32 \u2192 64 \u2192 128)","text":"Reason Explanation Feature complexity increases Early layers detect simple edges; deeper layers detect shapes &amp; objects Spatial size decreases After pooling, width/height \u2193 \u2192 channels \u2191 Information preservation Losing pixels \u2192 compensate with more channels Compute balance FLOPs stay manageable Hardware efficiency GPUs optimized for powers of 2"},{"location":"Computer-Vision/CNNs/#3-layer-wise-feature-evolution","title":"3\ufe0f\u20e3 LAYER-WISE FEATURE EVOLUTION","text":"CNN Layer Spatial Size Kernel Count What It Learns Conv-1 High (224\u00d7224) 16 Edges, lines Conv-2 Medium (112\u00d7112) 32 Corners, textures Conv-3 Low (56\u00d756) 64 Shapes Conv-4 Very Low (28\u00d728) 128 Object parts Conv-5 Tiny (14\u00d714) 256+ Full objects"},{"location":"Computer-Vision/CNNs/#4-why-not-start-with-128-kernels","title":"4\ufe0f\u20e3 WHY NOT START WITH 128 KERNELS?","text":"Reason Explanation Overfitting Early layers don\u2019t need many patterns High memory Too many feature maps Slow training FLOPs explode Redundancy Early patterns are simple"},{"location":"Computer-Vision/CNNs/#5-why-not-keep-same-number-eg-all-32","title":"5\ufe0f\u20e3 WHY NOT KEEP SAME NUMBER (E.G., ALL 32)?","text":"Problem Impact Bottleneck Network can\u2019t represent complex patterns Information loss Pooling removes details Lower accuracy Insufficient representation power"},{"location":"Computer-Vision/CNNs/#6-math-compute-balance-important","title":"6\ufe0f\u20e3 MATH &amp; COMPUTE BALANCE (IMPORTANT)","text":"Layer H \u00d7 W Channels Total Activations Early 224\u00d7224 16 ~800K Mid 56\u00d756 64 ~200K Deep 14\u00d714 256 ~50K <p>\ud83d\udc49 As H\u00d7W \u2193 \u2192 Channels \u2191 \ud83d\udc49 Keeps compute roughly stable</p>"},{"location":"Computer-Vision/CNNs/#7-biological-intuition-human-vision","title":"7\ufe0f\u20e3 BIOLOGICAL INTUITION (HUMAN VISION)","text":"CNN Layer Human Brain Equivalent Early Conv Retina / V1 (edges) Mid Conv V2 / V4 (shapes) Deep Conv IT Cortex (objects) <p>Human vision also uses hierarchical feature expansion.</p>"},{"location":"Computer-Vision/CNNs/#8-industry-practice-examples","title":"8\ufe0f\u20e3 INDUSTRY PRACTICE EXAMPLES","text":"Model Kernel Pattern VGG 64 \u2192 128 \u2192 256 \u2192 512 ResNet 64 \u2192 128 \u2192 256 \u2192 512 MobileNet 32 \u2192 64 \u2192 128 EfficientNet Scaled via compound rule"},{"location":"Computer-Vision/CNNs/#9-when-to-break-the-rule","title":"9\ufe0f\u20e3 WHEN TO BREAK THE RULE","text":"Scenario What to Do Small dataset Use fewer kernels Mobile device Cap at 128 Medical imaging Use more kernels No GPU Reduce channels"},{"location":"Computer-Vision/CNNs/#why-dense-layers-use-32-64-128-neurons","title":"WHY DENSE LAYERS USE 32, 64, 128\u2026 NEURONS","text":""},{"location":"Computer-Vision/CNNs/#1-what-a-dense-neuron-count-means","title":"1\ufe0f\u20e3 WHAT A DENSE NEURON COUNT MEANS","text":"Term Meaning Dense neuron A decision unit combining all input features 32 / 64 / 128 Model capacity (how complex a decision it can learn) More neurons More expressive power <p>\ud83d\udc49 <code>Dense layers do reasoning, not feature extraction</code></p>"},{"location":"Computer-Vision/CNNs/#2-why-powers-of-2-32-64-128","title":"2\ufe0f\u20e3 WHY POWERS OF 2 (32, 64, 128)?","text":"Reason Explanation Compute efficiency CPUs/GPUs optimized for powers of 2 Memory alignment Faster matrix multiplication Balanced scaling Easy to grow/shrink capacity Industry convention Proven by decades of practice"},{"location":"Computer-Vision/CNNs/#3-role-of-dense-layer-in-cnn-pipeline","title":"3\ufe0f\u20e3 ROLE OF DENSE LAYER IN CNN PIPELINE","text":"Stage CNN Does Dense Does Before Dense Extract features \u2014 Dense layer Combine features Learn decision boundaries Output layer Predict class/value Final decision"},{"location":"Computer-Vision/CNNs/#4-how-many-neurons-should-a-dense-layer-have","title":"4\ufe0f\u20e3 HOW MANY NEURONS SHOULD A DENSE LAYER HAVE?","text":"Scenario Dense Size Why Very small dataset 16\u201332 Avoid overfitting Medium dataset 64\u2013128 Balanced Large dataset 256\u2013512 Higher capacity Mobile / Edge 32\u201364 Low memory Medical / Finance 128\u2013512 Complex patterns"},{"location":"Computer-Vision/CNNs/#5-why-not-too-many-neurons","title":"5\ufe0f\u20e3 WHY NOT TOO MANY NEURONS?","text":"Problem Impact Overfitting Memorizes training data High parameters Slow training Poor generalization Bad test accuracy Cost More memory &amp; compute"},{"location":"Computer-Vision/CNNs/#6-why-not-too-few-neurons","title":"6\ufe0f\u20e3 WHY NOT TOO FEW NEURONS?","text":"Problem Impact Underfitting Can\u2019t learn patterns Weak decision boundary Low accuracy Information bottleneck Loses features"},{"location":"Computer-Vision/CNNs/#7-parameter-growth-very-important","title":"7\ufe0f\u20e3 PARAMETER GROWTH (VERY IMPORTANT)","text":"<p>Assume <code>Flatten = 512 features</code></p> Dense Neurons Parameters Dense(32) 512\u00d732 + 32 = 16,416 Dense(64) 512\u00d764 + 64 = 32,832 Dense(128) 512\u00d7128 + 128 = 65,664 Dense(256) 512\u00d7256 + 256 = 131,328 <p>\ud83d\udc49 Parameters grow `linearly \u00d7 neurons</p>"},{"location":"Computer-Vision/CNNs/#8-common-dense-layer-patterns","title":"8\ufe0f\u20e3 COMMON DENSE LAYER PATTERNS","text":"Pattern Usage 128 \u2192 64 Gradual compression 256 \u2192 128 \u2192 64 Deep reasoning 64 \u2192 Output Lightweight models GAP \u2192 Dense(32) Modern CNNs"},{"location":"Computer-Vision/CNNs/#9-modern-best-practice-important","title":"9\ufe0f\u20e3 MODERN BEST PRACTICE (IMPORTANT)","text":"Old CNN Modern CNN Huge Dense layers Global Average Pooling Millions of params Few dense neurons Overfitting risk Better generalization"},{"location":"Computer-Vision/CNNs/#256-cnn-feature-detectors","title":"256 CNN FEATURE DETECTORS","text":""},{"location":"Computer-Vision/CNNs/#group-a-edge-orientation-features-32","title":"\ud83d\udd34 GROUP A: EDGE &amp; ORIENTATION FEATURES (32)","text":"# Feature Detector Name 1 Vertical edge 2 Horizontal edge 3 Diagonal edge (\u2198) 4 Diagonal edge (\u2197) 5 Thick vertical edge 6 Thick horizontal edge 7 Thin edge 8 Double edge 9 Broken edge 10 Curved edge 11 Edge junction 12 T-junction 13 L-corner 14 Y-junction 15 Edge crossing 16 Edge termination 17 Shadow edge 18 Highlight edge 19 Edge gradient 20 Edge contrast 21 Edge symmetry 22 Edge asymmetry 23 Parallel edges 24 Converging edges 25 Diverging edges 26 Step edge 27 Ramp edge 28 Zigzag edge 29 Stair-step edge 30 Soft boundary 31 Sharp boundary 32 Occluded edge"},{"location":"Computer-Vision/CNNs/#group-b-corners-geometric-shapes-32","title":"\ud83d\udfe0 GROUP B: CORNERS &amp; GEOMETRIC SHAPES (32)","text":"# Feature Detector Name 33 Acute corner 34 Right-angle corner 35 Obtuse corner 36 Rounded corner 37 Sharp corner 38 Inner corner 39 Outer corner 40 Corner cluster 41 Rectangle fragment 42 Square fragment 43 Triangle fragment 44 Polygon edge 45 Star-like junction 46 Box boundary 47 Frame structure 48 Grid intersection 49 Window-like shape 50 Door-frame shape 51 Panel edge 52 Cross shape 53 Plus shape 54 Diamond shape 55 Arrow shape 56 Chevron pattern 57 U-shape 58 V-shape 59 W-shape 60 Z-shape 61 Symmetric corner 62 Asymmetric corner 63 Corner repetition 64 Multi-corner pattern"},{"location":"Computer-Vision/CNNs/#group-c-texture-surface-patterns-48","title":"\ud83d\udfe1 GROUP C: TEXTURE &amp; SURFACE PATTERNS (48)","text":"# Feature Detector Name 65 Fine texture 66 Coarse texture 67 Smooth surface 68 Rough surface 69 Grainy texture 70 Speckle noise 71 Striped texture 72 Dotted texture 73 Checkerboard 74 Repetitive pattern 75 Random pattern 76 Fabric-like texture 77 Brick-like texture 78 Wood grain 79 Marble texture 80 Metal texture 81 Plastic surface 82 Glossy surface 83 Matte surface 84 Shadow texture 85 Highlight texture 86 Gradient texture 87 Blurry region 88 Sharp texture 89 Directional texture 90 Flow texture 91 Ripple pattern 92 Wave pattern 93 Cracked surface 94 Scratched surface 95 Spotted pattern 96 Noise suppression 97 Uniform region 98 Irregular texture 99 Density variation 100 Micro-texture 101 Macro-texture 102 Texture boundary 103 Texture contrast 104 Texture symmetry 105 Texture asymmetry 106 Repeating blobs 107 Cellular texture 108 Layered texture 109 Fractal texture 110 Background texture 111 Foreground texture 112 Texture anomaly"},{"location":"Computer-Vision/CNNs/#group-d-shapes-blobs-48","title":"\ud83d\udfe2 GROUP D: SHAPES &amp; BLOBS (48)","text":"# Feature Detector Name 113 Circular blob 114 Elliptical blob 115 Irregular blob 116 Solid blob 117 Hollow blob 118 Ring shape 119 Arc shape 120 Crescent shape 121 Curve fragment 122 S-curve 123 Spiral fragment 124 Loop 125 Enclosed region 126 Open contour 127 Shape boundary 128 Shape interior 129 Symmetric shape 130 Asymmetric shape 131 Shape repetition 132 Shape cluster 133 Organic shape 134 Mechanical shape 135 Smooth contour 136 Jagged contour 137 Fat shape 138 Thin shape 139 Elongated shape 140 Compact shape 141 Nested shapes 142 Overlapping shapes 143 Partial shape 144 Occluded shape 145 Shape alignment 146 Shape orientation 147 Shape scale 148 Shape deformation 149 Shape symmetry axis 150 Shape imbalance 151 Shape center 152 Shape edge emphasis 153 Shape skeleton 154 Silhouette fragment 155 Outline detection 156 Filled region 157 Negative space 158 Shape anomaly 159 Salient blob 160 Background blob"},{"location":"Computer-Vision/CNNs/#group-e-object-parts-48","title":"\ud83d\udd35 GROUP E: OBJECT PARTS (48)","text":"# Feature Detector Name 161 Eye-like pattern 162 Nose-like pattern 163 Mouth-like pattern 164 Ear-like pattern 165 Face fragment 166 Hand-like pattern 167 Finger-like pattern 168 Arm-like pattern 169 Leg-like pattern 170 Human silhouette 171 Wheel-like pattern 172 Tire texture 173 Vehicle window 174 Headlight pattern 175 License-plate region 176 Building window 177 Roof edge 178 Door pattern 179 Text character stroke 180 Logo fragment 181 Symbol fragment 182 Medical organ edge 183 Tumor boundary 184 Vessel-like structure 185 Bone contour 186 Lung texture 187 Leaf shape 188 Branch pattern 189 Petal pattern 190 Animal fur 191 Feather texture 192 Scale texture 193 Clothing fold 194 Fabric seam 195 Button-like pattern 196 Tool handle 197 Mechanical joint 198 Screw-like pattern 199 Circuit trace 200 PCB component 201 Barcode stripe 202 QR pattern 203 Screen reflection 204 UI icon fragment 205 Document margin 206 Table border 207 Chart line 208 Object-part junction"},{"location":"Computer-Vision/CNNs/#group-f-context-semantic-specialized-features-48","title":"\ud83d\udfe3 GROUP F: CONTEXT, SEMANTIC &amp; SPECIALIZED FEATURES (48)","text":"# Feature Detector Name 209 Object-background contrast 210 Foreground separation 211 Spatial layout 212 Relative position 213 Center bias 214 Border bias 215 Scale context 216 Depth cue 217 Perspective cue 218 Occlusion context 219 Lighting direction 220 Shadow context 221 Reflection cue 222 Color contrast 223 Color harmony 224 Color anomaly 225 Motion blur cue 226 Focus cue 227 Attention hotspot 228 Saliency cue 229 Scene geometry 230 Scene boundary 231 Indoor context 232 Outdoor context 233 Road context 234 Sky context 235 Vegetation context 236 Water context 237 Medical abnormality 238 Industrial defect 239 Manufacturing flaw 240 Surface anomaly 241 Rare pattern 242 Dataset bias feature 243 Noise artifact 244 Compression artifact 245 Sensor artifact 246 Lens distortion 247 Blur artifact 248 Glare artifact 249 Edge-case pattern 250 Outlier feature 251 Confidence booster 252 Decision boundary cue 253 High-activation trigger 254 Suppression feature 255 Safety-critical cue 256 Unknown emergent feature <p>There is no single fixed \u201cmaximum\u201d kernel count in production, but in real enterprise / FAANG-grade systems there is a very clear practical ceiling beyond which kernels stop helping.</p> <p>In production CNNs, kernel counts rarely exceed 1024 channels per layer. Most enterprise systems cap between 256\u2013512, with 1024 used only in very deep, high-end backbones.</p>"},{"location":"Computer-Vision/CNNs/#1-common-enterprise-practice","title":"1\ufe0f\u20e3 COMMON ENTERPRISE PRACTICE","text":"Layer Depth Typical Kernel Count Used Where Early layers 16 \u2013 64 Edge detection Mid layers 64 \u2013 256 Shapes &amp; textures Deep layers 256 \u2013 512 Object parts Very deep (bottleneck) 512 \u2013 1024 High-level semantics"},{"location":"Computer-Vision/CNNs/#2-real-models-used-in-production","title":"2\ufe0f\u20e3 REAL MODELS USED IN PRODUCTION","text":"Model Max Kernels (Channels) Production Usage VGG-16 512 Legacy enterprise ResNet-50 2048 (internal bottleneck) Very common ResNet-101/152 2048 High-accuracy systems DenseNet-201 ~1920 Medical imaging EfficientNet-B7 ~2560 Large-scale cloud YOLOv5/YOLOv8 512 \u2013 1024 Real-time detection ConvNeXt-XL 2048+ Modern enterprise <p>\u26a0\ufe0f Important:  In ResNet, 2048 is after bottleneck expansion Actual conv kernels are usually 512</p>"},{"location":"Computer-Vision/CNNs/#3-why-enterprise-models-stop-around-5121024","title":"3\ufe0f\u20e3 WHY ENTERPRISE MODELS STOP AROUND 512\u20131024","text":"Reason Explanation Diminishing returns Accuracy gain becomes marginal Memory explosion Feature maps become huge Latency SLA Inference slows down Cost GPU hours increase Overfitting risk Too many features"},{"location":"Computer-Vision/CNNs/#4-when-1024-kernels-are-used","title":"4\ufe0f\u20e3 WHEN 1024+ KERNELS ARE USED","text":"Scenario Why Medical imaging Subtle patterns Satellite imagery High resolution Autonomous driving Complex scenes Research models Accuracy benchmarking Offline batch inference No latency pressure"},{"location":"Computer-Vision/CNNs/#5-when-enterprise-avoids-large-kernel-counts","title":"5\ufe0f\u20e3 WHEN ENTERPRISE AVOIDS LARGE KERNEL COUNTS","text":"Constraint Typical Limit Mobile / Edge \u2264128 Real-time APIs \u2264256 Cost-sensitive systems \u2264256 Low-data domains \u2264128"},{"location":"Computer-Vision/CNNs/#6-compute-impact-example","title":"6\ufe0f\u20e3 COMPUTE IMPACT EXAMPLE","text":"<p>Assume feature map size <code>14\u00d714</code></p> Kernels Activations Memory Impact 256 ~50K Low 512 ~100K Medium 1024 ~200K High 2048 ~400K Very high"},{"location":"Computer-Vision/CNNs/#enterprise-rule-of-thumb","title":"ENTERPRISE RULE-OF-THUMB","text":"<pre><code>If latency matters \u2192 cap at 256\nIf accuracy matters \u2192 go to 512\nIf research / heavy vision \u2192 1024+\n</code></pre>"},{"location":"Computer-Vision/CNNs/#cnn-training-parameters-best-practices-recommended-values","title":"CNN TRAINING PARAMETERS \u2014 BEST PRACTICES &amp; RECOMMENDED VALUES","text":""},{"location":"Computer-Vision/CNNs/#core-training-parameters","title":"\ud83d\udd34 CORE TRAINING PARAMETERS","text":"Parameter What It Controls Best Practice Recommended Values (Prod) Learning Rate (LR) Step size of weight update Start low, use scheduler <code>0.001</code> (Adam), <code>0.01</code> (SGD) Epochs Training duration Stop using validation loss <code>30\u2013100</code> Batch Size Samples per update Fit GPU memory <code>32\u2013128</code>"},{"location":"Computer-Vision/CNNs/#optimizer-parameters","title":"\ud83d\udfe0 OPTIMIZER PARAMETERS","text":"Parameter Best Practice Recommended Value Optimizer Adam for most CNNs <code>Adam</code> Momentum (SGD) Use if SGD <code>0.9</code> Weight Decay Always use <code>1e-4</code> Beta1 (Adam) Keep default <code>0.9</code> Beta2 (Adam) Keep default <code>0.999</code>"},{"location":"Computer-Vision/CNNs/#learning-rate-control-very-important","title":"\ud83d\udfe1 LEARNING RATE CONTROL (VERY IMPORTANT)","text":"Technique Best Practice Recommended LR Scheduler Always use Yes Warmup For large batch <code>5\u201310 epochs</code> Reduce on Plateau Safe default <code>factor=0.1</code> Cosine Decay Vision models Widely used Step Decay Legacy CNNs Optional"},{"location":"Computer-Vision/CNNs/#regularization-parameters","title":"\ud83d\udfe2 REGULARIZATION PARAMETERS","text":"Parameter Best Practice Recommended Value Dropout Only in Dense layers <code>0.2\u20130.5</code> BatchNorm Use everywhere Enabled Label Smoothing Classification <code>0.1</code> Early Stopping Prevent overfit <code>patience=5\u201310</code>"},{"location":"Computer-Vision/CNNs/#data-pipeline-parameters","title":"\ud83d\udd35 DATA PIPELINE PARAMETERS","text":"Parameter Best Practice Recommended Shuffle Always ON <code>True</code> Data Augmentation Light \u2192 strong Flip, rotate, crop Input Size Match pretrained <code>224\u00d7224</code> Normalization Use ImageNet stats Mean &amp; Std"},{"location":"Computer-Vision/CNNs/#loss-function-selection","title":"\ud83d\udfe3 LOSS FUNCTION SELECTION","text":"Task Best Practice Recommended Binary classification BCE + Sigmoid Default Multi-class Cross-Entropy Softmax Imbalanced data Focal Loss Detection Segmentation Dice + CE Medical"},{"location":"Computer-Vision/CNNs/#performance-stability","title":"\ud83d\udfe4 PERFORMANCE &amp; STABILITY","text":"Parameter Best Practice Recommended Mixed Precision Use on GPU FP16 Gradient Clipping Prevent explosion <code>1.0</code> Gradient Accumulation Simulate big batch <code>2\u20138</code> Distributed Training Scale training DDP"},{"location":"Computer-Vision/CNNs/#production-mlops-best-practices","title":"\ud83d\udd10 PRODUCTION &amp; MLOPS BEST PRACTICES","text":"Practice Why It\u2019s Needed Fixed random seed Reproducibility Checkpointing Failure recovery Save best model Deployment safety Experiment tracking Audit &amp; compare Model registry Rollback"},{"location":"Computer-Vision/CNNs/#safe-default-config","title":"\ud83c\udfaf SAFE DEFAULT CONFIG","text":"Parameter Value Optimizer Adam Learning Rate <code>0.001</code> Batch Size <code>32</code> Epochs <code>50</code> Scheduler ReduceLROnPlateau Dropout <code>0.3</code> Weight Decay <code>1e-4</code> Early Stopping Enabled"},{"location":"Computer-Vision/CNNs/#cnn-model-evaluation-metrics","title":"CNN MODEL EVALUATION METRICS","text":""},{"location":"Computer-Vision/CNNs/#1-classification-metrics-image-classification","title":"\ud83d\udd34 1\ufe0f\u20e3 CLASSIFICATION METRICS (Image Classification)","text":"Metric What It Measures Formula (Intuition) Why It\u2019s Important When to Use Accuracy Correct predictions (TP+TN)/All Simple performance Balanced datasets Precision Prediction correctness TP/(TP+FP) Reduces false alarms High FP cost Recall (Sensitivity) Detection ability TP/(TP+FN) Missed detection Medical F1-Score Precision-Recall balance Harmonic mean Balanced metric Imbalanced data Specificity True negative rate TN/(TN+FP) Avoid false positives Medical screening ROC-AUC Class separation Area under curve Threshold-free Binary classification PR-AUC Precision-Recall tradeoff Area under curve Rare positives Fraud/medical Top-K Accuracy Rank-based correctness Top-K hit Multi-class ImageNet"},{"location":"Computer-Vision/CNNs/#2-confusion-matrix-foundational","title":"\ud83d\udfe0 2\ufe0f\u20e3 CONFUSION MATRIX (FOUNDATIONAL)","text":"Element Meaning Why It Matters TP Correct positive Correct detection TN Correct negative Correct rejection FP False positive False alarm FN False negative Missed detection"},{"location":"Computer-Vision/CNNs/#3-object-detection-metrics","title":"\ud83d\udfe1 3\ufe0f\u20e3 OBJECT DETECTION METRICS","text":"Metric Measures Why Used Notes IoU Box overlap Localization quality \u22650.5 common mAP Mean Average Precision Overall detection Industry standard AP@0.5 Precision at IoU=0.5 COCO / VOC Baseline AP@[.5:.95] Strict detection COCO Hard metric Recall@K Object coverage Missed objects Safety systems FPS Speed Real-time needs Production SLA"},{"location":"Computer-Vision/CNNs/#4-segmentation-metrics","title":"\ud83d\udfe2 4\ufe0f\u20e3 SEGMENTATION METRICS","text":"Metric What It Measures Why Important Use Case IoU (Jaccard) Mask overlap Pixel accuracy Segmentation Dice Coefficient Similarity Small objects Medical Pixel Accuracy Correct pixels Simple baseline General Mean IoU Class-wise overlap Balanced classes Multi-class Boundary F1 Edge accuracy Shape precision Medical"},{"location":"Computer-Vision/CNNs/#5-regression-metrics-cnn-regression","title":"\ud83d\udd35 5\ufe0f\u20e3 REGRESSION METRICS (CNN Regression)","text":"Metric Measures Why Used MSE Squared error Penalize big errors RMSE Error scale Interpretability MAE Absolute error Robust to outliers R\u00b2 Variance explained Model quality"},{"location":"Computer-Vision/CNNs/#6-probabilistic-confidence-metrics","title":"\ud83d\udfe3 6\ufe0f\u20e3 PROBABILISTIC &amp; CONFIDENCE METRICS","text":"Metric Purpose Why Needed Log Loss Confidence quality Penalizes overconfidence Brier Score Calibration Probability accuracy ECE Expected calibration error Trustworthiness"},{"location":"Computer-Vision/CNNs/#7-robustness-generalization-metrics","title":"\ud83d\udfe4 7\ufe0f\u20e3 ROBUSTNESS &amp; GENERALIZATION METRICS","text":"Metric Purpose Why Cross-domain accuracy Domain shift Production realism Noise robustness Stability Real-world data Adversarial accuracy Security Safety-critical Out-of-distribution (OOD) Unknown inputs Reliability"},{"location":"Computer-Vision/CNNs/#8-production-level-metrics","title":"\ud83d\udd10 8\ufe0f\u20e3 PRODUCTION-LEVEL METRICS","text":"Metric Why Needed Latency SLA compliance Throughput Scale GPU utilization Cost Memory usage Stability Drift metrics Model decay"},{"location":"Computer-Vision/CNNs/#9-metric-selection-guide-very-important","title":"\ud83c\udfaf 9\ufe0f\u20e3 METRIC SELECTION GUIDE (VERY IMPORTANT)","text":"Use Case Must-Track Metrics Balanced classification Accuracy, F1 Medical diagnosis Recall, Dice Fraud detection Precision, PR-AUC Object detection mAP, IoU Segmentation Dice, mIoU Real-time systems FPS, Latency"},{"location":"Computer-Vision/CNNs/#1-classification-metrics","title":"\ud83d\udd34 1\ufe0f\u20e3 CLASSIFICATION METRICS","text":"Metric What It Measures Why It\u2019s Important Bad Good Excellent Accuracy Overall correctness Quick health check &lt; 70% 70\u201385% &gt; 85% Precision FP control Avoid false alarms &lt; 60% 60\u201380% &gt; 80% Recall (Sensitivity) FN control Catch positives &lt; 60% 60\u201385% &gt; 85% F1-Score Precision-Recall balance Imbalanced data &lt; 0.60 0.60\u20130.80 &gt; 0.80 Specificity True negative rate Avoid over-flagging &lt; 70% 70\u201390% &gt; 90% ROC-AUC Class separability Threshold-free &lt; 0.70 0.70\u20130.85 &gt; 0.85 PR-AUC Rare positive quality Imbalanced sets &lt; 0.50 0.50\u20130.75 &gt; 0.75 Top-5 Accuracy Rank-based correctness Large classes &lt; 85% 85\u201395% &gt; 95%"},{"location":"Computer-Vision/CNNs/#2-object-detection-metrics","title":"\ud83d\udfe0 2\ufe0f\u20e3 OBJECT DETECTION METRICS","text":"Metric What It Measures Why Used Bad Good Excellent IoU Box overlap Localization quality &lt; 0.50 0.50\u20130.70 &gt; 0.70 mAP@0.5 Detection accuracy Industry standard &lt; 0.50 0.50\u20130.75 &gt; 0.75 mAP@[.5:.95] Strict detection COCO benchmark &lt; 0.35 0.35\u20130.55 &gt; 0.55 Recall Object coverage Safety-critical &lt; 60% 60\u201380% &gt; 80% FPS Inference speed Real-time systems &lt; 15 15\u201330 &gt; 30"},{"location":"Computer-Vision/CNNs/#3-segmentation-metrics","title":"\ud83d\udfe2 3\ufe0f\u20e3 SEGMENTATION METRICS","text":"Metric What It Measures Why Important Bad Good Excellent Pixel Accuracy Correct pixels Baseline quality &lt; 80% 80\u201390% &gt; 90% IoU (Jaccard) Mask overlap Core metric &lt; 0.50 0.50\u20130.70 &gt; 0.70 Mean IoU Class balance Multi-class &lt; 0.45 0.45\u20130.65 &gt; 0.65 Dice Coefficient Similarity Medical imaging &lt; 0.60 0.60\u20130.80 &gt; 0.80 Boundary F1 Edge precision Fine structures &lt; 0.55 0.55\u20130.75 &gt; 0.75"},{"location":"Computer-Vision/CNNs/#4-regression-metrics-cnn-regression","title":"\ud83d\udd35 4\ufe0f\u20e3 REGRESSION METRICS (CNN Regression)","text":"Metric What It Measures Why Used Bad Good Excellent MAE Avg absolute error Robustness High Medium Low RMSE Large error penalty Sensitivity High Medium Low MSE Squared error Optimization High Medium Low R\u00b2 Score Variance explained Fit quality &lt; 0.50 0.50\u20130.80 &gt; 0.80 <p><code>(Lower is better for MAE/RMSE/MSE)</code></p>"},{"location":"Computer-Vision/CNNs/#5-probability-calibration-metrics","title":"\ud83d\udfe3 5\ufe0f\u20e3 PROBABILITY &amp; CALIBRATION METRICS","text":"Metric Purpose Bad Good Excellent Log Loss Confidence quality &gt; 0.8 0.3\u20130.8 &lt; 0.3 Brier Score Probability accuracy &gt; 0.25 0.10\u20130.25 &lt; 0.10 ECE Calibration error &gt; 0.10 0.05\u20130.10 &lt; 0.05"},{"location":"Computer-Vision/CNNs/#6-robustness-generalization","title":"\ud83d\udfe4 6\ufe0f\u20e3 ROBUSTNESS &amp; GENERALIZATION","text":"Metric Purpose Bad Good Excellent Cross-domain accuracy drop Domain shift &gt; 20% 10\u201320% &lt; 10% Noise robustness Stability Poor Moderate High OOD detection AUROC Unknown inputs &lt; 0.70 0.70\u20130.85 &gt; 0.85"},{"location":"Computer-Vision/CNNs/#7-production-system-metrics","title":"\ud83d\udd10 7\ufe0f\u20e3 PRODUCTION / SYSTEM METRICS","text":"Metric Bad Good Excellent Latency (ms) &gt; 500 100\u2013500 &lt; 100 Throughput (req/s) &lt; 10 10\u201350 &gt; 50 GPU Utilization &lt; 40% 40\u201370% 70\u201390% Drift (monthly) High Moderate Low"},{"location":"Computer-Vision/CNNs/#8-quick-metric-selection-target","title":"\ud83c\udfaf 8\ufe0f\u20e3 QUICK METRIC SELECTION + TARGET","text":"Use Case Primary Metric Target General classification F1 &gt; 0.80 Medical diagnosis Recall / Dice &gt; 0.85 Fraud / risk Precision / PR-AUC &gt; 0.80 Object detection mAP@0.5 &gt; 0.75 Segmentation Dice / mIoU &gt; 0.75 Real-time systems FPS + Latency &gt; 30 FPS <p><code>A model is \u201cexcellent\u201d only when accuracy is high, recall is safe, confidence is calibrated, and latency meets SLA.</code></p>"},{"location":"Computer-Vision/CNNs/#memory-rule","title":"\ud83e\udde0 MEMORY RULE","text":"<pre><code>Accuracy = correctness\nRecall   = safety\nPrecision= trust\nmAP/Dice = vision quality\nLatency  = production readiness\n</code></pre>"},{"location":"Computer-Vision/CNNs/#cnn-256-kernel-feature-detectors","title":"CNN \u2014 256 KERNEL FEATURE DETECTORS","text":""},{"location":"Computer-Vision/CNNs/#group-a-edges-orientations-132","title":"\ud83d\udd34 GROUP A \u2014 EDGES &amp; ORIENTATIONS (1\u201332)","text":"ID Feature Example 3\u00d73 Kernel 1 Vertical edge <code>[-1 0 1; -1 0 1; -1 0 1]</code> 2 Horizontal edge <code>[-1 -1 -1; 0 0 0; 1 1 1]</code> 3 Diagonal \u2198 <code>[-1 0 0; 0 0 0; 0 0 1]</code> 4 Diagonal \u2197 <code>[0 0 -1; 0 0 0; 1 0 0]</code> 5 Thick vertical <code>[-2 0 2; -2 0 2; -2 0 2]</code> 6 Thick horizontal <code>[-2 -2 -2; 0 0 0; 2 2 2]</code> 7 Soft vertical <code>[-0.5 0 0.5; -0.5 0 0.5; -0.5 0 0.5]</code> 8 Soft horizontal <code>[-0.5 -0.5 -0.5; 0 0 0; 0.5 0.5 0.5]</code> 9 Double edge <code>[-1 0 1; 1 0 -1; -1 0 1]</code> 10 Broken edge <code>[-1 0 1; 0 0 0; -1 0 1]</code> 11 Edge junction <code>[-1 1 -1; 1 1 1; -1 1 -1]</code> 12 T-junction <code>[0 1 0; 1 1 1; 0 -1 0]</code> 13 L-corner <code>[-1 -1 0; -1 1 1; 0 1 1]</code> 14 Y-junction <code>[-1 1 -1; 1 1 1; 0 1 0]</code> 15 Edge crossing <code>[-1 0 1; 0 0 0; 1 0 -1]</code> 16 Edge termination <code>[-1 0 0; 0 0 0; 1 0 0]</code> 17\u201332 Edge variants Same patterns, rotated / scaled"},{"location":"Computer-Vision/CNNs/#group-b-corners-geometry-3364","title":"\ud83d\udfe0 GROUP B \u2014 CORNERS &amp; GEOMETRY (33\u201364)","text":"ID Feature Example Kernel 33 Acute corner <code>[-1 0 1; 0 1 0; 1 0 -1]</code> 34 Right angle <code>[-1 -1 0; -1 1 1; 0 1 1]</code> 35 Rounded corner <code>[-0.5 0 0.5; 0 1 0; 0.5 0 -0.5]</code> 36 Inner corner <code>[1 -1 -1; -1 1 -1; -1 -1 1]</code> 37 Outer corner <code>[-1 1 1; 1 -1 1; 1 1 -1]</code> 38 Box corner <code>[-1 -1 1; -1 1 1; 1 1 1]</code> 39 Grid intersection <code>[-1 1 -1; 1 1 1; -1 1 -1]</code> 40 Frame edge <code>[-1 -1 -1; -1 8 -1; -1 -1 -1]</code> 41\u201364 Geometry variants Rotations / scale changes"},{"location":"Computer-Vision/CNNs/#group-c-texture-patterns-65112","title":"\ud83d\udfe1 GROUP C \u2014 TEXTURE &amp; PATTERNS (65\u2013112)","text":"ID Feature Example Kernel 65 Checker texture <code>[1 -1 1; -1 1 -1; 1 -1 1]</code> 66 Stripe texture <code>[1 1 1; -1 -1 -1; 1 1 1]</code> 67 Grainy <code>[0.5 -0.5 0.5; -0.5 0.5 -0.5; 0.5 -0.5 0.5]</code> 68 Smooth region <code>[1 1 1; 1 1 1; 1 1 1]/9</code> 69 Ripple <code>[0 1 0; 1 -4 1; 0 1 0]</code> 70 Noise suppress <code>[1 2 1; 2 4 2; 1 2 1]/16</code> 71 Cracked texture <code>[-1 1 -1; 1 -1 1; -1 1 -1]</code> 72 Dotted <code>[0 1 0; 1 0 1; 0 1 0]</code> 73\u2013112 Texture variants Frequency &amp; direction changes"},{"location":"Computer-Vision/CNNs/#group-d-blobs-shapes-113160","title":"\ud83d\udfe2 GROUP D \u2014 BLOBS &amp; SHAPES (113\u2013160)","text":"ID Feature Example Kernel 113 Blob center <code>[0 1 0; 1 -4 1; 0 1 0]</code> 114 Ring <code>[-1 1 -1; 1 0 1; -1 1 -1]</code> 115 Filled blob <code>[1 1 1; 1 -8 1; 1 1 1]</code> 116 Hollow blob <code>[-1 -1 -1; -1 8 -1; -1 -1 -1]</code> 117 Elongated blob <code>[0 0 0; 1 -2 1; 0 0 0]</code> 118 Compact shape <code>[1 1 1; 1 1 1; 1 1 1]/9</code> 119\u2013160 Shape variants Scale / orientation"},{"location":"Computer-Vision/CNNs/#group-e-object-parts-161208","title":"\ud83d\udd35 GROUP E \u2014 OBJECT PARTS (161\u2013208)","text":"ID Feature Example Kernel 161 Eye-like <code>[-1 0 -1; 1 2 1; -1 0 -1]</code> 162 Nose-like <code>[0 -1 0; 1 2 1; 0 -1 0]</code> 163 Mouth-like <code>[-1 -1 -1; 1 2 1; -1 -1 -1]</code> 164 Wheel-like <code>[-1 1 -1; 1 0 1; -1 1 -1]</code> 165 Window <code>[-1 -1 -1; -1 4 -1; -1 -1 -1]</code> 166 Organ edge <code>[-1 0 1; -2 0 2; -1 0 1]</code> 167\u2013208 Part variants Abstract learned patterns"},{"location":"Computer-Vision/CNNs/#group-f-context-semantic-209256","title":"\ud83d\udfe3 GROUP F \u2014 CONTEXT &amp; SEMANTIC (209\u2013256)","text":"ID Feature Example Kernel 209 Foreground focus <code>[0 0 0; 0 1 0; 0 0 0]</code> 210 Background suppress <code>[-1 -1 -1; -1 8 -1; -1 -1 -1]</code> 211 Spatial layout <code>[1 0 -1; 0 0 0; -1 0 1]</code> 212 Saliency cue <code>[0 1 0; 1 -4 1; 0 1 0]</code> 213 Depth cue <code>[-1 -2 -1; 0 0 0; 1 2 1]</code> 214 Lighting <code>[1 1 0; 1 0 -1; 0 -1 -1]</code> 215\u2013256 Semantic variants Highly abstract, learned"},{"location":"Computer-Vision/CNNs/#memory-rule_1","title":"\ud83e\udde0 MEMORY RULE","text":"<pre><code>1\u201364   \u2192 edges &amp; corners\n65\u2013128 \u2192 textures\n129\u2013192\u2192 shapes &amp; parts\n193\u2013256\u2192 semantics &amp; context\n</code></pre>"},{"location":"Computer-Vision/CNNs/#cnn-kernel-memory-rule-1-1024","title":"CNN KERNEL MEMORY RULE (1 \u2192 1024)","text":"Kernel Range Feature Level What Kernels Learn Typical Examples One-Line Memory Rule 1 \u2013 16 Primitive features Basic intensity changes Vertical edge, horizontal edge See light vs dark 17 \u2013 32 Oriented edges Direction &amp; contrast Thick/thin edges, gradients Which direction? 33 \u2013 64 Corners &amp; junctions Edge interactions L-corner, T-junction, crossings Where edges meet 65 \u2013 128 Textures &amp; patterns Repeating surface patterns Fabric, brick, grain What surface is this? 129 \u2013 256 Shapes &amp; blobs Geometric structures Circles, curves, blobs What shape is this? 257 \u2013 384 Object parts (simple) Local semantic parts Eye, wheel, handle Which part? 385 \u2013 512 Object parts (complex) Composed parts Face region, car front How parts combine 513 \u2013 768 Object-level semantics Whole objects Person, car, organ What object is this? 769 \u2013 1024 Context &amp; scene Global meaning Road scene, indoor/outdoor What\u2019s happening here?"},{"location":"Computer-Vision/Overview/","title":"Computer Vision","text":"<p>Computer Vision (CV) is a field of Artificial Intelligence (AI) that enables computers to see, interpret, and understand images and videos the way humans do\u2014and often at a much larger scale and speed.</p> <p>Computer Vision (CV) is a branch of Artificial Intelligence (AI) that helps computers to interpret and understand visual information much like humans. </p> <ul> <li>Computer Vision teaches machines to understand visual data (images/videos).</li> </ul>"},{"location":"Computer-Vision/Overview/#mathematical-prerequisites-for-computer-vision","title":"Mathematical Prerequisites for Computer Vision","text":""},{"location":"Computer-Vision/Overview/#1-linear-algebra","title":"1. Linear Algebra","text":"<ul> <li>Linear Algebra</li> <li>Vectors</li> <li>Matrices and Tensors</li> <li>Eigenvalues and Eigenvectors</li> <li>Singular Value Decomposition</li> </ul>"},{"location":"Computer-Vision/Overview/#2-probability-and-statistics","title":"2. Probability and Statistics","text":"<ul> <li>Probability and Statistics</li> <li>Probability Distributions</li> <li>Bayesian Inference and Bayes' Theorem</li> <li>Markov Chains</li> <li>Kalman Filters</li> </ul>"},{"location":"Computer-Vision/Overview/#3-signal-processing","title":"3. Signal Processing","text":"<ul> <li>Signal Processing</li> <li>Image Filtering and Convolution</li> <li>Discrete Fourier Transform (DFT)</li> <li>Fast Fourier Transform (FFT)</li> <li>Principal Component Analysis (PCA)</li> </ul>"},{"location":"Computer-Vision/Overview/#key-concepts-in-computer-vision","title":"Key Concepts in Computer Vision","text":""},{"location":"Computer-Vision/Overview/#1-image-processing","title":"1. Image Processing","text":"<p>a. Image Transformation</p> <ul> <li>Image Transformation</li> <li>Geometric Transformations</li> <li>Fourier Transform</li> <li>Intensity Transformation</li> </ul> <p>b. 2. Image Enhancement</p> <ul> <li>Image Enhancement</li> <li>Histogram Equalization</li> <li>Contrast Enhancement</li> <li>Image Sharpening</li> <li>Color Correction</li> </ul> <p>c. 3. Noise Reduction Techniques</p> <ul> <li>Noise Reduction Techniques</li> <li>Median Filtering</li> <li>Bilateral Filtering</li> <li>Wavelet Denoising</li> </ul> <p>d. Morphological Operations</p> <ul> <li>Morphological Operations</li> <li>Erosion and Dilation</li> <li>Opening</li> <li>Closing</li> <li>Morphological Gradient</li> </ul>"},{"location":"Computer-Vision/Overview/#2-feature-extraction","title":"2. Feature Extraction","text":"<p>a. Edge Detection Techniques</p> <ul> <li>Computer Vision Algorithms</li> <li>Edge Detection Techniques</li> <li>Canny Edge Detector</li> <li>Sobel Operator</li> <li>Laplacian of Gaussian (LoG)</li> </ul> <p>b. Corner and Interest Point Detection</p> <ul> <li>Harris Corner Detection</li> </ul> <p>c. Feature Descriptors</p> <ul> <li>Feature Descriptors</li> <li>SIFT (Scale-Invariant Feature Transform)</li> <li>SURF (Speeded-Up Robust Features)</li> <li>ORB (Oriented FAST and Rotated BRIEF)</li> <li>HOG (Histogram of Oriented Gradients)</li> </ul>"},{"location":"Computer-Vision/Overview/#popular-libraries-for-computer-vision","title":"Popular Libraries for Computer Vision","text":"<p>To implement computer vision tasks effectively, various libraries are used:</p> <ol> <li> <p>OpenCV: Mostly used open-source library for computer vision tasks like image processing, video capture and real-time applications.</p> </li> <li> <p>TensorFlow: A popular deep learning framework that includes tools for building and training computer vision models.</p> </li> <li> <p>PyTorch: Another deep learning library that provides great flexibility for computer vision tasks for research and development.</p> </li> <li> <p>scikit-image: A part of the scikit-learn ecosystem, this library provides algorithms for image processing and computer vision.</p> </li> </ol>"},{"location":"Computer-Vision/Overview/#deep-learning-for-computer-vision","title":"Deep Learning for Computer Vision","text":"<p>Deep learning has greatly enhanced computer vision by allowing machines to understand and analyze visual data and its key deep learning models include:</p>"},{"location":"Computer-Vision/Overview/#1-convolutional-neural-networks-cnns","title":"1. Convolutional Neural Networks (CNNs)","text":"<p>Convolutional Neural Networks are designed for learning spatial hierarchies of features from images and its key components include:</p> <ul> <li>Deep Learning for Computer Vision</li> <li>Deep learning</li> <li>Convolutional Neural Networks</li> <li>Convolutional Layers</li> <li>Pooling Layers</li> <li>Fully Connected Layers</li> </ul>"},{"location":"Computer-Vision/Overview/#2-generative-adversarial-networks-gans","title":"2. Generative Adversarial Networks (GANs)","text":"<p>It consists of two networks (generator and discriminator) that work against each other to create realistic images. There are various types of GANs each designed for specific tasks and improvements:</p> <ul> <li>Generative Adversarial Networks (GANs)</li> <li>Deep Convolutional GAN (DCGAN)</li> <li>Conditional GAN (cGAN)</li> <li>Cycle-Consistent GAN (CycleGAN)</li> <li>Super-Resolution GAN (SRGAN)</li> <li>StyleGAN</li> </ul>"},{"location":"Computer-Vision/Overview/#3-variational-autoencoders-vaes","title":"3. Variational Autoencoders (VAEs)","text":"<p>They are the probabilistic version of autoencoders which forces the model to learn a distribution over the latent space rather than a fixed point, some other autoencoders used in computer vision are:</p> <ul> <li>Autoencoders</li> <li>Variational Autoencoders (VAEs)</li> <li>Denoising Autoencoders (DAE)</li> <li>Convolutional Autoencoder (CAE)</li> </ul>"},{"location":"Computer-Vision/Overview/#4-vision-transformers-vit","title":"4. Vision Transformers (ViT)","text":"<p>They are inspired by transformers models to treat images and sequence of patches and process them using self-attention mechanisms, some common vision transformers include:</p> <ul> <li>Vision Transformers (ViT)</li> <li>Swin Transformer</li> <li>CvT (Convolutional Vision Transformer)</li> </ul>"},{"location":"Computer-Vision/Overview/#5-vision-language-models","title":"5. Vision Language Models","text":"<p>They integrate visual and textual information to perform image processing and natural language understanding.</p> <ul> <li>Vision language models</li> <li>CLIP (Contrastive Language-Image Pre-training)</li> <li>ALIGN (A Large-scale ImaGe and Noisy-text)</li> <li>BLIP (Bootstrapping Language-Image Pre-training)</li> </ul>"},{"location":"Computer-Vision/Overview/#computer-vision-tasks","title":"Computer Vision Tasks","text":""},{"location":"Computer-Vision/Overview/#1-image-classification","title":"1. Image Classification","text":"<p>It involves analyzing an image and assigning it a specific label or category based on its content such as identifying whether an image contains a cat, dog or car.</p> <p>Its techniques are as follows:</p> <ul> <li>Computer Vision Tasks</li> <li>Image Classification</li> <li>Image Classification using Support Vector Machine (SVM)</li> <li>Image Classification using RandomForest</li> <li>Image Classification using CNN</li> <li>Image Classification using TensorFlow</li> <li>Image Classification using PyTorch Lightning</li> </ul> <p>There are various types for Image Classification which are as follows:</p> <ul> <li>Dataset for Image Classification.</li> <li>Multiclass classification</li> <li>Multilabel classification</li> <li>Zero-shot classification</li> </ul>"},{"location":"Computer-Vision/Overview/#2-object-detection","title":"2. Object Detection","text":"<p>It involves identifying and locating objects within an image by drawing bounding boxes around them.</p> <p>It includes below following Techniques:</p> <ul> <li>Top Computer Vision Models</li> <li>Object Detection</li> <li>YOLO (You Only Look Once)</li> <li>SSD (Single Shot Multibox Detector)</li> <li>Region-Based Convolutional Neural Networks (R-CNNs)</li> <li>Fast R-CNN</li> <li>Faster R-CNN</li> <li>Mask R-CNN</li> <li>Object Detection using TensorFlow</li> <li>Object Detection using PyTorch</li> </ul> <p>Type of Object Detection Concepts are as follows:</p> <ul> <li>Bounding Box Regression</li> <li>Intersection over Union (IoU)</li> <li>Region Proposal Networks (RPN)</li> <li>Non-Maximum Suppression (NMS)</li> </ul>"},{"location":"Computer-Vision/Overview/#3-image-segmentation","title":"3. Image Segmentation","text":"<p>It involves partitioning an image into distinct regions or segments to identify objects or boundaries at a pixel level.</p> <p>Types of image segmentation are:</p> <ul> <li>Image Segmentation</li> <li>Semantic Segmentation</li> <li>Instance Segmentation</li> <li>Panoptic Segmentation</li> </ul> <p>We can perform image segmentation using the following methods:</p> <ul> <li>Image Segmentation using K Means Clustering</li> <li>Image Segmentation using UNet</li> <li>Image Segmentation using TensorFlow</li> <li>Image Segmentation with Mask R-CNN</li> </ul>"},{"location":"Data-processing/categorical-data/","title":"Categorical data","text":"\u2705 categorical data <ul> <li> <p>One-Hot Encoding</p> </li> <li> <p>Label Encoding</p> </li> <li> <p>Binary Encoding based on a specific value using the  column</p> </li> </ul> \ud83d\udccc One-Hot Encoding? <p>Creates one new column for each category, and assigns 1 to the present category, 0 to others.</p> <pre><code>pd.get_dummies(df['InternetService'])\n</code></pre> <pre><code>df_encoded = pd.get_dummies(df['InternetService']).astype(int)\n</code></pre> <pre><code>| InternetService | DSL | Fiber optic | No |\n| --------------- | --- | ----------- | -- |\n| DSL             | 1   | 0           | 0  |\n| Fiber optic     | 0   | 1           | 0  |\n| No              | 0   | 0           | 1  |\n</code></pre> <p>\u2705 When to use:</p> <ul> <li> <p>When categories are non-ordinal (no natural order).</p> </li> <li> <p>Works well with tree-based models like Random Forest, XGBoost.</p> </li> </ul> \ud83d\udccc Label Encoding? <p>Converts each category into a unique integer.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['InternetService_Label'] = le.fit_transform(df['InternetService'])\n</code></pre> InternetService Label DSL 0 Fiber optic 1 No 2 <p>\u26a0\ufe0f Caution: Implies ordinal relationship between categories (e.g., 0 &lt; 1 &lt; 2), which may mislead linear models (e.g., logistic regression).</p> <p>\u2705 When to use:</p> <ul> <li> <p>Categories with ordinal meaning.</p> </li> <li> <p>Or when using models that can handle ordinal encodings well.</p> </li> </ul> \ud83d\udccc Binary Encoding Based on a Specific Value <p>Creates a single column indicating presence (1) or absence (0) of a specific category.</p> <pre><code>df['FiberOptic_Flag'] = (df['InternetService'] == 'Fiber optic').astype(int)\n</code></pre> InternetService FiberOptic_Flag DSL 0 Fiber optic 1 No 0 <p>\u2705 When to use:</p> <ul> <li> <p>You only care about one category (e.g., checking if service is Fiber).</p> </li> <li> <p>For binary classification or simplified logic.</p> </li> </ul> \ud83d\udccc Summary Encoding Type # Columns Values Suitable For One-Hot Encoding Many 0/1 Most ML models Label Encoding One 0, 1, 2, ... Tree-based models, ordinal data Binary (Specific) One 0/1 Focus on one category only \ud83d\udccc Recommendation based on model types <p>\u2705 If you're using Tree-based models like:</p> <ul> <li> <p>Random Forest</p> </li> <li> <p>XGBoost / LightGBM</p> </li> <li> <p>Decision Tree</p> </li> </ul> <p>\ud83d\udfe9 Recommendation:</p> <p>\u27a1\ufe0f Label Encoding or One-Hot Encoding \u2014 both work, but Label Encoding is faster and often fine for trees because they split based on thresholds, not order.</p> <p>\u2705 If you're using Linear models like:</p> <ul> <li> <p>Logistic Regression</p> </li> <li> <p>Linear Regression</p> </li> <li> <p>SVM (with linear kernel)</p> </li> </ul> <p>\ud83d\udfe8 Recommendation:</p> <p>\u27a1\ufe0f One-Hot Encoding</p> <p>Because Label Encoding creates false ordinal relationships, which harms linear model performance.</p>"},{"location":"Data-processing/exploratory-data-analysis/","title":"Exploratory Data Analysis(EDA)","text":""},{"location":"Data-processing/exploratory-data-analysis/#exploratory-data-analysis-eda-tools-used-in-machine-learning","title":"Exploratory Data Analysis (EDA) tools used in Machine Learning","text":""},{"location":"Data-processing/exploratory-data-analysis/#popular-eda-tools-and-libraries","title":"\ud83e\uddf0 Popular EDA Tools and Libraries","text":"Tool / Library Description Language Pandas Profiling Auto-generates a detailed EDA report from a pandas DataFrame. Python Sweetviz Generates beautiful, high-density visualizations and comparisons between datasets. Python D-Tale Combines pandas with a Flask web server for visual data exploration in a browser. Python AutoViz Automatically visualizes any dataset with just one line of code. Python EDA (Dataprep) Simple, interactive EDA with summary statistics, correlations, and missing value analysis. Python Lux Augments pandas DataFrames with visual recommendations. Python Tidyverse (ggplot2) A collection of R packages including tools for data wrangling and visualization. R DataExplorer A powerful EDA package for automated report generation in R. R Orange A visual programming tool for EDA and ML without coding. GUI-based Tableau / Power BI Business intelligence tools that allow drag-and-drop EDA and visual analytics. GUI-based Qlik Sense Interactive data visualization and EDA in enterprise environments. GUI-based Kibana For time-series and log-based data exploration in Elasticsearch. Web-based"},{"location":"Data-processing/exploratory-data-analysis/#popular-python-libraries-used-in-manual-eda","title":"\ud83d\udd0d Popular Python Libraries Used in Manual EDA","text":"Library Use Case pandas Data manipulation and descriptive stats matplotlib Basic plotting seaborn Advanced statistical visualization plotly Interactive and dynamic visualizations missingno Missing value visualization scipy / statsmodels Statistical summaries, tests"},{"location":"Data-processing/exploratory-data-analysis/#pandas-profiling","title":"Pandas Profiling","text":"<pre><code>!pip install ydata-profiling\n</code></pre> <pre><code>from ydata_profiling import ProfileReport\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom ydata_profiling import ProfileReport\n\n# Load the Iris dataset\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = data.target\n\n# Generate the profile report\nprofile = ProfileReport(df, title=\"Iris Dataset Profile Report\", explorative=True)\n\n# Save the report as an HTML file\nprofile.to_file(\"iris_profile_report.html\")\n</code></pre> <p>Click here to view the Iris Profile Report</p>"},{"location":"Data-processing/exploratory-data-analysis/#autoviz","title":"autoviz","text":"<pre><code>!pip install autoviz\n</code></pre> <pre><code>from autoviz.AutoViz_Class import AutoViz_Class\nav = AutoViz_Class()\n\nreport = av.AutoViz(\"\", dfte=df, depVar=\"target\")\n</code></pre>"},{"location":"Data-processing/exploratory-data-analysis/#sweetviz","title":"sweetviz","text":"<pre><code>!pip install sweetviz\n</code></pre> <pre><code>!pip install numpy==1.24.4\n</code></pre> <pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\nimport sweetviz as sv\n\n# Load the Iris datasetdata\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = data.target\n</code></pre> <pre><code>#Generate a report\nreport = sv.analyze(df)\nreport.show_html('iris_report.html')\n</code></pre> <p>Click here to view the Iris sweetviz Report</p> <pre><code>from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Compare two Datasets\n\ncompare_report = sv.compare([train_df, \"Training Data\"], [test_df, \"Test Data\"])\ncompare_report.show_html('compare_report.html')\n</code></pre> <p>Click here to view the Iris sweetviz Report</p> <pre><code>import seaborn as sns\n\ntitanic_df = sns.load_dataset('titanic')\n\nreport = sv.analyze(titanic_df, target_feat='survived')\nreport.show_html('titanic_report.html')\n</code></pre> <p>Click here to view the Iris sweetviz titanic Report</p>"},{"location":"Data-processing/sql/","title":"Basic SQL","text":""},{"location":"Data-processing/sql/#introduction-to-sql","title":"Introduction to SQL","text":""},{"location":"Data-processing/sql/#what-is-sql","title":"What is SQL?SQL Lesson 1: SELECT queries","text":"<p>SQL, or Structured Query Language, is a language designed to allow both technical and non-technical users to query, manipulate, and transform data from a relational database. And due to its simplicity, SQL databases provide safe and scalable storage for millions of websites and mobile applications.</p> <p>There are many popular SQL databases including SQLite, MySQL, Postgres, Oracle and Microsoft SQL Server. All of them support the common SQL language standard, which is what this site will be teaching, but each implementation can differ in the additional features and storage types it supports.</p> <p>To retrieve data from a SQL database, we need to write SELECT statements.</p>"},{"location":"Data-processing/sql/#exercise","title":"Exercise","text":"<p>We will be using a database with data about some of Pixar's classic movies for most of our exercises. This first exercise will only involve the Movies table, and the default query below currently shows all the properties of each movie. </p>"},{"location":"Data-processing/sql/#table-movies","title":"Table: moviesSQL Lesson 2: Queries with constraints","text":"id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110  1. Find the title of each film  <pre><code>SELECT title FROM movies;\n</code></pre>  2. Find the director of each film  <pre><code>SELECT director FROM movies;\n</code></pre>  3. Find the title and director of each film  <pre><code>SELECT title,director FROM movies;\n</code></pre>  4. Find the title and year of each film  <pre><code>SELECT title,year FROM movies;\n</code></pre>  5. Find all the information about each film  <pre><code>SELECT * FROM movies;\n</code></pre> <p>Now we know how to select for specific columns of data from a table, but if you had a table with a hundred million rows of data, reading through all the rows would be inefficient and perhaps even impossible.</p> <p>In order to filter certain results from being returned, we need to use a WHERE clause in the query. The clause is applied to each row of data by checking specific column values to determine whether it should be included in the results or not.</p> <p>Select query with constraints</p> <pre><code>SELECT column, another_column, \u2026\nFROM mytable\nWHERE condition\n    AND/OR another_condition\n    AND/OR \u2026;\n</code></pre> Operator Condition SQL Example =, !=, &lt;, &lt;=, &gt;, &gt;= Standard numerical operators col_name != 4 BETWEEN \u2026 AND \u2026 Number is within range of two values col_name BETWEEN 1.5 AND 10.5 NOT BETWEEN \u2026 AND \u2026 Number is not within range of two values col_name NOT BETWEEN 1 AND 10 IN (\u2026) Number exists in a list col_name IN (2, 4, 6) NOT IN (\u2026) Number does not exist in a list col_name NOT IN (1, 3, 5)"},{"location":"Data-processing/sql/#exercise_1","title":"ExerciseSQL Lesson 3: Queries with constraints","text":"1. Find the movie with a row id of 6  <pre><code>SELECT * FROM movies where id=6\n</code></pre>  2. Find the movies released in the years between 2000 and 2010  <pre><code>SELECT * FROM movies where year between 2000 and 2010;\n</code></pre>  3. Find the movies not released in the years between 2000 and 2010  <pre><code>SELECT * FROM movies where year not between 2000 and 2010;\n</code></pre>  4. Find the first 5 Pixar movies and their release year  <pre><code>SELECT title,Year FROM movies where id&lt;=5;\n</code></pre> <p>When writing WHERE clauses with columns containing text data, SQL supports a number of useful operators to do things like case-insensitive string comparison and wildcard pattern matching. We show a few common text-data specific operators below:</p> Operator Condition Example = Case sensitive exact string comparison (single equals) col_name = \"abc\" != or &lt;&gt; Case sensitive exact string inequality comparison col_name != \"abcd\" LIKE Case insensitive exact string comparison col_name LIKE \"ABC\" NOT LIKE Case insensitive exact string inequality comparison col_name NOT LIKE \"ABCD\" % Matches any sequence of characters (used with LIKE/NOT LIKE) col_name LIKE \"%AT%\" (matches \"AT\", \"ATTIC\", \"CAT\", \"BATS\") _ Matches a single character (used with LIKE/NOT LIKE) col_name LIKE \"AN_\" (matches \"AND\", not \"AN\") IN (\u2026) String exists in a list col_name IN (\"A\", \"B\", \"C\") NOT IN (\u2026) String does not exist in a list col_name NOT IN (\"D\", \"E\", \"F\")"},{"location":"Data-processing/sql/#exercise_2","title":"ExerciseSQL Lesson 4: Filtering and sorting Query results","text":"1. Find all the Toy Story movies  <pre><code>SELECT title FROM movies where title like 'Toy Story%'\n</code></pre>  2. Find all the movies directed by John Lasseter  <pre><code>SELECT title FROM movies where director like 'John Lasseter%'\n</code></pre>  3. Find all the movies (and director) not directed by John Lasseter  <pre><code>SELECT title,director FROM movies where director not like 'John Lasseter%'\n</code></pre>  4. Find all the WALL-* movies  <pre><code>SELECT * FROM movies where title like 'WALL-%'\n</code></pre> <p>Even though the data in a database may be unique, the results of any particular query may not be \u2013 take our Movies table for example, many different movies can be released the same year. In such cases, SQL provides a convenient way to discard rows that have a duplicate column value by using the DISTINCT keyword.</p> <p>Since the DISTINCT keyword will blindly remove duplicate rows, we will learn in a future lesson how to discard duplicates based on specific columns using grouping and the GROUP BY clause.</p>"},{"location":"Data-processing/sql/#ordering-results","title":"Ordering results","text":"<p>To help with this, SQL provides a way to sort your results by a given column in ascending or descending order using the ORDER BY clause.</p> <p>When an ORDER BY clause is specified, each row is sorted alpha-numerically based on the specified column's value.</p> <pre><code>SELECT column, another_column, \u2026\nFROM mytable\nWHERE condition(s)\nORDER BY column ASC/DESC;\n</code></pre>"},{"location":"Data-processing/sql/#limiting-results-to-a-subset","title":"Limiting results to a subset","text":"<p>Another clause which is commonly used with the ORDER BY clause are the LIMIT and OFFSET clauses, which are a useful optimization to indicate to the database the subset of the results you care about. The LIMIT will reduce the number of rows to return, and the optional OFFSET will specify where to begin counting the number rows from.</p> <pre><code>SELECT column, another_column, \u2026\nFROM mytable\nWHERE condition(s)\nORDER BY column ASC/DESC\nLIMIT num_limit OFFSET num_offset;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_3","title":"ExerciseSQL Review: Simple SELECT Queries","text":"1. List all directors of Pixar movies (alphabetically), without duplicates  <pre><code>SELECT DISTINCT(director) FROM movies ORDER BY director\n</code></pre>  2. List the last four Pixar movies released (ordered from most recent to least)  <pre><code>SELECT title,year FROM movies ORDER BY year desc  LIMIT 4\n</code></pre>  3. List the first five Pixar movies sorted alphabetically  <pre><code>SELECT title FROM movies ORDER BY title  LIMIT 5\n</code></pre>  4. List the next five Pixar movies sorted alphabetically  <pre><code>SELECT * FROM movies ORDER BY title  LIMIT 5 OFFSET 5\n</code></pre>"},{"location":"Data-processing/sql/#table-north_american_cities","title":"Table: north_american_cities","text":"City Country Population Latitude Longitude Guadalajara Mexico 1,500,800 20.659699 -103.349609 Toronto Canada 2,795,060 43.653226 -79.383184 Houston United States 2,195,914 29.760427 -95.369803 New York United States 8,405,837 40.712784 -74.005941 Philadelphia United States 1,553,165 39.952584 -75.165222 Havana Cuba 2,106,146 23.054070 -82.345189 Mexico City Mexico 8,555,500 19.432608 -99.133208 Phoenix United States 1,513,367 33.448377 -112.074037 Los Angeles United States 3,884,307 34.052234 -118.243685 Ecatepec de Morelos Mexico 1,742,000 19.601841 -99.050674 Montreal Canada 1,717,767 45.501689 -73.567256 Chicago United States 2,718,782 41.878114 -87.629798"},{"location":"Data-processing/sql/#exercise_4","title":"ExerciseSQL Lesson 6: Multi-table queries with JOINs","text":"1. List all the Canadian cities and their populations  <pre><code>SELECT city,population FROM north_american_cities \nwhere country = 'Canada';\n</code></pre>  2. Order all the cities in the United States by their latitude from north to south  <pre><code>SELECT city, latitude \nFROM north_american_cities \nWHERE country = 'United States' \nORDER BY latitude DESC;\n</code></pre>  3. List all the cities west of Chicago, ordered from west to east  <pre><code>SELECT city, longitude\nFROM north_american_cities\nWHERE longitude &lt; -87.629798\nORDER BY longitude ASC;\n</code></pre>  4. List the two largest cities in Mexico (by population)  <pre><code>SELECT city, population\nFROM north_american_cities\nWHERE country = 'Mexico'\nORDER BY population DESC\nLIMIT 2;\n</code></pre>  5. List the third and fourth largest cities (by population) in the United States and their population  <pre><code>SELECT city, population\nFROM north_american_cities\nWHERE country = 'United States'\nORDER BY population DESC\nLIMIT 2 OFFSET 2;\n</code></pre>"},{"location":"Data-processing/sql/#database-normalization","title":"Database normalization","text":"<p>Database normalization is useful because it minimizes duplicate data in any single table, and allows for data in the database to grow independently of each other (ie. Types of car engines can grow independent of each type of car). As a trade-off, queries get slightly more complex since they have to be able to find data from different parts of the database, and performance issues can arise when working with many large tables.</p> <p>In order to answer questions about an entity that has data spanning multiple tables in a normalized database, we need to learn how to write a query that can combine all that data and pull out exactly the information we need.</p>"},{"location":"Data-processing/sql/#multi-table-queries-with-joins","title":"Multi-table queries with JOINs","text":"<p>Tables that share information about a single entity need to have a primary key that identifies that entity uniquely across the database. One common primary key type is an auto-incrementing integer (because they are space efficient), but it can also be a string, hashed value, so long as it is unique.</p> <p>Using the JOIN clause in a query, we can combine row data across two separate tables using this unique key. The first of the joins that we will introduce is the INNER JOIN.</p> <pre><code>SELECT column, another_table_column, \u2026\nFROM mytable\nINNER JOIN another_table \n    ON mytable.id = another_table.id\nWHERE condition(s)\nORDER BY column, \u2026 ASC/DESC\nLIMIT num_limit OFFSET num_offset;\n</code></pre> <p>The INNER JOIN is a process that matches rows from the first table and the second table which have the same key (as defined by the ON constraint) to create a result row with the combined columns from both tables. </p>"},{"location":"Data-processing/sql/#table-movies-read-only","title":"Table: movies (Read-only)","text":"id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110"},{"location":"Data-processing/sql/#table-boxoffice-read-only","title":"Table: boxoffice (Read-only)","text":"movie_id rating domestic_sales international_sales 5 8.2 380843261 555900000 14 7.4 268492764 475066843 8 8.0 206445654 417277164 12 6.4 191452396 368400000 3 7.9 245852179 239163000 6 8.0 261441092 370001000 9 8.5 223808164 297503696 11 8.4 415004880 648167031 1 8.3 191796233 170162503 7 7.2 244082982 217900167 10 8.3 293004164 438338580 4 8.1 289916256 272900000 2 7.2 162798565 200600000 13 7.2 237283207 301700000"},{"location":"Data-processing/sql/#exercise_5","title":"ExerciseSQL Lesson 7: OUTER JOINs","text":"1. Find the domestic and international sales for each movie  <pre><code>SELECT \n    movies.title,\n    boxoffice.domestic_sales,\n    boxoffice.international_sales\nFROM \n    movies\nJOIN \n    boxoffice\nON \n    movies.id = boxoffice.movie_id;\n</code></pre>  2. Show the sales numbers for each movie that did better internationally rather than domestically  <pre><code>SELECT \n    movies.title,\n    boxoffice.domestic_sales,\n    boxoffice.international_sales\nFROM \n    movies\nJOIN \n    boxoffice\nON \n    movies.id = boxoffice.movie_id\nWHERE \n    boxoffice.international_sales &gt; boxoffice.domestic_sales;\n</code></pre>  3. List all the movies by their ratings in descending order  <pre><code>SELECT \n    movies.title,\n    boxoffice.rating\nFROM \n    movies\nJOIN \n    boxoffice\nON \n    movies.id = boxoffice.movie_id\nORDER BY \n    boxoffice.rating DESC;\n</code></pre> <p>Depending on how you want to analyze the data, the INNER JOIN we used last lesson might not be sufficient because the resulting table only contains data that belongs in both of the tables.</p> <p>If the two tables have asymmetric data, which can easily happen when data is entered in different stages, then we would have to use a LEFT JOIN, RIGHT JOIN or FULL JOIN instead to ensure that the data you need is not left out of the results.</p>"},{"location":"Data-processing/sql/#select-query-with-leftrightfull-joins-on-multiple-tables","title":"Select query with LEFT/RIGHT/FULL JOINs on multiple tables","text":"<pre><code>SELECT column, another_column, \u2026\nFROM mytable\nINNER/LEFT/RIGHT/FULL JOIN another_table \n    ON mytable.id = another_table.matching_id\nWHERE condition(s)\nORDER BY column, \u2026 ASC/DESC\nLIMIT num_limit OFFSET num_offset;\n</code></pre> <p>Like the INNER JOIN these three new joins have to specify which column to join the data on. - When joining table A to table B, a LEFT JOIN simply includes rows from A regardless of whether a matching row is found in B. - The RIGHT JOIN is the same, but reversed, keeping rows in B regardless of whether a match is found in A. - Finally, a FULL JOIN simply means that rows from both tables are kept, regardless of whether a matching row exists in the other table.</p>"},{"location":"Data-processing/sql/#table-buildings-read-only","title":"Table: buildings (Read-only)","text":"building_name capacity 1e 24 1w 32 2e 16 2w 20"},{"location":"Data-processing/sql/#table-employees-read-only","title":"Table: employees (Read-only)","text":"role name building years_employed Engineer Becky A. 1e 4 Engineer Dan B. 1e 2 Engineer Sharon F. 1e 6 Engineer Dan M. 1e 4 Engineer Malcom S. 1e 1 Artist Tylar S. 2w 2 Artist Sherman D. 2w 8 Artist Jakob J. 2w 6 Artist Lillia A. 2w 7 Artist Brandon J. 2w 7 Manager Scott K. 1e 9 Manager Shirlee M. 1e 3 Manager Daria O. 2w 6"},{"location":"Data-processing/sql/#exercise_6","title":"ExerciseSQL Lesson 8: A short note on NULLs","text":"1. Find the list of all buildings that have employees  <pre><code>SELECT DISTINCT building FROM employees;\n</code></pre>  2. Find the list of all buildings and their capacity  <pre><code>SELECT * FROM buildings;\n</code></pre>  3. List all buildings and the distinct employee roles in each building (including empty buildings)  <pre><code>SELECT b.building_name, e.role\nFROM buildings b\nLEFT JOIN employees e ON b.building_name = e.building\nGROUP BY b.building_name, e.role\nORDER BY b.building_name, e.role;\n</code></pre> <p>It's always good to reduce the possibility of NULL values in databases because they require special attention when constructing queries, constraints (certain functions behave differently with null values) and when processing the results.</p> <p>An alternative to NULL values in your database is to have data-type appropriate default values, like 0 for numerical data, empty strings for text data, etc. But if your database needs to store incomplete data, then NULL values can be appropriate if the default values will skew later analysis (for example, when taking averages of numerical data).</p> <p>Sometimes, it's also not possible to avoid NULL values, as we saw in the last lesson when outer-joining two tables with asymmetric data. In these cases, you can test a column for NULL values in a WHERE clause by using either the IS NULL or IS NOT NULL constraint.</p>"},{"location":"Data-processing/sql/#select-query-with-constraints-on-null-values","title":"Select query with constraints on NULL values","text":"<pre><code>SELECT column, another_column, \u2026\nFROM mytable\nWHERE column IS/IS NOT NULL\nAND/OR another_condition\nAND/OR \u2026;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_7","title":"ExerciseSQL Lesson 9: Queries with expressions","text":"1. Find the name and role of all employees who have not been assigned to a building  <pre><code>SELECT name, role\nFROM employees\nWHERE building IS NULL OR building = '';\n</code></pre>  2. Find the names of the buildings that hold no employees  <pre><code>SELECT b.building_name\nFROM buildings b\nLEFT JOIN employees e ON b.building_name = e.building\nWHERE e.building IS NULL;\n</code></pre> <p>In addition to querying and referencing raw column data with SQL, you can also use expressions to write more complex logic on column values in a query. These expressions can use mathematical and string functions along with basic arithmetic to transform values when the query is executed, as shown in this physics example.</p>"},{"location":"Data-processing/sql/#example-query-with-expressions","title":"Example query with expressions","text":"<pre><code>SELECT particle_speed / 2.0 AS half_particle_speed\nFROM physics_data\nWHERE ABS(particle_position) * 10.0 &gt; 500;\n</code></pre> <p>Each database has its own supported set of mathematical, string, and date functions that can be used in a query, which you can find in their own respective docs.</p> <p>The use of expressions can save time and extra post-processing of the result data, but can also make the query harder to read, so we recommend that when expressions are used in the SELECT part of the query, that they are also given a descriptive alias using the AS keyword.</p>"},{"location":"Data-processing/sql/#select-query-with-expression-aliases","title":"Select query with expression aliases","text":"<pre><code>SELECT col_expression AS expr_description, \u2026\nFROM mytable;\n</code></pre> <p>In addition to expressions, regular columns and even tables can also have aliases to make them easier to reference in the output and as a part of simplifying more complex queries.</p>"},{"location":"Data-processing/sql/#example-query-with-both-column-and-table-name-aliases","title":"Example query with both column and table name aliases","text":"<pre><code>SELECT column AS better_column_name, \u2026\nFROM a_long_widgets_table_name AS mywidgets\nINNER JOIN widget_sales\n  ON mywidgets.id = widget_sales.widget_id;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_8","title":"ExerciseSQL Lesson 10: Queries with aggregates","text":"1. List all movies and their combined sales in millions of dollars  <pre><code>SELECT title, (domestic_sales + international_sales) / 1000000 AS gross_sales_millions\nFROM movies\n  JOIN boxoffice\n    ON movies.id = boxoffice.movie_id;\n</code></pre>  2. List all movies and their ratings in percent  <pre><code>SELECT title, rating * 10 AS rating_percent\nFROM movies\n  JOIN boxoffice\n    ON movies.id = boxoffice.movie_id;\n</code></pre>  3. List all movies that were released on even number years  <pre><code>SELECT title, year\nFROM movies\nWHERE year % 2 = 0;\n</code></pre> <p>In addition to the simple expressions that we introduced last lesson, SQL also supports the use of aggregate expressions (or functions) that allow you to summarize information about a group of rows of data. With the Pixar database that you've been using, aggregate functions can be used to answer questions like, \"How many movies has Pixar produced?\", or \"What is the highest grossing Pixar film each year?\".</p>"},{"location":"Data-processing/sql/#select-query-with-aggregate-functions-over-all-rows","title":"Select query with aggregate functions over all rows","text":"<pre><code>SELECT AGG_FUNC(column_or_expression) AS aggregate_description, \u2026\nFROM mytable\nWHERE constraint_expression;\n</code></pre> <p>Without a specified grouping, each aggregate function is going to run on the whole set of result rows and return a single value. And like normal expressions, giving your aggregate functions an alias ensures that the results will be easier to read and process.</p>"},{"location":"Data-processing/sql/#common-aggregate-functions","title":"Common aggregate functions","text":"<p>Here are some common aggregate functions that we are going to use in our examples:</p> Function Description COUNT(*), COUNT(column) Counts the number of rows in the group if no column is specified; otherwise counts non-NULL values in the specified column. MIN(column) Finds the smallest numerical value in the specified column for all rows in the group. MAX(column) Finds the largest numerical value in the specified column for all rows in the group. AVG(column) Finds the average numerical value in the specified column for all rows in the group. SUM(column) Finds the sum of all numerical values in the specified column for the rows in the group."},{"location":"Data-processing/sql/#grouped-aggregate-functions","title":"Grouped aggregate functions","text":"<p>In addition to aggregating across all the rows, you can instead apply the aggregate functions to individual groups of data within that group (ie. box office sales for Comedies vs Action movies). This would then create as many results as there are unique groups defined as by the GROUP BY clause.</p>"},{"location":"Data-processing/sql/#select-query-with-aggregate-functions-over-groups","title":"Select query with aggregate functions over groups","text":"<pre><code>SELECT AGG_FUNC(column_or_expression) AS aggregate_description, \u2026\nFROM mytable\nWHERE constraint_expression\nGROUP BY column;\n</code></pre> <p>The GROUP BY clause works by grouping rows that have the same value in the column specified.</p>"},{"location":"Data-processing/sql/#select-query-with-having-constraint","title":"Select query with HAVING constraint","text":"<pre><code>SELECT group_by_column, AGG_FUNC(column_expression) AS aggregate_result_alias, \u2026\nFROM mytable\nWHERE condition\nGROUP BY column\nHAVING group_condition;\n</code></pre> <p>The HAVING clause constraints are written the same way as the WHERE clause constraints, and are applied to the grouped rows. With our examples, this might not seem like a particularly useful construct, but if you imagine data with millions of rows with different properties, being able to apply additional constraints is often necessary to quickly make sense of the data.</p>"},{"location":"Data-processing/sql/#table-employees","title":"Table: employees","text":"role name building years_employed Engineer Becky A. 1e 4 Engineer Dan B. 1e 2 Engineer Sharon F. 1e 6 Engineer Dan M. 1e 4 Engineer Malcom S. 1e 1 Artist Tylar S. 2w 2 Artist Sherman D. 2w 8 Artist Jakob J. 2w 6 Artist Lillia A. 2w 7 Artist Brandon J. 2w 7 Manager Scott K. 1e 9 Manager Shirlee M. 1e 3 Manager Daria O. 2w 6"},{"location":"Data-processing/sql/#exercise_9","title":"ExerciseSQL Lesson 11: Order of execution of a Query","text":"1. Find the longest time that an employee has been at the studio  <pre><code>SELECT MAX(years_employed) AS longest_time\nFROM employees;\n</code></pre>  2. For each role, find the average number of years employed by employees in that role  <pre><code>SELECT role, AVG(years_employed) as Average_years_employed\nFROM employees\nGROUP BY role;\n</code></pre>  3. Find the total number of employee years worked in each building  <pre><code>SELECT building, SUM(years_employed) as Total_years_employed\nFROM employees\nGROUP BY building;\n</code></pre>  4. Find the number of Artists in the studio (without a HAVING clause)   <pre><code>SELECT role, COUNT(*) as Number_of_artists\nFROM employees\nWHERE role = \"Artist\";\n</code></pre>  5. Find the number of Employees of each role in the studio  <pre><code>SELECT role, COUNT(*)\nFROM employees\nGROUP BY role;\n</code></pre>  6. Find the total number of years employed by all Engineers  <pre><code>SELECT role, SUM(years_employed)\nFROM employees\nGROUP BY role\nHAVING role = \"Engineer\";\n</code></pre> <p>Now that we have an idea of all the parts of a query, we can now talk about how they all fit together in the context of a complete query.</p>"},{"location":"Data-processing/sql/#complete-select-query","title":"Complete SELECT query","text":"<pre><code>SELECT DISTINCT column, AGG_FUNC(column_or_expression), \u2026\nFROM mytable\n    JOIN another_table\n      ON mytable.column = another_table.column\n    WHERE constraint_expression\n    GROUP BY column\n    HAVING constraint_expression\n    ORDER BY column ASC/DESC\n    LIMIT count OFFSET COUNT;\n</code></pre>"},{"location":"Data-processing/sql/#query-order-of-execution","title":"Query order of execution","text":"<ol> <li><code>`FROM and JOINs</code></li> <li><code>WHERE</code></li> <li><code>GROUP BY</code></li> <li><code>HAVING</code></li> <li><code>SELECT</code></li> <li><code>DISTINCT</code></li> <li><code>ORDER BY</code></li> <li><code>LIMIT / OFFSET</code></li> </ol>"},{"location":"Data-processing/sql/#table-movies-read-only_1","title":"Table: movies (Read-only)","text":"id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110"},{"location":"Data-processing/sql/#table-boxoffice-read-only_1","title":"Table: boxoffice (Read-only)","text":"movie_id rating domestic_sales international_sales 5 8.2 380,843,261 555,900,000 14 7.4 268,492,764 475,066,843 8 8.0 206,445,654 417,277,164 12 6.4 191,452,396 368,400,000 3 7.9 245,852,179 239,163,000 6 8.0 261,441,092 370,001,000 9 8.5 223,808,164 297,503,696 11 8.4 415,004,880 648,167,031 1 8.3 191,796,233 170,162,503 7 7.2 244,082,982 217,900,167 10 8.3 293,004,164 438,338,580 4 8.1 289,916,256 272,900,000 2 7.2 162,798,565 200,600,000 13 7.2 237,283,207 301,700,000"},{"location":"Data-processing/sql/#exercise_10","title":"ExerciseSQL Lesson 12: Creating tables","text":"1. Find the number of movies each director has directed  <pre><code>SELECT director, COUNT(id) as Num_movies_directed\nFROM movies\nGROUP BY director;\n</code></pre>  2. Find the total domestic and international sales that can be attributed to each director  <pre><code>SELECT director, SUM(domestic_sales + international_sales) as Cumulative_sales_from_all_movies\nFROM movies\n    INNER JOIN boxoffice\n        ON movies.id = boxoffice.movie_id\nGROUP BY director;\n</code></pre> <p>When you have new entities and relationships to store in your database, you can create a new database table using the CREATE TABLE statement.</p>"},{"location":"Data-processing/sql/#create-table-statement-w-optional-table-constraint-and-default-value","title":"Create table statement w/ optional table constraint and default value","text":"<pre><code>CREATE TABLE IF NOT EXISTS mytable (\n    column DataType TableConstraint DEFAULT default_value,\n    another_column DataType TableConstraint DEFAULT default_value,\n    \u2026\n);\n</code></pre>"},{"location":"Data-processing/sql/#table-data-types","title":"Table data types","text":"<p>Different databases support different data types, but the common types support numeric, string, and other miscellaneous things like dates, booleans, or even binary data. Here are some examples that you might use in real code.</p> Data Type Description INTEGER, BOOLEAN Store whole integer values like counts or ages. Boolean may be represented as 0 or 1. FLOAT, DOUBLE, REAL Store precise numerical data with fractional values; different types indicate different floating point precisions. CHARACTER(num_chars) Fixed-length text data type that stores a specific number of characters; may truncate longer values. VARCHAR(num_chars) Variable-length text data type with a max character limit; more efficient for large tables than fixed-length types. TEXT Stores strings and text of varying length, typically without a specified max length. DATE, DATETIME Store date and time stamps; useful for time series and event data, but can be complex with timezones. BLOB Stores binary large objects (binary data); opaque to database, requiring proper metadata for retrieval."},{"location":"Data-processing/sql/#table-constraints","title":"Table constraints","text":"<p>We aren't going to dive too deep into table constraints in this lesson, but each column can have additional table constraints on it which limit what values can be inserted into that column. This is not a comprehensive list, but will show a few common constraints that you might find useful.</p> Constraint Description PRIMARY KEY Values in this column are unique and identify a single row in the table. AUTOINCREMENT Automatically fills and increments integer values with each row insertion (not supported in all databases). UNIQUE Values in this column must be unique, but unlike PRIMARY KEY, it does not necessarily identify a row. NOT NULL Values inserted in this column cannot be NULL. CHECK (expression) Validates values based on a condition/expression, e.g., ensuring positive values or specific formats. FOREIGN KEY Ensures that each value in this column corresponds to a valid value in another table\u2019s column, enforcing referential integrity."},{"location":"Data-processing/sql/#movies-table-schema","title":"Movies table schema","text":"<pre><code>CREATE TABLE movies (\n    id INTEGER PRIMARY KEY,\n    title TEXT,\n    director TEXT,\n    year INTEGER, \n    length_minutes INTEGER\n);\n</code></pre>"},{"location":"Data-processing/sql/#exercise_11","title":"ExerciseSQL Lesson 13: Inserting rows","text":"1. Create a new table named Database with the following columns:    \u2013 Name A string (text) describing the name of the database    \u2013 Version A number (floating point) of the latest version of this database    \u2013 Download_count An integer count of the number of times this database was downloaded    - This table has no constraints.  <pre><code>CREATE TABLE Database (\n    Name TEXT,\n    Version FLOAT,\n    Download_count INTEGER\n);\n</code></pre> <p>What is a Schema? We previously described a table in a database as a two-dimensional set of rows and columns, with the columns being the properties and the rows being instances of the entity in the table. In SQL, the database schema is what describes the structure of each table, and the datatypes that each column of the table can contain.</p>"},{"location":"Data-processing/sql/#inserting-new-data","title":"Inserting new data","text":"<p>When inserting data into a database, we need to use an INSERT statement, which declares which table to write into, the columns of data that we are filling, and one or more rows of data to insert. In general, each row of data you insert should contain values for every corresponding column in the table. You can insert multiple rows at a time by just listing them sequentially.</p>"},{"location":"Data-processing/sql/#insert-statement-with-values-for-all-columns","title":"Insert statement with values for all columns","text":"<pre><code>INSERT INTO mytable\nVALUES (value_or_expr, another_value_or_expr, \u2026),\n       (value_or_expr_2, another_value_or_expr_2, \u2026),\n       \u2026;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_12","title":"ExerciseSQL Lesson 13: Updating rows","text":"1. Add the studio's new production, Toy Story 4 to the list of movies (you can use any director)  <pre><code>INSERT INTO movies VALUES (4, \"Toy Story 4\", \"El Directore\", 2015, 90);\n</code></pre>  2. Toy Story 4 has been released to critical acclaim! It had a rating of 8.7, and made 340 million domestically and 270 million internationally. Add the record to the BoxOffice table.  <pre><code>INSERT INTO boxoffice VALUES (4, 8.7, 340000000, 270000000);\n</code></pre> <p>In addition to adding new data, a common task is to update existing data, which can be done using an UPDATE statement. Similar to the INSERT statement, you have to specify exactly which table, columns, and rows to update. In addition, the data you are updating has to match the data type of the columns in the table schema.</p>"},{"location":"Data-processing/sql/#update-statement-with-values","title":"Update statement with values","text":"<pre><code>UPDATE mytable\nSET column = value_or_expr, \n    other_column = another_value_or_expr, \n    \u2026\nWHERE condition;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_13","title":"ExerciseSQL Lesson 14: Deleting rows","text":"1. The director for A Bug's Life is incorrect, it was actually directed by John Lasseter  <pre><code>UPDATE movies\nSET director = \"John Lasseter\"\nWHERE id = 2;\n</code></pre>  2. The year that Toy Story 2 was released is incorrect, it was actually released in 1999  <pre><code>UPDATE movies\nSET year = 1999\nWHERE id = 3;\n</code></pre>  3. Both the title and director for Toy Story 8 is incorrect! The title should be \"Toy Story 3\" and it was directed by Lee Unkrich  <pre><code>UPDATE movies\nSET title = \"Toy Story 3\", director = \"Lee Unkrich\"\nWHERE id = 11;\n</code></pre> <p>When you need to delete data from a table in the database, you can use a DELETE statement, which describes the table to act on, and the rows of the table to delete through the WHERE clause.</p>"},{"location":"Data-processing/sql/#delete-statement-with-condition","title":"Delete statement with condition","text":"<pre><code>DELETE FROM mytable\nWHERE condition;\n</code></pre> <p>If you decide to leave out the WHERE constraint, then all rows are removed, which is a quick and easy way to clear out a table completely (if intentional).</p>"},{"location":"Data-processing/sql/#taking-extra-care","title":"Taking extra care","text":"<p>Like the UPDATE statement from last lesson, it's recommended that you run the constraint in a SELECT query first to ensure that you are removing the right rows. Without a proper backup or test database, it is downright easy to irrevocably remove data, so always read your DELETE statements twice and execute once.</p>"},{"location":"Data-processing/sql/#exercise_14","title":"ExerciseSQL Lesson 15: Altering tables","text":"1. This database is getting too big, lets remove all movies that were released before 2005.  <pre><code>DELETE FROM movies\nwhere year &lt; 2005;\n</code></pre>  2. Andrew Stanton has also left the studio, so please remove all movies directed by him.  <pre><code>DELETE FROM movies\nwhere director = \"Andrew Stanton\";\n</code></pre> <p>As your data changes over time, SQL provides a way for you to update your corresponding tables and database schemas by using the ALTER TABLE statement to add, remove, or modify columns and table constraints.</p>"},{"location":"Data-processing/sql/#adding-columns","title":"Adding columns","text":"<p>The syntax for adding a new column is similar to the syntax when creating new rows in the CREATE TABLE statement. You need to specify the data type of the column along with any potential table constraints and default values to be applied to both existing and new rows. In some databases like MySQL, you can even specify where to insert the new column using the FIRST or AFTER clauses, though this is not a standard feature.</p>"},{"location":"Data-processing/sql/#altering-table-to-add-new-columns","title":"Altering table to add new column(s)","text":"<pre><code>ALTER TABLE mytable\nADD column DataType OptionalTableConstraint \n    DEFAULT default_value;\n</code></pre>"},{"location":"Data-processing/sql/#removing-columns","title":"Removing columns","text":"<p>Dropping columns is as easy as specifying the column to drop, however, some databases (including SQLite) don't support this feature. Instead you may have to create a new table and migrate the data over.</p>"},{"location":"Data-processing/sql/#altering-table-to-remove-columns","title":"Altering table to remove column(s)","text":"<pre><code>ALTER TABLE mytable\nDROP column_to_be_deleted;\n</code></pre>"},{"location":"Data-processing/sql/#renaming-the-table","title":"Renaming the table","text":"<p>If you need to rename the table itself, you can also do that using the RENAME TO clause of the statement.</p>"},{"location":"Data-processing/sql/#altering-table-name","title":"Altering table name","text":"<pre><code>ALTER TABLE mytable\nRENAME TO new_table_name;\n</code></pre>"},{"location":"Data-processing/sql/#exercise_15","title":"ExerciseSQL Lesson 16: Dropping tables","text":"1. Add a column named Aspect_ratio with a FLOAT data type to store the aspect-ratio each movie was released in.  <pre><code>ALTER TABLE Movies\n  ADD COLUMN Aspect_ratio FLOAT DEFAULT 2.39;\n</code></pre>  2. Add another column named Language with a TEXT data type to store the language that the movie was released in. Ensure that the default for this language is English.  <pre><code>ALTER TABLE Movies\n  ADD COLUMN Language TEXT DEFAULT \"English\";\n</code></pre> <p>In some rare cases, you may want to remove an entire table including all of its data and metadata, and to do so, you can use the DROP TABLE statement, which differs from the DELETE statement in that it also removes the table schema from the database entirely.</p>"},{"location":"Data-processing/sql/#drop-table-statement","title":"Drop table statement","text":"<pre><code>DROP TABLE IF EXISTS mytable;\n</code></pre> <p>Like the CREATE TABLE statement, the database may throw an error if the specified table does not exist, and to suppress that error, you can use the IF EXISTS clause.</p> <p>In addition, if you have another table that is dependent on columns in table you are removing (for example, with a FOREIGN KEY dependency) then you will have to either update all dependent tables first to remove the dependent rows or to remove those tables entirely.</p>"},{"location":"Data-processing/sql/#exercise_16","title":"Exercise","text":"1. We've sadly reached the end of our lessons, lets clean up by removing the Movies table  <pre><code>DROP TABLE Movies;\n</code></pre>  2. And drop the BoxOffice table as well  <pre><code>DROP TABLE BoxOffice;\n</code></pre>"},{"location":"DeepLearning/Overview/","title":"Overview","text":"\u2705 Deep Learning \ud83d\udccc What is Deep Learning? <p>Deep Learning is a subset of Machine Learning (ML) that uses algorithms called artificial neural networks, inspired by the structure and function of the human brain. Deep learning is particularly powerful when working with unstructured data like images, audio, text, or videos.</p> <p>The \"deep\" in deep learning refers to the number of layers in these neural networks. A neural network is composed of layers of interconnected nodes (neurons). A deep neural network has many hidden layers between the input and output layers, allowing it to learn and represent data at various levels of abstraction.</p> \ud83d\udccc Key Concepts <ol> <li>Neural Networks</li> </ol> <p>A neural network is made up of layers of nodes (neurons):</p> <ul> <li> <p>Input layer (where data is fed)</p> </li> <li> <p>Hidden layers (where computation happens)</p> </li> <li> <p>Output layer (where the result is produced)</p> </li> <li> <p>Deep Neural Networks (DNN)</p> </li> </ul> <p>A \"deep\" network has multiple hidden layers. These allow it to learn complex patterns.</p> <ol> <li> <p>Common Deep Learning Architectures:</p> </li> <li> <p>CNN (Convolutional Neural Networks) \u2013 for images</p> </li> <li> <p>RNN (Recurrent Neural Networks) \u2013 for sequences, e.g., text</p> </li> <li> <p>Transformers \u2013 modern architectures used in NLP (like ChatGPT)</p> </li> </ol> <p></p> <p></p> <p></p> <p></p> <p></p> \ud83d\udccc What is a Neural Network? <p>Neural networks are machine learning models that mimic the complex functions of the human brain. These models consist of interconnected nodes or neurons that process data, learn patterns and enable tasks such as pattern recognition and decision-making.</p> <p></p> <p></p> <p></p> <p></p> \ud83d\udccc Understanding Neural Networks in Deep Learning <p>Neural networks are capable of learning and identifying patterns directly from data without pre-defined rules. These networks are built from several key components:</p> <ol> <li> <p>Neurons: The basic units that receive inputs, each neuron is governed by a threshold and an activation function.</p> </li> <li> <p>Connections: Links between neurons that carry information, regulated by weights and biases.</p> </li> <li> <p>Weights and Biases: These parameters determine the strength and influence of connections.</p> </li> <li> <p>Propagation Functions: Mechanisms that help process and transfer data across layers of neurons.</p> </li> <li> <p>Learning Rule: The method that adjusts weights and biases over time to improve accuracy.</p> </li> </ol> \ud83d\udccc Neural networks follows a structured, three-stage process: <ol> <li> <p>Input Computation: Data is fed into the network.</p> </li> <li> <p>Output Generation: Based on the current parameters, the network generates an output.</p> </li> <li> <p>Iterative Refinement: The network refines its output by adjusting weights and biases, gradually improving its performance on diverse tasks.</p> </li> </ol> \ud83d\udccc In an adaptive learning environment: <ul> <li> <p>The neural network is exposed to a simulated scenario or dataset.</p> </li> <li> <p>Parameters such as weights and biases are updated in response to new data or conditions.</p> </li> <li> <p>With each adjustment, the network\u2019s response evolves allowing it to adapt effectively to different tasks or environments.</p> </li> </ul> <p></p> \ud83d\udccc Layers in Neural Network Architecture: <p></p> \ud83d\udccc What is Forward Propagation? <p>When data is input into the network, it passes through the network in the forward direction, from the input layer through the hidden layers to the output layer. This process is known as forward propagation. Here\u2019s what happens during this phase:</p> <p>1. Linear Transformation: Each neuron in a layer receives inputs which are multiplied by the weights associated with the connections. These products are summed together and a bias is added to the sum. This can be represented mathematically as:</p> <p></p> <p>where</p> <ul> <li> <p>w represents the weights</p> </li> <li> <p>x represents the inputs</p> </li> <li> <p>b is the bias</p> </li> </ul> <p>2. Activation: The result of the linear transformation (denoted as z) is then passed through an activation function. The activation function is crucial because it introduces non-linearity into the system, enabling the network to learn more complex patterns. Popular activation functions include <code>ReLU</code>, <code>sigmoid</code> and <code>tanh</code>.</p> <p>Forward Propagation is the process of passing input data through the neural network layer-by-layer to get an output (or prediction).</p> <ul> <li> <p>Computing weighted sums</p> </li> <li> <p>Applying activation functions</p> </li> <li> <p>Passing the output to the next layer, until reaching the final prediction</p> </li> </ul> <p>It's like feeding data forward through the network.</p> \ud83d\uded2 Real-Time Example: Predicting Purchase Decision in E-Commerce <p>Imagine you're building a model to predict whether a customer will buy a product or not, based on:</p> Feature Value Time on website 10 minutes Pages visited 5 Previous purchases 2 <p>Feed this input into a small neural network to predict: Buy (1) or Not Buy (0).</p> \ud83e\udde0 Neural Network Structure <p>Let\u2019s say your network looks like this:</p> <ul> <li> <p>Input Layer: 3 neurons (for 3 input features)</p> </li> <li> <p>Hidden Layer: 2 neurons</p> </li> <li> <p>Output Layer: 1 neuron (Buy or Not)</p> </li> </ul> <p>Input \u2192 [Hidden1, Hidden2] \u2192 Output</p> \u2797 Step-by-Step Forward Propagation <p>\ud83c\udfaf Inputs</p> <pre><code>x = [10, 5, 2]  # Time, Pages, Purchases\n</code></pre> <p>\ud83d\udd17 Weights (Randomly initialized)</p> <p>Hidden Layer weights:</p> <pre><code>w1 = [[0.2, 0.4, 0.1],   # Weights for Hidden1\n      [0.5, 0.3, 0.2]]   # Weights for Hidden2\n</code></pre> <p>Output Layer weights:</p> <pre><code>w2 = [0.6, 0.9]  # Weights from Hidden1 and Hidden2 to Output\n</code></pre> <p>\ud83d\udcc8 Step 1: Input \u2192 Hidden Layer</p> <p>For Hidden1:</p> <pre><code>z1 = 10*0.2 + 5*0.4 + 2*0.1 = 2 + 2 + 0.2 = 4.2\na1 = sigmoid(4.2) \u2248 0.985\n</code></pre> <p>For Hidden2:</p> <pre><code>z2 = 10*0.5 + 5*0.3 + 2*0.2 = 5 + 1.5 + 0.4 = 6.9\na2 = sigmoid(6.9) \u2248 0.999\n</code></pre> <p>Now, hidden layer outputs:</p> <p>hidden_output = [0.985, 0.999]</p> <p>\ud83e\uddee Step 2: Hidden \u2192 Output</p> <pre><code>z3 = 0.985*0.6 + 0.999*0.9 = 0.591 + 0.899 = 1.49\na3 = sigmoid(1.49) \u2248 0.816\n</code></pre> <p>\u2705 Final Prediction: 0.816</p> <p>This means:</p> <p><code>There's an 81.6% chance that the customer will buy the product.</code></p> <p>\ud83e\udde0 Summary</p> Step Layer Formula Value 1 Hidden1 <code>z = x\u00b7w + b</code> \u2192 <code>sigmoid(z)</code> <code>0.985</code> 2 Hidden2 same as above <code>0.999</code> 3 Output <code>z = hidden\u00b7w + b</code> \u2192 <code>sigmoid(z)</code> <code>0.816</code> \ud83d\udccc Why It Matters <p>Forward propagation is how the neural network generates predictions before learning. Once predictions are made, we compare them to the actual result, and then backpropagation is used to update weights to improve future predictions.</p> <p>Simple Python example using NumPy:</p> <pre><code>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Inputs\nx = np.array([10, 5, 2])\n\n# Weights\nw1 = np.array([[0.2, 0.4, 0.1],\n               [0.5, 0.3, 0.2]])\nw2 = np.array([0.6, 0.9])\n\n# Forward pass\nhidden_input = np.dot(w1, x)\nhidden_output = sigmoid(hidden_input)\n\nfinal_input = np.dot(w2, hidden_output)\noutput = sigmoid(final_input)\n\nprint(f\"Final output (purchase probability): {output:.3f}\")\n</code></pre> <p>Final output (purchase probability): 0.816</p> \ud83d\udccc What is Backpropagation? <p>After forward propagation, the network evaluates its performance using a loss function which measures the difference between the actual output and the predicted output. The goal of training is to minimize this loss. This is where backpropagation comes into play:</p> <ol> <li> <p>Loss Calculation: The network calculates the loss which provides a measure of error in the predictions. The loss function could vary; common choices are mean squared error for regression tasks or cross-entropy loss for classification.</p> </li> <li> <p>Gradient Calculation: The network computes the gradients of the loss function with respect to each weight and bias in the network. This involves applying the chain rule of calculus to find out how much each part of the output error can be attributed to each weight and bias.</p> </li> <li> <p>Weight Update: Once the gradients are calculated, the weights and biases are updated using an optimization algorithm like stochastic gradient descent (SGD). The weights are adjusted in the opposite direction of the gradient to minimize the loss. The size of the step taken in each update is determined by the learning rate.</p> </li> </ol> <p>Backpropagation is the learning process in deep learning. After forward propagation (i.e., making a prediction), the model:</p> <ol> <li> <p>Compares the predicted output to the actual value using a loss function</p> </li> <li> <p>Calculates how wrong the prediction was (the error)</p> </li> <li> <p>Moves backward through the network, adjusting the weights so that future predictions improve</p> </li> </ol> <p>\ud83d\udc49 Forward propagation = prediction</p> <p>\ud83d\udc49 Backpropagation = learning</p> \ud83d\udd01 Using the Same Example: E-Commerce Purchase Prediction <p>\ud83e\udde0 Setup Recap</p> <ul> <li> <p>Input: [10, 5, 2]</p> </li> <li> <p>Predicted output: 0.816 (from forward propagation)</p> </li> <li> <p>Actual output (label): 1 (customer actually bought)</p> </li> <li> <p>Loss function: Binary Cross-Entropy</p> </li> </ul> \u2699\ufe0f Step-by-Step Backpropagation <p>We use gradient descent to update the weights by calculating the gradient of the loss w.r.t. each weight.</p> <p>\ud83d\udd27 1. Compute Loss (Binary Cross Entropy)</p> <p></p> <p>This is the error we want to minimize.</p> <p>\ud83d\udd04 2. Compute Gradients (Chain Rule)</p> <p>We use the chain rule of calculus to backpropagate the error.</p> <p>Let\u2019s focus on the output neuron and then the hidden layer.</p> <p>\ud83e\uddee a. Output Layer</p> <p>We calculate how much the output neuron contributed to the error.</p> <p>Let\u2019s denote:</p> <p></p> <p>\ud83d\udd01 b. Hidden Layer</p> <p>We now calculate how the hidden neurons contributed to the output error.</p> <p></p> <p>\ud83d\udd01 3. Update Weights</p> <p></p> <p>\ud83d\udd04 This Process Repeats...</p> <p>In each epoch (training cycle), the network:</p> <ul> <li> <p>Performs forward pass (predict)</p> </li> <li> <p>Calculates loss (compare with actual)</p> </li> <li> <p>Performs backward pass (update weights)</p> </li> </ul> <p>Over many epochs, the network learns patterns and improves accuracy.</p> <p>\ud83c\udf93 Visualization</p> <p>Input \u2192 Hidden Layer \u2192 Output (forward)        \u2190 Gradients \u2190           (backward)</p> <p>\u2705 Summary</p> Stage What Happens Forward Prop Predict output Compare Calculate loss Backward Prop Compute gradients Update Adjust weights <p>Here\u2019s a mini example in NumPy (gradient calculation):</p> <p><pre><code>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_deriv(x):\n    return x * (1 - x)\n\n# Input and label\nX = np.array([[10, 5, 2]])\ny = np.array([[1]])\n\n# Weights\nw1 = np.random.rand(3, 2)\nw2 = np.random.rand(2, 1)\n\n# Forward\nhidden_input = np.dot(X, w1)\nhidden_output = sigmoid(hidden_input)\nfinal_input = np.dot(hidden_output, w2)\noutput = sigmoid(final_input)\n\n# Loss and backprop\nerror = y - output\nd_output = error * sigmoid_deriv(output)\nd_hidden = d_output.dot(w2.T) * sigmoid_deriv(hidden_output)\n\n# Update weights\nlr = 0.1\nw2 += hidden_output.T.dot(d_output) * lr\nw1 += X.T.dot(d_hidden) * lr\n\nprint(f\"Updated output: {output}\")\n</code></pre> <pre><code>Updated output: [[0.7875618]]\n</code></pre></p> <p>\ud83d\udce6 No Libraries Required (uses only numpy)</p> <p><pre><code># Code inside the notebook\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Input features: [Time on website, Pages visited, Previous purchases]\nX = np.array([[10, 5, 2]])  # shape (1,3)\ny = np.array([[1]])         # Target: Buy (1)\n\n# Initialize weights (3 inputs \u2192 2 hidden, 2 hidden \u2192 1 output)\nnp.random.seed(1)\nw1 = np.random.rand(3, 2)  # weights from input \u2192 hidden\nw2 = np.random.rand(2, 1)  # weights from hidden \u2192 output\n\nlearning_rate = 0.1\nepochs = 1000\n\nfor epoch in range(epochs):\n    # Forward propagation\n    hidden_input = np.dot(X, w1)\n    hidden_output = sigmoid(hidden_input)\n\n    final_input = np.dot(hidden_output, w2)\n    output = sigmoid(final_input)\n\n    # Backpropagation\n    error = y - output\n    d_output = error * sigmoid_derivative(output)\n\n    error_hidden = d_output.dot(w2.T)\n    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n\n    # Update weights\n    w2 += hidden_output.T.dot(d_output) * learning_rate\n    w1 += X.T.dot(d_hidden) * learning_rate\n\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch} \u2192 Loss: {np.mean(np.abs(error)):.4f}, Output: {output[0][0]:.4f}\")\n</code></pre> <pre><code>Epoch   0 \u2192 Loss: 0.3706, Output: 0.6294  \nEpoch 100 \u2192 Loss: 0.1836, Output: 0.8164  \nEpoch 200 \u2192 Loss: 0.1310, Output: 0.8690  \nEpoch 300 \u2192 Loss: 0.1058, Output: 0.8942  \nEpoch 400 \u2192 Loss: 0.0906, Output: 0.9094  \nEpoch 500 \u2192 Loss: 0.0803, Output: 0.9197  \nEpoch 600 \u2192 Loss: 0.0727, Output: 0.9273  \nEpoch 700 \u2192 Loss: 0.0668, Output: 0.9332  \nEpoch 800 \u2192 Loss: 0.0622, Output: 0.9378  \nEpoch 900 \u2192 Loss: 0.0583, Output: 0.9417  \n</code></pre></p> <p>Loss vs. Epoch plot</p> <pre><code>import matplotlib.pyplot as plt\n\n# Epochs and corresponding loss values\nepochs = list(range(0, 1000, 100))\nlosses = [0.3706, 0.1836, 0.1310, 0.1058, 0.0906, 0.0803, 0.0727, 0.0668, 0.0622, 0.0583]\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, losses, marker='o', linestyle='-', linewidth=2)\nplt.title('Loss vs. Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Here is the Loss vs. Epoch plot. You can see the loss steadily decreases over time, which shows the model is learning and improving its predictions.</p> \ud83d\udccc Iteration <p>This process of forward propagation, loss calculation, backpropagation and weight update is repeated for many iterations over the dataset. Over time, this iterative process reduces the loss and the network's predictions become more accurate.</p> <p>Through these steps, neural networks can adapt their parameters to better approximate the relationships in the data, thereby improving their performance on tasks such as classification, regression or any other predictive modeling.</p> \ud83d\udccc Example of Email Classification <p>Let's consider a record of an email dataset:</p> <p></p> <p>To classify this email, we will create a feature vector based on the analysis of keywords such as \"free\" \"win\" and \"offer\"</p> <p>The feature vector of the record can be presented as:</p> <ul> <li> <p>\"free\": Present (1)</p> </li> <li> <p>\"win\": Absent (0)</p> </li> <li> <p>\"offer\": Present (1)</p> </li> </ul> \ud83d\udccc How Neurons Process Data in a Neural Network <p>In a neural network, input data is passed through multiple layers, including one or more hidden layers. Each neuron in these hidden layers performs several operations, transforming the input into a usable output.</p> <ol> <li> <p>Input Layer: The input layer contains 3 nodes that indicates the presence of each keyword.</p> </li> <li> <p>Hidden Layer: The input vector is passed through the hidden layer. Each neuron in the hidden layer performs two primary operations: a weighted sum followed by an activation function.</p> </li> </ol> <p>Weights:</p> <ul> <li> <p>Neuron H1: [0.5,\u22120.2,0.3]</p> </li> <li> <p>Neuron H2: [0.4,0.1,\u22120.5]</p> </li> </ul> <p>Input Vector: [1,0,1]</p> <p>Weighted Sum Calculation</p> <ul> <li> <p>For H1: (1\u00d70.5)+(0\u00d7\u22120.2)+(1\u00d70.3)=0.5+0+0.3=0.8</p> </li> <li> <p>For H2: (1\u00d70.4)+(0\u00d70.1)+(1\u00d7\u22120.5)=0.4+0\u22120.5=\u22120.1</p> </li> </ul> <p>Activation Function</p> <p>Here we will use <code>ReLu activation function</code>:</p> <ul> <li> <p>H1 Output: ReLU(0.8)= 0.8</p> </li> <li> <p>H2 Output: ReLu(-0.1) = 0</p> </li> </ul> <p>3. Output Layer</p> <p>The activated values from the hidden neurons are sent to the output neuron where they are again processed using a weighted sum and an activation function.</p> <ul> <li> <p>Output Weights: [0.7, 0.2]</p> </li> <li> <p>Input from Hidden Layer: [0.8, 0]</p> </li> <li> <p>Weighted Sum: (0.8\u00d70.7)+(0\u00d70.2)=0.56+0=0.56</p> </li> <li> <p>Activation (Sigmoid): </p> </li> </ul> <p>4. Final Classification</p> <ul> <li> <p>The output value of approximately 0.636 indicates the probability of the email being spam.</p> </li> <li> <p>Since this value is greater than 0.5, the neural network classifies the email as spam (1).</p> </li> </ul> <p></p> \ud83d\udccc Learning of a Neural Network <p>1. Learning with Supervised Learning</p> <p>In supervised learning, a neural network learns from labeled input-output pairs provided by a teacher. The network generates outputs based on inputs and by comparing these outputs to the known desired outputs, an error signal is created. The network iteratively adjusts its parameters to minimize errors until it reaches an acceptable performance level.</p> <p>2. Learning with Unsupervised Learning</p> <p>Unsupervised learning involves data without labeled output variables. The primary goal is to understand the underlying structure of the input data (X). Unlike supervised learning, there is no instructor to guide the process. Instead, the focus is on modeling data patterns and relationships, with techniques like clustering and association commonly used.</p> <p>3. Learning with Reinforcement Learning</p> <p>Reinforcement learning enables a neural network to learn through interaction with its environment. The network receives feedback in the form of rewards or penalties, guiding it to find an optimal policy or strategy that maximizes cumulative rewards over time. This approach is widely used in applications like gaming and decision-making.</p> \ud83d\udccc Types of Neural Networks \ud83e\udde0 1. Feedforward Neural Network (FNN) <ul> <li> <p>Description: The simplest type; data flows in one direction (input \u2192 hidden \u2192 output).</p> </li> <li> <p>Use Case: Basic classification/regression tasks.</p> </li> <li> <p>Example: Predicting house prices, email spam detection.</p> </li> </ul> \ud83d\udd01 2. Recurrent Neural Network (RNN) <ul> <li> <p>Description: Designed for sequential data. It has memory of previous inputs.</p> </li> <li> <p>Use Case: Time series, speech recognition, text generation.</p> </li> <li> <p>Example: Language modeling, stock price prediction.</p> </li> </ul> \ud83d\udd04 Variants of RNN: <ul> <li> <p>LSTM (Long Short-Term Memory): Solves vanishing gradient problem; better for long sequences.</p> </li> <li> <p>GRU (Gated Recurrent Unit): A simpler alternative to LSTM.</p> </li> </ul> \ud83d\uddbc\ufe0f 3. Convolutional Neural Network (CNN) <ul> <li> <p>Description: Uses filters/kernels to detect spatial patterns in images.</p> </li> <li> <p>Use Case: Image classification, object detection, facial recognition.</p> </li> <li> <p>Example: Self-driving cars, medical imaging.</p> </li> </ul> \ud83e\uddee 4. Radial Basis Function Network (RBFN) <ul> <li> <p>Description: Uses radial basis functions as activation functions; good for pattern recognition.</p> </li> <li> <p>Use Case: Function approximation, time-series prediction.</p> </li> <li> <p>Example: Signal classification.</p> </li> </ul> \ud83d\udd78\ufe0f 5. Modular Neural Network (MNN) <ul> <li> <p>Description: Combines multiple networks (modules) that work independently and combine their outputs.</p> </li> <li> <p>Use Case: When tasks can be split across different models.</p> </li> <li> <p>Example: Multi-modal tasks (e.g., combining image + text inputs).</p> </li> </ul> \ud83c\udf10 6. Generative Adversarial Networks (GANs) <ul> <li> <p>Description: Consists of two networks \u2014 Generator &amp; Discriminator \u2014 competing against each other.</p> </li> <li> <p>Use Case: Image generation, data augmentation, deepfake creation.</p> </li> <li> <p>Example: Creating realistic human faces, art generation.</p> </li> </ul> \ud83d\udd24 7. Transformer Networks <ul> <li> <p>Description: Uses self-attention mechanism; excels at handling long-range dependencies.</p> </li> <li> <p>Use Case: NLP tasks (translation, summarization, Q&amp;A).</p> </li> <li> <p>Example: ChatGPT, BERT, GPT, T5</p> </li> </ul> \ud83e\udd16 8. Autoencoders <ul> <li> <p>Description: Learns compressed representations of data (encoder) and reconstructs them (decoder).</p> </li> <li> <p>Use Case: Dimensionality reduction, denoising, anomaly detection.</p> </li> <li> <p>Example: Recommender systems, image compression.</p> </li> </ul> \ud83e\uddf1 9. Self-Organizing Maps (SOM) <ul> <li> <p>Description: Unsupervised network that reduces dimensions and clusters data.</p> </li> <li> <p>Use Case: Exploratory data analysis, visualization.</p> </li> <li> <p>Example: Customer segmentation.</p> </li> </ul> \ud83d\udcca Summary Table Neural Network Type Key Use Case Feedforward Neural Network General classification/regression CNN Image and video analysis RNN / LSTM / GRU Text, speech, sequential data GAN Image &amp; video generation Autoencoder Dimensionality reduction, anomaly detection Transformer Natural Language Processing (NLP) RBFN Function approximation SOM Clustering, dimensionality reduction Modular NN Complex multi-task systems \ud83d\udccc How to choose the right neural network for your problem? <p>Choosing the right neural network for your problem depends on three key factors:</p> <ol> <li> <p>\u2705 Nature of the data</p> </li> <li> <p>\u2705 Type of problem</p> </li> <li> <p>\u2705 Resources (compute, time, data volume)</p> </li> </ol> \ud83d\udd0d 1. Understand Your Data Type Data Type Description Common Networks Images Photos, videos, medical scans CNN, GAN Sequences Time-series, speech, stock data RNN, LSTM, GRU, Transformer Text Sentences, documents RNN, LSTM, Transformer Tabular Excel/CSV data (structured rows/columns) Feedforward Neural Network Mixed Modal Combining text + image + numbers Modular NN, Multimodal Transformers Unlabeled No ground truth (unsupervised) Autoencoders, SOM, GAN \ud83d\udd27 2. Match Problem Type to Model Problem Type Recommended Networks Classification FNN, CNN, RNN, Transformers Regression FNN, RBFN, LSTM Object Detection CNN (YOLO, Faster R-CNN) Image Generation GANs Text Generation Transformers (GPT), LSTM Translation Transformer (like T5, BERT, MarianMT) Anomaly Detection Autoencoders, LSTM (for time series) Clustering/Segmentation SOM, CNN (for image segmentation), k-Means + Autoencoders Recommendation Systems Autoencoders, Transformers, Embedding models \ud83e\udde0 3. Consider Model Complexity and Resources Factor Light Models Heavy Models Training Data Size Small \u2192 FNN, SVM Large \u2192 CNN, Transformers Hardware CPU \u2192 FNN, Autoencoders GPU \u2192 CNN, Transformers Real-time need Fast \u2192 FNN, LSTM Slower \u2192 BERT, GPT \ud83d\udcca Decision Flowchart (Simplified) <pre><code>\u2192 Do you have images?\n     \u2192 Yes \u2192 CNN or GAN\n     \u2192 No \u2192\n\u2192 Do you have sequential data?\n     \u2192 Yes \u2192 RNN / LSTM / Transformer\n     \u2192 No \u2192\n\u2192 Is your data tabular (structured)?\n     \u2192 Yes \u2192 FNN (MLP)\n\u2192 Is your problem text-based (NLP)?\n     \u2192 Yes \u2192 Transformer (e.g., BERT/GPT)\n\u2192 Is your data unlabeled?\n     \u2192 Yes \u2192 Autoencoder / SOM / GAN\n</code></pre> \ud83c\udfaf Example Use Cases Use Case Best Neural Network Detecting spam emails RNN, LSTM, Transformer Diagnosing diseases from X-rays CNN Predicting stock prices LSTM, GRU Translating English to French Transformer (T5, MarianMT) Chatbot like ChatGPT Transformer (GPT) Recommending movies on Netflix Autoencoder, Transformer <p>\u2705 Tips</p> <ul> <li> <p>Start simple: Begin with FNN or Logistic Regression if you\u2019re unsure.</p> </li> <li> <p>Use pre-trained models: Especially for NLP and vision (e.g., BERT, ResNet).</p> </li> <li> <p>Use AutoML: Tools like Google AutoML, H2O.ai, or AutoKeras can auto-select the best architecture.</p> </li> <li> <p>Don\u2019t overcomplicate: Deep learning isn\u2019t always better than traditional ML.</p> </li> </ul> \ud83d\udccc A Neural Network Playground <p>A Neural Network Playground</p> <p>A Neural Network Playground</p> <p>A Neural Network Playground</p>"},{"location":"DeepLearning/Vanishing/","title":"Vanishing and Exploding Gradients Problems","text":"\u2705 Vanishing and Exploding Gradients Problems \ud83d\udccc What is Vanishing Gradient? <p>The vanishing gradient problem is a challenge that emerges during backpropagation when the derivatives or slopes of the activation functions become progressively smaller as we move backward through the layers of a neural network. This phenomenon is particularly prominent in deep networks with many layers, hindering the effective training of the model. The weight updates becomes extremely tiny, or even exponentially small, it can significantly prolong the training time, and in the worst-case scenario, it can halt the training process altogether.</p> <p>Why the Problem Occurs?</p> <p>During backpropagation, the gradients propagate back through the layers of the network, they decrease significantly. This means that as they leave the output layer and return to the input layer, the gradients become progressively smaller. As a result, the weights associated with the initial levels, which accommodate these small gradients, are updated little or not at each iteration of the optimization process.</p> <p>The vanishing gradient problem is particularly associated with the sigmoid and hyperbolic tangent (tanh) activation functions because their derivatives fall within the range of 0 to 0.25 and 0 to 1, respectively. Consequently, extreme weights becomes very small, causing the updated weights to closely resemble the original ones. This persistence of small updates contributes to the vanishing gradient issue.</p> <p>The sigmoid and tanh functions limit the input values \u200b\u200bto the ranges [0,1] and [-1,1], so that they saturate at 0 or 1 for sigmoid and -1 or 1 for Tanh. The derivatives at points becomes zero as they are moving. In these regions, especially when inputs are very small or large, the gradients are very close to zero. </p> <p>When training deep neural networks (especially RNNs, LSTMs, and deep feedforward networks), we use backpropagation to update weights.</p> <p>The update depends on gradients \u2014 numbers that tell us how much to change the weights.</p> <p>If these gradients become very small as they move backward through the layers, they can \u201cvanish\u201d (approach zero).</p> <p>When this happens:</p> <ul> <li> <p>The earlier layers barely get updated.</p> </li> <li> <p>The network learns very slowly or stops learning.</p> </li> </ul> <p>Why it Happens?</p> <p>In backpropagation, gradients are multiplied many times by the derivative of the activation function.</p> <p>For example:</p> <ul> <li>If the derivative is small (e.g., 0.1), multiplying it through 50 layers gives:</li> </ul> <p></p> <ul> <li>Sigmoid and tanh activations squash numbers between small ranges, so their derivatives are small, which worsens the problem.</li> </ul>"},{"location":"DeepLearning/Vanishing/#real-life-example-the-whisper-game","title":"Real-Life Example: The Whisper Game","text":"<p>Imagine a group of 50 people standing in a line.</p> <ol> <li> <p>Person 1 whispers a message: \u201cThe train leaves at 8.\u201d</p> </li> <li> <p>Each person passes it on quietly to the next.</p> </li> <li> <p>By the time it reaches Person 50, the message becomes: </p> <p>\u201cT...ain...8\u201d (almost lost).</p> </li> </ol> <p>Why?</p> <ul> <li> <p>Every person slightly loses information.</p> </li> <li> <p>The earlier the person, the more the message is lost before it reaches the end.</p> </li> </ul> <p>This is the same as vanishing gradients:</p> <ul> <li> <p>The \u201cmessage\u201d = gradient information.</p> </li> <li> <p>Passing through people = layers in the neural network.</p> </li> <li> <p>Whispering quietly = multiplying by small derivatives (like sigmoid output\u2019s slope).</p> </li> </ul> <p>Impact on RNNs</p> <p>RNNs process sequences step by step (like passing the message from one person to the next).</p> <ul> <li>If gradients vanish, early time steps (earlier words in a sentence) get forgotten.</li> </ul> <p>How We Solve It</p> <ul> <li> <p>Use ReLU instead of sigmoid/tanh (avoids tiny derivatives).</p> </li> <li> <p>Use LSTM or GRU (special gates to keep gradients flowing).</p> </li> <li> <p>Use Batch Normalization or Residual Connections.</p> </li> </ul>"},{"location":"DeepLearning/Components/ActivationFunctions/","title":"Activation Functions","text":"\u2705 Activation functions in Neural Networks \ud83d\udccc What is Activation functions in Neural Networks? <ul> <li> <p>While building a neural network, one key decision is selecting the Activation Function for both the hidden layer and the output layer. </p> </li> <li> <p>It is a mathematical function applied to the output of a neuron</p> </li> <li> <p>It introduces non-linearity into the model, allowing the network to learn and represent complex patterns in the data.</p> </li> <li> <p>Without this non-linearity feature a neural network would behave like a linear regression model no matter how many layers it has.</p> </li> <li> <p>Activation function decides whether a neuron should be activated by calculating the weighted sum of inputs and adding a bias term.</p> </li> </ul> <p>1. Weighted Sum of Inputs:</p> <ul> <li>In a neural network, each input to a neuron is multiplied by a weight. These weights represent the importance or strength of each input.</li> </ul> <p>Formula:</p> <p></p> <p>2. Adding Bias:</p> <ul> <li>The bias b is a value added to the weighted sum. It helps the neuron to shift the activation function curve to the left or right, allowing the model to better fit the data.</li> </ul> <p></p> <p></p> <p>What happens next?</p> <p></p> <p></p> <p></p> <p>Intuition behind the sigmoid:</p> <ul> <li> <p>It's a smooth curve that squashes any input value into a range between 0 and 1.</p> </li> <li> <p>Useful to interpret the output as a probability or \"activation level\".</p> </li> </ul> <p></p> \ud83d\udccc Introducing Non-Linearity in Neural Network? <p>Imagine you want to classify apples and bananas based on their shape and color.</p> <ul> <li> <p>If we use a linear function it can only separate them using a straight line.</p> </li> <li> <p>But real-world data is often more complex like overlapping colors, different lighting, etc.</p> </li> <li> <p>By adding a non-linear activation function like ReLU, Sigmoid or Tanh the network can create curved decision boundaries to separate them correctly.</p> </li> </ul> <p>Why is Non-Linearity Important in Neural Networks?</p> <ul> <li> <p>Neural networks consist of neurons that operate using weights, biases and activation functions.</p> </li> <li> <p>In the learning process these weights and biases are updated based on the error produced at the output\u2014a process known as backpropagation. Activation functions enable backpropagation by providing gradients that are essential for updating the weights and biases.</p> </li> <li> <p>Without non-linearity even deep networks would be limited to solving only simple, linearly separable problems. Activation functions help neural networks to model highly complex data distributions and solve advanced deep learning tasks. Adding non-linear activation functions introduce flexibility and enable the network to learn more complex and abstract patterns from data.</p> </li> </ul> <p>Mathematical Proof of Need of Non-Linearity in Neural Networks</p> <p>To illustrate the need for non-linearity in neural networks with a specific example let's consider a network with two input nodes  (i1 and i2), a single hidden layer containing neurons h1 and h2 \u200b   and an output neuron (out).</p> <ul> <li> <p>Non-linearity means that the relationship between input and output is not a straight line. In simple terms the output does not change proportionally with the input.</p> </li> <li> <p>A common choice is the ReLU function defined as <code>\u03c3(x)=max(0,x)</code>.</p> </li> </ul> <p></p> <p></p> <p></p> <ul> <li>\u03c3(x)=max(0,x) is a mathematical function called the ReLU function, which stands for Rectified Linear Unit.</li> </ul> <p>Explanation:</p> <p>\u03c3(x) here is the function notation.The Greek letter \u03c3 (sigma) is just a symbol for the function;sometimes people use different symbols, but it just means <code>a function of x.</code></p> <p>The function outputs the maximum value between 0 and the input x.</p> <p>What it means in simple terms:</p> <ul> <li> <p>If x is positive or zero, then \u03c3(x)=x.</p> </li> <li> <p>If x is negative, then \u03c3(x)=0.</p> </li> </ul> <p>Example:</p> x sigma(x) = max(0, x) -3 0 0 0 2 2 5 5 <p>Where is this used?</p> <p>In neural networks, ReLU is a popular activation function used in neurons to decide whether a neuron should \"fire\" or not, by transforming the input signal.</p> <p>Why use ReLU?</p> <ul> <li> <p>It introduces non-linearity in the network.</p> </li> <li> <p>It is simple and fast to compute.</p> </li> <li> <p>Helps with the problem of vanishing gradients (compared to sigmoid or tanh).</p> </li> </ul> \ud83d\udccc Common activation functions used in neural networks and how ReLU compares to them. <p></p> <p></p> <p></p> Activation Output Range Shape Pros Cons ReLU [0, \u221e) Piecewise linear (zero + line) Fast, simple, avoids vanishing gradient Can \"die\" (neurons output zero forever) for negative inputs Sigmoid (0, 1) Smooth S-shape Output interpretable as probability Vanishing gradient, slow training Tanh (-1, 1) Smooth S-shape centered at zero Zero-centered outputs Vanishing gradient, slower <p></p> <ul> <li> <p>ReLU: Outputs zero for all negative values, then increases linearly for positive values.</p> </li> <li> <p>Sigmoid: Smooth S-shaped curve between 0 and 1, saturates at both ends.</p> </li> <li> <p>Tanh: Smooth S-shaped curve between -1 and 1, zero-centered.</p> </li> </ul> <p>You can see how ReLU sharply cuts off negative values and grows linearly, which helps neural networks learn efficiently.</p>"},{"location":"DeepLearning/Models/BERT/","title":"BERT","text":"\u2705 BERT \ud83d\udccc What is BERT?"},{"location":"DeepLearning/Models/BERT/#referance-link","title":"Referance Link","text":"<p>BERT</p>"},{"location":"DeepLearning/Models/CNN/","title":"Convolutional Neural Network (CNN)","text":"\u2705 Convolutional Neural Network (CNN) \ud83d\udccc What is Convolutional Neural Network (CNN)? <p>Convolutional Neural Network (CNN) is an advanced version of artificial neural networks (ANNs), primarily designed to extract features from grid-like matrix datasets. This is particularly useful for visual datasets such as images or videos, where data patterns play a crucial role. CNNs are widely used in computer vision applications due to their effectiveness in processing visual data.</p> <p>CNNs consist of multiple layers like the input layer, Convolutional layer, pooling layer, and fully connected layers.</p> <p></p> \ud83d\udccc How Convolutional Layers Works? <p>Convolution Neural Networks are neural networks that share their parameters.</p> <p>Imagine you have an image. It can be represented as a cuboid having its length, width (dimension of the image), and height (i.e the channel as images generally have red, green, and blue channels). </p> <p></p> <p>Now imagine taking a small patch of this image and running a small neural network, called a filter or kernel on it, with say, K outputs and representing them vertically.</p> <p>Now slide that neural network across the whole image, as a result, we will get another image with different widths, heights, and depths. Instead of just R, G, and B channels now we have more channels but lesser width and height. This operation is called Convolution. If the patch size is the same as that of the image it will be a regular neural network. Because of this small patch, we have fewer weights. </p> <p></p> \ud83d\udccc Mathematical Overview of Convolution <ul> <li> <p>Convolution layers consist of a set of learnable filters (or kernels) having small widths and heights and the same depth as that of input volume (3 if the input layer is image input).</p> </li> <li> <p>For example, if we have to run convolution on an image with dimensions 34x34x3. The possible size of filters can be axax3, where \u2018a\u2019 can be anything like 3, 5, or 7 but smaller as compared to the image dimension.</p> </li> <li> <p>During the forward pass, we slide each filter across the whole input volume step by step where each step is called stride (which can have a value of 2, 3, or even 4 for high-dimensional images)</p> </li> </ul> <p>and compute the dot product between the kernel weights and patch from input volume.</p> <p>As we slide our filters we\u2019ll get a 2-D output for each filter and we\u2019ll stack them together as a result, we\u2019ll get output volume having a depth equal to the number of filters. The network will learn all the filters.</p> <p>x</p>"},{"location":"DeepLearning/Models/ComputerVision/","title":"Computer Vision","text":"\u2705 Computer Vision \ud83d\udccc What is Computer Vision? <p>Computer Vision (CV) in artificial intelligence (AI) help machines to interpret and understand visual information similar to how humans use their eyes and brains. It involves teaching computers to analyze and understand images and videos, helping them \"see\" the world. From identifying objects in images to recognizing faces in a crowd, it is revolutionizing industries such as healthcare, automotive, security and entertainment.</p> <p></p> \ud83d\udccc Key Concepts of Computer Vision <ol> <li> <p>Image Processing: This involves improving or changing an image to make it clearer or easier to analyze. It includes cleaning up images by removing noise, improving contrast or adjusting the lighting.</p> </li> <li> <p>Object Detection: This allows the machine to find and identify specific objects within an image or video. For example, it can detect faces in a photo or find cars in a traffic scene.</p> </li> <li> <p>Image Classification: It involves categorizing an image into a specific class or label such as identifying whether a given image is of a dog or a cat.</p> </li> <li> <p>Feature Extraction: It is the process of identifying unique patterns or features in an image that can be used for further analysis like shapes, colors or textures.</p> </li> </ol> \ud83d\udccc How Does Computer Vision Work? <ul> <li> <p>Image Acquisition: It involves collecting images or videos using cameras, sensors or other devices. The quality of the image and its type (black-and-white, color or 3D) affects how the system will process the data.</p> </li> <li> <p>Preprocessing: Raw images are often not perfect, so they are cleaned up first. This might include adjusting the brightness, sharpening the image or removing unwanted noise to help the system see better.</p> </li> <li> <p>Feature Detection: In this, the system looks for key elements in the image like edges, patterns or shapes. This helps the system focus on the important parts of the image.</p> </li> <li> <p>Pattern Recognition: This compares what it detects in the image to known patterns or examples. Using machine learning, the system can recognize objects, classify images or even understand relationships in the image.</p> </li> <li> <p>Decision Making: After recognizing patterns, the system uses this information to make decisions such as identifying a dog in the image or recognizing a stop sign in a video.</p> </li> </ul> \ud83d\udccc Tasks of Computer Vision <ol> <li> <p>Object Recognition: This is used for identifying objects in an image such as recognizing a car, dog or tree. It\u2019s used in surveillance, self-driving cars and checking products in factories.</p> </li> <li> <p>Face Recognition: This involves identifying people based on their facial features. It is used in security systems, unlocking smartphones and identifying people in photos or videos.</p> </li> <li> <p>Image Segmentation: Segmentation breaks an image into smaller parts for easier analysis. For example, in medical imaging, different organs may be segmented to focus on specific areas.</p> </li> <li> <p>Optical Character Recognition (OCR): OCR helps in recognizing text in images such as scanning documents or extracting text from pictures of signs. It\u2019s used in document scanners, translation apps and more.</p> </li> </ol> \ud83d\udccc Key Techniques in Computer Vision <ol> <li> <p>Convolutional Neural Networks (CNNs): CNNs are a type of deep learning model that has changed the field of CV. These networks can automatically learn and recognize patterns in images. They are excellent for tasks like object detection, image classification and segmentation.</p> </li> <li> <p>Feature Matching: This technique matches key points between images. It\u2019s used in applications like creating panoramas where multiple images are stitched together to form one large image.</p> </li> <li> <p>Optical Flow: It helps track movement in videos by analyzing how pixels change from one frame to the next. It\u2019s used in things like tracking moving objects or detecting motion in surveillance videos.</p> </li> <li> <p>Generative Adversarial Networks (GANs): GANs are used in advanced CV tasks such as generating realistic images or improving low-quality images. They work by having two components challenging each other to improve their results.</p> </li> </ol> \ud83d\udccc Popular Libraries for Computer Vision <ol> <li> <p>OpenCV: Mostly used open-source library for computer vision tasks like image processing, video capture and real-time applications.</p> </li> <li> <p>TensorFlow: A popular deep learning framework that includes tools for building and training computer vision models.</p> </li> <li> <p>PyTorch: Another deep learning library that provides great flexibility for computer vision tasks for research and development.</p> </li> </ol> \ud83d\udccc Object Detection <p>It involves identifying and locating objects within an image by drawing bounding boxes around them.</p> <p>It includes below following Techniques:</p> <ol> <li>Yolo (You Only Look Once).</li> </ol>"},{"location":"DeepLearning/Models/LSTM/","title":"LSTM (Long Short-Term Memory)","text":"\u2705 LSTM - Long Short Term Memory \ud83d\udccc What is LSTM - Long Short Term Memory? <p>Long Short-Term Memory (LSTM) is an enhanced version of the Recurrent Neural Network (RNN) designed by Hochreiter and Schmidhuber. LSTMs can capture long-term dependencies in sequential data making them ideal for tasks like language translation, speech recognition and time series forecasting.</p> <p>Unlike traditional RNNs which use a single hidden state passed through time LSTMs introduce a memory cell that holds information over extended periods addressing the challenge of learning long-term dependencies.</p> <p></p> <p></p> <p></p> <p>Problem with Long-Term Dependencies in RNN</p> <p>Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous time steps.</p> \ud83d\udccc Understanding LSTM Networks <p></p> <p></p> <p>Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN,capable of learning long-term dependencies.</p> <p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p> <p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p> <p></p> <p>The repeating module in a standard RNN contains a single layer.</p> <p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p> <p></p> <p></p>"},{"location":"DeepLearning/Models/LSTM/#the-core-idea-behind-lstms","title":"The Core Idea Behind LSTMs","text":"<p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.</p>"},{"location":"DeepLearning/Models/LSTM/#memory-cell","title":"Memory Cell","text":"<ol> <li> <p>You can Add information</p> </li> <li> <p>You can Remove Information</p> </li> </ol> <p></p> <p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p> <p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.</p> <p></p> <p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d</p> <p>An LSTM has three of these gates, to protect and control the cell state.</p> <p>Step-by-Step LSTM Walk Through</p> <p>The first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at ht\u22121  and xt, and outputs a number between 0  and 1 for each number in the cell state Ct\u22121.</p> <p>A 1 represents \u201ccompletely keep this\u201d while a 0 represents \u201ccompletely get rid of this.\u201d</p> <p>Let\u2019s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p> <p></p> <p>The next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, C\u0303 t , that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.</p> <p>In the example of our language model, we\u2019d want to add the gender of the new subject to the cell state, to replace the old one we\u2019re forgetting.</p> <p></p> <p>It\u2019s now time to update the old cell state, Ct\u22121 , into the new cell state Ct . The previous steps already decided what to do, we just need to actually do it.</p> <p>We multiply the old state by ft , forgetting the things we decided to forget earlier. Then we add it\u2217C\u0303 t. This is the new candidate values, scaled by how much we decided to update each state value.</p> <p>In the case of the language model, this is where we\u2019d actually drop the information about the old subject\u2019s gender and add the new information, as we decided in the previous steps.</p> <p></p> <p>Finally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh  (to push the values to be between \u22121  and 1 ) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p> <p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that\u2019s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that\u2019s what follows next.</p> <p></p>"},{"location":"DeepLearning/Models/LSTM/#variants-on-long-short-term-memory","title":"Variants on Long Short Term Memory","text":"<p>What I\u2019ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it\u2019s worth mentioning some of them.</p> <p>One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding \u201cpeephole connections.\u201d This means that we let the gate layers look at the cell state.</p> <p></p> <p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p> <p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we\u2019re going to input something in its place. We only input new values to the state when we forget something older.</p> <p></p> <p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p> <p></p> <p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There\u2019s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).</p> <p>Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they\u2019re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p>"},{"location":"DeepLearning/Models/LSTM/#conclusion","title":"Conclusion","text":"<p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p> <p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p> <p>LSTMs were a big step in what we can accomplish with RNNs. It\u2019s natural to wonder: is there another big step? A common opinion among researchers is: \u201cYes! There is a next step and it\u2019s attention!\u201d The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this \u2013 it might be a fun starting point if you want to explore attention! There\u2019s been a number of really exciting results using attention, and it seems like a lot more are around the corner\u2026</p> <p>Attention isn\u2019t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models \u2013 such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) \u2013 also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p>"},{"location":"DeepLearning/Models/LSTM/#extra-understanding","title":"Extra Understanding","text":"<p>Long Short terl Memory</p> <ol> <li> <p>Memory Cell</p> </li> <li> <p>Forget Cell</p> </li> <li> <p>Input Cell</p> </li> </ol>"},{"location":"DeepLearning/Models/LSTM/#relu","title":"Relu","text":"<p>Can actually cause dead neurons duraing back propagation.</p> <ul> <li><code>My name is Ganesh and I want to eat Pizza</code></li> </ul> <p>When I am trying to use the RNN, what RNN will do?</p> <p><code>Each and every word it will try to convert in to vectors to make sure it consider that each and every word is important.</code></p> <p>Suppose if something we have missed, suppose let say Pizza dependent on some word like it. This case you know that \"eat\" is just before the \"Pizza\" so it is being able to capture the context by with the help of RNN and also it will be able to capture the context. But over here you can see another word \"I\" I is definitly related to \"Ganesh\" I is basically giving the context of \"Ganesh\". This is only one word distance. But think about \"I\" and \"My\" can also be related. There should be some kind of context and Important context. Here you can see the distance is quite long.</p> <p>What memory do? (Sometime you need to remember something, sometimes you need to forget something )</p>"},{"location":"DeepLearning/Models/LSTM/#lstm-context-switching-example","title":"LSTM Context Switching Example","text":""},{"location":"DeepLearning/Models/LSTM/#example","title":"Example","text":"<ol> <li>Ganesh likes AIML \u2192 This is one context.  </li> <li>Ram likes DevSecOps \u2192 This is another context.  </li> </ol> <p>From context 1 to context 2, a switch happens because the topic (or subject) changes \u2014 we are now talking about a different person.</p> <p>Let\u2019s say in the previous step (cell), we passed the information:</p> <p>\"Ganesh likes AIML\"</p> <p>In the next step, the context changes. When this happens, if we want our neural network to predict future words, it should only be based on the new context. It should not remember the previous context at this moment.</p> <p>So, the old context (<code>Ganesh likes AIML</code>) will be forgotten, and the new context will be added.</p>"},{"location":"DeepLearning/Models/LSTM/#forget-gate-behavior","title":"Forget Gate Behavior","text":"<p>Most of the information in such a context-switching scenario will be close to [0] because the forget gate will filter it out.</p> <ul> <li>The forget gate uses a sigmoid activation function that outputs values between [0, 1].</li> <li>If the previous information is similar to the new context \u2192 values will be close to 1 (retain it).</li> <li>If it is different \u2192 values will be close to 0 (forget it).</li> </ul> <p>When values are near 0, the memory cell is instructed to forget the old information. This is done through pointwise multiplication in the forward pass.</p>"},{"location":"DeepLearning/Models/LSTM/#adding-new-information","title":"Adding New Information","text":"<p>We now want to store the new word <code>\"friend\"</code> in the memory cell.</p> <ol> <li>Sigmoid gate decides how much of the new information should be stored (values in <code>[0, 1]</code>).</li> <li>The same input passes through a tanh activation function, producing values in the range <code>[-1, 1]</code>.</li> <li>Outputs from sigmoid and tanh are combined element-wise.</li> <li>The result is added to the memory cell, updating it with only the important parts of the new context.</li> </ol>"},{"location":"DeepLearning/Models/LSTM/#summary","title":"Summary","text":"<ul> <li>Pointwise operation ensures that only important new information passes through and merges with the memory cell.</li> <li>Old, irrelevant context is discarded when the forget gate outputs values close to <code>0</code>.</li> </ul>"},{"location":"DeepLearning/Models/LSTM/#referance-link","title":"Referance Link:\u2705 Bidirectional LSTM","text":"<p>Understanding LSTM Networks</p>"},{"location":"DeepLearning/Models/LSTM/#bidirectional-lstm-bilstm-explanation","title":"Bidirectional LSTM (BiLSTM) Explanation","text":""},{"location":"DeepLearning/Models/LSTM/#1-what-is-a-bidirectional-lstm","title":"1. What is a Bidirectional LSTM?","text":"<p>A Bidirectional LSTM (BiLSTM) is an extension of the traditional LSTM (Long Short-Term Memory) network. Instead of processing the sequence in only one direction (forward in time), BiLSTM processes the sequence in both directions:</p> <ul> <li>Forward LSTM \u2192 Reads the sequence from start to end.</li> <li>Backward LSTM \u2192 Reads the sequence from end to start.</li> </ul> <p>The outputs from both directions are then combined (either concatenated, summed, or averaged) to make the final prediction.</p>"},{"location":"DeepLearning/Models/LSTM/#2-why-use-bidirectional-lstm","title":"2. Why use Bidirectional LSTM?","text":"<p>Some tasks require understanding both past and future context in a sequence.</p> <ul> <li>Standard LSTM: Has access only to past information.</li> <li>BiLSTM: Has access to both past and future information at each time step.</li> </ul> <p>This is especially useful in: - Natural Language Processing (NLP) - Speech Recognition - Handwriting Recognition - Named Entity Recognition (NER) - Sentiment Analysis</p>"},{"location":"DeepLearning/Models/LSTM/#3-real-life-example","title":"3. Real-Life Example","text":""},{"location":"DeepLearning/Models/LSTM/#problem","title":"Problem:","text":"<p>We want to predict the sentiment (Positive/Negative) of a sentence.</p> <p>Sentence:  </p> <p><code>\"The movie was not bad\"</code></p>"},{"location":"DeepLearning/Models/LSTM/#why-bilstm-helps","title":"Why BiLSTM helps:","text":"<ul> <li>If we read only forward, we might think \"The movie was not...\" is negative.</li> <li>But when reading backward as well, we see the word \"bad\" preceded by \"not\", which changes the meaning to positive.</li> <li>A BiLSTM can use both forward and backward context to correctly understand that the sentiment is Positive.</li> </ul>"},{"location":"DeepLearning/Models/LSTM/#4-how-it-works-step-by-step","title":"4. How it works step-by-step:","text":"<ol> <li>Input sentence is tokenized into a sequence of words:</li> </ol> <p>[\"The\", \"movie\", \"was\", \"not\", \"bad\"]</p> <ol> <li>Forward LSTM processes:</li> </ol> <p>The \u2192 movie \u2192 was \u2192 not \u2192 bad</p> <ol> <li>At each word position, both forward and backward hidden states are concatenated:</li> </ol> <p>Hidden_state_t = [Forward_hidden_t ; Backward_hidden_t]</p> <ol> <li>Final combined representation is passed to a classifier (e.g., Softmax for sentiment prediction).</li> </ol>"},{"location":"DeepLearning/Models/LSTM/#5-diagram","title":"5. Diagram","text":"<p>Input: The movie was not bad Forward \u2192 h1 \u2192 h2 \u2192 h3 \u2192 h4 \u2192 h5 Backward \u2190 h1'\u2190 h2'\u2190 h3'\u2190 h4'\u2190 h5' | | | | | Output: [h1;h1'] [h2;h2'] ... [h5;h5']</p>"},{"location":"DeepLearning/Models/LSTM/#6-advantages","title":"6. Advantages","text":"<ul> <li>Uses full context of the sequence (past + future).</li> <li>Improves performance in many NLP tasks where meaning depends on both sides of the word.</li> </ul>"},{"location":"DeepLearning/Models/LSTM/#7-limitations","title":"7. Limitations","text":"<ul> <li>Cannot be used for real-time predictions where future data is not yet available.</li> <li>More computational cost compared to a single LSTM.</li> </ul>"},{"location":"DeepLearning/Models/LSTM/#8-summary-table","title":"8. Summary Table","text":"Feature LSTM BiLSTM Direction Forward only Forward + Backward Context Past only Past + Future Computation Time Lower Higher Use in Real-time Yes No Accuracy in NLP Good Often Better"},{"location":"DeepLearning/Models/RNN/","title":"Recurrent Neural Network (RNN)","text":"\u2705 Recurrent Neural Networks(RNN) \ud83d\udccc What is Recurrent Neural Networks(RNN)? <p>Recurrent Neural Networks (RNNs) differ from regular neural networks in how they process information. While standard neural networks pass information in one direction i.e from input to output, RNNs feed information back into the network at each step.</p> <p></p> <p></p> <p></p> <ul> <li> <p>Imagine reading a sentence and you try to predict the next word, you don\u2019t rely only on the current word but also remember the words that came before.</p> </li> <li> <p>RNNs work similarly by \u201cremembering\u201d past information and passing the output from one step as input to the next i.e it considers all the earlier words to choose the most likely next word. This memory of previous steps helps the network understand context and make better predictions.</p> </li> </ul> \ud83d\udcccKey Components of RNNs <p>There are mainly two components of RNNs</p> <p>1. Recurrent Neurons</p> <p>The fundamental processing unit in RNN is a Recurrent Unit. They hold a hidden state that maintains information about previous inputs in a sequence. Recurrent units can \"remember\" information from prior steps by feeding back their hidden state, allowing them to capture dependencies across time.</p> <p></p> <p>2. RNN Unfolding</p> <p>RNN unfolding or unrolling is the process of expanding the recurrent structure over time steps. During unfolding each step of the sequence is represented as a separate layer in a series illustrating how information flows across each time step.</p> <p></p> \ud83d\udcccRecurrent Neural Network Architecture <p>RNNs share similarities in input and output structures with other deep learning architectures but differ significantly in how information flows from input to output. Unlike traditional deep neural networks where each dense layer has distinct weight matrices. RNNs use shared weights across time steps, allowing them to remember information over sequences.</p> <p></p> <p></p> <p>In deep learning, especially when discussing Recurrent Neural Networks (RNNs), the expression</p> <p>Y=f(X,h,W,U,V,B,C)</p> <p>is essentially describing the forward pass equation, where Y is the output and f is a function of the inputs, hidden state, and parameters.</p> <p>The Parameters and Variables</p> Symbol Meaning X Input at the current time step t h Hidden state from the previous time step (t\u22121) W Weight matrix for input-to-hidden connections U Weight matrix for hidden-to-hidden (recurrent) connections V Weight matrix for hidden-to-output connections B Bias vector for hidden layer C Bias vector for output layer <p>This function defines the entire RNN operation where the state matrix S holds each element si representing the network's state at each time step i.</p> <p></p> <p>How does RNN work?</p> <p>At each time step RNNs process units with a fixed activation function. These units have an internal hidden state that acts as memory that retains information from previous time steps. This memory allows the network to store past knowledge and adapt based on new inputs.</p> <p>Updating the Hidden State in RNNs</p> <p></p> <p>Meaning of terms</p> Symbol Meaning h\u209c Hidden state at time step t h\u209c\u208b\u2081 Hidden state from the previous time step (t\u22121) x\u209c Input vector at time step t W\u2095\u2095 Weight matrix for hidden-to-hidden (recurrent) connection W\u2093\u2095 Weight matrix for input-to-hidden connection tanh Non-linear activation function that squashes output to the range (\u22121, 1) \ud83d\udcccBackpropagation Through Time (BPTT) in RNNs <p>Since RNNs process sequential data Backpropagation Through Time (BPTT) is used to update the network's parameters. The loss function L(\u03b8) depends on the final hidden state h3 and each hidden state relies on preceding ones forming a sequential dependency chain:</p> <p></p> <p>In BPTT, gradients are backpropagated through each time step. This is essential for updating network parameters based on temporal dependencies.</p> <ol> <li>Simplified Gradient Calculation:</li> </ol> <p></p> <p>It\u2019s basically the chain rule from calculus applied to neural network training.</p> <p>What each term means</p> Symbol Meaning L(\u03b8) Loss function, depends on all model parameters \u03b8 (including weight matrices like W) W Weight matrix (can be W\u2093\u2095, W\u2095\u2095, etc.) h\u2083 Hidden state at time step t = 3 \u2202L(\u03b8) / \u2202h\u2083 How much the loss changes if h\u2083 changes (error signal at time step 3) \u2202h\u2083 / \u2202W How much h\u2083 changes if W changes (sensitivity of hidden state to weights) <p>2. Handling Dependencies in Layers: Each hidden state is updated based on its dependencies:</p> <p></p> Symbol Meaning h\u2083 Hidden state at time step 3 W Weight matrix (could be recurrent W\u2095\u2095) h\u2082 Hidden state at time step 2 b Bias vector \u03c3 Activation function (e.g., tanh, ReLU, sigmoid) <p>The gradient is then calculated for each state, considering dependencies from previous hidden states.</p> <p>3. Gradient Calculation with Explicit and Implicit Parts: The gradient is broken down into explicit and implicit parts summing up the indirect paths from each hidden state to the weights.</p> <p></p> <p>4. Final Gradient Expression: The final derivative of the loss function with respect to the weight matrix W is computed:</p> <p></p> <p>This iterative process is the essence of backpropagation through time.</p> \ud83d\udcccTypes Of Recurrent Neural Networks* <p>There are four types of RNNs based on the number of inputs and outputs in the network:</p> \ud83d\udccc1. One-to-One RNN* <p>This is the simplest type of neural network architecture where there is a single input and a single output. It is used for straightforward classification tasks such as binary classification where no sequential data is involved.</p> <p></p> \ud83d\udccc2. One-to-Many RNN* <p>In a One-to-Many RNN the network processes a single input to produce multiple outputs over time. This is useful in tasks where one input triggers a sequence of predictions (outputs). For example in image captioning a single image can be used as input to generate a sequence of words as a caption.</p> <p></p> \ud83d\udccc3. Many-to-One RNN* <p>The Many-to-One RNN receives a sequence of inputs and generates a single output. This type is useful when the overall context of the input sequence is needed to make one prediction. In sentiment analysis the model receives a sequence of words (like a sentence) and produces a single output like positive, negative or neutral.</p> <p></p> \ud83d\udccc4. Many-to-Many RNN* <p>The Many-to-Many RNN type processes a sequence of inputs and generates a sequence of outputs. In language translation task a sequence of words in one language is given as input and a corresponding sequence in another language is generated as output.</p> <p></p> \ud83d\udcccVariants of Recurrent Neural Networks (RNNs)* <p>There are several variations of RNNs, each designed to address specific challenges or optimize for certain tasks:</p> <p>1. Vanilla RNN</p> <p>This simplest form of RNN consists of a single hidden layer where weights are shared across time steps. Vanilla RNNs are suitable for learning short-term dependencies but are limited by the vanishing gradient problem, which hampers long-sequence learning.</p> <p>2. Bidirectional RNNs</p> <p>Bidirectional RNNs process inputs in both forward and backward directions, capturing both past and future context for each time step. This architecture is ideal for tasks where the entire sequence is available, such as named entity recognition and question answering.</p> <p>3. Long Short-Term Memory Networks (LSTMs)</p> <p>Long Short-Term Memory Networks (LSTMs) introduce a memory mechanism to overcome the vanishing gradient problem. Each LSTM cell has three gates:</p> <ul> <li> <p>Input Gate: Controls how much new information should be added to the cell state.</p> </li> <li> <p>Forget Gate: Decides what past information should be discarded.</p> </li> <li> <p>Output Gate: Regulates what information should be output at the current step. This selective memory enables LSTMs to handle long-term dependencies, making them ideal for tasks where earlier context is critical.</p> </li> </ul> <p>4. Gated Recurrent Units (GRUs)</p> <p>Gated Recurrent Units (GRUs) simplify LSTMs by combining the input and forget gates into a single update gate and streamlining the output mechanism. This design is computationally efficient, often performing similarly to LSTMs and is useful in tasks where simplicity and faster training are beneficial.</p> \ud83d\udcccHow RNN Differs from Feedforward Neural Networks?* <p>Feedforward Neural Networks (FNNs) process data in one direction from input to output without retaining information from previous inputs. This makes them suitable for tasks with independent inputs like image classification. However FNNs struggle with sequential data since they lack memory.</p> <p>Recurrent Neural Networks (RNNs) solve this by incorporating loops that allow information from previous steps to be fed back into the network. This feedback enables RNNs to remember prior inputs making them ideal for tasks where context is important.</p> <p></p> \ud83d\udcccImplementing a Text Generator Using Recurrent Neural Networks (RNNs)* <p>We will create a character-based text generator using Recurrent Neural Network (RNN) in TensorFlow and Keras. We'll implement an RNN that learns patterns from a text sequence to generate new text character-by-character.</p>"},{"location":"DeepLearning/Models/RNN/#example","title":"Example:","text":""},{"location":"DeepLearning/Models/RNN/#1-importing-necessary-libraries","title":"1. Importing Necessary Libraries","text":"<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense\n</code></pre> <ul> <li> <p><code>numpy</code> for array manipulation, <code>tensorflow / keras</code> for the model.</p> </li> <li> <p><code>Sequential</code> builds a stack of layers. <code>SimpleRNN</code> = vanilla RNN layer. <code>Dense</code> = fully connected output layer.</p> </li> </ul>"},{"location":"DeepLearning/Models/RNN/#2-defining-the-input-text-and-prepare-character-set","title":"2. Defining the Input Text and Prepare Character Set","text":"<p>We define the input text and identify unique characters in the text which we\u2019ll encode for our model.</p> <pre><code>text = \"Kurudusonnehalli Main Rd, Sonnenahalli Colony, Krishnarajapuram, Bengaluru, Karnataka 560049\"\nchars = sorted(list(set(text)))\nchar_to_index = {char: i for i, char in enumerate(chars)}\nindex_to_char = {i: char for i, char in enumerate(chars)}\n</code></pre> <ul> <li> <p>You build a vocabulary of all distinct characters in <code>text</code>.</p> </li> <li> <p><code>chars</code> is the sorted list of unique characters (spaces, commas, digits, letters, etc.).</p> </li> <li> <p><code>char_to_index</code> / <code>index_to_char</code> convert between character \u2194 integer index (one-hot indexing).</p> </li> </ul> <p>In this specific string:</p> <ul> <li> <p><code>len(text) = 92</code> characters total.</p> </li> <li> <p><code>len(chars) = 31</code> distinct characters (so vocabulary size = 31).</p> </li> </ul>"},{"location":"DeepLearning/Models/RNN/#3-creating-sequences-and-labels","title":"3. Creating Sequences and Labels","text":"<p>To train the RNN, we need sequences of fixed length (seq_length) and the character following each sequence as the label.</p> <pre><code>seq_length = 3\nsequences = []\nlabels = []\n\nfor i in range(len(text) - seq_length):\n    seq = text[i:i + seq_length]\n    label = text[i + seq_length]\n    sequences.append([char_to_index[char] for char in seq])\n    labels.append(char_to_index[label])\n\nX = np.array(sequences)\ny = np.array(labels)\n</code></pre> <ul> <li> <p>You create sliding windows of length <code>seq_length=3</code>.</p> </li> <li> <p>For each window seq = <code>text[i:i+3]</code>, the label is the next character <code>text[i+3]</code>.</p> </li> <li> <p>Example: if <code>text = \"abcdef...\"</code> and <code>seq_length=3</code>, samples are <code>(\"abc\" -&gt; \"d\"), (\"bcd\" -&gt; \"e\"), ....</code></p> </li> <li> <p>Number of training samples = <code>len(text) - seq_length = 89.</code></p> </li> </ul> <p>So after this:</p> <ul> <li> <p><code>X</code> (as indices) shape: <code>(89, 3)</code> \u2014 89 sequences, each 3 indices long.</p> </li> <li> <p><code>y</code> (as indices) shape: <code>(89,)</code> \u2014 1 label per sequence.</p> </li> </ul>"},{"location":"DeepLearning/Models/RNN/#4-converting-sequences-and-labels-to-one-hot-encoding","title":"4. Converting Sequences and Labels to One-Hot Encoding","text":"<p>For training we convert X and y into one-hot encoded tensors.</p> <pre><code>X_one_hot = tf.one_hot(X, len(chars))\ny_one_hot = tf.one_hot(y, len(chars))\n</code></pre> <ul> <li> <p><code>tf.one_hot(X, len(chars))</code> converts each index to a one-hot vector of length 31.</p> </li> <li> <p>Final training tensors:</p> </li> <li> <p><code>X_one_hot</code>: shape <code>(num_samples, timesteps, input_dim) = (89, 3, 31)</code>.</p> </li> <li> <p><code>y_one_hot</code>: shape <code>(num_samples, num_classes) = (89, 31)</code>.</p> </li> </ul> <p>Notes:</p> <ul> <li> <p><code>X_one_hot</code> dtype is <code>float32</code> by default. Keras accepts <code>tf.Tensor</code> inputs.</p> </li> <li> <p>For large vocabularies, one-hot is memory-inefficient \u2014 embeddings are preferred.</p> </li> </ul>"},{"location":"DeepLearning/Models/RNN/#5-building-the-rnn-model","title":"5. Building the RNN Model","text":"<p>We create a simple RNN model with a hidden layer of 50 units and a Dense output layer with softmax activation.</p> <pre><code>model = Sequential()\nmodel.add(SimpleRNN(50, input_shape=(seq_length, len(chars)), activation='relu'))\nmodel.add(Dense(len(chars), activation='softmax'))\n</code></pre> <p>Two layers:</p> <ol> <li> <p><code>SimpleRNN(50, input_shape=(3, 31), activation='relu')</code></p> </li> <li> <p>Units = 50 \u2192 hidden state vector size = 50.</p> </li> <li> <p>Input shape = <code>(timesteps, features) = (3, 31)</code>.</p> </li> <li> <p>Default <code>return_sequences=False</code> so the layer outputs the final hidden state <code>h_T</code> for the whole sequence (shape (<code>batch_size, 50)</code>).</p> </li> <li> <p>Math (per time step t):</p> </li> <li> <p>pre-activation: <code>z_t = x_t @ W_xh + h_{t-1} @ W_hh + b_h</code></p> </li> <li> <p><code>W_xh</code>: shape <code>(input_dim, units) = (31, 50)</code></p> </li> <li> <p><code>W_hh</code>: shape <code>(units, units) = (50, 50)</code></p> </li> <li> <p><code>b_h</code>: shape <code>(50,)</code></p> </li> <li> <p>hidden update: <code>h_t = relu(z_t)</code> (here ReLU used instead of tanh)</p> </li> <li> <p>Initial hidden state <code>h_0</code> defaults to zeros.</p> </li> <li> <p><code>Dense(len(chars), activation='softmax')</code></p> </li> <li> <p>Fully connected layer from 50 \u2192 31 output logits.</p> </li> <li> <p><code>softmax</code> turns logits into a probability distribution over the 31 characters:</p> <ul> <li> <p>logits: <code>logits = h_T @ W_hy + b_y (W_hy</code> shape <code>(50, 31))</code></p> </li> <li> <p><code>y_pred = softmax(logits)</code></p> </li> </ul> </li> </ol>"},{"location":"DeepLearning/Models/RNN/#6-compiling-and-training-the-model","title":"6. Compiling and Training the Model","text":"<p>We compile the model using the categorical_crossentropy loss and train it for 100 epochs.</p> <pre><code>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_one_hot, y_one_hot, epochs=100)\n</code></pre> <ul> <li> <p><code>optimizer='adam'</code>: adaptive optimizer (default lr=0.001).</p> </li> <li> <p><code>loss='categorical_crossentropy'</code>: appropriate for multi-class one-hot labels:</p> </li> </ul> <p></p> <ul> <li> <p><code>metrics=['accuracy']</code>: computes fraction where <code>argmax(y_pred) == argmax(y_true)</code>.</p> </li> <li> <p><code>model.fit(...)</code>:</p> </li> <li> <p>Default <code>batch_size=32</code>. With 89 samples you get 3 batches per epoch: 32 + 32 + 25.</p> </li> <li> <p><code>epochs=100</code> \u2014 small dataset; 100 epochs likely causes overfitting.</p> </li> </ul>"},{"location":"DeepLearning/Models/RNN/#7-generating-new-text-using-the-trained-model","title":"7. Generating New Text Using the Trained Model","text":"<p>After training we use a starting sequence to generate new text character by character.</p> <pre><code>start_seq = \"Kurudusonnehalli\"\ngenerated_text = start_seq\n\nfor i in range(40):\n    x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n    x_one_hot = tf.one_hot(x, len(chars))\n    prediction = model.predict(x_one_hot)\n    next_index = np.argmax(prediction)\n    next_char = index_to_char[next_index]\n    generated_text += next_char\n\nprint(\"Generated Text:\")\nprint(generated_text)\n</code></pre> <ul> <li> <p><code>start_seq = \"Kurudusonnehalli\"</code> seeds generation (must be characters that exist in <code>char_to_index</code>).</p> </li> <li> <p>Each iteration:</p> </li> <li> <p>Take last <code>seq_length</code> characters of <code>generated_text</code>.</p> </li> <li> <p>Convert to indices \u2192 shape <code>(1, 3)</code> (a batch of 1).</p> </li> <li> <p>One-hot \u2192 <code>(1, 3, 31)</code>.</p> </li> <li> <p><code>model.predict</code> returns probabilities shape <code>(1, 31)</code>.</p> </li> <li> <p><code>np.argmax</code> picks the highest-probability character (greedy).`</p> </li> <li> <p>Append predicted char to <code>generated_text</code> and repeat.</p> </li> <li> <p>After 40 iterations you have 43 characters (initial 3 + 40 predicted).</p> </li> </ul> <p>Greedy vs sampling</p> <ul> <li><code>argmax</code> = greedy; produces deterministic and often boring/repetitive text.</li> </ul>"},{"location":"DeepLearning/Models/RNN/#8-under-the-hood-gradients-bptt","title":"8. Under-the-hood: gradients &amp; BPTT","text":"<ul> <li> <p>Training uses Backpropagation Through Time (BPTT) unrolled for 3 time steps (because seq_length=3).</p> </li> <li> <p>Gradients flow from <code>loss</code> \u2192 <code>softmax</code> \u2192 <code>Dense</code> \u2192 ```h_T \u2192 back through RNN time steps via W_hh and activation derivatives.</p> </li> <li> <p>Using <code>relu</code> inside RNN can lead to \u201cdying ReLU\u201d (zero gradients) for some weights \u2014 <code>tanh</code> is more common for SimpleRNN.</p> </li> </ul>"},{"location":"DeepLearning/Models/Transformer/","title":"Transformer Networks","text":"\u2705 Transformer \ud83d\udccc What is Transformer? <p>The Transformer is a neural network architecture that relies entirely on a mechanism called self-attention to process sequential data, like text. Unlike previous models such as Recurrent Neural Networks (RNNs) that process data word-by-word in order, the Transformer processes the entire input sequence at once. This design allows for massive parallelization, dramatically speeding up training time and enabling it to learn long-range dependencies in data more effectively.</p> \ud83d\udccc A High-Level Look <p>Let\u2019s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p> <p></p> <p>A Transformer is typically composed of two main parts:</p> <ol> <li> <p>The Encoder: This part processes the input sentence (e.g., \"Je suis \u00e9tudiant\"). It reads the entire sentence at once and builds a rich numerical representation (a set of vectors) for each word that captures its meaning in the context of the full sentence.</p> </li> <li> <p>The Decoder: This part generates the output sentence (e.g., \"I am a student\") word by word. At each step, it looks at the representations created by the encoder and the words it has already generated to decide which word to produce next.</p> </li> </ol> <p>The encoder's job is to read and understand the input sentence in its entirety.</p> <p>Think of it as a specialist reader. It takes the input sentence, \"Je suis \u00e9tudiant,\" and looks at all the words at once. Using a mechanism called self-attention, it figures out how each word in the sentence relates to all the other words in that same sentence. It learns that \"Je\" is the subject and \"suis\" is the verb connected to it, for example.</p> <p>The final output of the encoder isn't another sentence; it's a set of numerical representations (vectors). You can think of these as rich, context-aware \"notes\" that capture the meaning of the input sentence.</p>"},{"location":"DeepLearning/Models/Transformer/#the-decoder-the-writer","title":"The Decoder: The Writer \u270d\ufe0f","text":"<p>The decoder's job is to generate the output sentence word by word.</p> <p>It's a generative process. To start, it might be given a special \"start\" token. Then, to generate the first word (\"I\"), it relies on two key sources of information:</p> <ol> <li> <p>What it has already written: In the beginning, this is just the \"start\" token. Later, when generating \"am,\" it will look back at \"I.\" This is a form of self-attention, but it's \"masked\" so the decoder can't cheat by looking ahead at words it hasn't generated yet.</p> </li> <li> <p>The Encoder's Notes: This is the critical connection you mentioned! The decoder pays attention to the numerical representations created by the encoder. When it's trying to generate the English translation, it looks across the encoded French sentence to find the most relevant information. As it generates \"student,\" it will pay strong attention to the encoder's notes for the word \"\u00e9tudiant.\" This is often called encoder-decoder attention or cross-attention.</p> </li> </ol> <p>This process repeats\u2014generating one word at a time, looking at the encoder's notes and its own previous output\u2014until it produces a special \"end of sentence\" token.</p> <p><code>we see an encoding component, a decoding component, and connections between them.</code></p> <p></p> <p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2013 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p> <p>The Encoder Stack: Deepening the Understanding</p> <p>When the input \"Je suis \u00e9tudiant\" enters the first encoder, the model might learn some basic relationships. As the output of that encoder is passed to the second, and then the third, and so on, the model builds a more sophisticated understanding.</p> <ul> <li> <p>Layer 1: Might learn simple grammatical connections.</p> </li> <li> <p>Layers 2-4: Could start to understand the semantic meaning and disambiguate words.</p> </li> <li> <p>Layers 5-6: Can capture long-range dependencies and subtle contextual nuances across the entire sentence.</p> </li> </ul> <p>By the time the data emerges from the top of the sixth encoder, the model has a very rich, multi-layered representation of the source sentence's meaning.</p> <p></p> <p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p> <p></p> <p>1. The Self-Attention Layer: The Contextualizer</p> <p>For every single word, the self-attention mechanism generates a score against every other word in the sentence. A high score means the words are highly relevant to each other. It effectively asks, \"To understand the meaning of this specific word in this context, which other words should I pay the most attention to?\"</p> <p>2. The Feed-Forward Network: The Processor</p> <p>After the self-attention layer has gathered and blended the contextual information, the output is passed to a simple but crucial Feed-Forward Neural Network (FFN)</p> <p>This is a standard, fully connected neural network. Importantly, it processes the representation for each word independently. While the self-attention layer was all about inter-word communication, the FFN allows for a deeper, more complex transformation of each word's individual representation.</p> <p>The final output of the encoder layer, which is then passed up to the next encoder in the stack.</p> <p>Identical in Structure, but No Shared Weights</p> <ul> <li> <p>Identical Structure: Every encoder in the stack has this same Attention-FFN setup. This makes the architecture neat and consistent.</p> </li> <li> <p>No Shared Weights: Each of the six encoder layers learns its own unique set of parameters (weights). This is what allows for the hierarchical learning we discussed. If they all had the same weights, stacking them would be redundant. Because they are independent, Encoder Layer 1 can learn to focus on syntactic relationships, while Encoder Layer 6 can learn to capture more abstract semantic meaning.</p> </li> </ul> <p>The encoder\u2019s inputs first flow through a self-attention layer \u2013 a layer that helps the encoder look at other words in the input sentence as it encodes a specific word.</p> <p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p> <p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence</p> <p></p> <p>Bringing The Tensors Into The Picture</p> <p>Now that we\u2019ve seen the major components of the model, let\u2019s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p> <p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.</p> <p></p> <p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 \u2013 In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. The size of this list is hyperparameter we can set \u2013 basically it would be the length of the longest sentence in our training dataset.</p> <p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p> <p></p> <p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p> <p>Next, we\u2019ll switch up the example to a shorter sentence and we\u2019ll look at what happens in each sub-layer of the encoder.</p>"},{"location":"DeepLearning/Models/Transformer/#now-were-encoding","title":"Now We\u2019re Encoding!","text":"<p>As we\u2019ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a \u2018self-attention\u2019 layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#self-attention-at-a-high-level","title":"Self-Attention at a High Level","text":"<p>Don\u2019t be fooled by me throwing around the word \u201cself-attention\u201d like it\u2019s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p> <p>Say the following sentence is an input sentence we want to translate:</p> <p><code>The animal didn't cross the street because it was too tired</code></p> <p>What does \u201cit\u201d in this sentence refer to? Is it referring to the street or to the animal? It\u2019s a simple question to a human, but not as simple to an algorithm.</p> <p>When the model is processing the word \u201cit\u201d, self-attention allows it to associate \u201cit\u201d with \u201canimal\u201d.</p> <p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p> <p>If you\u2019re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it\u2019s processing. Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing.</p> <p></p> <p>Self-Attention in Detail</p> <p>Let\u2019s first look at how to calculate self-attention using vectors, then proceed to look at how it\u2019s actually implemented \u2013 using matrices.</p> <p>The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a </p> <ol> <li> <p>Query vector</p> </li> <li> <p>Key vector</p> </li> <li> <p>Value vector.</p> </li> </ol> <p>These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>"},{"location":"DeepLearning/Models/Transformer/#transformer-attention-vector-dimensions","title":"Transformer Attention Vector Dimensions","text":""},{"location":"DeepLearning/Models/Transformer/#1-context","title":"1. Context","text":"<p>In the Transformer architecture, embedding vectors and encoder input/output vectors typically have a dimensionality of 512. However, the vectors used inside multi-head attention (query, key, and value vectors) often have a smaller dimensionality \u2014 in this example, 64.</p>"},{"location":"DeepLearning/Models/Transformer/#2-why-smaller-dimensions-for-attention","title":"2. Why Smaller Dimensions for Attention?","text":"<ul> <li>Design Choice: The reduced dimensionality is not mandatory, but is chosen to make the computation of multi-head attention more efficient.</li> <li>Computation Cost: Multi-head attention involves matrix multiplications for multiple attention heads.   By reducing the vector size, the computation for each head remains (mostly) constant regardless of the total embedding size.</li> </ul>"},{"location":"DeepLearning/Models/Transformer/#3-example-multi-head-attention-dimension-split","title":"3. Example: Multi-Head Attention Dimension Split","text":""},{"location":"DeepLearning/Models/Transformer/#given","title":"Given:","text":"<ul> <li>Embedding size = 512</li> <li>Number of heads = 8</li> <li>Dimension per head = 512 \u00f7 8 = 64</li> </ul>"},{"location":"DeepLearning/Models/Transformer/#process","title":"Process:","text":"<ol> <li>The input embedding (512-d) is projected into:</li> <li>Query vectors (Q) \u2192 64 dimensions per head</li> <li>Key vectors (K) \u2192 64 dimensions per head</li> <li> <p>Value vectors (V) \u2192 64 dimensions per head</p> </li> <li> <p>Each attention head operates independently on its 64-dimensional vectors.</p> </li> <li> <p>The outputs from all heads are concatenated back to 512 dimensions.</p> </li> </ol>"},{"location":"DeepLearning/Models/Transformer/#4-benefits-of-this-design","title":"4. Benefits of This Design","text":"<ul> <li>Efficiency: Reduces the matrix multiplication cost per head.</li> <li>Parallelization: Multiple heads run in parallel without exploding computational requirements.</li> <li>Flexibility: The per-head dimension (64 here) can be tuned based on memory and compute budgets.</li> </ul>"},{"location":"DeepLearning/Models/Transformer/#5-visual-representation","title":"5. Visual Representation","text":"<p>What are the \u201cquery\u201d, \u201ckey\u201d, and \u201cvalue\u201d vectors?</p>"},{"location":"DeepLearning/Models/Transformer/#query-key-and-value-vectors-in-attention","title":"Query, Key, and Value Vectors in Attention","text":""},{"location":"DeepLearning/Models/Transformer/#1-what-are-q-k-and-v","title":"1. What Are Q, K, and V?","text":"<p>In the attention mechanism (used in Transformers), each input token is represented by three vectors:</p> <ul> <li>Query (Q) \u2192 What am I looking for?</li> <li>Key (K) \u2192 What do I contain?</li> <li>Value (V) \u2192 The actual content I can offer.</li> </ul> <p>They are learned projections of the same input embedding.</p>"},{"location":"DeepLearning/Models/Transformer/#2-how-are-they-created","title":"2. How Are They Created?","text":"<p>For each token embedding x (e.g., 512-dimensional), we compute:</p> <p>Q = x \u00d7 W_Q K = x \u00d7 W_K V = x \u00d7 W_V</p> <p>Where: - <code>W_Q</code>, <code>W_K</code>, <code>W_V</code> are learnable weight matrices. - The resulting Q, K, and V vectors are usually smaller in dimension (e.g., 64 for each head).</p>"},{"location":"DeepLearning/Models/Transformer/#3-analogy","title":"3. Analogy","text":"<p>Imagine a library:</p> <ul> <li>Query: Your search request (e.g., \"books about AI\").</li> <li>Key: The labels on each book in the library (metadata).</li> <li>Value: The full text/content of the book.</li> </ul> <p>The attention mechanism matches Query with Keys to decide which Values to retrieve.</p>"},{"location":"DeepLearning/Models/Transformer/#4-how-they-work-in-attention","title":"4. How They Work in Attention","text":"<ol> <li>Similarity Calculation    Compare each Query with all Keys using a dot product:</li> </ol> <p>score = Q \u00b7 K^T</p> <p>This produces a measure of how relevant each key is to the query.</p> <ol> <li>Scaling and Softmax Scale scores by <code>\u221ad_k</code> (dimension of key) and apply softmax to get attention weights:</li> </ol> <p>attention_weights = softmax(score / \u221ad_k)</p> <ol> <li>Weighted Sum of Values Multiply each Value vector by its corresponding attention weight and sum:</li> </ol> <p>output = \u03a3 (attention_weight \u00d7 V)</p> <p>This output is a context vector \u2014 a blend of relevant values based on the query.</p>"},{"location":"DeepLearning/Models/Transformer/#5-formula","title":"5. Formula","text":"<p>The scaled dot-product attention formula is:</p>"},{"location":"DeepLearning/Models/Transformer/#6-example","title":"6. Example","text":"<p>Sentence: <code>\"The cat sat on the mat\"</code></p> <ul> <li>The word \"cat\" (as a query) might have high similarity to \"sat\" and \"mat\" in the key space,   so the context vector for \"cat\" will focus more on values from those words.</li> </ul>"},{"location":"DeepLearning/Models/Transformer/#7-visual-diagram-conceptual","title":"7. Visual Diagram (Conceptual)","text":"<p>Embedding \u2192 Linear(W_Q) \u2192 Query (Q) \u2192 Linear(W_K) \u2192 Key (K) \u2192 Linear(W_V) \u2192 Value (V)</p> <p>Q \u00d7 K^T \u2192 Scale \u2192 Softmax \u2192 Weights \u00d7 V \u2192 Context Vector</p>"},{"location":"DeepLearning/Models/Transformer/#8-key-takeaway","title":"8. Key Takeaway","text":"<ul> <li>Query: What information this token is looking for.  </li> <li>Key: What information this token contains.  </li> <li>Value: The actual information content to pass forward.  </li> <li>Attention finds matches between Q and K to decide which V matters most.</li> </ul> <p>They\u2019re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you\u2019ll know pretty much all you need to know about the role each of these vectors plays.</p> <p>The second step in calculating self-attention is to calculate a score. Say we\u2019re calculating the self-attention for the first word in this example, \u201cThinking\u201d. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p> <p>The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So if we\u2019re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.</p> <p></p> <p>The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper \u2013 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they\u2019re all positive and add up to 1.</p> <p></p> <p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word.</p> <p>The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p> <p>The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p> <p></p> <p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let\u2019s look at that now that we\u2019ve seen the intuition of the calculation on the word level.</p>"},{"location":"DeepLearning/Models/Transformer/#matrix-calculation-of-self-attention","title":"Matrix Calculation of Self-Attention","text":"<p>Finally, since we\u2019re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#the-beast-with-many-heads","title":"The Beast With Many Heads","text":"<p>The paper further refined the self-attention layer by adding a mechanism called \u201cmulti-headed\u201d attention. This improves the performance of the attention layer in two ways:</p> <ol> <li> <p>It expands the model\u2019s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we\u2019re translating a sentence like \u201cThe animal didn\u2019t cross the street because it was too tired\u201d, it would be useful to know which word \u201cit\u201d refers to.</p> </li> <li> <p>It gives the attention layer multiple \u201crepresentation subspaces\u201d. As we\u2019ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p> </li> </ol> <p></p> <p>If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p> <p></p> <p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2013 it\u2019s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p> <p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p> <p></p> <p>That\u2019s pretty much all there is to multi-headed self-attention. It\u2019s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p> <p></p> <p>Now that we have touched upon attention heads, let\u2019s revisit our example from before to see where the different attention heads are focusing as we encode the word \u201cit\u201d in our example sentence:</p> <p></p> <p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#representing-the-order-of-the-sequence-using-positional-encoding","title":"Representing The Order of The Sequence Using Positional Encoding","text":"<p>One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p> <p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they\u2019re projected into Q/K/V vectors and during dot-product attention.</p> <p></p> <p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#the-residuals","title":"The Residuals","text":"<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.</p> <p></p> <p>If we\u2019re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p> <p></p> <p>This goes for the sub-layers of the decoder as well. If we\u2019re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#the-decoder-side","title":"The Decoder Side","text":"<p>Now that we\u2019ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let\u2019s take a look at how they work together.</p> <p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence:</p> <p></p> <p>The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</p> <p></p> <p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p> <p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.</p> <p>The \u201cEncoder-Decoder Attention\u201d layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>"},{"location":"DeepLearning/Models/Transformer/#the-final-linear-and-softmax-layer","title":"The Final Linear and Softmax Layer","text":"<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That\u2019s the job of the final Linear layer which is followed by a Softmax Layer.</p> <p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p> <p>Let\u2019s assume that our model knows 10,000 unique English words (our model\u2019s \u201coutput vocabulary\u201d) that it\u2019s learned from its training dataset. This would make the logits vector 10,000 cells wide \u2013 each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p> <p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#recap-of-training","title":"Recap Of Training","text":"<p>Now that we\u2019ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p> <p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p> <p>To visualize this, let\u2019s assume our output vocabulary only contains six words(\u201ca\u201d, \u201cam\u201d, \u201ci\u201d, \u201cthanks\u201d, \u201cstudent\u201d, and \u201c\u201d (short for \u2018end of sentence\u2019)). <p></p> <p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word \u201cam\u201d using the following vector:</p> <p></p> <p>Following this recap, let\u2019s discuss the model\u2019s loss function \u2013 the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>"},{"location":"DeepLearning/Models/Transformer/#the-loss-function","title":"The Loss Function","text":"<p>Say we are training our model. Say it\u2019s our first step in the training phase, and we\u2019re training it on a simple example \u2013 translating \u201cmerci\u201d into \u201cthanks\u201d.</p> <p>What this means, is that we want the output to be a probability distribution indicating the word \u201cthanks\u201d. But since this model is not yet trained, that\u2019s unlikely to happen just yet.</p> <p></p> <p>How do you compare two probability distributions? We simply subtract one from the other.</p> <p>But note that this is an oversimplified example. More realistically, we\u2019ll use a sentence longer than one word. For example \u2013 input: \u201cje suis \u00e9tudiant\u201d and expected output: \u201ci am a student\u201d. What this really means, is that we want our model to successively output probability distributions where:</p> <p>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000) The first probability distribution has the highest probability at the cell associated with the word \u201ci\u201d The second probability distribution has the highest probability at the cell associated with the word \u201cam\u201d And so on, until the fifth output distribution indicates \u2018\u2019 symbol, which also has a cell associated with it from the 10,000 element vocabulary. <p></p> <p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p> <p></p>"},{"location":"DeepLearning/Models/Transformer/#hugginface","title":"Hugginface","text":"<p>Of course. That's an extensive list of models, many of which belong to the same architectural families.</p> <p>Creating a unique entry for every single model would be extremely long and repetitive. Instead, I've created a table that groups them by their core architecture or family. This approach provides a clearer understanding of what each model type is designed for.</p> <p>Here is the requested format, summarizing the major model families and prominent examples from your list.</p> <p>Of course. That's an extensive list of models, many of which belong to the same architectural families.</p> <p>Creating a unique entry for every single model would be extremely long and repetitive. Instead, I've created a table that groups them by their core architecture or family. This approach provides a clearer understanding of what each model type is designed for.</p> <p>Here is the requested format, summarizing the major model families and prominent examples from your list.</p> Model / Family Short Description Primary Use Cases BERT Family Encoder-only models. They read an entire text sequence at once to build a deep bidirectional understanding of context. They are excellent \"understanding\" models. \u2022 Text Classification\u2022 Sentiment Analysis\u2022 Named Entity Recognition (NER)\u2022 Extractive Question Answering <code>BERT</code>, <code>RoBERTa</code>, <code>ALBERT</code>, <code>DistilBERT</code>, <code>ELECTRA</code>, <code>CamemBERT</code>, <code>FlauBERT</code> Variations on the original BERT, often optimized for size, speed, training efficiency, or pre-trained on specific languages (e.g., CamemBERT for French). Same as the family's general use cases. GPT Family Decoder-only models. Autoregressive models that generate text one word at a time, based on the preceding words. They are excellent \"generation\" models. \u2022 Content Generation (stories, articles)\u2022 Chatbots &amp; Conversational AI\u2022 Summarization\u2022 General-purpose instruction following <code>GPT-2</code>, <code>GPT-J</code>, <code>GPT-Neo</code>, <code>LLaMA</code>, <code>Llama2/3</code>, <code>Gemma</code>, <code>Mistral</code>, <code>Falcon</code>, <code>Phi-3</code>, <code>OPT</code> These are foundational large language models (LLMs) that power most modern generative AI applications. They vary in size, training data, and performance. Same as the family's general use cases. T5 / BART Family Encoder-Decoder models. A versatile sequence-to-sequence (seq2seq) framework that treats every NLP task as a \"text-to-text\" problem. \u2022 Machine Translation\u2022 Text Summarization\u2022 Generative Question Answering\u2022 Code Generation &amp; Data-to-Text tasks <code>T5</code>, <code>BART</code>, <code>FLAN-T5</code>, <code>PEGASUS</code>, <code>MarianMT</code>, <code>MBart</code> T5 is a canonical text-to-text model. BART is optimized for denoising. PEGASUS is specialized for abstractive summarization. MarianMT is focused on translation. Same as the family's general use cases. Mixture-of-Experts (MoE) Sparse models. A variation of other architectures (usually decoder-only) that uses multiple \"expert\" sub-networks. Only a fraction of the model is used for any given input, making them very efficient to run for their size. \u2022 High-performance, scalable generation\u2022 Efficiently serving very large models\u2022 Multi-task, multi-domain capabilities <code>Mixtral</code>, <code>Qwen2MoE</code>, <code>NLLB-MoE</code>, <code>SwitchTransformers</code> Implementations of the MoE architecture. For example, Mixtral is a GPT-family model with MoE layers. NLLB-MoE is an Encoder-Decoder model for translation. Use cases align with their base architecture (e.g., Mixtral for generation, NLLB-MoE for translation). Alternative Architectures Non-Transformer or Hybrid models. These models aim to solve some of the Transformer's inefficiencies, especially its quadratic complexity with long sequences. \u2022 Long-context document analysis\u2022 Processing DNA or time-series data\u2022 Computationally efficient generation <code>Mamba</code>, <code>RWKV</code>, <code>Zamba</code>, <code>Jamba</code>, <code>RecurrentGemma</code> These use State Space Models (SSMs) or linear RNN approaches to offer faster inference and handle extremely long contexts better than standard attention mechanisms. Jamba is a hybrid of Mamba and Transformer blocks. Same as the family's general use cases. Specialized Models Models fine-tuned or pre-trained for a specific domain. While based on the architectures above, their training data gives them expert-level capabilities in a niche. \u2022 Code: Generation, completion, debugging\u2022 Biology: Protein folding, sequence analysis\u2022 Vision: Image captioning, VQA <code>CodeGen</code>, <code>CodeLlama</code>, <code>Starcoder2</code> (Code) <code>BioGpt</code> (Biomedical) Code models are trained on billions of lines of code. Biomedical models are trained on scientific literature and medical texts. Domain-specific tasks as listed. Long Context Models Transformer variants optimized for long sequences. These models use modified attention mechanisms to handle inputs of many thousands or even millions of tokens. \u2022 Summarizing entire books or reports\u2022 Question answering over large document sets\u2022 \"Retrieval Augmented Generation\" (RAG) over long chat histories <code>Longformer</code>, <code>BigBird</code>, <code>Transformer-XL</code> Use sparse, sliding window, or global attention patterns to reduce the computational cost of the self-attention mechanism, enabling it to process much longer inputs. Same as the family's general use cases. Model Name Availability Hugging Face Hosted? Notes (License / Access) BERT Open-source (Free) \u2705 Yes Apache 2.0 license, released by Google; base model on HF. ALBERT Open-source (Free) \u2705 Yes Apache 2.0 license, Google; multiple variants on HF. DistilBERT Open-source (Free) \u2705 Yes Hugging Face compressed BERT variant, Apache 2.0. RoBERTa Open-source (Free) \u2705 Yes Meta AI; HF hosts base &amp; large versions. ELECTRA Open-source (Free) \u2705 Yes Google; HF hosts small, base, large variants. DeBERTaV3 Open-source (Free) \u2705 Yes Microsoft; HF hosts pretrained checkpoints. BioGPT Open-source (Free) \u2705 Yes Microsoft biomedical GPT; HF hosts weights. CodeGen Open-source (Free) \u2705 Yes Salesforce code LLM; HF models available. LLaMA Open-source (Free for research) \u2705 Yes (restricted) Meta; HF gated repo requires approval. GPT-J Open-source (Free) \u2705 Yes EleutherAI; full model weights on HF. GPT-Neo Open-source (Free) \u2705 Yes EleutherAI; HF hosts multiple sizes. GPT-NeoX-20B Open-source (Free) \u2705 Yes EleutherAI; HF hosts full weights. GPT-2 Open-source (Free) \u2705 Yes OpenAI; HF hosts small to XL variants. XLNet Open-source (Free) \u2705 Yes Google/CMU; HF hosts pretrained checkpoints. GLM-130B Open-source (Free) \u2705 Yes THUDM; large bilingual model, hosted on HF. DeepSeek-V3 Open-source (Free) \u274c No MIT license; currently distributed outside HF. Qwen2.5 Open-source (Free) \u2705 Yes Alibaba; HF hosts multiple versions incl. MoE. GPT-4.5 Paid / Proprietary \u274c No OpenAI; API-only access, no HF hosting. Claude 3 Paid / Proprietary \u274c No Anthropic; API-only, no open weights. Gemini 1.5 Paid / Proprietary \u274c No Google DeepMind; API-only, no HF hosting."},{"location":"DeepLearning/Models/Transformer/#referance-link","title":"Referance Link","text":"<p>Transformer</p> <p>Transformer huggingface</p>"},{"location":"GraphDatabases/neo4j-cypher/","title":"cypher","text":""},{"location":"GraphDatabases/neo4j-cypher/#create","title":"CREATE","text":"<ul> <li>The <code>CREATE</code> clause allows you to create nodes and relationships.</li> <li>To define these entities, <code>CREATE</code> uses a syntax similar to that of <code>MATCH</code>. However, while patterns only need to evaluate to either true or false, the syntax for <code>CREATE</code> needs to specify exactly what nodes and relationships to create.</li> </ul>"},{"location":"GraphDatabases/neo4j-cypher/#syntax-for-nodes","title":"Syntax for nodes","text":"<p>The <code>CREATE</code> clause allows you to create one or more nodes. Each node can be assigned <code>labels</code> and <code>properties</code>. You can bind each node to a <code>variable</code> that you can refer to later in the query. Multiple <code>labels</code> are separated by <code>colons</code>.</p> <p>Requirements:</p> <p>represent people who can have multiple roles (e.g., <code>actor</code>, <code>director</code>) in a graph database.</p> <p>Cyper Query:</p> <pre><code>CREATE \n(charlie:Person:Actor {name: 'Charlie Sheen'}), \n(oliver:Person:Director {name: 'Oliver Stone'})\n</code></pre> <p>1\ufe0f\u20e3 CREATE</p> <ul> <li>Tells Neo4j to <code>create new nodes</code> in the graph.</li> <li>This does not check if they already exist (duplicates possible).</li> <li>Used when you are sure the data is new.</li> </ul> <p>2\ufe0f\u20e3 (charlie:Person:Actor {name: 'Charlie Sheen'})</p> <p>This creates one node with:</p> Component Meaning <code>charlie</code> Variable name (used only within this query) <code>:Person</code> Label \u2192 general classification <code>:Actor</code> Label \u2192 specific role <code>{name: 'Charlie Sheen'}</code> Properties (key\u2013value data) <p>\ud83d\udccc Requirement satisfied here:</p> <p>A single entity can belong to multiple categories (<code>Person</code> + <code>Actor</code>) without duplication.</p> <p>3\ufe0f\u20e3 (oliver:Person:Director {name: 'Oliver Stone'})</p> <p>Similarly creates:</p> Component Meaning <code>oliver</code> Query variable <code>:Person</code> General entity type <code>:Director</code> Role <code>{name: 'Oliver Stone'}</code> Property <p>\ud83d\udccc Requirement satisfied here:</p> <p>Another person, but with a different role, while still being a Person.</p> <p>\ud83e\udde9 Why use multiple labels instead of a property?</p> <p>\u274c Bad (less efficient)</p> <pre><code>(:Person {name:'Charlie Sheen', role:'Actor'})\n</code></pre> <p>\u2705 Good (your query)</p> <pre><code>(:Person:Actor {name:'Charlie Sheen'})\n</code></pre> <p>Why?</p> Reason Benefit Labels are indexed Faster queries Multiple labels allowed One node, many roles Cleaner queries <code>MATCH (:Actor)</code> Scales well Enterprise-grade KG <p>\ud83d\udd0e Example Queries Enabled by This Design</p> <p>Find all actors</p> <pre><code>MATCH (a:Actor) RETURN a.name\n</code></pre> <p>Find all directors</p> <pre><code>MATCH (d:Director) RETURN d.name\n</code></pre> <p>Find all people</p> <pre><code>MATCH (p:Person) RETURN p.name\n</code></pre> <p>Find people who are both Actor AND Director</p> <pre><code>MATCH (p:Person:Actor:Director) RETURN p.name\n</code></pre> <p>Multiple labels can also be separated by an ampersand &amp;, in the same manner as it is used in label expressions. Separation by colon : and ampersand &amp; cannot be mixed in the same clause.</p> <p>Cyper Query:</p> <pre><code>CREATE \n(charlie:Person&amp;Actor {name: 'Charlie Sheen'}), \n(oliver:Person&amp;Director {name: 'Oliver Stone'})\n</code></pre> <p>Both of the above queries create two nodes, bound to the variables charlie and oliver, each with a Person label and a name property. The node representing <code>Charlie Sheen</code> also has the label Actor while the node representing <code>Oliver Stone</code> is assigned the label Director.</p>"},{"location":"GraphDatabases/neo4j-cypher/#syntax-for-relationships","title":"Syntax for relationships","text":"<p>\ud83d\udccc Cypher Relationship Syntax \u2013 Explained</p> <p>\ud83d\udd39 Query</p> <pre><code>CREATE\n  (charlie:Person:Actor {name: 'Charlie Sheen'})\n    -[:ACTED_IN {role: 'Bud Fox'}]-&gt;\n  (wallStreet:Movie {title: 'Wall Street'})\n    &lt;-[:DIRECTED]-\n  (oliver:Person:Director {name: 'Oliver Stone'})\n</code></pre> <p>This query creates the Person nodes for Charlie Sheen and Oliver Stone and the Movie node for Wall Street. It also created the relationships of the types ACTED_IN and DIRECTED between them.</p>"},{"location":"GraphDatabases/neo4j-cypher/#reusing-variables","title":"Reusing variables","text":"<p>The previous example created a path between the specified nodes. Note, that these newly created nodes and relationships are not connected to what was previously in the graph. To connect them to already existing data, bind the desired nodes and relationships to variables. These variables can then be passed along to subsequent clauses in a query that target pre-existing elements in the graph.</p> <p>Cyper Query:</p> <pre><code>MATCH (charlie:Person {name: 'Charlie Sheen'}), (oliver:Person {name: 'Oliver Stone'})\nCREATE (charlie)-[:ACTED_IN {role: 'Bud Fox'}]-&gt;(wallStreet:Movie {title: 'Wall Street'})&lt;-[:DIRECTED]-(oliver)\n</code></pre> <p>In this example, the MATCH clause finds the nodes Charlie Sheen and Oliver Stone and binds them to the charlie and oliver variables respectively. These variables are then passed along to the subsequent CREATE clause, which creates new relationships from the bound nodes.</p> <p>You can also reuse variables from the same CREATE, both in the same or a later clause. This way, you can, for example, define constructs that are more complex than just a linear path.</p> <p>Cyper Query:</p> <pre><code>CREATE p = (charlie:Person:Actor {name: 'Charlie Sheen'})-[:ACTED_IN {role: 'Bud Fox'}]-&gt;(wallStreet:Movie {title: 'Wall Street'})&lt;-[:DIRECTED]-(oliver:Person:Director {name: 'Oliver Stone'}), (wallStreet)&lt;-[:ACTED_IN {role: 'Gordon Gekko'}]-(michael:Person:Actor {name: 'Michael Douglas'})\nRETURN length(p)\n</code></pre> <p>Creates all three nodes for Charlie Sheen, Oliver Stone and Michael Douglas and connects them all to the node representing the Wall Street movie. It then returns the length of the path from Charlie Sheen to Oliver Stone.</p> <p>Note that when repeating a node\u2019s variable, you may not add labels or properties to the repetition.</p>"},{"location":"GraphDatabases/neo4j-cypher/#example","title":"Example:","text":"<pre><code>CREATE\n(Ganesh:Person:Architect{name:'Ganesh kinkar giri'})\n</code></pre> <p>OP:</p> <pre><code>Added 2 labels, created 1 node, set 1 property, completed after 11 ms.\n</code></pre> <pre><code>MATCH (a:Person) return a.name\n</code></pre> <p>OP:</p> <pre><code>\"Ganesh kinkar giri\"\n</code></pre> <pre><code>MATCH (n) RETURN n LIMIT 25\n</code></pre> <p>Table:</p> <pre><code>{\n  \"identity\": 0,\n  \"labels\": [\n    \"Person\",\n    \"Architect\"\n  ],\n  \"properties\": {\n    \"name\": \"Ganesh kinkar giri\"\n  },\n  \"elementId\": \"4:8667926f-58ca-43b3-a452-a345911d9507:0\"\n}\n</code></pre> <pre><code>CREATE\n(Taneesh:Person:SoftwareEngineer{name:'Taneesh GIRI'}),\n(Rakesh:Person:Tester{name:'Rakesh Panda'}),\n(Srabani:Person:Manager{name:'Srabani Patra'}),\n(Mivaan:Person:Director{name:'Mivaan Giri'})\n</code></pre> <p></p> <pre><code>MATCH (n) RETURN n LIMIT 10\n</code></pre> <p></p> <p></p> <p></p> <pre><code>MATCH (a:Architect) RETURN a.name\n</code></pre> <p></p> <pre><code>MATCH (a:Person) RETURN a.name\n</code></pre> <p></p>"},{"location":"GraphDatabases/neo4j-setup/","title":"neo4j setup","text":""},{"location":"GraphDatabases/neo4j-setup/#pvc","title":"PVC","text":"<p>pvc.yaml</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: neo4j-data-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"GraphDatabases/neo4j-setup/#secret","title":"Secret","text":"<p>Secret.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: neo4j-auth\n  namespace: ganesh-aimlapp\ntype: Opaque\nstringData:\n  auth: neo4j/admin123\n</code></pre>"},{"location":"GraphDatabases/neo4j-setup/#deployment","title":"Deployment","text":"<p>Deployment-Neo4j.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: neo4j\n  namespace: ganesh-aimlapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: neo4j\n  template:\n    metadata:\n      labels:\n        app: neo4j\n    spec:\n      containers:\n        - name: neo4j\n          image: neo4j:5\n          imagePullPolicy: IfNotPresent\n\n          ports:\n            - name: http\n              containerPort: 7474\n            - name: bolt\n              containerPort: 7687\n\n          env:\n            # \ud83d\udd10 Authentication\n            - name: NEO4J_AUTH\n              valueFrom:\n                secretKeyRef:\n                  name: neo4j-auth\n                  key: auth\n\n            # \u2705 REQUIRED FIX for K8s env var injection\n            - name: NEO4J_server_config_strict__validation_enabled\n              value: \"false\"\n\n            # \ud83e\udde0 Memory tuning\n            - name: NEO4J_dbms_memory_heap_initial__size\n              value: \"2G\"\n            - name: NEO4J_dbms_memory_heap_max__size\n              value: \"4G\"\n            - name: NEO4J_dbms_memory_pagecache_size\n              value: \"2G\"\n\n            # \ud83c\udf10 Listen on all interfaces\n            - name: NEO4J_server_default__listen__address\n              value: \"0.0.0.0\"\n\n            # \ud83d\udd11 CRITICAL: Bolt configuration for Ingress usage\n            - name: NEO4J_dbms_connector_bolt_listen__address\n              value: \"0.0.0.0:7687\"\n            - name: NEO4J_dbms_connector_bolt_advertised__address\n              value: \"localhost:7687\"\n\n          volumeMounts:\n            - name: neo4j-data\n              mountPath: /data\n\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 7474\n            initialDelaySeconds: 30\n            periodSeconds: 10\n\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 7474\n            initialDelaySeconds: 60\n            periodSeconds: 20\n\n      volumes:\n        - name: neo4j-data\n          persistentVolumeClaim:\n            claimName: neo4j-data-pvc\n</code></pre>"},{"location":"GraphDatabases/neo4j-setup/#service","title":"Service","text":"<p>Service-Neo4j.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: neo4j\n  namespace: ganesh-aimlapp\nspec:\n  type: ClusterIP\n  selector:\n    app: neo4j\n  ports:\n    - name: http\n      port: 7474\n      targetPort: 7474\n    - name: bolt\n      port: 7687\n      targetPort: 7687\n</code></pre>"},{"location":"GraphDatabases/neo4j-setup/#ingress","title":"Ingress","text":"<p>ingress-Neo4j.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: neo4j\n  namespace: ganesh-aimlapp\n  annotations:\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"\n    # \u26d4\ufe0f No ssl-redirect\n    # \u26d4\ufe0f No cookie affinity\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: neo4j.apps.kubernetes.aimledu.cloud\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: neo4j\n                port:\n                  number: 7474\n</code></pre>"},{"location":"GraphDatabases/neo4j-setup/#local-port-forward","title":"Local Port forward","text":"<pre><code>kubectl -n ganesh-aimlapp port-forward service/neo4j 7687:7687\nkubectl -n ganesh-aimlapp port-forward service/neo4j 7474:7474\n</code></pre> <p>http://127.0.0.1:7474/browser/</p> <p></p> <p></p> <p></p>"},{"location":"LinearAlgebra/Overview/","title":"Overview","text":"\u2705 Linear Algebra For Machine Learning \ud83d\udccc What is Linear Algebra? <p>Linear algebra is a core mathematical foundation for machine learning, as most datasets and models are represented using vectors and matrices. It allows efficient computation, data manipulation and optimization, making complex tasks manageable.</p> <ul> <li> <p>Data in ML is represented as vectors (features) and matrices (datasets).</p> </li> <li> <p>Operations like dot product, matrix multiplication and transformations power ML algorithms.</p> </li> <li> <p>Key concepts such as eigenvalues, eigenvectors and decompositions simplify dimensionality reduction, optimization and training.</p> </li> <li> <p>Algorithms like PCA, SVD, regression, SVMs and neural networks rely heavily on linear algebra.</p> </li> </ul> <p></p> <p></p> <p></p> \ud83d\udccc Fundamental Concepts in Linear Algebra for Machine Learning <p>In machine learning, vectors, matrices and scalars play key roles in handling and processing data.</p> 1. Vectors <p>Vectors are quantities that have both magnitude and direction, often represented as arrows in space.</p> <p></p> 2. Matrices <p>Matrices are rectangular arrays of numbers, arranged in rows and columns. Matrices are used to represent linear transformations, systems of linear equations and data transformations in machine learning.</p> <p></p> <p>A matrix is a rectangular array of numbers arranged in rows and columns.</p> <p>Notation: an m\u00d7n matrix A has m rows and n columns and entries \ud835\udc4e\ud835\udc56\ud835\udc57 (row i, column j):</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> 3. Scalars <p>Scalars are single numerical values, without direction, magnitude only. Scalars are just single numbers that can multiply vectors or matrices. In machine learning, they\u2019re used to adjust things like the weights in a model or the learning rate during training</p> <p></p> Operations in Linear Algebra <p></p> Linear Transformations <p>Linear transformations are basic operations in linear algebra that change vectors and matrices while keeping important properties like straight lines and proportionality. In machine learning, they are key for tasks like preparing data, creating features and training models. This section covers the definition, types and uses of linear transformations.</p> <p></p> Matrix Operations <p>Matrix operations are central to linear algebra and widely used in machine learning for data handling, transformations and model training. The most common ones are:</p> <p></p> Eigenvalues and Eigenvectors <p>Eigenvalues and eigenvectors describe how matrices transform space, making them fundamental in many ML algorithms.</p> <p></p> Solving Linear Systems of equations <p>Linear systems are common in machine learning for parameter estimation and optimization. Key methods include:</p> <p></p> Applications of Linear Algebra in Machine Learning <p>Linear algebra powers many ML algorithms by enabling data manipulation, model representation and optimization. Key applications include:</p> <ul> <li> <p>PCA (Principal Component Analysis): Reduces dimensionality by computing covariance, eigenvalues/eigenvectors and projecting data onto principal components.</p> </li> <li> <p>SVD (Singular Value Decomposition): Factorizes a matrix into A = U\u03a3VT, used for dimensionality reduction, compression and noise filtering.</p> </li> <li> <p>Linear Regression: Models relationships via matrix form Y = X\u03b2+ \u03f5, solved using the normal equation XTX\u03b2 = XTY.</p> </li> <li> <p>SVM (Support Vector Machines): Uses the kernel trick and optimization to find decision boundaries for classification and regression.</p> </li> <li> <p>Neural Networks: Depend on matrix multiplications, gradient descent and weight initialization for training deep models.</p> </li> </ul>"},{"location":"MCP/mcp/","title":"MCP","text":"\u2705 Model Context Protocol (MCP) \ud83d\udccc What is Model Context Protocol (MCP)? <p>MCP is an open protocol that standardizes how applications provide context to large language models (LLMs).</p> <p>Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. MCP enables you to build agents and complex workflows on top of LLMs and connects your models with the world.</p> \ud83d\udccc MCP provides: <ul> <li> <p>A growing list of pre-built integrations that your LLM can directly plug into</p> </li> <li> <p>A standardized way to build custom integrations for AI applications</p> </li> <li> <p>An open protocol that everyone is free to implement and use</p> </li> <li> <p>The flexibility to change between different apps and take your context with you</p> </li> </ul> \ud83d\udccc Concepts of MCP: <p>MCP follows a client-server architecture where an MCP host \u2014 an AI application.</p> <p>Client establishes connections to one or more MCP servers.</p> <p>The MCP host accomplishes this by creating one MCP client for each MCP server. Each MCP client maintains a dedicated one-to-one connection with its corresponding MCP server.</p> <p>The key participants in the MCP architecture are:</p> <ol> <li> <p>MCP Host: The AI application that coordinates and manages one or multiple MCP clients</p> </li> <li> <p>MCP Client: A component that maintains a connection to an MCP server and obtains context from an MCP server for the MCP host to use</p> </li> <li> <p>MCP Server: A program that provides context to MCP clients</p> </li> </ol> <p>For example: Visual Studio Code acts as an MCP host. When Visual Studio Code establishes a connection to an MCP server, such as the Sentry MCP server, the Visual Studio Code runtime instantiates an MCP client object that maintains the connection to the Sentry MCP server. When Visual Studio Code subsequently connects to another MCP server, such as the local filesystem server, the Visual Studio Code runtime instantiates an additional MCP client object to maintain this connection, hence maintaining a one-to-one relationship of MCP clients to MCP servers.</p> <p></p> \ud83d\udccc MCP consists of two layers: <ul> <li> <p>Data layer: Defines the JSON-RPC based protocol for client-server communication, including lifecycle management, and core primitives, such as tools, resources, prompts and notifications.</p> </li> <li> <p>Transport layer: Defines the communication mechanisms and channels that enable data exchange between clients and servers, including transport-specific connection establishment, message framing, and authorization.</p> </li> </ul> <p>Conceptually the data layer is the inner layer, while the transport layer is the outer layer.</p> \ud83d\udccc Data layer: <p>The data layer implements a JSON-RPC 2.0 based exchange protocol that defines the message structure and semantics. This layer includes:</p> <ul> <li> <p>Lifecycle management: Handles connection initialization, capability negotiation, and connection termination between clients and servers</p> </li> <li> <p>Server features: Enables servers to provide core functionality including tools for AI actions, resources for context data, and prompts for interaction templates from and to the client</p> </li> <li> <p>Client features: Enables servers to ask the client to sample from the host LLM, elicit input from the user, and log messages to the client</p> </li> <li> <p>Utility features: Supports additional capabilities like notifications for real-time updates and progress tracking for long-running operations</p> </li> </ul> \ud83d\udccc Transport layer: <p>The transport layer manages communication channels and authentication between clients and servers. It handles connection establishment, message framing, and secure communication between MCP participants.</p> <p>MCP supports two transport mechanisms:</p> <ul> <li> <p>Stdio transport: Uses standard input/output streams for direct process communication between local processes on the same machine, providing optimal performance with no network overhead.</p> </li> <li> <p>Streamable HTTP transport: Uses HTTP POST for client-to-server messages with optional Server-Sent Events for streaming capabilities. This transport enables remote server communication and supports standard HTTP authentication methods including bearer tokens, API keys, and custom headers. MCP recommends using OAuth to obtain authentication tokens.</p> </li> </ul> <p>The transport layer abstracts communication details from the protocol layer, enabling the same JSON-RPC 2.0 message format across all transport mechanisms.</p> \ud83d\udccc Primitives: <p>MCP primitives are the most important concept within MCP. They define what clients and servers can offer each other. These primitives specify the types of contextual information that can be shared with AI applications and the range of actions that can be performed.</p> <p>MCP defines three core primitives that servers can expose:</p> <ul> <li> <p>Tools: Executable functions that AI applications can invoke to perform actions (e.g., file operations, API calls, database queries)</p> </li> <li> <p>Resources: Data sources that provide contextual information to AI applications (e.g., file contents, database records, API responses)</p> </li> <li> <p>Prompts: Reusable templates that help structure interactions with language models (e.g., system prompts, few-shot examples)</p> </li> </ul> <p>Each primitive type has associated methods for discovery <code>(*/list)</code>, retrieval <code>(*/get)</code>, and in some cases, execution <code>(tools/call)</code>. MCP clients will use the <code>*/list</code> methods to discover available primitives. For example, a client can first list all available tools <code>(tools/list)</code> and then execute them. This design allows listings to be dynamic.</p> <p>MCP also defines primitives that clients can expose. These primitives allow MCP server authors to build richer interactions.</p> <ul> <li> <p>Sampling: Allows servers to request language model completions from the client\u2019s AI application. This is useful when servers\u2019 authors want access to a language model, but want to stay model independent and not include a language model SDK in their MCP server. They can use the <code>sampling/complete</code> method to request a language model completion from the client\u2019s AI application.</p> </li> <li> <p>Elicitation: Allows servers to request additional information from users. This is useful when servers\u2019 authors want to get more information from the user, or ask for confirmation of an action. They can use the <code>elicitation/request</code> method to request additional information from the user.</p> </li> <li> <p>Logging: Enables servers to send log messages to clients for debugging and monitoring purposes.</p> </li> </ul> \ud83d\udccc Connect to Remote MCP Servers: <p>Remote MCP servers extend AI applications\u2019 capabilities beyond your local environment, providing access to internet-hosted tools, services, and data sources. By connecting to remote MCP servers, you transform AI assistants from helpful tools into informed teammates capable of handling complex, multi-step projects with real-time access to external resources.</p> <p>Remote MCP servers function similarly to local MCP servers but are hosted on the internet rather than your local machine. They expose tools, prompts, and resources that Claude can use to perform tasks on your behalf. These servers can integrate with various services such as project management tools, documentation systems, code repositories, and any other API-enabled service.</p> <p>The key advantage of remote MCP servers is their accessibility. Unlike local servers that require installation and configuration on each device, remote servers are available from any MCP client with an internet connection. This makes them ideal for web-based AI applications, integrations that emphasize ease-of-use and services that require server-side processing or authentication.</p> \ud83d\udccc What are Custom Connectors?: <p>Custom Connectors serve as the bridge between Claude and remote MCP servers. They allow you to connect Claude directly to the tools and data sources that matter most to your workflows, enabling Claude to operate within your favorite software and draw insights from the complete context of your external tools.</p> <p>With Custom Connectors, you can:</p> <ul> <li> <p>Connect Claude to existing remote MCP servers provided by third-party developers</p> </li> <li> <p>Build your own remote MCP servers to connect with any tool</p> </li> </ul> \ud83d\udccc Best Practices for Using Remote MCP Servers: <p>When working with remote MCP servers, consider these recommendations to ensure a secure and efficient experience:</p> <p>Security considerations: Always verify the authenticity of remote MCP servers before connecting. Only connect to servers from trusted sources, and review the permissions requested during authentication. Be cautious about granting access to sensitive data or systems.</p> <p>Managing multiple connectors: You can connect to multiple remote MCP servers simultaneously. Organize your connectors by purpose or project to maintain clarity. Regularly review and remove connectors you no longer use to keep your workspace organized and secure.</p> \ud83d\udccc Build an MCP Server: <p>Core MCP Concepts</p> <p>MCP servers can provide three main types of capabilities:</p> <ol> <li> <p>Resources: File-like data that can be read by clients (like API responses or file contents)</p> </li> <li> <p>Tools: Functions that can be called by the LLM (with user approval)</p> </li> <li> <p>Prompts: Pre-written templates that help users accomplish specific tasks</p> </li> </ol> <p>System requirements</p> <ul> <li> <p>Python 3.10 or higher installed.</p> </li> <li> <p>You must use the Python MCP SDK 1.2.0 or higher.</p> </li> </ul> <pre><code># Create a new directory for our project\n\nuv init weather\ncd weather\n\n# Create virtual environment and activate it\n\nuv venv\nsource .venv/bin/activate\n\n# Install dependencies\n\nuv add \"mcp[cli]\" httpx\n\n# Create  server file\n\ntouch weather.py\n</code></pre> \ud83d\udccc Building your server: <p>Importing packages and setting up the instance</p> <p><code>weather.py</code></p> <pre><code>from typing import Any\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"weather\")\n\n# Constants\nNWS_API_BASE = \"https://api.weather.gov\"\nUSER_AGENT = \"weather-app/1.0\"\n</code></pre> <p>The FastMCP class uses Python type hints and docstrings to automatically generate tool definitions, making it easy to create and maintain MCP tools.</p> <p>Helper functions</p> <p>Next, let\u2019s add our helper functions for querying and formatting the data from the National Weather Service API:</p> <pre><code>async def make_nws_request(url: str) -&gt; dict[str, Any] | None:\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n    headers = {\n        \"User-Agent\": USER_AGENT,\n        \"Accept\": \"application/geo+json\"\n    }\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(url, headers=headers, timeout=30.0)\n            response.raise_for_status()\n            return response.json()\n        except Exception:\n            return None\n\ndef format_alert(feature: dict) -&gt; str:\n    \"\"\"Format an alert feature into a readable string.\"\"\"\n    props = feature[\"properties\"]\n    return f\"\"\"\nEvent: {props.get('event', 'Unknown')}\nArea: {props.get('areaDesc', 'Unknown')}\nSeverity: {props.get('severity', 'Unknown')}\nDescription: {props.get('description', 'No description available')}\nInstructions: {props.get('instruction', 'No specific instructions provided')}\n\"\"\"\n</code></pre> <p>Implementing tool execution</p> <p>The tool execution handler is responsible for actually executing the logic of each tool.</p> <pre><code>@mcp.tool()\nasync def get_alerts(state: str) -&gt; str:\n    \"\"\"Get weather alerts for a US state.\n\n    Args:\n        state: Two-letter US state code (e.g. CA, NY)\n    \"\"\"\n    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n    data = await make_nws_request(url)\n\n    if not data or \"features\" not in data:\n        return \"Unable to fetch alerts or no alerts found.\"\n\n    if not data[\"features\"]:\n        return \"No active alerts for this state.\"\n\n    alerts = [format_alert(feature) for feature in data[\"features\"]]\n    return \"\\n---\\n\".join(alerts)\n\n@mcp.tool()\nasync def get_forecast(latitude: float, longitude: float) -&gt; str:\n    \"\"\"Get weather forecast for a location.\n\n    Args:\n        latitude: Latitude of the location\n        longitude: Longitude of the location\n    \"\"\"\n    # First get the forecast grid endpoint\n    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n    points_data = await make_nws_request(points_url)\n\n    if not points_data:\n        return \"Unable to fetch forecast data for this location.\"\n\n    # Get the forecast URL from the points response\n    forecast_url = points_data[\"properties\"][\"forecast\"]\n    forecast_data = await make_nws_request(forecast_url)\n\n    if not forecast_data:\n        return \"Unable to fetch detailed forecast.\"\n\n    # Format the periods into a readable forecast\n    periods = forecast_data[\"properties\"][\"periods\"]\n    forecasts = []\n    for period in periods[:5]:  # Only show next 5 periods\n        forecast = f\"\"\"\n{period['name']}:\nTemperature: {period['temperature']}\u00b0{period['temperatureUnit']}\nWind: {period['windSpeed']} {period['windDirection']}\nForecast: {period['detailedForecast']}\n\"\"\"\n        forecasts.append(forecast)\n\n    return \"\\n---\\n\".join(forecasts)\n</code></pre> <p>Running the server</p> <p>Finally, let\u2019s initialize and run the server:</p> <pre><code>if __name__ == \"__main__\":\n    # Initialize and run the server\n    mcp.run(transport='stdio')\n</code></pre> <p>Your server is complete! Run <code>uv run weather.py</code> to start the MCP server, which will listen for messages from MCP hosts.</p> <p>Let\u2019s now test your server from an existing MCP host, Claude for Desktop.</p> \ud83d\udccc Testing your server with Claude for Desktop: <p>First, make sure you have Claude for Desktop installed.</p> <p>We\u2019ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> in a text editor. Make sure to create the file if it doesn\u2019t exist.</p> <p>You\u2019ll then add your servers in the <code>mcpServers</code> key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.</p> <pre><code>{\n  \"mcpServers\": {\n    \"weather\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather\",\n        \"run\",\n        \"weather.py\"\n      ]\n    }\n  }\n}\n</code></pre> \ud83d\udccc Building MCP with LLMs: <p>MCP development using LLMs such as Claude or any LLM</p> \ud83d\udccc Build an MCP Client: <p>How to build an LLM-powered chatbot client that connects to MCP servers.</p> <p>Setting Up Your Environment</p> <p>First, create a new Python project with uv</p> <pre><code># Create project directory\nuv init mcp-client\ncd mcp-client\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\n# On Windows:\n.venv\\Scripts\\activate\n# On Unix or macOS:\nsource .venv/bin/activate\n\n# Install required packages\nuv add mcp anthropic python-dotenv\n\n# Remove boilerplate files\n# On Windows:\ndel main.py\n# On Unix or macOS:\nrm main.py\n\n# Create our main file\ntouch client.py\n</code></pre> <p>Setting Up Your API Key</p> <p>You\u2019ll need an Anthropic API key from the Anthropic Console.</p> <p>Create a .env file to store it:</p> <pre><code># Create .env file\ntouch .env\n</code></pre> <p>Add your key to the .env file:</p> <pre><code>ANTHROPIC_API_KEY=&lt;your key here&gt;\n</code></pre> <p>Add .env to your .gitignore:</p> <pre><code>echo \".env\" &gt;&gt; .gitignore\n</code></pre> <p>Creating the Client</p> <p>Basic Client Structure</p> <p>First, let\u2019s set up our imports and create the basic client class:</p> <pre><code>import asyncio\nfrom typing import Optional\nfrom contextlib import AsyncExitStack\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()  # load environment variables from .env\n\nclass MCPClient:\n    def __init__(self):\n        # Initialize session and client objects\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.anthropic = Anthropic()\n    # methods will go here\n</code></pre> <p>Server Connection Management</p> <p>Next, we\u2019ll implement the method to connect to an MCP server:</p> <pre><code>async def connect_to_server(self, server_script_path: str):\n    \"\"\"Connect to an MCP server\n\n    Args:\n        server_script_path: Path to the server script (.py or .js)\n    \"\"\"\n    is_python = server_script_path.endswith('.py')\n    is_js = server_script_path.endswith('.js')\n    if not (is_python or is_js):\n        raise ValueError(\"Server script must be a .py or .js file\")\n\n    command = \"python\" if is_python else \"node\"\n    server_params = StdioServerParameters(\n        command=command,\n        args=[server_script_path],\n        env=None\n    )\n\n    stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n    self.stdio, self.write = stdio_transport\n    self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n\n    await self.session.initialize()\n\n    # List available tools\n    response = await self.session.list_tools()\n    tools = response.tools\n    print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n</code></pre> <p>Query Processing Logic</p> <p>Now let\u2019s add the core functionality for processing queries and handling tool calls:</p> <pre><code>async def process_query(self, query: str) -&gt; str:\n    \"\"\"Process a query using Claude and available tools\"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": query\n        }\n    ]\n\n    response = await self.session.list_tools()\n    available_tools = [{\n        \"name\": tool.name,\n        \"description\": tool.description,\n        \"input_schema\": tool.inputSchema\n    } for tool in response.tools]\n\n    # Initial Claude API call\n    response = self.anthropic.messages.create(\n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1000,\n        messages=messages,\n        tools=available_tools\n    )\n\n    # Process response and handle tool calls\n    final_text = []\n\n    assistant_message_content = []\n    for content in response.content:\n        if content.type == 'text':\n            final_text.append(content.text)\n            assistant_message_content.append(content)\n        elif content.type == 'tool_use':\n            tool_name = content.name\n            tool_args = content.input\n\n            # Execute tool call\n            result = await self.session.call_tool(tool_name, tool_args)\n            final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n\n            assistant_message_content.append(content)\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": assistant_message_content\n            })\n            messages.append({\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"tool_result\",\n                        \"tool_use_id\": content.id,\n                        \"content\": result.content\n                    }\n                ]\n            })\n\n            # Get next response from Claude\n            response = self.anthropic.messages.create(\n                model=\"claude-3-5-sonnet-20241022\",\n                max_tokens=1000,\n                messages=messages,\n                tools=available_tools\n            )\n\n            final_text.append(response.content[0].text)\n\n    return \"\\n\".join(final_text)\n</code></pre> <p>Interactive Chat Interface</p> <p>Now we\u2019ll add the chat loop and cleanup functionality:</p> <pre><code>async def chat_loop(self):\n    \"\"\"Run an interactive chat loop\"\"\"\n    print(\"\\nMCP Client Started!\")\n    print(\"Type your queries or 'quit' to exit.\")\n\n    while True:\n        try:\n            query = input(\"\\nQuery: \").strip()\n\n            if query.lower() == 'quit':\n                break\n\n            response = await self.process_query(query)\n            print(\"\\n\" + response)\n\n        except Exception as e:\n            print(f\"\\nError: {str(e)}\")\n\nasync def cleanup(self):\n    \"\"\"Clean up resources\"\"\"\n    await self.exit_stack.aclose()\n</code></pre> <p>Main Entry Point</p> <p>Finally, we\u2019ll add the main execution logic:</p> <pre><code>async def main():\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python client.py &lt;path_to_server_script&gt;\")\n        sys.exit(1)\n\n    client = MCPClient()\n    try:\n        await client.connect_to_server(sys.argv[1])\n        await client.chat_loop()\n    finally:\n        await client.cleanup()\n\nif __name__ == \"__main__\":\n    import sys\n    asyncio.run(main())\n</code></pre> <p>Running the Client</p> <p>To run your client with any MCP server:</p> <pre><code>uv run client.py path/to/server.py # python server\nuv run client.py path/to/build/index.js # node server\n</code></pre> \ud83d\udccc Inspector: <p>MCP Inspector for testing and debugging Model Context Protocol servers</p> <p>The MCP Inspector is an interactive developer tool for testing and debugging MCP servers.</p> \ud83d\udccc MCP Toolbox for Databases - GCP: <p>MCP Toolbox for Databases is an open source MCP server that helps you build Gen AI tools so that your agents can access data in your database. Google\u2019s Agent Development Kit (ADK) has built in support for The MCP Toolbox for Databases.</p> <p></p>"},{"location":"MachineLearning/Overview/","title":"Overview","text":"\u2705 What is Artificial Intelligence (AI)? <p>Artificial Intelligence (AI) is the field of computer science that focuses on creating machines or software that can simulate human intelligence.</p> \ud83d\udccc Key aspects of AI include: <ul> <li> <p>Mimicking human intelligence: The goal of AI is to enable systems to \"think\" and \"act\" like humans, or at least to perform tasks in a way that suggests intelligence.</p> </li> <li> <p>Learning from data: A core part of modern AI is machine learning, where systems are trained on vast amounts of data to identify patterns and make predictions or decisions without being explicitly programmed for every scenario.</p> </li> <li> <p>Problem-solving: AI systems are designed to solve complex problems by systematically searching through possible actions to reach a desired goal.</p> </li> </ul> \ud83d\udccc AI is an umbrella term that encompasses various subfields, such as: <ul> <li> <p>Machine Learning (ML): The use of algorithms and data to enable systems to learn and improve over time.</p> </li> <li> <p>Deep Learning: A subset of machine learning that uses multi-layered neural networks inspired by the human brain.</p> </li> <li> <p>Natural Language Processing (NLP): The ability of computers to understand, interpret, and generate human language.</p> </li> <li> <p>Computer Vision: Enabling machines to \"see\" and interpret visual information from images and videos.</p> </li> <li> <p>Generative AI: Models that can create new, original content like text, images, or audio.</p> </li> </ul> \u2705 What is Machine learning (ML)? <p>Machine learning (ML) is a subfield of artificial intelligence that empowers computers to learn and improve from experience without being explicitly programmed for every possible scenario. Instead of following a rigid set of rules, ML algorithms are trained on vast amounts of data to identify patterns, make predictions, and generate insights.</p> \ud83d\udccc Core component of ML <p></p> \ud83d\udccc Core concepts of ML <ul> <li> <p>Learning from Data: The fundamental idea behind machine learning is that a system can be given a dataset and, through a process of statistical analysis and pattern recognition, it can learn the underlying relationships within that data. The more data it receives, the better it becomes at its task.</p> </li> <li> <p>The Model: The \"brain\" of a machine learning system is the model. This is the piece of software that has been trained on the data. For example, a model trained to predict house prices would have learned the mathematical relationship between factors like location, square footage, and the final sale price.</p> </li> <li> <p>Trial and Error: Machine learning often involves an iterative process. The model makes a prediction, its accuracy is evaluated, and then the model is adjusted to reduce its errors. This cycle of \"evaluate and optimize\" is how the system continuously refines its performance.</p> </li> </ul> \ud83d\udccc Types of Machine Learning <p>There are three main types of machine learning.</p> <ol> <li> <p>Supervised Learning: This is the most common type. The algorithm is trained on a \"labeled\" dataset, meaning the data includes both the input and the correct output. The model learns to map the input to the output, and once trained, it can predict the output for new, unseen data.</p> </li> <li> <p>Unsupervised Learning: In this case, the algorithm is given \"unlabeled\" data\u2014it has no prior knowledge of the correct outputs. The goal is for the algorithm to discover hidden patterns and structures on its own. </p> </li> <li> <p>Reinforcement Learning: Focuses on training an agent to make optimal decisions through trial and error using rewards.</p> </li> </ol> \ud83d\udccc Underfitting and Overfitting <p>Machine learning models aim to perform well on both training data and new, unseen data and is considered \"good\" if:</p> <ol> <li> <p>It learns patterns effectively from the training data.</p> </li> <li> <p>It generalizes well to new, unseen data.</p> </li> <li> <p>It avoids memorizing the training data (overfitting) or failing to capture relevant patterns (underfitting).</p> </li> </ol> <p>To evaluate how well a model learns and generalizes, we monitor its performance on both the training data and a separate validation or test dataset which is often measured by its accuracy or prediction errors.</p> <p>However, achieving this balance can be challenging.</p> <p>Two common issues that affect a model's performance and generalization ability are overfitting and underfitting.</p> <p>These problems are major contributors to poor performance in machine learning models. Let's us understand what they are and how they contribute to ML models.</p> \ud83d\udccc Bias and Variance in Machine Learning <p>Bias and variance are two key sources of error in machine learning models that directly impact their performance and generalization ability.</p> <p>Bias: is the error that happens when a machine learning model is too simple and doesn't learn enough details from the data. It's like assuming all birds can only be small and fly, so the model fails to recognize big birds like ostriches or penguins that can't fly and get biased with predictions.</p> <ul> <li> <p>These assumptions make the model easier to train but may prevent it from capturing the underlying complexities of the data.</p> </li> <li> <p>High bias typically leads to underfitting, where the model performs poorly on both training and testing data because it fails to learn enough from the data.</p> </li> <li> <p>Example: A linear regression model applied to a dataset with a non-linear relationship.</p> </li> </ul> <p>Variance: Error that happens when a machine learning model learns too much from the data, including random noise.</p> <ul> <li> <p>A high-variance model learns not only the patterns but also the noise in the training data, which leads to poor generalization on unseen data.</p> </li> <li> <p>High variance typically leads to overfitting, where the model performs well on training data but poorly on testing data.</p> </li> </ul> \ud83d\udccc Overfitting and Underfitting: The Core Issues <p>1. Overfitting in Machine Learning</p> <p>Overfitting happens when a model learns too much from the training data, including details that don\u2019t matter (like noise or outliers).</p> <ul> <li> <p>For example, imagine fitting a very complicated curve to a set of points. The curve will go through every point, but it won\u2019t represent the actual pattern.</p> </li> <li> <p>As a result, the model works great on training data but fails when tested on new data.</p> </li> </ul> <p>Overfitting models are like students who memorize answers instead of understanding the topic. They do well in practice tests (training) but struggle in real exams (testing).</p> <p>Reasons for Overfitting:</p> <ul> <li> <p>High variance and low bias.</p> </li> <li> <p>The model is too complex.</p> </li> <li> <p>The size of the training data.</p> </li> </ul> <p>2. Underfitting in Machine Learning</p> <p>Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what\u2019s going on in the data.</p> <ul> <li> <p>For example, imagine drawing a straight line to fit points that actually follow a curve. The line misses most of the pattern.</p> </li> <li> <p>In this case, the model doesn\u2019t work well on either the training or testing data.</p> </li> </ul> <p>Underfitting models are like students who don\u2019t study enough. They don\u2019t do well in practice tests or real exams. Note: The underfitting model has High bias and low variance.</p> <p>Reasons for Underfitting:</p> <ol> <li> <p>The model is too simple, So it may be not capable to represent the complexities in the data.</p> </li> <li> <p>The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.</p> </li> <li> <p>The size of the training dataset used is not enough.</p> </li> <li> <p>Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.</p> </li> <li> <p>Features are not scaled.</p> </li> </ol> <p>Let's visually understand the concept of underfitting, proper fitting, and overfitting.</p> <p></p> <ul> <li> <p>Underfitting : Straight line trying to fit a curved dataset but cannot capture the data's patterns, leading to poor performance on both training and test sets.</p> </li> <li> <p>Overfitting: A squiggly curve passing through all training points, failing to generalize performing well on training data but poorly on test data.</p> </li> <li> <p>Appropriate Fitting: Curve that follows the data trend without overcomplicating to capture the true patterns in the data.</p> </li> </ul> \ud83d\udccc Balance Between Bias and Variance <p>The relationship between bias and variance is often referred to as the bias-variance tradeoff, which highlights the need for balance:</p> <ul> <li> <p>Increasing model complexity reduces bias but increases variance (risk of overfitting).</p> </li> <li> <p>Simplifying the model reduces variance but increases bias (risk of underfitting).</p> </li> </ul> <p>The goal is to find an optimal balance where both bias and variance are minimized, resulting in good generalization performance.</p> <p>Imagine you're trying to predict the price of houses based on their size, and you decide to draw a line or curve that best fits the data points on a graph. How well this line captures the trend in the data depends on the complexity of the model you use.</p> <p></p> <ul> <li> <p>When a model is too simple, like fitting a straight line to curved data, it has high bias and fails to capture the true relationship, leading to underfitting. For example, a linear model cannot represent a non-linear increase in house prices with size.</p> </li> <li> <p>However, if the model becomes too complex, like a fourth-degree polynomial that adjusts to every point, it develops high variance, overfits the training data, and struggles to generalize to new data. This is overfitting, where the model performs well on training but poorly on testing.</p> </li> <li> <p>An ideal model strikes a balance with low bias and low variance, capturing the overall pattern without overreacting to noise. For instance, a smooth second-degree polynomial fits the data well without being overly complex.</p> </li> </ul> \ud83d\udccc How to Address Overfitting and Underfitting? Underfitting \u2013 Techniques to Reduce Overfitting \u2013 Techniques to Reduce \u2705 Increase model complexity (e.g., deeper trees, more layers). \u2705 Improve the quality of training data to focus on meaningful patterns. \u2705 Increase the number of features, perform feature engineering. \u2705 Increase the training data to improve generalization. \u2705 Remove noise from the data. \u2705 Reduce model complexity (e.g., prune trees, reduce layers). \u2705 Increase the number of epochs or training duration. \u2705 Use early stopping \u2013 stop training when validation loss starts increasing. \u2705 Apply Ridge (L2) or Lasso (L1) regularization. \u2705 Use dropout in neural networks."},{"location":"MachineLearning/SupervisedLearning/Classification/","title":"Classification","text":"\u2705 What is Classification in Supervised Learning? <p>Classification is a type of Supervised Learning where the model learns from labeled data to predict discrete categories or classes.</p> <p>Classification teaches a machine to sort things into categories. It learns by looking at examples with labels (like emails marked \"spam\" or \"not spam\"). After learning, it can decide which category new items belong to, like identifying if a new email is spam or not.</p> <p>For example a classification model might be trained on dataset of images labeled as either dogs or cats and it can be used to predict the class of new and unseen images as dogs or cats based on their features such as color, texture and shape.</p> \ud83d\udccc Types of Classification <p>When we talk about classification in machine learning, we\u2019re talking about the process of sorting data into categories based on specific features or characteristics. There are different types of classification problems depending on how many categories (or classes) we are working with and how they are organized. There are two main classification types in machine learning:</p> <ol> <li>Binary Classification</li> </ol> <p>This is the simplest kind of classification. In binary classification, the goal is to sort the data into two distinct categories.</p> <p>Think of it like a simple choice between two options. Imagine a system that sorts emails into either spam or not spam. It works by looking at different features of the email like certain keywords or sender details, and decides whether it\u2019s spam or not. It only chooses between these two options.</p> <ol> <li>Multiclass Classification</li> </ol> <p>Here, instead of just two categories, the data needs to be sorted into more than two categories. The model picks the one that best matches the input. Think of an image recognition system that sorts pictures of animals into categories like cat, dog, and bird.</p> <p>Basically, machine looks at the features in the image (like shape, color, or texture) and chooses which animal the picture is most likely to be based on the training it received.</p> \ud83d\udccc Examples of Machine Learning Classification in Real Life <ul> <li> <p>Email spam filtering</p> </li> <li> <p>Credit risk assessment: Algorithms predict whether a loan applicant is likely to default by analyzing factors such as credit score, income, and loan history. This helps banks make informed lending decisions and minimize financial risk.</p> </li> <li> <p>Medical diagnosis: Machine learning models classify whether a patient has a certain condition (e.g., cancer or diabetes) based on medical data such as test results, symptoms, and patient history. This aids doctors in making quicker, more accurate diagnoses, improving patient care.</p> </li> <li> <p>Image classification: Applied in fields such as facial recognition, autonomous driving, and medical imaging.</p> </li> <li> <p>Sentiment analysis: Determining whether the sentiment of a piece of text is positive, negative, or neutral. Businesses use this to understand customer opinions, helping to improve products and services.</p> </li> <li> <p>Fraud detection: Algorithms detect fraudulent activities by analyzing transaction patterns and identifying anomalies crucial in protecting against credit card fraud and other financial crimes.</p> </li> <li> <p>Recommendation systems: Used to recommend products or content based on past user behavior, such as suggesting movies on Netflix or products on Amazon. This personalization boosts user satisfaction and sales for businesses.</p> </li> </ul> \ud83d\udccc Key characteristics of Classification Models <ol> <li> <p>Class Separation: Classification relies on distinguishing between distinct classes. The goal is to learn a model that can separate or categorize data points into predefined classes based on their features.</p> </li> <li> <p>Decision Boundaries: The model draws decision boundaries in the feature space to differentiate between classes. These boundaries can be linear or non-linear.</p> </li> <li> <p>Sensitivity to Data Quality: Classification models are sensitive to the quality and quantity of the training data. Well-labeled, representative data ensures better performance, while noisy or biased data can lead to poor predictions.</p> </li> <li> <p>Handling Imbalanced Data: Classification problems may face challenges when one class is underrepresented. Special techniques like resampling or weighting are used to handle class imbalances.</p> </li> <li> <p>Interpretability: Some classification algorithms, such as Decision Trees, offer higher interpretability, meaning it's easier to understand why a model made a particular prediction.</p> </li> </ol> \ud83d\udccc Classification Algorithms <p>Now, for implementation of any classification model it is essential to understand Logistic Regression, which is one of the most fundamental and widely used algorithms in machine learning for classification tasks. There are various types of classifiers algorithms. Some of them are : </p> <p>Linear Classifiers: Linear classifier models create a linear decision boundary between classes. They are simple and computationally efficient. Some of the linear classification models are as follows: </p> <ol> <li> <p>Logistic Regression</p> </li> <li> <p>Support Vector Machines having kernel = 'linear'</p> </li> <li> <p>Single-layer Perceptron</p> </li> <li> <p>Stochastic Gradient Descent (SGD) Classifier</p> </li> </ol> <p>Non-linear Classifiers: Non-linear models create a non-linear decision boundary between classes. They can capture more complex relationships between input features and target variable. Some of the non-linear classification models are as follows:</p> <ol> <li> <p>K-Nearest Neighbours</p> </li> <li> <p>Kernel SVM</p> </li> <li> <p>Naive Bayes</p> </li> <li> <p>Decision Tree Classification</p> </li> <li> <p>Random Forests</p> </li> <li> <p>AdaBoost</p> </li> <li> <p>Bagging Classifier</p> </li> <li> <p>Voting Classifier</p> </li> <li> <p>Extra Trees Classifier</p> </li> <li> <p>Multi-layer Artificial Neural Networks</p> </li> </ol> \ud83d\udccc Logistic Regression in Machine Learning <p>Logistic Regression is a supervised machine learning algorithm used for classification problems. Unlike linear regression which predicts continuous values it predicts the probability that an input belongs to a specific class.</p> <p>It is used for binary classification where the output can be one of two possible categories such as Yes/No, True/False or 0/1. It uses sigmoid function to convert inputs into a probability value between 0 and 1. </p> <p></p> <p></p> <p></p> \ud83d\udccc Types of Logistic Regression <p>Logistic regression can be classified into three main types based on the nature of the dependent variable:</p> <ol> <li> <p>Binomial Logistic Regression: This type is used when the dependent variable has only two possible categories. Examples include Yes/No, Pass/Fail or 0/1. It is the most common form of logistic regression and is used for binary classification problems.</p> </li> <li> <p>Multinomial Logistic Regression: This is used when the dependent variable has three or more possible categories that are not ordered. For example, classifying animals into categories like \"cat,\" \"dog\" or \"sheep.\" It extends the binary logistic regression to handle multiple classes.</p> </li> <li> <p>Ordinal Logistic Regression: This type applies when the dependent variable has three or more categories with a natural order or ranking. Examples include ratings like \"low,\" \"medium\" and \"high.\" It takes the order of the categories into account when modeling.</p> </li> </ol> \ud83d\udccc Assumptions of Logistic Regression <p>Understanding the assumptions behind logistic regression is important to ensure the model is applied correctly, main assumptions are:</p> <ol> <li> <p>Independent observations: Each data point is assumed to be independent of the others means there should be no correlation or dependence between the input samples.</p> </li> <li> <p>Binary dependent variables: It takes the assumption that the dependent variable must be binary, means it can take only two values. For more than two categories SoftMax functions are used.</p> </li> <li> <p>Linearity relationship between independent variables and log odds: The model assumes a linear relationship between the independent variables and the log odds of the dependent variable which means the predictors affect the log odds in a linear way.</p> </li> <li> <p>No outliers: The dataset should not contain extreme outliers as they can distort the estimation of the logistic regression coefficients.</p> </li> <li> <p>Large sample size: It requires a sufficiently large sample size to produce reliable and stable results.</p> </li> </ol> \ud83d\udccc Understanding Sigmoid Function <ol> <li> <p>The sigmoid function is a important part of logistic regression which is used to convert the raw output of the model into a probability value between 0 and 1.</p> </li> <li> <p>This function takes any real number and maps it into the range 0 to 1 forming an \"S\" shaped curve called the sigmoid curve or logistic curve. Because probabilities must lie between 0 and 1, the sigmoid function is perfect for this purpose.</p> </li> <li> <p>In logistic regression, we use a threshold value usually 0.5 to decide the class label.</p> <ul> <li> <p>If the sigmoid output is same or above the threshold, the input is classified as Class 1.</p> </li> <li> <p>If it is below the threshold, the input is classified as Class 0.</p> </li> </ul> </li> </ol> \ud83d\udccc How does Logistic Regression work? <p>The logit function is commonly used in Logistic Regression, especially when working with binary classification problems. It maps probabilities (values between 0 and 1) to real numbers (\u2212\u221e to +\u221e). Here's a full explanation and example of how it works:</p> \ud83d\udccc What is the logit function? <p>The logit function is defined as:</p> <p></p> <p>Logistic Function (Sigmoid)</p> <p>The inverse of the logit function is the sigmoid function:</p> <p></p> \ud83d\udccc How to Evaluate Logistic Regression Model? <p></p> \ud83d\udccc Differences Between Linear and Logistic Regression? Feature Linear Regression Logistic Regression Purpose Predict continuous dependent variable Predict categorical dependent variable Problem Type Regression Classification Prediction Output Continuous value (e.g., price, age) Categorical value (e.g., 0 or 1, Yes or No) Curve Best fit line S-curve (Sigmoid function) Estimation Method Least Squares Estimation Maximum Likelihood Estimation Relationship Requirement Requires linear relationship Does not require linear relationship Collinearity Can handle some collinearity Should have little or no collinearity Target Variable Continuous Categorical \ud83d\udccc Key Concepts Aspect Details Objective Predict class labels (e.g., Yes/No, Spam/Not Spam, Disease/No Disease) Input Features (X) Output Categorical label (Y) Type Supervised Learning Examples Email spam detection, Disease prediction, Image recognition \ud83d\udccc Real-time Example: Medical Diagnosis System <p>Imagine a hospital wants to predict whether a patient has Diabetes based on medical measurements.</p> <p>Sample Dataset</p> Glucose Blood Pressure BMI Age Outcome 148 72 33.6 50 1 85 66 26.6 31 0 <ul> <li> <p>Input Features (X): Glucose, Blood Pressure, BMI, Age</p> </li> <li> <p>Target (Y): Outcome \u2192 1 (Has Diabetes), 0 (No Diabetes)</p> </li> </ul> \ud83d\udccc Steps in Classification Project <ol> <li>Data Collection</li> </ol> <p>Medical records, lab test results, etc.</p> <ol> <li> <p>Data Preprocessing</p> <ul> <li> <p>Handle missing values</p> </li> <li> <p>Normalize/scale features</p> </li> <li> <p>Encode categorical variables (if any)</p> </li> </ul> </li> <li> <p>Exploratory Data Analysis (EDA)</p> <ul> <li>Visualize class balance, distributions, correlations.</li> </ul> </li> <li> <p>Feature Selection</p> <ul> <li>Use correlation or feature importance (from models).</li> </ul> </li> <li> <p>Model Building</p> </li> <li> <p>Common models for classification:</p> <ul> <li> <p>Logistic Regression</p> </li> <li> <p>Decision Tree</p> </li> <li> <p>Random Forest</p> </li> <li> <p>Support Vector Machine (SVM)</p> </li> <li> <p>K-Nearest Neighbors (KNN)</p> </li> <li> <p>Naive Bayes</p> </li> <li> <p>Neural Networks</p> </li> </ul> </li> </ol> <pre><code>from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <ol> <li> <p>Model Evaluation</p> </li> <li> <p>Use classification metrics:</p> <ul> <li> <p>Accuracy</p> </li> <li> <p>Precision</p> </li> <li> <p>Recall</p> </li> <li> <p>F1 Score</p> </li> <li> <p>Confusion Matrix</p> </li> <li> <p>ROC AUC</p> </li> </ul> </li> </ol> <pre><code>from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n</code></pre> \ud83d\udccc Evaluation Example Predicted \u2193 / Actual \u2192 Positive Negative Positive TP FP Negative FN TN \ud83d\udccc Real-World Classification Use Cases Domain Use Case Classes Finance Fraud Detection Fraud / Not Fraud HR/Recruiting Resume Screening Suitable / Not Suitable Healthcare Disease Prediction Positive / Negative Retail Customer Churn Churn / Retain Security Intrusion Detection Attack / Normal Email Spam Filter Spam / Not Spam Telecom Call Drop Reason Prediction Technical / Customer-based \ud83d\udccc Tools/Libraries for Classification <ul> <li> <p>Python: <code>scikit-learn, xgboost, lightgbm, catboost, tensorflow, pytorch</code></p> </li> <li> <p>Visualization: <code>matplotlib, seaborn, plotly</code></p> </li> </ul> \u2705 Use Cases \ud83d\udccc Predicting Diabetes - (Classification) 1. Imports Library <pre><code>import pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n</code></pre> 2. Load Data <p>Pima Indians Diabetes Database</p> <pre><code>df = pd.read_csv('diabetes.csv')\ndf.head()\n</code></pre> Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 0 8 183 64 0 0 23.3 0.672 32 1 1 89 66 23 94 28.1 0.167 21 0 0 137 40 35 168 43.1 2.288 33 1 3. Data Preprocessing 1. Handle Missing Data <ul> <li>Check for NaN or null values</li> </ul> <pre><code>print(df.isnull().sum())\n</code></pre> Column Name Missing Values Pregnancies 0 Glucose 0 BloodPressure 0 SkinThickness 0 Insulin 0 BMI 0 DiabetesPedigreeFunction 0 Age 0 Outcome 0 <ul> <li> <p>Drop rows or columns with too many missing values</p> </li> <li> <p>Impute missing values:</p> <ul> <li> <p>Mean/Median (numerical)</p> </li> <li> <p>Mode (categorical)</p> </li> <li> <p>Forward/Backward fill</p> </li> <li> <p>Model-based imputation</p> </li> </ul> </li> <li> <p>Check &amp; Remove Duplicate value</p> <ul> <li>Detect and remove duplicate rows </li> </ul> </li> </ul> <pre><code>print(df.duplicated().sum())\n</code></pre> <ul> <li> <p>Handle Outliers</p> </li> <li> <p>Identify outliers using:</p> <ul> <li> <p>IQR (Interquartile Range)</p> </li> <li> <p>Z-score</p> </li> <li> <p>Boxplot</p> </li> </ul> </li> <li> <p>Remove or cap outliers</p> </li> <li> <p>Data Type Correction</p> </li> <li> <p>Ensure columns have correct data types:</p> <ul> <li> <p>e.g., convert <code>object to int, datetime, or float</code></p> </li> <li> <p>Use <code>pd.to_numeric() or pd.to_datetime()</code></p> </li> </ul> </li> <li> <p>Normalize / Scale Values</p> <ul> <li> <p>Standardize features (<code>StandardScaler, MinMaxScaler</code>)</p> </li> <li> <p>Normalize data if using distance-based models (e.g., KNN, SVM)</p> </li> </ul> </li> <li> <p>Fix Structural Errors</p> <ul> <li> <p>Inconsistent formatting (e.g., <code>yes, Yes, Y</code>)</p> </li> <li> <p>Incorrect spelling or labels</p> </li> <li> <p>Strip whitespaces</p> </li> <li> <p>Format phone numbers, dates, currencies</p> </li> </ul> </li> <li> <p>Handle Categorical Variables</p> </li> <li> <p>Encode using:</p> <ul> <li> <p>One-hot encoding (<code>pd.get_dummies</code>)</p> </li> <li> <p>Label encoding</p> </li> <li> <p>Frequency/target encoding (advanced)</p> </li> </ul> </li> <li> <p>Remove Irrelevant or Redundant Features</p> <ul> <li> <p>Drop ID columns, unnecessary time stamps</p> </li> <li> <p>Use correlation or variance analysis to drop low-impact features</p> </li> </ul> </li> <li> <p>Binning / Discretization</p> <ul> <li>Convert continuous variables to categories (e.g., age groups)</li> </ul> </li> <li> <p>Text Cleaning (for NLP)</p> <ul> <li> <p>Lowercasing, stopword removal, stemming/lemmatization, punctuation removal</p> </li> <li> <p>Text Cleaning (for NLP)**</p> </li> </ul> </li> <li> <p>Feature Engineering</p> <ul> <li>Create new features from existing data (e.g., BMI from weight/height)</li> </ul> </li> <li> <p>Consistency Checks</p> <ul> <li> <p>Ensure dates are in logical order (e.g., start_date &lt; end_date)</p> </li> <li> <p>Check valid ranges (e.g., age &gt; 0)</p> </li> </ul> </li> </ul> \u2705 Data Cleaning Template (Python Code) <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Load your dataset\ndf = pd.read_csv(\"your_dataset.csv\")  # replace with your actual file path\n\n# 1. Check for missing values\nprint(\"Missing values:\\n\", df.isnull().sum())\n\n# 2. Replace zero values in specific columns (common in medical datasets)\ncols_with_zero_as_nan = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndf[cols_with_zero_as_nan] = df[cols_with_zero_as_nan].replace(0, np.nan)\n\n# 3. Impute missing values with median\nimputer = SimpleImputer(strategy='median')\ndf[cols_with_zero_as_nan] = imputer.fit_transform(df[cols_with_zero_as_nan])\n\n# 4. Remove duplicates\ndf = df.drop_duplicates()\n\n# 5. Convert data types (if necessary)\n# Example: df['Age'] = df['Age'].astype(int)\n\n# 6. Detect and remove outliers using IQR\ndef remove_outliers_iqr(df, columns):\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower = Q1 - 1.5 * IQR\n        upper = Q3 + 1.5 * IQR\n        df = df[(df[col] &gt;= lower) &amp; (df[col] &lt;= upper)]\n    return df\n\ndf = remove_outliers_iqr(df, cols_with_zero_as_nan + ['Age'])\n\n# 7. Normalize/Standardize data\nscaler = StandardScaler()\nnumerical_features = df.drop(columns=['Outcome']).columns\ndf[numerical_features] = scaler.fit_transform(df[numerical_features])\n\n# 8. Final check\nprint(\"\\nCleaned Data Preview:\\n\", df.head())\nprint(\"\\nData Types:\\n\", df.dtypes)\nprint(\"\\nShape of cleaned data:\", df.shape)\n</code></pre> \ud83d\udccc Exploratory Data Analysis (EDA) \u2013 Complete Guide with Python <p>\u2705 Typical EDA Activities</p> Step Description 1\ufe0f\u20e3 Understand dataset structure (rows, columns, datatypes) 2\ufe0f\u20e3 Descriptive statistics (mean, median, mode, std) 3\ufe0f\u20e3 Null/missing values analysis 4\ufe0f\u20e3 Value distributions and outliers 5\ufe0f\u20e3 Correlation analysis 6\ufe0f\u20e3 Feature relationships (scatter, box, violin plots) 7\ufe0f\u20e3 Target variable balance check (classification) \ud83d\udccc Exploratory Data Analysis (EDA) \u2013 Complete Guide with Python <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load dataset\ndf = pd.read_csv(\"diabetes.csv\")\n\n# 1. Shape &amp; basic info\nprint(\"Dataset shape:\", df.shape)\nprint(df.info())\n\n# 2. Summary statistics\nprint(df.describe())\n\n# 3. Check class balance\nsns.countplot(data=df, x='Outcome')\nplt.title(\"Class Distribution (0 = No Diabetes, 1 = Diabetes)\")\nplt.show()\n\n# 4. Missing value check\nprint(df.isnull().sum())\n\n# 5. Correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n# 6. Pairplot (optional for small data)\nsns.pairplot(df, hue='Outcome')\nplt.show()\n\n# 7. Distribution of numerical features\ndf.hist(figsize=(12, 10), bins=20)\nplt.suptitle(\"Histograms of Features\")\nplt.show()\n\n# 8. Box plots to detect outliers\nplt.figure(figsize=(12, 8))\nfor i, column in enumerate(df.columns[:-1], 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(data=df, y=column)\n    plt.title(f'Boxplot of {column}')\nplt.tight_layout()\nplt.show()\n\n# 9. Check skewness\nprint(df.skew())\n</code></pre> \ud83d\udccc Key Questions to Answer During EDA: <ul> <li> <p>Are there any missing values or outliers?</p> </li> <li> <p>Are there highly correlated features?</p> </li> <li> <p>Is the target variable (e.g., Outcome) imbalanced?</p> </li> <li> <p>Which features differ significantly between classes?</p> </li> </ul> \ud83d\udccc Heatmap Explanation (Correlation Heatmap) <p>A heatmap is a graphical representation of data using colors to indicate the strength of correlation between variables. In EDA, a correlation heatmap is commonly used to understand relationships between numerical features.</p> \ud83d\udccc What Is Correlation? <p>Correlation measures the linear relationship between two variables. The value ranges from:</p> <ul> <li> <p>+1 \u2192 perfect positive correlation (as one increases, so does the other)</p> </li> <li> <p>0 \u2192 no correlation</p> </li> <li> <p>\u20131 \u2192 perfect negative correlation (as one increases, the other decreases)</p> </li> </ul> \ud83d\udccc How to Read a Correlation Heatmap <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n</code></pre> Element Description <code>df.corr()</code> Calculates pairwise correlation between all numerical columns. <code>annot=True</code> Shows the correlation coefficient numbers in each cell. <code>cmap='coolwarm'</code> Color map; red = high positive correlation, blue = high negative. <code>fmt=\".2f\"</code> Shows values up to 2 decimal places. \ud83d\udccc What to Look for in Heatmaps Goal Example \ud83d\udd0d Identify multicollinearity If two features have correlation &gt; 0.9 or &lt; \u20130.9, one can be removed. \ud83c\udfaf Target association Look for features highly correlated with the target variable (<code>Outcome</code>). \ud83e\uddfc Data cleaning Helps identify redundant variables. \ud83d\udccc Example Interpretation Feature 1 Feature 2 Correlation Interpretation Glucose Outcome 0.47 Moderate positive correlation \u2013 higher glucose relates to diabetes BMI Outcome 0.31 Slight positive correlation Age Pregnancies 0.54 Older individuals tend to have more pregnancies in the dataset \ud83d\udccc Example Output of Correlation with Outcome <p></p> <p>Correlation with Outcome</p> Feature Correlation with Outcome Glucose 0.47 \u2705 BMI 0.31 \u2705 Age 0.23 \u2705 DiabetesPedigreeFunction 0.17 \u2705 Pregnancies 0.22 SkinThickness 0.07 BloodPressure 0.06 Insulin 0.13 Outcome 1.00 (self) <p>Correlation with Age </p> Feature Correlation with Age Pregnancies 0.54 \u2705 Glucose 0.26 \u2705 BloodPressure 0.24 \u2705 Outcome 0.24 \u2705 BMI 0.036 Age 1.00 (self) DiabetesPedigreeFunction 0.034 SkinThickness - 0.11 \u2705 Insulin - 0.042 \ud83d\udccc Key Insights <ul> <li> <p><code>Glucose</code> has the strongest positive correlation with diabetes. Makes sense biologically.</p> </li> <li> <p><code>BMI</code>, <code>Age</code>, and <code>DiabetesPedigreeFunction</code> also have a moderate correlation with the <code>Outcome</code>.</p> </li> <li> <p><code>BloodPressure</code>, <code>SkinThickness</code>, and <code>Insulin</code> have weak correlation, but may still be useful in multivariate models.</p> </li> </ul> \ud83d\udccc Telco Customer Churn <p>Telco Customer Churn</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    roc_auc_score, roc_curve\n)\n\n# Load dataset\ndf = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\n# Drop customerID\ndf.drop('customerID', axis=1, inplace=True)\n\n# Convert TotalCharges to numeric\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n\n# Fill missing values with median\ndf['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n\n# Encode binary variables\nbinary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\nfor col in binary_cols:\n    df[col] = df[col].map({'Yes': 1, 'No': 0})\n\n# One-hot encode categorical features\ndf = pd.get_dummies(df, drop_first=True)\n\n# Boxplot for numerical features to detect outliers\nplt.figure(figsize=(14, 6))\ndf[['tenure', 'MonthlyCharges', 'TotalCharges']].boxplot()\nplt.title(\"Boxplot for Numerical Features\")\nplt.grid(True)\nplt.show()\n\n# Define X and y\nX = df.drop('Churn', axis=1)\ny = df['Churn']\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Logistic Regression with class balancing\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=10000)\nlog_reg.fit(X_train, y_train)\n\n# Predict\ny_pred = log_reg.predict(X_test)\ny_proba = log_reg.predict_proba(X_test)[:, 1]\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nAccuracy Score: {accuracy:.2f}\")\n\n# Classification Report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, digits=2))\n\n# Confusion Matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\", color='darkorange')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Find optimal threshold\noptimal_threshold = thresholds[np.argmax(tpr - fpr)]\nprint(f\"\\nOptimal Threshold: {optimal_threshold:.2f}\")\n\n# Predict with new threshold\ny_pred_new = (y_proba &gt;= optimal_threshold).astype(int)\n\n# Adjusted Classification Report\nprint(\"\\nClassification Report with Adjusted Threshold:\\n\")\nprint(classification_report(y_test, y_pred_new, digits=2))\n\n# Adjusted Confusion Matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred_new), annot=True, fmt='d', cmap='Greens')\nplt.title(\"Confusion Matrix (Adjusted Threshold)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n</code></pre> <p></p> <p></p>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/","title":"Cross Validation","text":"\u2705 Cross Validation in Machine Learning \ud83d\udccc What is Cross Validation in Machine Learning? <p>Cross-validation is a technique used to check how well a machine learning model performs on unseen data. It splits the data into several parts, trains the model on some parts and tests it on the remaining part repeating this process multiple times. Finally the results from each validation step are averaged to produce a more accurate estimate of the model's performance.</p> <p>The main purpose of cross validation is to prevent overfitting. If you want to make sure your machine learning model is not just memorizing the training data but is capable of adapting to real-world data cross-validation is a commonly used technique.</p> <p></p> <p>In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. Let\u2019s load the iris data set to fit a linear support vector machine on it:</p> <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn import svm\n\nX, y = datasets.load_iris(return_X_y=True)\nX.shape, y.shape\n</code></pre> <p>We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=0)\n\nX_train.shape, y_train.shape\nX_test.shape, y_test.shape\n\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n</code></pre>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#what-is-c-in-svm","title":"\ud83d\udd39 What is C in SVM?","text":"<p><code>C</code>is the regularization parameter in SVM.</p> <p>It controls the trade-off between:</p> <ul> <li> <p>Having a wide margin (simpler model, better generalization)</p> </li> <li> <p>Classifying training points correctly (lower training error)</p> </li> </ul>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#intuition-margin-vs-misclassification","title":"\ud83d\udd38 Intuition (Margin vs Misclassification)","text":"<ul> <li> <p>Large C (e.g., C=1000):</p> </li> <li> <p>The model tries very hard to classify all training points correctly.</p> </li> <li> <p>Narrow margin, less tolerance for misclassification.</p> </li> <li> <p>Risk of overfitting (memorizes training data, may fail on unseen data).</p> </li> <li> <p>Small C (e.g., C=0.01):</p> </li> <li> <p>The model allows some misclassifications.</p> </li> <li> <p>Wider margin, focuses more on generalization.</p> </li> <li> <p>Risk of underfitting (too simple, misses patterns).</p> </li> </ul> <p>When evaluating different settings (\u201chyperparameters\u201d) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can \u201cleak\u201d into the model and evaluation metrics no longer report on generalization performance.To solve this problem, yet another part of the dataset can be held out as a so-called \u201cvalidation set\u201d: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</p> <p>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</p> <p>A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k \u201cfolds\u201d:</p> <ul> <li> <p>A model is trained using <code>k -1</code> of the folds as training data;</p> </li> <li> <p>the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).</p> </li> </ul> <p>The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</p>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#computing-cross-validated-metrics","title":"Computing cross-validated metrics","text":"<p>The simplest way to use cross-validation is to call the <code>cross_val_score</code> helper function on the estimator and the dataset.</p> <p>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</p> <pre><code>from sklearn.model_selection import cross_val_score\nclf = svm.SVC(kernel='linear', C=1, random_state=42)\nscores = cross_val_score(clf, X, y, cv=5)\nscores\n</code></pre> <p>The mean score and the standard deviation are hence given by:</p> <pre><code>print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n0.98 accuracy with a standard deviation of 0.02\n</code></pre> <p>By default, the score computed at each CV iteration is the <code>score</code> method of the estimator. It is possible to change this by using the scoring parameter:</p> <pre><code>from sklearn import metrics\nscores = cross_val_score(\n    clf, X, y, cv=5, scoring='f1_macro')\nscores\n</code></pre>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#string-name-scorers","title":"String name scorers","text":""},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#scikit-learn-scoring-reference","title":"Scikit-learn Scoring Reference","text":""},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#classification","title":"Classification","text":"Scoring String Name Function Comment <code>accuracy</code> <code>metrics.accuracy_score</code> <code>balanced_accuracy</code> <code>metrics.balanced_accuracy_score</code> <code>top_k_accuracy</code> <code>metrics.top_k_accuracy_score</code> <code>average_precision</code> <code>metrics.average_precision_score</code> <code>neg_brier_score</code> <code>metrics.brier_score_loss</code> <code>f1</code> <code>metrics.f1_score</code> for binary targets <code>f1_micro</code> <code>metrics.f1_score</code> micro-averaged <code>f1_macro</code> <code>metrics.f1_score</code> macro-averaged <code>f1_weighted</code> <code>metrics.f1_score</code> weighted average <code>f1_samples</code> <code>metrics.f1_score</code> by multilabel sample <code>neg_log_loss</code> <code>metrics.log_loss</code> requires <code>predict_proba</code> support <code>precision</code>, etc. <code>metrics.precision_score</code> suffixes apply as with <code>f1</code> <code>recall</code>, etc. <code>metrics.recall_score</code> suffixes apply as with <code>f1</code> <code>jaccard</code>, etc. <code>metrics.jaccard_score</code> suffixes apply as with <code>f1</code> <code>roc_auc</code> <code>metrics.roc_auc_score</code> <code>roc_auc_ovr</code> <code>metrics.roc_auc_score</code> <code>roc_auc_ovo</code> <code>metrics.roc_auc_score</code> <code>roc_auc_ovr_weighted</code> <code>metrics.roc_auc_score</code> <code>roc_auc_ovo_weighted</code> <code>metrics.roc_auc_score</code> <code>d2_log_loss_score</code> <code>metrics.d2_log_loss_score</code>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#clustering","title":"Clustering","text":"Scoring String Name Function Comment <code>adjusted_mutual_info_score</code> <code>metrics.adjusted_mutual_info_score</code> <code>adjusted_rand_score</code> <code>metrics.adjusted_rand_score</code> <code>completeness_score</code> <code>metrics.completeness_score</code> <code>fowlkes_mallows_score</code> <code>metrics.fowlkes_mallows_score</code> <code>homogeneity_score</code> <code>metrics.homogeneity_score</code> <code>mutual_info_score</code> <code>metrics.mutual_info_score</code> <code>normalized_mutual_info_score</code> <code>metrics.normalized_mutual_info_score</code> <code>rand_score</code> <code>metrics.rand_score</code> <code>v_measure_score</code> <code>metrics.v_measure_score</code>"},{"location":"MachineLearning/SupervisedLearning/CrossValidation/#regression","title":"Regression","text":"Scoring String Name Function Comment <code>explained_variance</code> <code>metrics.explained_variance_score</code> <code>neg_max_error</code> <code>metrics.max_error</code> <code>neg_mean_absolute_error</code> <code>metrics.mean_absolute_error</code> <code>neg_mean_squared_error</code> <code>metrics.mean_squared_error</code> <code>neg_root_mean_squared_error</code> <code>metrics.root_mean_squared_error</code> <code>neg_mean_squared_log_error</code> <code>metrics.mean_squared_log_error</code> <code>neg_root_mean_squared_log_error</code> <code>metrics.root_mean_squared_log_error</code> <code>neg_median_absolute_error</code> <code>metrics.median_absolute_error</code> <code>r2</code> <code>metrics.r2_score</code> <code>neg_mean_poisson_deviance</code> <code>metrics.mean_poisson_deviance</code> <code>neg_mean_gamma_deviance</code> <code>metrics.mean_gamma_deviance</code> <code>neg_mean_absolute_percentage_error</code> <code>metrics.mean_absolute_percentage_error</code> <code>d2_absolute_error_score</code> <code>metrics.d2_absolute_error_score</code> <p>In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.</p> <p>When the cv argument is an integer, cross_val_score uses the <code>KFold</code> or <code>StratifiedKFold</code> strategies by default</p> <p>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</p> <pre><code>from sklearn.model_selection import ShuffleSplit\nn_samples = X.shape[0]\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(clf, X, y, cv=cv)\n</code></pre> \ud83d\udccc Types of Cross-Validation <p>There are several types of cross validation techniques which are as follows:</p> <p>1. Holdout Validation</p> <p>In Holdout Validation we perform training on the 50% of the given dataset and rest 50% is used for the testing purpose. It's a simple and quick way to evaluate a model. The major drawback of this method is that we perform training on the 50% of the dataset, it may possible that the remaining 50% of the data contains some important information which we are leaving while training our model that can lead to higher bias.</p> <p>Process: Split dataset into training and test sets (commonly 70:30 or 80:20).</p> <p>Pros: Simple, fast.</p> <p>Cons: High variance (depends heavily on split).</p> <p>When to use: Very large datasets where k-fold isn\u2019t necessary.</p> <p>2. LOOCV (Leave One Out Cross Validation)</p> <p>In this method we perform training on the whole dataset but leaves only one data-point of the available dataset and then iterates for each data-point. In LOOCV the model is trained on n\u22121 samples and tested on the one omitted sample repeating this process for each data point in the dataset. It has some advantages as well as disadvantages also.</p> <ul> <li> <p>An advantage of using this method is that we make use of all data points and hence it is low bias.</p> </li> <li> <p>The major drawback of this method is that it leads to higher variation in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation.</p> </li> <li> <p>Another drawback is it takes a lot of execution time as it iterates over the number of data points we have.</p> </li> </ul> <p>Process: k = number of samples. Train on all except one sample, test on the one sample. Repeat for each sample.</p> <p>Pros: Almost unbiased performance estimate.</p> <p>Cons: Extremely computationally expensive for large datasets; can have high variance.</p> <p>3. Stratified Cross-Validation</p> <p>It is a technique used in machine learning to ensure that each fold of the cross-validation process maintains the same class distribution as the entire dataset. This is particularly important when dealing with imbalanced datasets where certain classes may be under represented. In this method:</p> <ul> <li> <p>The dataset is divided into k folds while maintaining the proportion of classes in each fold.</p> </li> <li> <p>During each iteration, one-fold is used for testing and the remaining folds are used for training.</p> </li> <li> <p>The process is repeated k times with each fold serving as the test set exactly once.</p> </li> </ul> <p>Stratified Cross-Validation is essential when dealing with classification problems where maintaining the balance of class distribution is crucial for the model to generalize well to unseen data.</p> <p>4. K-Fold Cross Validation</p> <p>In K-Fold Cross Validation we split the dataset into k number of subsets known as folds then we perform training on the all the subsets but leave one (k-1) subset for the evaluation of the trained model. In this method, we iterate k times with a different subset reserved for testing purpose each time.</p> <p>Note: <code>It is always suggested that the value of k should be 10 as the lower value of k takes towards validation and higher value of k leads to LOOCV method.</code></p> <p>Example of K Fold Cross Validation</p> <p>The diagram below shows an example of the training subsets and evaluation subsets generated in k-fold cross-validation. Here we have total 25 instances. In first iteration we use the first 20 percent of data for evaluation and the remaining 80 percent for training like [1-5] testing and [5-25] training while in the second iteration we use the second subset of 20 percent for evaluation and the remaining three subsets of the data for training like [5-10] testing and [1-5 and 10-25] training and so on.</p> <p></p> Iteration Training Set Observations Testing Set Observations 1 [5-24] [0-4] 2 [0-4, 10-24] [5-9] 3 [0-9, 15-24] [10-14] 4 [0-14, 20-24] [15-19] 5 [0-19] [20-24] <p>Each iteration uses different subsets for testing and training, ensuring that all data points are used for both training and testing.</p> <p>5. Time Series Cross-Validation (Rolling/Expanding Window)</p> <p>Process:</p> <ul> <li> <p>Train on first chunk, validate on next chunk.</p> </li> <li> <p>Move the window forward in time.</p> </li> </ul> <p>Pros: Preserves temporal order, avoids leakage from future.</p> <p>Cons: Less training data in early folds.</p> <p>Use case: Forecasting, stock prices, sensor data.</p> \ud83d\udccc Comparison between K-Fold Cross-Validation and Hold Out Method <p>K-Fold Cross-Validation and Hold Out Method are widely used technique and sometimes they are confusing so here is the quick comparison between them:</p> Feature K-Fold Cross-Validation Hold-Out Method Definition The dataset is divided into 'k' subsets (folds). Each fold gets a turn to be the test set while the others are used for training. The dataset is split into two sets: one for training and one for testing. Training Sets \u2705 The model is trained 'k' times, each time on a different training subset. \u26a0\ufe0f The model is trained once on the training set. Testing Sets \u2705 The model is tested 'k' times, each time on a different test subset. \u26a0\ufe0f The model is tested once on the test set. Bias \u2705 Less biased due to multiple splits and testing. \u274c Can have higher bias due to a single split. Variance \u2705 Lower variance, as it tests on multiple splits. \u274c Higher variance, as results depend on the single split. Computation Cost \u274c High, as the model is trained and tested 'k' times. \u2705 Low, as the model is trained and tested only once. Use in Model Selection \u2705 Better for tuning and evaluating model performance due to reduced bias. \u274c Less reliable for model selection, as it might give inconsistent results. Data Utilization \u2705 The entire dataset is used for both training and testing. \u274c Only a portion of the data is used for testing, so some data is not used for validation. Suitability for Small Datasets \u2705 Preferred for small datasets, as it maximizes data usage. \u274c Less ideal for small datasets, as a significant portion is held out for testing. Risk of Overfitting \u2705 Less prone to overfitting due to multiple training and testing cycles. \u274c Higher risk of overfitting as the model is trained on one set. \ud83d\udccc Advantages and Disadvantages of Cross Validation <p>Advantages:</p> <ol> <li> <p>Overcoming Overfitting: Cross validation helps to prevent overfitting by providing a more robust estimate of the model's performance on unseen data.</p> </li> <li> <p>Model Selection: Cross validation is used to compare different models and select the one that performs the best on average.</p> </li> <li> <p>Hyperparameter tuning: This is used to optimize the hyperparameters of a model such as the regularization parameter by selecting the values that result in the best performance on the validation set.</p> </li> <li> <p>Data Efficient: It allow the use of all the available data for both training and validation making it more data-efficient method compared to traditional validation techniques.</p> </li> </ol> <p>Disadvantages:</p> <ol> <li> <p>Computationally Expensive: It can be computationally expensive especially when the number of folds is large or when the model is complex and requires a long time to train.</p> </li> <li> <p>Time-Consuming: It can be time-consuming especially when there are many hyperparameters to tune or when multiple models need to be compared.</p> </li> <li> <p>Bias-Variance Tradeoff: The choice of the number of folds in cross validation can impact the bias-variance tradeoff i.e too few folds may result in high bias while too many folds may result in high variance.</p> </li> </ol> \ud83d\udccc Python implementation for k fold cross-validation <p>Step 1: Importing necessary libraries</p> <p>import from scikit learn.</p> <pre><code>from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\n</code></pre> <p>Step 2: Loading the dataset</p> <p>let's use the iris dataset which is a multi-class classification in-built dataset.</p> <pre><code>iris = load_iris()\nX, y = iris.data, iris.target\n</code></pre> <p>Step 3: Creating SVM classifier</p> <p>SVC is a Support Vector Classification model from scikit-learn.</p> <pre><code>svm_classifier = SVC(kernel='linear')\n</code></pre> <p>Step 4: Defining the number of folds for cross-validation</p> <p>Here we will be using 5 folds.</p> <pre><code>num_folds = 5\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n</code></pre> <p>Step 5: Performing k-fold cross-validation</p> <pre><code>cross_val_results = cross_val_score(svm_classifier, X, y, cv=kf)\n</code></pre> <p>Step 6: Evaluation metrics</p> <pre><code>print(\"Cross-Validation Results (Accuracy):\")\nfor i, result in enumerate(cross_val_results, 1):\n    print(f\"  Fold {i}: {result * 100:.2f}%\")\n\nprint(f'Mean Accuracy: {cross_val_results.mean()* 100:.2f}%')\n</code></pre> <p></p> <p>The output shows the accuracy scores from each of the 5 folds in the K-fold cross-validation process. The mean accuracy is the average of these individual scores which is approximately 97.33% indicating the model's overall performance across all the folds.</p>"},{"location":"MachineLearning/SupervisedLearning/HyperparameterTuning/","title":"Hyperparameter Tuning","text":"\u2705 Hyperparameter Tuning <p>Tuning the hyper-parameters of an estimator</p> <p>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include <code>C</code>, <code>kernel</code> and <code>gamma</code> for Support Vector Classifier, <code>alpha</code> for Lasso, etc.</p> <ul> <li>It is possible and recommended to search the hyper-parameter space for the best <code>cross validation</code> score.</li> </ul> <p>Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:</p> <pre><code>estimator.get_params()\n</code></pre> <p>What is an Estimator in Machine Learning?</p> <p>In scikit-learn (and generally in ML), an estimator is any object that can learn from data.</p> <ul> <li> <p>It must implement at least the methods:</p> </li> <li> <p><code>.fit(X, y)</code> \u2192 learns from the data (training).</p> </li> <li> <p><code>.predict(X)</code> \u2192 makes predictions on new data.</p> </li> </ul> <p>\ud83d\udc49 In simple words:</p> <p>An estimator = algorithm/model or transformer in sklearn.</p> <p>A search consists of:</p> <ul> <li> <p>an estimator (regressor or classifier such as <code>sklearn.svm.SVC()</code>);</p> </li> <li> <p>a parameter space;</p> </li> <li> <p>a method for searching or sampling candidates;</p> </li> <li> <p>a cross-validation scheme;</p> </li> <li> <p>a score function.</p> </li> </ul> <p>Two generic approaches to parameter search are provided in scikit-learn: for given values,</p> <ol> <li> <p><code>GridSearchCV</code> exhaustively considers all parameter combinations.</p> </li> <li> <p><code>RandomizedSearchCV</code> can sample a given number of candidates from a parameter space with a specified distribution.</p> </li> <li> <p>Both these tools have successive halving counterparts <code>HalvingGridSearchCV</code> and <code>HalvingRandomSearchCV</code>, which can be much faster at finding a good parameter combination.</p> </li> </ol> \ud83d\udccc What is Hyperparameter Tuning? <p>Hyperparameter tuning is the process of selecting the optimal values for a machine learning model's hyperparameters. These are typically set before the actual training process begins and control aspects of the learning process itself. They influence the model's performance its complexity and how fast it learns.</p> <p><code>For example the learning rate and number of neurons in a neural network in a neural network or the kernel size in a support vector machine can significantly impact how well the model trains and generalizes. The goal of hyperparameter tuning is to find the values that lead to the best performance on a given task.</code></p> <p>These settings can affect both the speed and quality of the model's performance.</p> <ul> <li> <p>A high learning rate can cause the model to converge too quickly possibly skipping over the optimal solution.</p> </li> <li> <p>A low learning rate might lead to slower convergence and require more time and computational resources.</p> </li> </ul> <p>Different models have different hyperparameters and they need to be tuned accordingly.</p> \ud83d\udccc Techniques for Hyperparameter Tuning <p>Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem. The two best strategies for Hyperparameter tuning are:</p> \ud83d\udccc 1. GridSearchCV  <p></p> <p>GridSearchCV is a brute-force technique for hyperparameter tuning. It trains the model using all possible combinations of specified hyperparameter values to find the best-performing setup. It is slow and uses a lot of computer power which makes it hard to use with big datasets or many settings. It works using below steps:</p> <ul> <li> <p>Create a grid of potential values for each hyperparameter.</p> </li> <li> <p>Train the model for every combination in the grid.</p> </li> <li> <p>Evaluate each model using cross-validation.</p> </li> <li> <p>Select the combination that gives the highest score.</p> </li> </ul> <p>For example if we want to tune two hyperparameters C and Alpha for a Logistic Regression Classifier model with the following sets of values:</p> <p>C = [0.1, 0.2, 0.3, 0.4, 0.5] Alpha = [0.01, 0.1, 0.5, 1.0]</p> <p></p> <p>The grid search technique will construct multiple versions of the model with all possible combinations of C and Alpha, resulting in a total of 5 * 4 = 20 different models. The best-performing combination is then chosen.</p> <p>Example: Tuning Logistic Regression with GridSearchCV</p> <p>The following code illustrates how to use GridSearchCV . In this below code:</p> <ul> <li>We generate sample data using make_classification.</li> <li>We define a range of C values using logarithmic scale.</li> <li>GridSearchCV tries all combinations from param_grid and uses 5-fold cross-validation.</li> <li>It returns the best hyperparameter (C) and its corresponding validation score</li> </ul> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\nlogreg = LogisticRegression()\n\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\nlogreg_cv.fit(X, y)\n\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n</code></pre> <p>Output:</p> <p><code>Tuned Logistic Regression Parameters: {'C': 0.006105402296585327} Best score is 0.853</code></p> <p>This represents the highest accuracy achieved by the model using the hyperparameter combination C = 0.0061.</p> <p>The best score of 0.853 means the model achieved 85.3% accuracy on the validation data during the grid search process.</p> <p>GridSearchCV is a method in scikit-learn that helps you find the best combination of hyperparameters for your model.</p> <ol> <li> <p>Takes a model and a set of hyperparameters to try (the \"grid\").</p> </li> <li> <p>Trains the model with all possible combinations of those hyperparameters.</p> </li> <li> <p>Uses cross-validation to evaluate each combination.</p> </li> <li> <p>Selects the combination that gives the best performance.</p> </li> </ol> <p>Why Use It?</p> <ul> <li> <p>Choosing hyperparameters manually can be guesswork.</p> </li> <li> <p>GridSearchCV ensures systematic, exhaustive search across given parameter ranges.</p> </li> <li> <p>Helps in avoiding underfitting or overfitting due to poor hyperparameter selection.</p> </li> </ul> \ud83d\udccc 3. Real-Time Example: Predicting Loan Default  <p>Imagine you\u2019re working in a bank\u2019s credit risk department.</p> <p>You want to predict whether a customer will default on their loan using a Random Forest Classifier.</p> <p>The challenge:</p> <ul> <li> <p>The model has multiple hyperparameters (<code>n_estimators, max_depth, min_samples_split</code>).</p> </li> <li> <p>You don\u2019t know the best combination.</p> </li> </ul> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# 1. Create synthetic dataset (Example: Loan Default)\nX, y = make_classification(n_samples=1000, n_features=10, \n                           n_informative=5, n_classes=2, random_state=42)\n\n# 2. Split into train &amp; test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 3. Define model\nrf = RandomForestClassifier(random_state=42)\n\n# 4. Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10]\n}\n\n# 5. Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=param_grid,\n                           cv=5,                # 5-fold cross-validation\n                           scoring='accuracy', # Metric to evaluate\n                           n_jobs=-1)          # Use all CPU cores\n\n# 6. Fit model\ngrid_search.fit(X_train, y_train)\n\n# 7. Print best parameters &amp; score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Score:\", grid_search.best_score_)\n\n# 8. Evaluate on test data\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n</code></pre> <p>How it works in this example</p> <p>Parameter Grid:</p> <pre><code>n_estimators \u2192 [50, 100, 200]\nmax_depth \u2192 [None, 5, 10]\nmin_samples_split \u2192 [2, 5, 10]\n</code></pre> <p>\u2192 Total combinations = 3 \u00d7 3 \u00d7 3 = 27 models.</p> <p>Cross-validation:</p> <p>For each combination, GridSearchCV:</p> <ul> <li> <p>Splits training data into 5 folds.</p> </li> <li> <p>Trains on 4 folds, validates on 1.</p> </li> <li> <p>Repeats 5 times \u2192 averages accuracy.</p> </li> </ul> <p>Selection:</p> <p>Picks the combination with the highest average validation accuracy.</p> \ud83d\udccc RandomizedSearchCV   <p>As the name suggests RandomizedSearchCV picks random combinations of hyperparameters from the given ranges instead of checking every single combination like GridSearchCV.</p> <p>In each iteration it tries a new random combination of hyperparameter values. It records the model\u2019s performance for each combination. After several attempts it selects the best-performing set.</p> <p>Example: Tuning Decision Tree with RandomizedSearchCV</p> <p>The following code illustrates how to use RandomizedSearchCV.In this example:</p> <ul> <li> <p>We define a range of values for each hyperparameter e.g, max_depth, min_samples_leaf etc.</p> </li> <li> <p>Random combinations are picked and evaluated using 5-fold cross-validation.</p> </li> <li> <p>The best combination and score are printed.</p> </li> </ul> <pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": randint(1, 9),\n    \"min_samples_leaf\": randint(1, 9),\n    \"criterion\": [\"gini\", \"entropy\"]\n}\n\ntree = DecisionTreeClassifier()\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\ntree_cv.fit(X, y)\n\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n</code></pre> <p>Output:</p> <p><code>Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 6, 'min_samples_leaf': 6} Best score is 0.8</code></p> <p>A score of 0.842 means the model performed with an accuracy of 84.2% on the validation set with following hyperparameters.</p> \ud83d\udccc Advantages of Hyperparameter tuning   <ul> <li> <p>Improved Model Performance: Finding the optimal combination of hyperparameters can significantly boost model accuracy and robustness.</p> </li> <li> <p>Reduced Overfitting and Underfitting: Tuning helps to prevent both overfitting and underfitting resulting in a well-balanced model.</p> </li> <li> <p>Enhanced Model Generalizability: By selecting hyperparameters that optimize performance on validation data the model is more likely to generalize well to unseen data.</p> </li> <li> <p>Optimized Resource Utilization: With careful tuning resources such as computation time and memory can be used more efficiently avoiding unnecessary work.</p> </li> <li> <p>Improved Model Interpretability: Properly tuned hyperparameters can make the model simpler and easier to interpret.</p> </li> </ul> \ud83d\udccc Challenges in Hyperparameter Tuning   <ul> <li> <p>Dealing with High-Dimensional Hyperparameter Spaces: The larger the hyperparameter space the more combinations need to be explored. This makes the search process computationally expensive and time-consuming especially for complex models with many hyperparameters.</p> </li> <li> <p>Handling Expensive Function Evaluations: Evaluating a model's performance can be computationally expensive, particularly for models that require a lot of data or iterations.</p> </li> <li> <p>Incorporating Domain Knowledge: It can help guide the hyperparameter search, narrowing down the search space and making the process more efficient. Using insights from the problem context can improve both the efficiency and effectiveness of tuning.</p> </li> <li> <p>Developing Adaptive Hyperparameter Tuning Methods: Dynamic adjustment of hyperparameters during training such as learning rate schedules or early stopping can lead to better model performance.</p> </li> </ul>"},{"location":"MachineLearning/SupervisedLearning/Overview/","title":"Overview","text":"\u2705 What is Supervised Machine Learning? <p>supervised learning\u00a0is a type of machine learning where a model is trained on labeled data\u2014meaning each input is paired with the correct output. the model learns by comparing its predictions with the actual answers provided in the training data. Over time, it adjusts itself to minimize errors and improve accuracy. The goal of supervised learning is to make accurate predictions when given new, unseen data.</p> <p>Supervised learning can be applied in various forms, including\u00a0supervised learning classification and supervised learning regression</p> \ud83d\udccc How Supervised Machine Learning Works? <p>Where\u00a0supervised learning algorithm\u00a0consists of input features and corresponding output labels.</p> <p>The process works through the following stages:</p> <ul> <li> <p>Training Data: The model is provided with a training dataset. This dataset is the foundation of the learning process and consists of:</p> <ul> <li> <p>Input Features: The variables or attributes used to make a prediction.</p> </li> <li> <p>Output Labels: The correct answers or target variables corresponding to the input features.</p> </li> </ul> </li> <li> <p>Learning Process: The supervised learning algorithm processes the training data to learn the relationship between the input features and the output labels. This is achieved by:</p> <ul> <li> <p>Initialization: The model's parameters are randomly initialized.</p> </li> <li> <p>Prediction: The model makes a prediction for a given input.</p> </li> <li> <p>Error Calculation: The model's prediction is compared to the actual label. The difference between these two is the error.</p> </li> <li> <p>Parameter Adjustment: The model's parameters are adjusted to minimize this error. This process is repeated iteratively over the entire training dataset until the model's performance on the training data is optimized.</p> </li> </ul> </li> </ul> <p>Validation Data: This is a crucial step to prevent overfitting. The validation set is a portion of the original data that is set  aside and not used during the initial training process. It is used to tune the model's hyperparameters and assess its  performance during the training phase. By evaluating the model on the validation data, we can see if it is learning  generalized patterns or simply memorizing the training data.</p> <p>Test Data: After the model has been trained and its hyperparameters have been tuned using the validation set,  it is evaluated one last time using a separate, unseen test dataset. The test dataset is the ultimate measure of the model\u2019s  accuracy and performance. Because the model has never seen this data before, the results on the test set  provide an unbiased estimate of how the model will perform on new, real-world data.</p> <p></p> <p>Training\u00a0phase involves feeding the algorithm labeled data, where each data point is paired with its correct output.  The algorithm learns to identify patterns and relationships between the input and output data.</p> <p>Testing\u00a0phase involves feeding the algorithm new, unseen data and evaluating its ability to predict the correct output  based on the learned patterns.</p> \ud83d\udccc Types of Supervised Learning in Machine Learning? <ol> <li> <p>Classification: Where the output is a categorical variable (e.g., spam vs. non-spam emails, yes vs. no).</p> </li> <li> <p>Regression: Where the output is a continuous variable (e.g., predicting house prices, stock prices).</p> </li> </ol> <p></p> <ul> <li>While training the model, data is usually split in the ratio of 80:20 i.e. 80% as training data and  the rest 20% as testing data. In training data, we feed input as well as output for 80% of data.  The model learns from training data only.</li> </ul> <p>Understand the classification and regression data</p> <p></p> <p>Both the above figures have labelled data set as follows:</p> <p>Figure A:\u00a0It is a dataset of a shopping store that is useful in predicting whether a customer will purchase a particular product under consideration or not based on his/ her gender, age, and salary.</p> <ul> <li> <p>Input:\u00a0Gender, Age, Salary</p> </li> <li> <p>Output:\u00a0Purchased i.e. 0 or 1; 1 means yes the customer will purchase and 0 means that the customer won't purchase it.</p> </li> </ul> <p>Figure B:\u00a0It is a Meteorological dataset that serves the purpose of predicting wind speed based on different parameters.</p> <ul> <li> <p>Input:\u00a0Dew Point, Temperature, Pressure, Relative Humidity, Wind Direction</p> </li> <li> <p>Output:\u00a0Wind Speed</p> </li> </ul> \ud83d\udccc Supervised Machine Learning Algorithms <ol> <li> <p>Linear Regression: Linear regression is a type of supervised learning regression algorithm that is used to predict a continuous output value. It is one of the simplest and most widely used algorithms in supervised learning.</p> </li> <li> <p>Logistic Regression: Logistic regression is a type of supervised learning classification algorithm that is used to predict a binary output variable.</p> </li> <li> <p>Decision Trees: Decision tree is a tree-like structure that is used to model decisions and their possible consequences. Each internal node in the tree represents a decision, while each leaf node represents a possible outcome.</p> </li> <li> <p>Random Forests: Random forests again are made up of multiple decision trees that work together to make predictions. Each tree in the forest is trained on a different subset of the input features and data. The final prediction is made by aggregating the predictions of all the trees in the forest.</p> </li> <li> <p>Support Vector Machine(SVM): The SVM algorithm creates a hyperplane to segregate n-dimensional space into classes and identify the correct category of new data points. The extreme cases that help create the hyperplane are called support vectors, hence the name Support Vector Machine.</p> </li> <li> <p>K-Nearest Neighbors (KNN): KNN works by finding k training examples closest to a given input and then predicts the class or value based on the majority class or average value of these neighbors. The performance of KNN can be influenced by the choice of k and the distance metric used to measure proximity.</p> </li> <li> <p>Gradient Boosting: Gradient Boosting combines weak learners, like decision trees, to create a strong model. It iteratively builds new models that correct errors made by previous ones.</p> </li> <li> <p>Naive Bayes Algorithm: The Naive Bayes algorithm is a supervised machine learning algorithm based on applying Bayes' Theorem with the \u201cnaive\u201d assumption that features are independent of each other given the class label.</p> </li> </ol> \ud83d\udccc Training a Supervised Learning Model Key Steps: <ol> <li> <p>Data Collection and Preprocessing(cleaning, feature engineering): Gather a labeled dataset consisting of input features and target output labels. Clean the data, handle missing values, and scale features as needed to ensure high quality for supervised learning algorithms.</p> </li> <li> <p>Exploratory Data Analysis(EDA): EDA (Exploratory Data Analysis) is the first step in the data analysis process. It involves visualizing and summarizing the main characteristics of a dataset to understand its structure, spot anomalies, test hypotheses, and check assumptions.</p> </li> <li> <p>Splitting the Data: Divide the data into training set (80%) and the test set (20%).</p> </li> <li> <p>Choosing the Model: Select appropriate algorithms based on the problem type. This step is crucial for effective supervised learning in AI.</p> </li> <li> <p>Training the Model: Feed the model input data and output labels, allowing it to learn patterns by adjusting internal parameters.</p> </li> <li> <p>Evaluating the Model: Test the trained model on the unseen test set and assess its performance using various metrics.</p> </li> <li> <p>Hyperparameter Tuning: Adjust settings that control the training process (e.g., learning rate) using techniques like grid search and cross-validation.</p> </li> <li> <p>Final Testing: Retrain the model on the complete dataset using the best hyperparameters testing its performance on the test set to ensure readiness for deployment.</p> </li> <li> <p>Model Deployment: Deploy the validated model to make predictions on new, unseen data.</p> </li> </ol> <p></p> \ud83d\udccc Advantages of Supervised Learning: <p>The power of supervised learning lies in its ability to accurately predict patterns and make data-driven decisions across a variety of applications. Here are some advantages of supervised learning listed below:</p> <ul> <li> <p>Supervised learning excels in accurately predicting patterns and making data-driven decisions.</p> </li> <li> <p>Labeled training data is crucial for enabling supervised learning models to learn input-output relationships effectively.</p> </li> <li> <p>Supervised machine learning encompasses tasks such as supervised learning classification and supervised learning regression.</p> </li> <li> <p>Applications include complex problems like image recognition and natural language processing.</p> </li> <li> <p>Established evaluation metrics (accuracy, precision, recall, F1-score) are essential for assessing supervised learning model performance.</p> </li> <li> <p>Advantages of supervised learning include creating complex models for accurate predictions on new data.</p> </li> </ul> \ud83d\udccc Disadvantages of Supervised Learning: <p>Despite the benefits of supervised learning methods, there are notable disadvantages of supervised learning:</p> <ol> <li> <p>Overfitting: Models can overfit training data, leading to poor performance on new data due to capturing noise in supervised machine learning.</p> </li> <li> <p>Feature Engineering: Extracting relevant features is crucial but can be time-consuming and requires domain expertise in supervised learning applications.</p> </li> <li> <p>Bias in Models: Bias in the training data may result in unfair predictions in supervised learning algorithms.</p> </li> <li> <p>Dependence on Labeled Data: Supervised learning relies heavily on labeled training data, which can be costly and time-consuming to obtain, posing a challenge for supervised learning techniques.</p> </li> </ol> \ud83d\udccc Conclusion: <p>Supervised learning is a powerful branch of machine learning that revolves around learning a class from examples provided during training. By using supervised learning algorithms, models can be trained to make predictions based on labeled data. The effectiveness of supervised machine learning lies in its ability to generalize from the training data to new, unseen data, making it invaluable for a variety of applications, from image recognition to financial forecasting.</p> <p>Understanding the types of supervised learning algorithms and the dimensions of supervised machine learning is essential for choosing the appropriate algorithm to solve specific problems. As we continue to explore the different types of supervised learning and refine these supervised learning techniques, the impact of supervised learning in machine learning will only grow, playing a critical role in advancing AI-driven solutions.</p> <p>Supervised machine learning algorithms in table:</p> Algorithm Regression / Classification Purpose Method Use Cases Linear Regression Regression Predict continuous output values Linear equation minimizing sum of squares of residuals Predicting continuous values (e.g., house prices, sales forecasting) Logistic Regression Classification Predict binary output variable Logistic function transforming linear relationship Binary classification tasks (e.g., spam detection, disease prediction) Decision Trees Both Model decisions and outcomes Tree-like structure with decisions and outcomes Classification and regression tasks Random Forests Both Improve classification and regression accuracy Combining multiple decision trees Reducing overfitting, improving prediction accuracy SVM Both Create hyperplane for classification or predict continuous values Maximizing margin between classes or predicting continuous values Classification and regression tasks (e.g., image recognition, stock prediction) KNN Both Predict class or value based on k closest neighbors Finding k closest neighbors and predicting based on majority or average Classification and regression tasks; sensitive to noisy data Gradient Boosting Both Combine weak learners to create strong model Iteratively correcting errors with new models Classification and regression tasks to improve prediction accuracy Naive Bayes Classification Predict class based on feature independence assumption Bayes' theorem with feature independence assumption Text classification, spam filtering, sentiment analysis, medical diagnosis"},{"location":"MachineLearning/SupervisedLearning/Regression/","title":"Regression","text":"\u2705 Regression <p>Regression in machine learning refers to a supervised learning technique where the goal is to predict a continuous numerical value based on one or more independent features. It finds relationships between variables so that predictions can be made. we have two types of variables present in regression:</p> <ul> <li> <p>Dependent Variable (Target): The variable we are trying to predict e.g house price.</p> </li> <li> <p>Independent Variables (Features): The input variables that influence the prediction e.g locality, number of rooms.</p> </li> </ul> <p>Regression analysis problem works with if output variable is a real or continuous value such as \u201csalary\u201d or \u201cweight\u201d. Many different regression models can be used but the simplest model in them is linear regression.</p> \ud83d\udccc Types of Regression <p>Regression can be classified into different types based on the number of predictor variables and the nature of the relationship between variables:</p> 1. Simple Linear Regression <p>Linear regression is one of the simplest and most widely used statistical models. This assumes that there is a linear relationship between the independent and dependent variables. This means that the change in the dependent variable is proportional to the change in the independent variables. For example predicting the price of a house based on its size.</p> 2. Multiple Linear Regression <p>Multiple linear regression extends simple linear regression by using multiple independent variables to predict target variable. For example predicting the price of a house based on multiple features such as size, location, number of rooms, etc.</p> 3. Polynomial Regression <p>Polynomial regression is used to model with non-linear relationships between the dependent variable and the independent variables. It adds polynomial terms to the linear regression model to capture more complex relationships. For example when we want to predict a non-linear trend like population growth over time we use polynomial regression.</p> 4. Ridge &amp; Lasso Regression <p>Ridge &amp; lasso regression are regularized versions of linear regression that help avoid overfitting by penalizing large coefficients. When there\u2019s a risk of overfitting due to too many features we use these type of regression algorithms.</p> 5. Support Vector Regression (SVR) <p>SVR is a type of regression algorithm that is based on the Support Vector Machine (SVM) algorithm. SVM is a type of algorithm that is used for classification tasks but it can also be used for regression tasks. SVR works by finding a hyperplane that minimizes the sum of the squared residuals between the predicted and actual values.</p> 6. Decision Tree Regression <p>Decision tree Uses a tree-like structure to make decisions where each branch of tree represents a decision and leaves represent outcomes. For example predicting customer behavior based on features like age, income, etc there we use decison tree regression.</p> 7. Random Forest Regression <p>Random Forest is a ensemble method that builds multiple decision trees and each tree is trained on a different subset of the training data. The final prediction is made by averaging the predictions of all of the trees. For example customer churn or sales data using this.</p> \ud83d\udccc Regression Evaluation Metrics <p>Evaluation in machine learning measures the performance of a model. Here are some popular evaluation metrics for regression:</p> <ul> <li> <p>Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values of the target variable.</p> </li> <li> <p>Mean Squared Error (MSE): The average squared difference between the predicted and actual values of the target variable.</p> </li> <li> <p>Root Mean Squared Error (RMSE): Square root of the mean squared error.</p> </li> <li> <p>R2 \u2013 Score: Higher values indicate better fit ranging from 0 to 1.</p> </li> </ul> \ud83d\udccc What is Mean Absolute Error (MAE)? <p>Mean Absolute Error calculates the average difference between the calculated values and actual values. It is also known as scale-dependent accuracy as it calculates error in observations taken on the same scale used to predict the accuracy of the machine learning model.</p> <p>MAE Formula:</p> <p></p> <p></p> \u2705 Real-Life Example <p>Suppose we are predicting house prices.</p> <p>You have the actual prices and predicted prices for 5 houses:</p> House Actual Price (in Lakhs) Predicted Price (in Lakhs) A 50 48 B 60 65 C 55 53 D 70 75 E 65 60 \ud83e\uddee Step-by-Step Calculation <p>Calculate the absolute error for each:</p> <p>|50 - 48| = 2 |60 - 65| = 5 |55 - 53| = 2 |70 - 75| = 5 |65 - 60| = 5  </p> <p>Sum of absolute errors:</p> <p>2 + 5 + 2 + 5 + 5 = 19</p> <p>Number of predictions = 5</p> <p>Calculate MAE:</p> <p>MAE= 19/5 = 3.8</p> <p>Interpretation:</p> <p>The MAE = 3.8 Lakhs</p> <p>On average, your model\u2019s predictions are off by 3.8 Lakhs from the actual prices.</p> \ud83d\udccc Python Code Example: <pre><code>from sklearn.metrics import mean_absolute_error\n\nactual = [50, 60, 55, 70, 65]\npredicted = [48, 65, 53, 75, 60]\n\nmae = mean_absolute_error(actual, predicted)\nprint(\"MAE:\", mae)\n</code></pre> \ud83d\udccc What is Mean Squared Error (MSE)? <p>Mean Squared Error (MSE) is a commonly used regression error metric that measures the average squared difference between the actual values and the predicted values.</p> <p></p> \u2705 Real-Life Example House Actual Price (Lakhs) Predicted Price (Lakhs) A 50 48 B 60 65 C 55 53 D 70 75 E 65 60 \ud83e\uddee Step-by-Step Calculation <p>Calculate squared errors:</p> <p>(50 - 48)^2 = 4 (60 - 65)^2 = 25 (55 - 53)^2 = 4 (70 - 75)^2 = 25 (65 - 60)^2 = 25  </p> <p>Sum of squared errors:</p> <p>4 + 25 + 4 + 25 + 25 = 83</p> <p>Number of samples = 5</p> <p>Calculate MSE:</p> <p>MSE = 83/5 = 16.6</p> <p>Interpretation:</p> <ul> <li> <p>MSE = 16.6</p> </li> <li> <p>On average, the squared difference between predicted and actual values is 16.6.</p> </li> <li> <p>Higher errors are penalized more because the error is squared.</p> </li> </ul> <p>Python Code</p> <pre><code>from sklearn.metrics import mean_squared_error\n\n# Actual and Predicted values\nactual = [50, 60, 55, 70, 65]\npredicted = [48, 65, 53, 75, 60]\n\n# Calculate MSE using scikit-learn\nmse = mean_squared_error(actual, predicted)\nprint(\"Mean Squared Error (MSE):\", mse)\n</code></pre> \ud83d\udccc What is Root Mean Squared Error (RMSE)? <p>Root Mean Squared Error (RMSE) is a standard regression metric that measures the square root of the average squared differences between predicted and actual values.</p> <p>It is the square root of Mean Squared Error (MSE) and brings the error back to the same unit as the target variable, making it easier to interpret.</p> <p></p> \u2705 Real-Life Example House Actual Price (Lakhs) Predicted Price (Lakhs) A 50 48 B 60 65 C 55 53 D 70 75 E 65 60 \ud83e\uddee Step-by-Step Calculation <p>(50 - 48)^2 = 4 (60 - 65)^2 = 25 (55 - 53)^2 = 4 (70 - 75)^2 = 25 (65 - 60)^2 = 25  </p> <p></p> <p>Interpretation</p> <ul> <li> <p>RMSE = 4.08 Lakhs</p> </li> <li> <p>On average, your model's predictions are off by about 4.08 Lakhs.</p> </li> <li> <p>Since RMSE penalizes larger errors more heavily (due to squaring), it's useful when large errors are especially undesirable.</p> </li> </ul> <p>Python Code</p> <pre><code>from sklearn.metrics import mean_squared_error\nimport numpy as np\n\nactual = [50, 60, 55, 70, 65]\npredicted = [48, 65, 53, 75, 60]\n\nmse = mean_squared_error(actual, predicted)\nrmse = np.sqrt(mse)\n\nprint(\"RMSE:\", rmse)\n</code></pre> <p>Manual MSE Calculation (without library)</p> <pre><code># Manual MSE calculation\nerrors = [(a - p) ** 2 for a, p in zip(actual, predicted)]\nmse_manual = sum(errors) / len(errors)\nprint(\"Manual MSE:\", mse_manual)\n</code></pre> \ud83d\udccc What is R2 \u2013 Score? <p>R\u00b2 Score (also called the coefficient of determination) is a metric that shows how well your regression model fits the data.</p> <p>It tells you the proportion of the variance in the dependent variable that is predictable from the independent variables.</p> <p></p> <p>Interpretation of R\u00b2:</p> R\u00b2 Value Meaning 1.0 Perfect fit (model explains 100% of the variance) 0.9 Very good fit 0.5 Moderate fit 0 Model does no better than the mean &lt; 0 Model is worse than just predicting the mean \u2705 Real-Life Example <pre><code>from sklearn.metrics import r2_score\n\nactual = [50, 60, 55, 70, 65]\npredicted = [48, 65, 53, 75, 60]\n\nr2 = r2_score(actual, predicted)\nprint(\"R\u00b2 Score:\", r2)\n</code></pre> \ud83d\udccc Summary: Metric What it Tells MAE Average error in same units as target MSE Penalizes large errors more RMSE Similar to MAE, but penalizes big errors R\u00b2 How well the model explains variability \ud83d\udccc Comparison of Common Error Metrics: Metric Description Sensitive to Outliers? Units? Use When... MAE (Mean Absolute Error) Average of absolute errors \u274c No Same as target You want a simple, robust metric. Errors should be equally weighted. MSE (Mean Squared Error) Average of squared errors \u2705 Yes Squared units You want to penalize large errors more heavily. Often used in optimization. RMSE (Root Mean Squared Error) Square root of MSE \u2705 Yes Same as target Similar to MSE, but easier to interpret (in original units). R\u00b2 Score % of variance explained by the model \u2796 Ratio (0 to 1 or &lt; 0) You want to know how well your model explains the outcome. \ud83d\udccc Quick Guide: <p>\u2705 Use MAE when:</p> <ul> <li> <p>Interpretability is important</p> </li> <li> <p>You want to treat all errors equally</p> </li> <li> <p>Data has outliers and you don\u2019t want to punish them too much</p> </li> </ul> <p>\u2705 Use MSE or RMSE when:</p> <ul> <li> <p>Large errors matter a lot (e.g., in finance, healthcare)</p> </li> <li> <p>You're training a model and want a smooth, differentiable loss function (MSE is widely used in optimization)</p> </li> </ul> <p>\u2705 Use R\u00b2 Score when:</p> <ul> <li> <p>You want to evaluate how well the model explains the data</p> </li> <li> <p>It\u2019s okay to have a relative performance measure (e.g., comparing models)</p> </li> </ul> \ud83d\udccc Life Expectancy (WHO): <p>life-expectancy-who</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Load dataset\ndf = pd.read_csv(\"Life Expectancy Data.csv\")\n\n# Map 'Status' to numeric\ndf['Status'] = df['Status'].map({'Developing': 0, 'Developed': 1})\n\n# Drop rows where target is missing\ndf = df[df['Life expectancy '].notnull()]\n\n# Box plot for Life Expectancy\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=df['Life expectancy '])\nplt.title('Box Plot of Life Expectancy')\nplt.xlabel('Life Expectancy')\nplt.show()\n\n# Box plots for all numeric columns\nnumeric_cols = df.select_dtypes(include='number').columns\nplt.figure(figsize=(14, 8))\ndf[numeric_cols].boxplot(rot=90)\nplt.title('Box Plots for Numeric Features')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Outlier detection for target column\nQ1 = df['Life expectancy '].quantile(0.25)\nQ3 = df['Life expectancy '].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\ndf = df[(df['Life expectancy '] &gt;= lower_bound) &amp; (df['Life expectancy '] &lt;= upper_bound)]\n\n# Impute missing values using median\ndf.fillna(df.median(numeric_only=True), inplace=True)\n\n# Drop non-feature columns\nX = df.drop(columns=['Country', 'Life expectancy '])\ny = df['Life expectancy ']\n\n# Optional: Check outlier stats\ndef find_outliers_iqr(data, feature):\n    Q1 = data[feature].quantile(0.25)\n    Q3 = data[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = data[(data[feature] &lt; lower_bound) | (data[feature] &gt; upper_bound)]\n    return outliers\n\noutlier_stats = {}\nfor col in X.columns:\n    outliers = find_outliers_iqr(X, col)\n    count = len(outliers)\n    percent = (count / len(X)) * 100\n    outlier_stats[col] = {'count': count, 'percent': percent}\nsorted_outlier_stats = dict(sorted(outlier_stats.items(), key=lambda x: x[1]['count'], reverse=True))\n\nprint(\"Top 10 features with most outliers:\")\nfor col, stats in list(sorted_outlier_stats.items())[:10]:\n    print(f\"{col}: {stats['count']} outliers ({stats['percent']:.2f}%)\")\n\n# Cap outliers using IQR\ndef cap_outliers_iqr(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    data[column] = data[column].clip(lower, upper)\n    return data\n\nfor col in X.columns:\n    X = cap_outliers_iqr(X, col)\n\n# Feature Engineering: Add GDP per capita\nX['GDP_per_capita'] = df['GDP'] / df['Population']\n\n# Feature Scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Linear Regression Model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Evaluation Metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mse ** 0.5\nr2 = r2_score(y_test, y_pred)\n\nprint(\"\\n\u2705 Model Evaluation Metrics:\")\nprint(f\"Mean Squared Error (MSE): {mse:.2f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.2f}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\nprint(f\"R\u00b2 Score: {r2:.2f}\")\n\n# Scatter plot: Actual vs Predicted\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.xlabel(\"Actual Life Expectancy\")\nplt.ylabel(\"Predicted Life Expectancy\")\nplt.title(\"Actual vs Predicted\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Correlation heatmap\ncorr_matrix = df.corr(numeric_only=True)\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix[['Life expectancy ']].sort_values(by='Life expectancy ', ascending=False), annot=True, cmap='coolwarm')\nplt.title(\"Feature Correlation with Life Expectancy\")\nplt.show()\n\n# \u2705 Plot feature importances\ncoefficients = model.coef_\nfeatures = X.columns\nimportance_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})\nimportance_df['Absolute Coefficient'] = importance_df['Coefficient'].abs()\nimportance_df = importance_df.sort_values(by='Absolute Coefficient', ascending=True)\n\n# Plot with value annotations\nplt.figure(figsize=(15, 8))\nbars = plt.barh(importance_df['Feature'], importance_df['Coefficient'], color='skyblue')\nplt.xlabel('Coefficient Value')\nplt.title('Feature Importances via Linear Regression Coefficients')\nplt.grid(True)\n\n# Annotate each bar with the coefficient value\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width + 0.1 if width &gt;= 0 else width - 0.1,  # offset based on sign\n             bar.get_y() + bar.get_height()/2,\n             f'{width:.2f}',\n             va='center',\n             ha='left' if width &gt;= 0 else 'right')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\u2705 Model Evaluation Metrics:</p> <p>Mean Squared Error (MSE): 13.99</p> <p>Mean Absolute Error (MAE): 2.79</p> <p>Root Mean Squared Error (RMSE): 3.74</p> <p>R\u00b2 Score: 0.85</p> <p></p> \ud83d\udccc Coefficient (\u03b2) in Linear Regression: <ul> <li> <p>Definition: It is the weight assigned to each feature by the regression model.</p> </li> <li> <p>Meaning: Shows the magnitude and direction of impact that a feature (independent variable) has on the target (dependent variable), assuming all other variables are held constant.</p> </li> <li> <p>Units: It is in the units of the target variable per unit of the feature.</p> </li> <li> <p>Interpretation:</p> <ul> <li> <p>Positive \u2192 as feature increases, the target tends to increase.</p> </li> <li> <p>Negative \u2192 as feature increases, the target tends to decrease.</p> </li> </ul> </li> <li> <p>Use: Used after training a model.</p> </li> </ul> <p>\ud83d\udccc Example:</p> <pre><code>If \u03b2(Income) = 0.3, it means a 1 unit increase in income increases the prediction by 0.3 units, assuming other features are constant.\n</code></pre> <p></p> \ud83d\udccc Correlation: <ul> <li> <p>Definition: A statistical measure that describes the linear relationship between two variables.</p> </li> <li> <p>Range: Always between -1 and +1:</p> <ul> <li> <p>+1 \u2192 perfect positive linear relationship</p> </li> <li> <p>-1 \u2192 perfect negative linear relationship</p> </li> <li> <p>0 \u2192 no linear relationship</p> </li> </ul> </li> <li> <p>Symmetric: corr(X, Y) = corr(Y, X)</p> </li> <li> <p>Use: Used during EDA to understand variable relationships.</p> </li> </ul> <p>\ud83d\udccc Example:</p> <pre><code>If corr(Income, LifeExpectancy) = 0.85, it means Income and LifeExpectancy are strongly positively related.\n</code></pre> <p></p> \ud83d\udccc Summary of What Was Done: <p>\u2705 Data Cleaning</p> <ul> <li> <p>Missing values filled using median \u2013 robust to outliers.</p> </li> <li> <p>'Status' encoded from categorical to numeric.</p> </li> <li> <p>Dropped 'Country' since it's a string and not helpful in raw form.</p> </li> </ul> <p>\u2705 Feature Engineering</p> <ul> <li> <p>Encoded categorical columns.</p> </li> <li> <p>Removed irrelevant or hard-to-encode non-numeric data.</p> </li> </ul> <p>\u2705 Best Predictors</p> <ul> <li> <p>Use the top 5 features from coefficients.head(5). Likely candidates based on previous work are:</p> </li> <li> <p>Adult Mortality</p> </li> <li> <p>HIV/AIDS</p> </li> <li> <p>Schooling</p> </li> <li> <p>BMI</p> </li> <li> <p>Income composition of resources</p> </li> </ul> <p>\u2705 Model Evaluation</p> <ul> <li> <p>R\u00b2 Score: 0.85 \u2192 85% of the variance in life expectancy is explained.</p> </li> <li> <p>MAE: 2.79 \u2192 On average, predictions are off by ~2.79 years.</p> </li> <li> <p>RMSE: 3.74 \u2192 Standard deviation of prediction error.</p> </li> </ul> <p>\u2705 Conclusion</p> <ul> <li> <p>The model is reasonably accurate for a first iteration.</p> </li> <li> <p>R\u00b2 = 0.85 indicates good performance.</p> </li> <li> <p>Further improvement possible with feature selection, outlier handling, or non-linear models like Random Forest or Gradient Boosting.</p> </li> </ul>"},{"location":"MachineLearning/SupervisedLearning/TuningDecisionThreshold/","title":"Tuning decision threshold","text":"\u2705 Tuning the decision threshold for class prediction <p>Classification is best divided into two parts:</p> <ul> <li> <p>the statistical problem of learning a model to predict, ideally, class probabilities;</p> </li> <li> <p>the decision problem to take concrete action based on those probability predictions.</p> </li> </ul> <p>Let\u2019s take a straightforward example related to weather forecasting: the first point is related to answering \u201cwhat is the chance that it will rain tomorrow?\u201d while the second point is related to answering \u201cshould I take an umbrella tomorrow?\u201d.</p> <p>When it comes to the scikit-learn API, the first point is addressed by providing scores using <code>predict_proba</code> or <code>decision_function</code>. </p> <p>The former returns conditional probability estimates P(y/X) for each class, while the latter returns a decision score for each class.</p> <ul> <li> <p>The decision corresponding to the labels is obtained with <code>predict</code>.</p> </li> <li> <p>In binary classification, a decision rule or action is then defined by thresholding the scores, leading to the prediction of a single class label for each sample.</p> </li> </ul> <p>For binary classification in scikit-learn, class labels predictions are obtained by hard-coded cut-off rules: a positive class is predicted when the conditional probability P(y/X) is greater than 0.5 (obtained with predict_proba) or if the decision score is greater than 0 (obtained with decision_function).</p> <p>Here, we show an example that illustrates the relatonship between conditional probability estimatesP(y/X) and class labels:</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nX, y = make_classification(random_state=0)\nclassifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\nclassifier.predict_proba(X[:4])\nclassifier.predict(X[:4])\n</code></pre> <p>While these hard-coded rules might at first seem reasonable as default behavior, they are most certainly not ideal for most use cases. Let\u2019s illustrate with an example.</p> <p>Consider a scenario where a predictive model is being deployed to assist physicians in detecting tumors. In this setting, physicians will most likely be interested in identifying all patients with cancer and not missing anyone with cancer so that they can provide them with the right treatment. In other words, physicians prioritize achieving a high recall rate. This emphasis on recall comes, of course, with the trade-off of potentially more false-positive predictions, reducing the precision of the model. That is a risk physicians are willing to take because the cost of a missed cancer is much higher than the cost of further diagnostic tests. Consequently, when it comes to deciding whether to classify a patient as having cancer or not, it may be more beneficial to classify them as positive for cancer when the conditional probability estimate is much lower than 0.5.</p>"},{"location":"MachineLearning/SupervisedLearning/TuningDecisionThreshold/#post-tuning-the-decision-threshold","title":"Post-tuning the decision threshold","text":""},{"location":"MachineLearning/SupervisedLearning/RegressionModels/DecisionTreeRegression/","title":"Decision Tree Regression","text":"\u2705 Decision Tree Regression \ud83d\udccc What is Decision Tree Regression? <p>A Decision Tree helps us to make decisions by mapping out different choices and their possible outcomes.It\u2019s used in machine learning for tasks like classification and prediction.</p> <p></p> <p></p> <p></p> <p></p> <p>A Decision Tree helps us make decisions by showing different options and how they are related. It has a tree-like structure that starts with one main question called the root node which represents the entire dataset. From there, the tree branches out into different possibilities based on features in the data.</p> <ul> <li> <p>Root Node: Starting point representing the whole dataset.</p> </li> <li> <p>Branches: Lines connecting nodes showing the flow from one decision to another.</p> </li> <li> <p>Internal Nodes: Points where decisions are made based on data features.</p> </li> <li> <p>Leaf Nodes: End points of the tree where the final decision or prediction is made.</p> </li> </ul> <p></p> <p>A Decision Tree also helps with decision-making by showing possible outcomes clearly. By looking at the \"branches\" we can quickly compare options and figure out the best choice.</p> <p>There are mainly two types of Decision Trees based on the target variable:</p> <ol> <li> <p>Classification Trees: Used for predicting categorical outcomes like spam or not spam. These trees split the data based on features to classify data into predefined categories.</p> </li> <li> <p>Regression Trees: Used for predicting continuous outcomes like predicting house prices. Instead of assigning categories, it provides numerical predictions based on the input features.</p> </li> </ol> \ud83d\udccc How Decision Trees Work? <ol> <li>1. Start with the Root Node: It begins with a main question at the root node which is derived from the dataset\u2019s features.</li> </ol> <p>2. Ask Yes/No Questions: From the root, the tree asks a series of yes/no questions to split the data into subsets based on specific attributes.</p> <p>3. Branching Based on Answers: Each question leads to different branches:</p> <ul> <li> <p>If the answer is yes, the tree follows one path.</p> </li> <li> <p>If the answer is no, the tree follows another path.</p> </li> </ul> <p>4. Continue Splitting: This branching continues through further decisions helps in reducing the data down step-by-step.</p> <p>5. Reach the Leaf Node: The process ends when there are no more useful questions to ask leading to the leaf node where the final decision or prediction is made.</p> <p>Let\u2019s look at a simple example to understand how it works. Imagine we need to decide whether to drink coffee based on the time of day and how tired we feel. The tree first checks the time:</p> <p>1. In the morning: It asks \u201cTired?\u201d</p> <ul> <li> <p>If yes, the tree suggests drinking coffee.</p> </li> <li> <p>If no, it says no coffee is needed.</p> </li> <li> <p>In the afternoon: It asks again \u201cTired?\u201d</p> </li> <li> <p>If yes, it suggests drinking coffee.</p> </li> <li> <p>If no, no coffee is needed.</p> </li> </ul> <p></p> \ud83d\udccc Splitting Criteria in Decision Trees <p>In a Decision Tree, the process of splitting data at each node is important. The splitting criteria finds the best feature to split the data on. Common splitting criteria include Gini Impurity and Entropy.</p> <ul> <li> <p>Gini Impurity: This criterion measures how \"impure\" a node is. The lower the Gini Impurity the better the feature splits the data into distinct categories.</p> </li> <li> <p>Entropy: This measures the amount of uncertainty or disorder in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.</p> </li> </ul> <p>These criteria help decide which features are useful for making the best split at each decision point in the tree.</p> \ud83d\udccc Gini Impurity and Entropy in Decision Tree <p>Gini Index</p> <ul> <li> <p>The Gini Index is the additional approach to dividing a decision tree.</p> </li> <li> <p>Purity and impurity in a junction are the primary focus of the Entropy and Information Gain framework.</p> </li> <li> <p>The Gini Index, also known as Impurity, calculates the likelihood that somehow a randomly picked instance would be erroneously cataloged. </p> </li> </ul> <p>1. Gini Impurity</p> <p></p> <p></p> <p>2. Entropy (Information Gain)</p> <p></p> <p></p> <p>3. Comparison</p> <p></p> \ud83d\udccc Example Dataset: Weather &amp; Play Tennis ID Outlook PlayTennis 1 Sunny No 2 Sunny No 3 Overcast Yes 4 Rain Yes 5 Rain Yes 6 Rain No 7 Overcast Yes 8 Sunny No 9 Sunny Yes 10 Rain Yes 11 Sunny Yes 12 Overcast Yes 13 Overcast Yes 14 Rain No <p>Step 1 \u2013 Parent Node Calculation (Before Split)</p> <p></p> <p>Step 2 \u2013 Split on \"Outlook\"</p> <p></p> <p>Overcast Group</p> <p></p> <p>Rain Group</p> <p></p> <p>Step 3 \u2013 Weighted Average After Split</p> <p></p> <p>Step 4 \u2013 Information Gain (for Entropy)</p> <p></p> <p>Step 4 \u2013 Information Gain (for Entropy)</p> <p></p> <p>\u2705 Summary Table</p> Group Yes No Gini Entropy Parent 9 5 0.46 0.94 Sunny 2 3 0.48 0.971 Overcast 4 0 0.00 0.000 Rain 3 2 0.48 0.971 Weighted Avg \u2014 \u2014 0.343 0.692 <p></p> <p></p> <p>Entropy v/s Gini Impurity: Now we have learned about Gini Impurity and Entropy and how it actually works. Also, we have seen how we can calculate Gini Impurity/Entropy for a split/feature. But the major question that arises here is why do we need to have both methods for computation and which is better. </p> <p></p> <p>The internal workings of both methods are similar, as they are used for computing the impurity of features after each split. However, Gini Impurity is generally more computationally efficient than entropy. The graph of entropy increases up to 1 and then starts decreasing, while Gini Impurity only goes up to 0.5 before decreasing, thus requiring less computational power. The range of entropy is from 0 to (log2C), whereas the range of Gini Impurity is from 0 to 0.5 (for binary classification).</p> <pre><code>Note: The range of Gini Impurity [0,0.5] is in case of a binary classification problem. In case of multi-class classification, you can calculate range using this method:\n</code></pre> <p></p> <p>However, the main reason for Gini Impurity's computational advantage is that it does not involve logarithmic functions, which are more computationally intensive. Therefore, Gini Impurity is often considered more efficient compared to entropy for selecting the best features.</p> <p>Pruning in Decision Trees</p> <ul> <li> <p>Pruning is an important technique used to prevent overfitting in Decision Trees. Overfitting occurs when a tree becomes too deep and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.</p> </li> <li> <p>This technique reduces the complexity of the tree by removing branches that have little predictive power. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.</p> </li> <li> <p>It is useful when a Decision Tree is too deep and starts to capture noise in the data.</p> </li> </ul> <p>Advantages of Decision Trees</p> <ul> <li> <p>Easy to Understand: Decision Trees are visual which makes it easy to follow the decision-making process.</p> </li> <li> <p>Versatility: Can be used for both classification and regression problems.</p> </li> <li> <p>No Need for Feature Scaling: Unlike many machine learning models, it don\u2019t require us to scale or normalize our data.</p> </li> <li> <p>Handles Non-linear Relationships: It capture complex, non-linear relationships between features and outcomes effectively.</p> </li> <li> <p>Interpretability: The tree structure is easy to interpret helps in allowing users to understand the reasoning behind each decision.</p> </li> <li> <p>Handles Missing Data: It can handle missing values by using strategies like assigning the most common value or ignoring missing data during splits.</p> </li> </ul> <p>Disadvantages of Decision Trees</p> <ul> <li> <p>Overfitting: They can overfit the training data if they are too deep which means they memorize the data instead of learning general patterns. This leads to poor performance on unseen data.</p> </li> <li> <p>Instability: It can be unstable which means that small changes in the data may lead to significant differences in the tree structure and predictions.</p> </li> <li> <p>Bias towards Features with Many Categories: It can become biased toward features with many distinct values which focuses too much on them and potentially missing other important features which can reduce prediction accuracy.</p> </li> <li> <p>Difficulty in Capturing Complex Interactions: Decision Trees may struggle to capture complex interactions between features which helps in making them less effective for certain types of data.</p> </li> <li> <p>Computationally Expensive for Large Datasets: For large datasets, building and pruning a Decision Tree can be computationally intensive, especially as the tree depth increases.</p> </li> </ul> <p>Applications of Decision Trees</p> <p>Decision Trees are used across various fields due to their simplicity, interpretability and versatility lets see some key applications:</p> <ol> <li> <p>Loan Approval in Banking: Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.</p> </li> <li> <p>Medical Diagnosis: In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.</p> </li> <li> <p>Predicting Exam Results in Education: Educational institutions use to predict whether a student will pass or fail based on factors like attendance, study time and past grades. This helps teachers identify at-risk students and offer targeted support.</p> </li> <li> <p>Customer Churn Prediction: Companies use Decision Trees to predict whether a customer will leave or stay based on behavior patterns, purchase history, and interactions. This allows businesses to take proactive steps to retain customers.</p> </li> <li> <p>Fraud Detection: In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud. By analyzing past transaction data and patterns, Decision Trees can identify suspicious activities and flag them for further investigation.</p> </li> </ol> <p>Example:</p> <pre><code># Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import fetch_california_housing\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"=== Decision Tree Regression Implementation ===\\n\")\n\n# 1. LOAD AND EXPLORE DATASET\nprint(\"1. Loading California Housing Dataset...\")\n# Load the California housing dataset - perfect for regression\ncalifornia_housing = fetch_california_housing()\nX = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\ny = pd.Series(california_housing.target, name='MedHouseValue')\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Target variable range: ${y.min():.2f} - ${y.max():.2f} (in hundreds of thousands)\")\n\n# Manually define feature descriptions to avoid parsing issues\nfeature_info = {\n    'MedInc': 'Median income in block group (in tens of thousands of USD)',\n    'HouseAge': 'Median house age in years',\n    'AveRooms': 'Average number of rooms per household',\n    'AveBedrms': 'Average number of bedrooms per household',\n    'Population': 'Population of the block group',\n    'AveOccup': 'Average number of household members',\n    'Latitude': 'Block group latitude',\n    'Longitude': 'Block group longitude'\n}\n\nprint(\"\\nFeature descriptions:\")\nfor feature, description in feature_info.items():\n    print(f\"- {feature}: {description}\")\n\n\n# Display basic statistics\nprint(\"\\nDataset Info:\")\nprint(X.describe().round(2))\nprint(f\"\\nTarget variable statistics:\")\nprint(y.describe().round(2))\n\n# 2. DATA PREPROCESSING\nprint(\"\\n2. Data Preprocessing...\")\n\n# Check for missing values\nprint(f\"Missing values in features: {X.isnull().sum().sum()}\")\nprint(f\"Missing values in target: {y.isnull().sum()}\")\n\n# Feature correlation analysis\nprint(\"\\nFeature correlation with target:\")\ncorrelations = X.corrwith(y).sort_values(ascending=False)\nprint(correlations.round(3))\n\n# 3. TRAIN-TEST SPLIT\nprint(\"\\n3. Splitting data into train and test sets...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\n\nprint(f\"Training set size: {X_train.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")\n\n# 4. MODEL TRAINING WITH HYPERPARAMETER TUNING\nprint(\"\\n4. Training Decision Tree Regressor with Hyperparameter Tuning...\")\n\n# Define hyperparameter grid for optimization\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, 15, None],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 5, 10],\n    'max_features': ['sqrt', 'log2', None] # 'auto' was removed in recent sklearn versions\n}\n\n# Initialize the regressor\ndt_regressor = DecisionTreeRegressor(random_state=42)\n\n# Perform grid search with cross-validation\nprint(\"Performing Grid Search with 5-fold Cross Validation...\")\ngrid_search = GridSearchCV(\n    estimator=dt_regressor,\n    param_grid=param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    verbose=0\n)\n\n# Fit the grid search\ngrid_search.fit(X_train, y_train)\n\n# Get the best model\nbest_dt = grid_search.best_estimator_\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score (negative MSE): {grid_search.best_score_:.4f}\")\n\n# Train a simple model for comparison\nsimple_dt = DecisionTreeRegressor(max_depth=5, random_state=42)\nsimple_dt.fit(X_train, y_train)\n\n# 5. MODEL EVALUATION\nprint(\"\\n5. Model Evaluation...\")\n\n# Make predictions\ny_train_pred_best = best_dt.predict(X_train)\ny_test_pred_best = best_dt.predict(X_test)\ny_train_pred_simple = simple_dt.predict(X_train)\ny_test_pred_simple = simple_dt.predict(X_test)\n\n# Calculate metrics for both models\ndef calculate_metrics(y_true, y_pred, model_name, dataset_type):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"\\n{model_name} - {dataset_type} Set Metrics:\")\n    print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"  R\u00b2 Score: {r2:.4f}\")\n\n    return mse, rmse, mae, r2\n\n# Evaluate both models\ncalculate_metrics(y_train, y_train_pred_best, \"Best Tuned Model\", \"Training\")\ncalculate_metrics(y_test, y_test_pred_best, \"Best Tuned Model\", \"Test\")\ncalculate_metrics(y_train, y_train_pred_simple, \"Simple Model\", \"Training\")\ncalculate_metrics(y_test, y_test_pred_simple, \"Simple Model\", \"Test\")\n\n# Cross-validation scores\ncv_scores_best = cross_val_score(best_dt, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_scores_simple = cross_val_score(simple_dt, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n\nprint(f\"\\nCross-validation RMSE (Best Model): {np.sqrt(-cv_scores_best.mean()):.4f} (+/- {np.sqrt(cv_scores_best.std() * 2):.4f})\")\nprint(f\"Cross-validation RMSE (Simple Model): {np.sqrt(-cv_scores_simple.mean()):.4f} (+/- {np.sqrt(cv_scores_simple.std() * 2):.4f})\")\n\n# 6. FEATURE IMPORTANCE ANALYSIS\nprint(\"\\n6. Feature Importance Analysis...\")\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance_best': best_dt.feature_importances_,\n    'importance_simple': simple_dt.feature_importances_\n}).sort_values('importance_best', ascending=False)\n\nprint(\"Top 5 Most Important Features (Best Model):\")\nprint(feature_importance.head())\n\n# 7. VISUALIZATIONS\nprint(\"\\n7. Generating Visualizations...\")\n\n# Set up the plotting style\nplt.style.use('default')\nfig = plt.figure(figsize=(20, 15))\n\n# Plot 1: Actual vs Predicted (Best Model)\nplt.subplot(3, 3, 1)\nplt.scatter(y_test, y_test_pred_best, alpha=0.6, color='blue', s=30)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs Predicted Values (Best Model)')\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Residuals Plot (Best Model)\nplt.subplot(3, 3, 2)\nresiduals = y_test - y_test_pred_best\nplt.scatter(y_test_pred_best, residuals, alpha=0.6, color='green', s=30)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot (Best Model)')\nplt.grid(True, alpha=0.3)\n\n# Plot 3: Feature Importance\nplt.subplot(3, 3, 3)\nplt.barh(feature_importance['feature'], feature_importance['importance_best'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Feature Importance (Best Model)')\nplt.gca().invert_yaxis()\n\n# Plot 4: Prediction Distribution\nplt.subplot(3, 3, 4)\nplt.hist(y_test, bins=30, alpha=0.7, label='Actual', color='blue', edgecolor='black')\nplt.hist(y_test_pred_best, bins=30, alpha=0.7, label='Predicted', color='red', edgecolor='black')\nplt.xlabel('House Value')\nplt.ylabel('Frequency')\nplt.title('Distribution: Actual vs Predicted')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 5: Model Comparison (MSE)\nplt.subplot(3, 3, 5)\nmodels = ['Simple Model\\n(max_depth=5)', 'Best Tuned Model']\ntrain_mse = [mean_squared_error(y_train, y_train_pred_simple), mean_squared_error(y_train, y_train_pred_best)]\ntest_mse = [mean_squared_error(y_test, y_test_pred_simple), mean_squared_error(y_test, y_test_pred_best)]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nplt.bar(x - width/2, train_mse, width, label='Train MSE', color='lightblue', edgecolor='black')\nplt.bar(x + width/2, test_mse, width, label='Test MSE', color='lightcoral', edgecolor='black')\n\nplt.xlabel('Model')\nplt.ylabel('Mean Squared Error')\nplt.title('Model Comparison: MSE')\nplt.xticks(x, models)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 6: Learning Curve (Tree Depth)\nplt.subplot(3, 3, 6)\ndepths = range(1, 21)\ntrain_scores = []\ntest_scores = []\n\nfor depth in depths:\n    dt_temp = DecisionTreeRegressor(max_depth=depth, random_state=42)\n    dt_temp.fit(X_train, y_train)\n    train_scores.append(mean_squared_error(y_train, dt_temp.predict(X_train)))\n    test_scores.append(mean_squared_error(y_test, dt_temp.predict(X_test)))\n\nplt.plot(depths, train_scores, 'o-', color='blue', label='Training MSE', linewidth=2)\nplt.plot(depths, test_scores, 'o-', color='red', label='Test MSE', linewidth=2)\nplt.xlabel('Tree Depth')\nplt.ylabel('Mean Squared Error')\nplt.title('Learning Curve: Effect of Tree Depth')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 7: Correlation Heatmap\nplt.subplot(3, 3, 7)\ncorrelation_matrix = X.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f')\nplt.title('Feature Correlation Heatmap')\n\n# Plot 8: Cross-validation Scores\nplt.subplot(3, 3, 8)\ncv_results = pd.DataFrame({\n    'Fold': range(1, 6),\n    'Best Model': -cv_scores_best,\n    'Simple Model': -cv_scores_simple\n})\n\nplt.plot(cv_results['Fold'], cv_results['Best Model'], 'o-', label='Best Model', linewidth=2, markersize=8)\nplt.plot(cv_results['Fold'], cv_results['Simple Model'], 's-', label='Simple Model', linewidth=2, markersize=8)\nplt.xlabel('CV Fold')\nplt.ylabel('Mean Squared Error')\nplt.title('Cross-Validation Performance')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 9: Tree Visualization (Simple Model)\nplt.subplot(3, 3, 9)\nplot_tree(simple_dt, max_depth=3, feature_names=X.columns, filled=True, fontsize=8)\nplt.title('Decision Tree Structure (Simplified View)')\n\nplt.tight_layout()\nplt.show()\n\n# 8. DETAILED ANALYSIS AND INSIGHTS\nprint(\"\\n8. Model Analysis and Insights...\")\n\n# Analyze overfitting\ntrain_r2_best = r2_score(y_train, y_train_pred_best)\ntest_r2_best = r2_score(y_test, y_test_pred_best)\ntrain_r2_simple = r2_score(y_train, y_train_pred_simple)\ntest_r2_simple = r2_score(y_test, y_test_pred_simple)\n\nprint(f\"\\nOverfitting Analysis:\")\nprint(f\"Best Model - Train R\u00b2: {train_r2_best:.4f}, Test R\u00b2: {test_r2_best:.4f}\")\nprint(f\"Simple Model - Train R\u00b2: {train_r2_simple:.4f}, Test R\u00b2: {test_r2_simple:.4f}\")\n\noverfitting_best = train_r2_best - test_r2_best\noverfitting_simple = train_r2_simple - test_r2_simple\nprint(f\"Overfitting Gap (Best): {overfitting_best:.4f}\")\nprint(f\"Overfitting Gap (Simple): {overfitting_simple:.4f}\")\n\n# Model complexity analysis\nprint(f\"\\nModel Complexity:\")\nprint(f\"Best Model - Tree Depth: {best_dt.get_depth()}, Leaves: {best_dt.get_n_leaves()}\")\nprint(f\"Simple Model - Tree Depth: {simple_dt.get_depth()}, Leaves: {simple_dt.get_n_leaves()}\")\n\n# Feature importance insights\nprint(f\"\\nKey Insights:\")\nprint(f\"\u2022 Most important feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance_best']:.3f})\")\nprint(f\"\u2022 Least important feature: {feature_importance.iloc[-1]['feature']} ({feature_importance.iloc[-1]['importance_best']:.3f})\")\n\n# Performance summary\nprint(f\"\\nFinal Model Performance Summary:\")\nprint(f\"\u2022 Best model RMSE on test set: ${np.sqrt(mean_squared_error(y_test, y_test_pred_best)):.2f} (hundreds of thousands)\")\nprint(f\"\u2022 This represents an average prediction error of ~${np.sqrt(mean_squared_error(y_test, y_test_pred_best))*100000:.0f}\")\nprint(f\"\u2022 Model explains {test_r2_best:.1%} of the variance in house prices\")\n\nprint(\"\\n=== Analysis Complete ===\")\n</code></pre>"},{"location":"MachineLearning/SupervisedLearning/RegressionModels/RandomForestRegression/","title":"Random Forest Regression","text":"\u2705 Random Forest  \ud83d\udccc What is Random Forest Algorithm? <p>Random Forest is a machine learning algorithm that uses many decision trees to make better predictions. Each tree looks at different random parts of the data and their results are combined by voting for classification or averaging for regression. This helps in improving accuracy and reducing errors.</p> <p></p> <p></p> <p></p> <p>Working of Random Forest Algorithm</p> <p>Create Many Decision Trees: The algorithm makes many decision trees each using a random part of the data. So every tree is a bit different.</p> <p>Pick Random Features: When building each tree it doesn\u2019t look at all the features (columns) at once. It picks a few at random to decide how to split the data. This helps the trees stay different from each other.</p> <p>Combine the Predictions:</p> <ul> <li> <p>For classification we choose a category as the final answer is the one that most trees agree on i.e majority voting.</p> </li> <li> <p>For regression we predict a number as the final answer is the average of all the trees predictions.</p> </li> </ul> <p>Implementing Random Forest for Classification Tasks</p> <p>Here we will predict survival rate of a person in titanic.</p> <ul> <li> <p>Import libraries and load the Titanic dataset.</p> </li> <li> <p>Remove rows with missing target values ('Survived').</p> </li> <li> <p>Select features like class, sex, age, etc and convert 'Sex' to numbers.</p> </li> <li> <p>Fill missing age values with the median.</p> </li> <li> <p>Split the data into training and testing sets, then train a Random Forest model.</p> </li> <li> <p>Predict on test data, check accuracy and print a sample prediction result.</p> </li> </ul> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ntitanic_data = pd.read_csv(url)\n\ntitanic_data = titanic_data.dropna(subset=['Survived'])\n\nX = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\ny = titanic_data['Survived']\n\nX.loc[:, 'Sex'] = X['Sex'].map({'female': 0, 'male': 1})\n\nX.loc[:, 'Age'].fillna(X['Age'].median(), inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\nrf_classifier.fit(X_train, y_train)\n\ny_pred = rf_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\\n\", classification_rep)\n\nsample = X_test.iloc[0:1]\nprediction = rf_classifier.predict(sample)\n\nsample_dict = sample.iloc[0].to_dict()\nprint(f\"\\nSample Passenger: {sample_dict}\")\nprint(f\"Predicted Survival: {'Survived' if prediction[0] == 1 else 'Did Not Survive'}\")\n</code></pre> <p></p> <p>We evaluated model's performance using a classification report to see how well it predicts the outcomes and used a random sample to check model prediction.</p> <p>Implementing Random Forest for Regression Tasks</p> <p>We will do house price prediction here.</p> <ul> <li> <p>Load the California housing dataset and create a DataFrame with features and target.</p> </li> <li> <p>Separate the features and the target variable.</p> </li> <li> <p>Split the data into training and testing sets (80% train, 20% test).</p> </li> <li> <p>Initialize and train a Random Forest Regressor using the training data.</p> </li> <li> <p>Predict house values on test data and evaluate using MSE and R\u00b2 score.</p> </li> <li> <p>Print a sample prediction and compare it with the actual value.</p> </li> </ul> <pre><code>import pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ncalifornia_housing = fetch_california_housing()\ncalifornia_data = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\ncalifornia_data['MEDV'] = california_housing.target\n\nX = california_data.drop('MEDV', axis=1)\ny = california_data['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n\nrf_regressor.fit(X_train, y_train)\n\ny_pred = rf_regressor.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nsingle_data = X_test.iloc[0].values.reshape(1, -1)\npredicted_value = rf_regressor.predict(single_data)\nprint(f\"Predicted Value: {predicted_value[0]:.2f}\")\nprint(f\"Actual Value: {y_test.iloc[0]:.2f}\")\n\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"R-squared Score: {r2:.2f}\")\n</code></pre> <p>Example:</p> <pre><code># Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score, confusion_matrix, classification_report,\n    precision_recall_fscore_support, roc_curve, auc\n)\nfrom sklearn.tree import plot_tree\nfrom scipy.stats import randint, uniform\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# 1. Load Cancer Dataset\nprint(\"Loading Breast Cancer Wisconsin Dataset...\")\ncancer_data = load_breast_cancer()\nX = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\ny = cancer_data.target\ntarget_names = cancer_data.target_names\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of classes: {len(np.unique(y))}\")\nprint(f\"Class names: {target_names}\")\nprint(f\"Class distribution:\")\nprint(f\"  Malignant (0): {np.sum(y == 0)} samples\")\nprint(f\"  Benign (1): {np.sum(y == 1)} samples\")\n\n# Display first few rows\nprint(\"\\nFirst 5 rows of the dataset:\")\nprint(X.head())\n\n# Check for missing values\nprint(f\"\\nMissing values: {X.isnull().sum().sum()}\")\n\n# Dataset description\nprint(f\"\\nDataset Description:\")\nprint(f\"This dataset contains {X.shape[0]} instances of breast cancer tumors\")\nprint(f\"with {X.shape[1]} features derived from digitized images of\")\nprint(f\"fine needle aspirate (FNA) of breast masses.\")\n\n# 2. Data Preprocessing\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA PREPROCESSING\")\nprint(\"=\"*60)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Testing set size: {X_test.shape[0]}\")\nprint(f\"Training class distribution:\")\nprint(f\"  Malignant (0): {np.sum(y_train == 0)} samples\")\nprint(f\"  Benign (1): {np.sum(y_train == 1)} samples\")\n\n# Feature scaling (important for cancer data due to varying scales)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert back to DataFrame for easier handling\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n\n# Display feature statistics\nprint(f\"\\nFeature scaling completed.\")\nprint(f\"Original feature ranges (first 5 features):\")\nfor i, col in enumerate(X.columns[:5]):\n    print(f\"  {col}: {X[col].min():.2f} to {X[col].max():.2f}\")\n\n# 3. HYPERPARAMETER OPTIMIZATION - LIMITED TO 100 MODELS TOTAL\nprint(\"\\n\" + \"=\"*60)\nprint(\"HYPERPARAMETER OPTIMIZATION FOR CANCER PREDICTION\")\nprint(\"LIMITED TO MAXIMUM 100 MODELS TOTAL\")\nprint(\"=\"*60)\n\n# Create base Random Forest model\nbase_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n# STEP 1: RANDOM SEARCH - REDUCED TO 70 ITERATIONS\nprint(\"\\nSTEP 1: RANDOM SEARCH OPTIMIZATION\")\nprint(\"Limited to 70 model evaluations\")\nprint(\"-\" * 40)\n\n# Define comprehensive parameter space for Random Search\nrandom_param_space = {\n    'n_estimators': randint(50, 300),                    # Reduced range for efficiency\n    'max_depth': [int(x) for x in np.linspace(3, 15, 8)] + [None],  # Reduced options\n    'min_samples_split': randint(2, 15),                 # Reduced range\n    'min_samples_leaf': randint(1, 8),                   # Reduced range\n    'max_features': ['sqrt', 'log2', 0.5],              # Reduced options\n    'bootstrap': [True, False],                          # Keep both options\n    'criterion': ['gini', 'entropy'],                   # Keep both options\n    'max_leaf_nodes': randint(15, 80),                   # Reduced range\n    'class_weight': [None, 'balanced'],                  # Keep both options\n}\n\n# Initialize Random Search - REDUCED TO 70 ITERATIONS\nrandom_search = RandomizedSearchCV(\n    estimator=base_rf,\n    param_distributions=random_param_space,\n    n_iter=70,                     # CHANGED: Reduced from 100 to 70\n    cv=5,                         # 5-fold cross-validation\n    scoring='roc_auc',            # Optimization metric (better for medical data)\n    n_jobs=-1,                    # Use all available cores\n    verbose=1,                    # Show progress\n    random_state=42\n)\n\n# Perform Random Search\nprint(\"Performing Random Search (70 model evaluations)...\")\nstart_time = time.time()\nrandom_search.fit(X_train_scaled, y_train)\nrandom_search_time = time.time() - start_time\n\nprint(f\"\\nRandom Search completed in {random_search_time:.2f} seconds\")\nprint(f\"Models evaluated in Random Search: 70\")\nprint(f\"Best Random Search Score (CV ROC-AUC): {random_search.best_score_:.4f}\")\nprint(f\"Best Random Search Parameters:\")\nfor param, value in random_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\n# STEP 2: GRID SEARCH - LIMITED TO MAXIMUM 30 COMBINATIONS\nprint(f\"\\nSTEP 2: GRID SEARCH OPTIMIZATION\")\nprint(\"Limited to maximum 30 model evaluations\")\nprint(\"-\" * 40)\n\n# Create focused parameter grid based on Random Search results\nbest_random_params = random_search.best_params_\n\n# Build SMALLER focused grid around best random search results - MAX 30 COMBINATIONS\ngrid_param_space = {\n    'n_estimators': [\n        best_random_params['n_estimators'],\n        min(300, best_random_params['n_estimators'] + 50)\n    ],  # Only 2 options\n    'max_depth': [\n        best_random_params['max_depth'],\n        best_random_params['max_depth'] + 2 if best_random_params['max_depth'] else None\n    ] if best_random_params['max_depth'] else [None, 10],  # Only 2 options\n    'min_samples_split': [\n        best_random_params['min_samples_split'],\n        max(2, best_random_params['min_samples_split'] - 1)\n    ],  # Only 2 options\n    'min_samples_leaf': [best_random_params['min_samples_leaf']],  # Only 1 option\n    'max_features': [best_random_params['max_features']],  # Only 1 option\n    'bootstrap': [best_random_params['bootstrap']],  # Only 1 option\n    'criterion': [best_random_params['criterion']],  # Only 1 option\n    'max_leaf_nodes': [\n        best_random_params['max_leaf_nodes'],\n        best_random_params['max_leaf_nodes'] + 10\n    ],  # Only 2 options\n    'class_weight': [best_random_params['class_weight']]  # Only 1 option\n}\n\n# Clean up None values and duplicates\nfor key, values in grid_param_space.items():\n    grid_param_space[key] = list(set([v for v in values if v is not None])) if None not in values else list(set(values))\n\n# Calculate total combinations to ensure we don't exceed 30\ntotal_combinations = 1\nfor key, values in grid_param_space.items():\n    total_combinations *= len(values)\n\nprint(f\"Total Grid Search combinations: {total_combinations}\")\n\n# If still too many combinations, further reduce the grid\nif total_combinations &gt; 30:\n    print(\"Reducing grid size to ensure maximum 30 combinations...\")\n    grid_param_space = {\n        'n_estimators': [best_random_params['n_estimators']],\n        'max_depth': [best_random_params['max_depth']],\n        'min_samples_split': [\n            best_random_params['min_samples_split'],\n            max(2, best_random_params['min_samples_split'] - 1),\n            best_random_params['min_samples_split'] + 1\n        ],\n        'min_samples_leaf': [\n            best_random_params['min_samples_leaf'],\n            best_random_params['min_samples_leaf'] + 1\n        ],\n        'max_features': [best_random_params['max_features']],\n        'bootstrap': [best_random_params['bootstrap']],\n        'criterion': [best_random_params['criterion']],\n        'max_leaf_nodes': [best_random_params['max_leaf_nodes']],\n        'class_weight': [best_random_params['class_weight']]\n    }\n\n    # Recalculate combinations\n    total_combinations = 1\n    for key, values in grid_param_space.items():\n        total_combinations *= len(values)\n    print(f\"Reduced Grid Search combinations: {total_combinations}\")\n\n# Initialize Grid Search\ngrid_search = GridSearchCV(\n    estimator=base_rf,\n    param_grid=grid_param_space,\n    cv=5,                         # 5-fold cross-validation\n    scoring='roc_auc',            # Optimization metric\n    n_jobs=-1,                    # Use all available cores\n    verbose=1                     # Show progress\n)\n\n# Perform Grid Search\nprint(f\"Performing Grid Search ({total_combinations} model evaluations)...\")\nstart_time = time.time()\ngrid_search.fit(X_train_scaled, y_train)\ngrid_search_time = time.time() - start_time\n\nprint(f\"\\nGrid Search completed in {grid_search_time:.2f} seconds\")\nprint(f\"Models evaluated in Grid Search: {total_combinations}\")\nprint(f\"TOTAL MODELS EVALUATED: {70 + total_combinations}\")\nprint(f\"Best Grid Search Score (CV ROC-AUC): {grid_search.best_score_:.4f}\")\nprint(f\"Best Grid Search Parameters:\")\nfor param, value in grid_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\n# STEP 3: COMPARISON OF MODELS\nprint(f\"\\n\" + \"=\"*60)\nprint(\"MODEL COMPARISON FOR CANCER PREDICTION\")\nprint(\"=\"*60)\n\n# Train baseline model (default parameters)\nbaseline_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\nbaseline_rf.fit(X_train_scaled, y_train)\n\n# Get the best models from hyperparameter tuning\nbest_random_rf = random_search.best_estimator_\nbest_grid_rf = grid_search.best_estimator_\n\n# Evaluate all models\nmodels = {\n    'Baseline (Default)': baseline_rf,\n    'Random Search Optimized': best_random_rf,\n    'Grid Search Optimized': best_grid_rf\n}\n\nresults_summary = []\n\nfor name, model in models.items():\n    # Predictions\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class (benign)\n\n    # Metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n\n    # ROC AUC\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    roc_auc = auc(fpr, tpr)\n\n    results_summary.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1,\n        'ROC-AUC': roc_auc\n    })\n\n    print(f\"\\n{name} Results:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\")\n    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results_summary)\nprint(f\"\\nSUMMARY TABLE:\")\nprint(results_df.to_string(index=False, float_format='%.4f'))\n\n# STEP 4: DETAILED CANCER PREDICTION ANALYSIS\nprint(f\"\\n\" + \"=\"*60)\nprint(\"CANCER PREDICTION MODEL EVALUATION\")\nprint(\"=\"*60)\n\n# Use the best performing model for detailed analysis\nbest_model = best_grid_rf\ny_pred_best = best_model.predict(X_test_scaled)\ny_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]  # Extract positive class probabilities\n\n# Detailed classification report\nprint(\"\\nDetailed Classification Report (Best Model):\")\nprint(classification_report(y_test, y_pred_best, target_names=target_names))\n\n# Feature importance analysis for cancer prediction - ORIGINAL VARIABLES ONLY\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': best_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 20 Most Important Features for Cancer Prediction (Original Variable Names):\")\nprint(feature_importance.head(20).to_string(index=False, float_format='%.6f'))\n\nprint(f\"\\nAll Features Ranked by Importance (Original Variable Names):\")\nprint(feature_importance.to_string(index=False, float_format='%.6f'))\n\n# STEP 5: COMPREHENSIVE VISUALIZATIONS\nprint(f\"\\n\" + \"=\"*60)\nprint(\"GENERATING CANCER PREDICTION VISUALIZATIONS\")\nprint(\"=\"*60)\n\n# Set up plotting\nplt.style.use('default')\nfig = plt.figure(figsize=(20, 15))\n\n# 1. Model Performance Comparison\nplt.subplot(2, 4, 1)\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\nbaseline_scores = [results_summary[0][metric] for metric in metrics]\nrandom_scores = [results_summary[1][metric] for metric in metrics]\ngrid_scores = [results_summary[2][metric] for metric in metrics]\n\nx = np.arange(len(metrics))\nwidth = 0.25\n\nplt.bar(x - width, baseline_scores, width, label='Baseline', alpha=0.8, color='lightcoral')\nplt.bar(x, random_scores, width, label='Random Search', alpha=0.8, color='skyblue')\nplt.bar(x + width, grid_scores, width, label='Grid Search', alpha=0.8, color='lightgreen')\n\nplt.xlabel('Metrics')\nplt.ylabel('Score')\nplt.title('Cancer Prediction Model Comparison', fontsize=14, fontweight='bold')\nplt.xticks(x, metrics, rotation=45)\nplt.legend()\nplt.ylim(0, 1.1)\n\n# 2. Confusion Matrix (Best Model)\nplt.subplot(2, 4, 2)\ncm_best = confusion_matrix(y_test, y_pred_best)\nsns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', \n            xticklabels=target_names, yticklabels=target_names)\nplt.title('Confusion Matrix (Best Model)', fontsize=14, fontweight='bold')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n# Add medical context\ntn, fp, fn, tp = cm_best.ravel()\nprint(f\"\\nConfusion Matrix Analysis:\")\nprint(f\"True Negatives (Correctly identified malignant): {tn}\")\nprint(f\"False Positives (Malignant predicted as benign): {fp}\")\nprint(f\"False Negatives (Benign predicted as malignant): {fn}\")\nprint(f\"True Positives (Correctly identified benign): {tp}\")\nprint(f\"False Positive Rate: {fp/(fp+tn):.4f}\")\nprint(f\"False Negative Rate: {fn/(fn+tp):.4f}\")\n\n# 3. ROC Curve\nplt.subplot(2, 4, 3)\nfpr_best, tpr_best, _ = roc_curve(y_test, y_pred_proba_best)\nroc_auc_best = auc(fpr_best, tpr_best)\n\nplt.plot(fpr_best, tpr_best, color='blue', linewidth=2,\n         label=f'Best Model (AUC = {roc_auc_best:.4f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Cancer Prediction', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\n\n# 4. Feature Importance (Top 15) - Original Names\nplt.subplot(2, 4, 4)\ntop_15_features = feature_importance.head(15)\nbars = plt.barh(range(len(top_15_features)), top_15_features['importance'])\nplt.yticks(range(len(top_15_features)), top_15_features['feature'])\nplt.xlabel('Importance Score')\nplt.title('Top 15 Feature Importance (Original Names)', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\n\n# 5. Prediction Confidence Distribution\nplt.subplot(2, 4, 5)\nplt.hist(y_pred_proba_best, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\nplt.xlabel('Prediction Confidence (Benign Class)')\nplt.ylabel('Frequency')\nplt.title('Cancer Prediction Confidence', fontsize=14, fontweight='bold')\nplt.axvline(np.mean(y_pred_proba_best), color='red', linestyle='--', \n            label=f'Mean: {np.mean(y_pred_proba_best):.3f}')\nplt.legend()\n\n# 6. Class Distribution\nplt.subplot(2, 4, 6)\nclass_counts = pd.Series(y).value_counts()\ncolors = ['lightcoral', 'lightgreen']\nplt.pie(class_counts.values, labels=target_names, autopct='%1.1f%%', \n        colors=colors, startangle=90)\nplt.title('Cancer Dataset Class Distribution', fontsize=14, fontweight='bold')\n\n# 7. Optimization Time Comparison\nplt.subplot(2, 4, 7)\nmethods = ['Random Search\\n(70 models)', 'Grid Search\\n({} models)'.format(total_combinations)]\ntimes = [random_search_time, grid_search_time]\ncolors = ['skyblue', 'lightgreen']\n\nbars = plt.bar(methods, times, color=colors, alpha=0.8)\nplt.ylabel('Time (seconds)')\nplt.title('Optimization Time Comparison', fontsize=14, fontweight='bold')\n\nfor bar, time_val in zip(bars, times):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01, \n             f'{time_val:.1f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# 8. Feature Correlation Heatmap (Top 10 features)\nplt.subplot(2, 4, 8)\ntop_10_feature_names = feature_importance.head(10)['feature'].values\ncorrelation_matrix = X[top_10_feature_names].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\nplt.title('Top 10 Features Correlation', fontsize=14, fontweight='bold')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\n\nplt.tight_layout()\nplt.show()\n\n# Additional Feature Importance Visualization with Original Names\nplt.figure(figsize=(12, 10))\n\n# Top 15 Feature Importance with Original Names\ntop_15_features = feature_importance.head(15)\nbars = plt.barh(range(len(top_15_features)), top_15_features['importance'])\nplt.yticks(range(len(top_15_features)), top_15_features['feature'])\nplt.xlabel('Importance Score')\nplt.title('Top 15 Feature Importance (Original Variable Names)', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\n\n# Add value labels on bars\nfor i, (idx, row) in enumerate(top_15_features.iterrows()):\n    plt.text(row['importance'] + 0.001, i, f'{row[\"importance\"]:.4f}', \n             va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Feature importance statistics\nprint(f\"\\nFeature Importance Statistics:\")\nprint(f\"Total number of features: {len(feature_importance)}\")\nprint(f\"Most important feature: {feature_importance.iloc[0]['feature']} (importance: {feature_importance.iloc[0]['importance']:.6f})\")\nprint(f\"Least important feature: {feature_importance.iloc[-1]['feature']} (importance: {feature_importance.iloc[-1]['importance']:.6f})\")\nprint(f\"Mean importance: {feature_importance['importance'].mean():.6f}\")\nprint(f\"Standard deviation: {feature_importance['importance'].std():.6f}\")\n\n# Top 10 features contribution percentage\ntop_10_importance = feature_importance.head(10)\ntotal_importance = feature_importance['importance'].sum()\nprint(f\"\\nTop 10 Features Contribution:\")\nfor idx, row in top_10_importance.iterrows():\n    percentage = (row['importance'] / total_importance) * 100\n    print(f\"{row['feature']}: {row['importance']:.6f} ({percentage:.2f}% of total importance)\")\n\nprint(f\"\\nTop 10 features account for {(top_10_importance['importance'].sum() / total_importance) * 100:.2f}% of total importance\")\n\n# STEP 6: MEDICAL INSIGHTS AND RECOMMENDATIONS\nprint(f\"\\n\" + \"=\"*60)\nprint(\"MEDICAL INSIGHTS AND RECOMMENDATIONS\")\nprint(\"=\"*60)\n\n# Performance improvement analysis\nbaseline_acc = results_summary[0]['Accuracy']\nrandom_acc = results_summary[1]['Accuracy']\ngrid_acc = results_summary[2]['Accuracy']\n\nrandom_improvement = ((random_acc - baseline_acc) / baseline_acc) * 100\ngrid_improvement = ((grid_acc - baseline_acc) / baseline_acc) * 100\n\nprint(f\"\\nPerformance Improvements for Cancer Prediction:\")\nprint(f\"Random Search vs Baseline: {random_improvement:+.2f}% accuracy improvement\")\nprint(f\"Grid Search vs Baseline: {grid_improvement:+.2f}% accuracy improvement\")\nprint(f\"Final ROC-AUC Score: {results_summary[2]['ROC-AUC']:.4f}\")\n\nprint(f\"\\nOptimization Efficiency:\")\nprint(f\"Total models evaluated: {70 + total_combinations} (\u2264 100)\")\nprint(f\"Random Search models: 70\")\nprint(f\"Grid Search models: {total_combinations}\")\nprint(f\"Total optimization time: {(random_search_time + grid_search_time):.2f} seconds\")\n\nprint(f\"\\nClinical Significance:\")\nprint(f\"- High ROC-AUC ({results_summary[2]['ROC-AUC']:.4f}) indicates excellent diagnostic capability\")\nprint(f\"- Low False Negative Rate minimizes missed cancer cases\")\nprint(f\"- Feature importance reveals key tumor characteristics for diagnosis\")\n\nprint(f\"\\nOptimal Hyperparameters for Cancer Prediction:\")\nfor param, value in grid_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\nKey Findings:\")\nprint(f\"1. Model achieves {grid_acc:.1%} accuracy in cancer diagnosis\")\nprint(f\"2. Random Forest handles the high-dimensional medical data effectively\")\nprint(f\"3. Hyperparameter optimization significantly improves performance\")\nprint(f\"4. Feature importance provides insights into most predictive characteristics\")\nprint(f\"5. Efficient optimization with limited model evaluations (\u2264 100)\")\n\n# Risk assessment for individual predictions\nprint(f\"\\nSample Risk Assessments (First 10 test cases):\")\n\nfor i in range(min(10, len(X_test))):\n    # Direct indexing since y_test is a NumPy array\n    true_class = target_names[y_test[i]]\n    pred_class = target_names[y_pred_best[i]]\n    confidence = y_pred_proba_best[i] if y_pred_best[i] == 1 else 1 - y_pred_proba_best[i]\n\n    risk_level = \"High Confidence\" if confidence &gt; 0.8 else \"Medium Confidence\" if confidence &gt; 0.6 else \"Low Confidence\"\n    status = \"\u2713 Correct\" if true_class == pred_class else \"\u2717 Incorrect\"\n\n    print(f\"Patient {i+1}: True={true_class}, Predicted={pred_class}\")\n    print(f\"           Confidence={confidence:.3f} ({risk_level}) {status}\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"CANCER PREDICTION MODEL ANALYSIS COMPLETE\")\nprint(\"=\"*60)\n\n# Final model summary\nprint(f\"\\nFINAL MODEL SUMMARY:\")\nprint(f\"Dataset: Breast Cancer Wisconsin (Diagnostic)\")\nprint(f\"Samples: {X.shape[0]} (Malignant: {np.sum(y == 0)}, Benign: {np.sum(y == 1)})\")\nprint(f\"Features: {X.shape[1]} tumor characteristics\")\nprint(f\"Best Model: Random Forest with optimized hyperparameters\")\nprint(f\"Performance: {grid_acc:.1%} accuracy, {results_summary[2]['ROC-AUC']:.4f} ROC-AUC\")\nprint(f\"Model Evaluations: {70 + total_combinations} total (\u2264 100 limit)\")\nprint(f\"Clinical Impact: Reliable tool for cancer diagnosis support\")\n</code></pre>"},{"location":"MachineLearning/SupervisedLearning/RegressionModels/RidgeLassoRegression/","title":"Ridge & Lasso Regression","text":"\u2705 Lasso vs Ridge vs Elastic Net  <p>Regularization methods like Lasso, Ridge and Elastic Net help improve linear regression models by preventing overfitting which address multicollinearity and helps in feature selection. These techniques increase the model\u2019s accuracy and stability.</p> \ud83d\udccc What is Ridge Regression (L2 Regularization)? <p>Ridge regression is a technique used to address overfitting by adding a penalty to the model's complexity. It introduces an L2 penalty (also called L2 regularization) which is the sum of the squares of the model's coefficients. This penalty term reduces the size of large coefficients but keeps all features in the model. This prevents overfitting with correlated features.</p> <p>Formula for Ridge Regression:</p> <p></p> \ud83d\udccc Lasso Regression (L1 Regularization)? <p>Lasso regression addresses overfitting by adding an L1 penalty i.e sum of absolute coefficients to the model's loss function. This encourages some coefficients to become exactly zero helps in effectively removing less important features. It also helps to simplify the model by selecting only the key features.</p> <p>Formula for Lasso Regression:</p> <p></p> \ud83d\udccc Elastic Net Regression (L1 + L2 Regularization)? <p>Elastic Net regression combines both L1 (Lasso) and L2 (Ridge) penalties to perform feature selection, manage multicollinearity and balancing coefficient shrinkage. This works well when there are many correlated features helps in avoiding the problem where Lasso might randomly pick one and ignore others.</p> <p>Formula for Elastic Net Regression:</p> <p></p> <p>Lasso vs Ridge vs Elastic Net</p> Features Lasso Regression Ridge Regression Elastic Net Regression Penalty Type L1 Penalty: Uses absolute values of coefficients. L2 Penalty: Uses the square of the coefficients. L1 + L2 Penalty: Combines absolute and square penalties. Effect on Coefficients Completely removes unnecessary features by setting coefficients to zero. Makes all coefficients smaller but does not set them to zero. Removes some features and reduces others by balancing both. Best Use Case Best when we want to remove irrelevant features. Best when all features matter but their impact should be reduced. Best when features are correlated and feature selection is needed. Hyperparameters Involved Alpha: Controls regularization strength (higher alpha = more shrinkage). Alpha: Same as Lasso for controlling regularization strength. Alpha + L1_ratio: Alpha controls strength; L1_ratio balances Lasso &amp; Ridge. Bias and Variance High bias, low variance. Low bias, high variance. Balanced bias and variance. Strengths Automatically chooses important features. Works well when features are related but shouldn\u2019t be completely removed. Combines Lasso\u2019s feature selection with Ridge\u2019s handling of correlations. Weaknesses May remove useful features if not tuned well. Keeps all features, which may not help in high-dimensional irrelevant data. Harder to tune due to having two parameters. Example With 100 features to predict house prices, sets irrelevant features (like house color) to zero. With 100 features, reduces the impact of all features but doesn\u2019t remove any. If \u201csize\u201d and \u201crooms\u201d are similar, removes one and shrinks the other. <p>Example:</p> <pre><code># Import necessary libraries\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the California Housing dataset\ndata = fetch_california_housing(as_frame=True)\ndf = data.frame\n\n# Preprocess the data\nX = df.drop(columns=['MedHouseVal'])\ny = df['MedHouseVal']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Fit Linear Regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\ny_pred_linear = linear_model.predict(X_test)\n\n# Fit Lasso Regression model with hyperparameter tuning\nlasso_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\nlasso_model = GridSearchCV(Lasso(), lasso_params, cv=5, scoring='r2')\nlasso_model.fit(X_train, y_train)\ny_pred_lasso = lasso_model.best_estimator_.predict(X_test)\n\n# Fit Ridge Regression model with hyperparameter tuning\nridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\nridge_model = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='r2')\nridge_model.fit(X_train, y_train)\ny_pred_ridge = ridge_model.best_estimator_.predict(X_test)\n\n# Fit Elastic Net Regression model with hyperparameter tuning\nelastic_params = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.5, 0.9]}\nelastic_model = GridSearchCV(ElasticNet(), elastic_params, cv=5, scoring='r2')\nelastic_model.fit(X_train, y_train)\ny_pred_elastic = elastic_model.best_estimator_.predict(X_test)\n\n# Evaluate models\nresults = pd.DataFrame({\n    'Model': ['Linear', 'Lasso', 'Ridge', 'Elastic Net'],\n    'MSE': [mean_squared_error(y_test, y_pred_linear),\n            mean_squared_error(y_test, y_pred_lasso),\n            mean_squared_error(y_test, y_pred_ridge),\n            mean_squared_error(y_test, y_pred_elastic)],\n    'MAE': [mean_absolute_error(y_test, y_pred_linear),\n            mean_absolute_error(y_test, y_pred_lasso),\n            mean_absolute_error(y_test, y_pred_ridge),\n            mean_absolute_error(y_test, y_pred_elastic)],\n    'R\u00b2': [r2_score(y_test, y_pred_linear),\n           r2_score(y_test, y_pred_lasso),\n           r2_score(y_test, y_pred_ridge),\n           r2_score(y_test, y_pred_elastic)]\n})\nprint(results)\n\n# Plot model performance\nresults.set_index('Model').plot(kind='bar', figsize=(10, 6))\nplt.title('Model Performance Comparison')\nplt.ylabel('Metric Value')\nplt.show()\n</code></pre>"},{"location":"MachineLearning/UnsupervisedLearning/Clustering/","title":"Clustering","text":"\u2705 Clustering \ud83d\udccc What is K means Clustering? <p>K-Means Clustering is an Unsupervised Machine Learning algorithm which groups unlabeled dataset into different clusters. It is used to organize data into groups based on their similarity.</p> \ud83d\udccc Understanding K-means Clustering <p></p> <p></p> <p>For example online store uses K-Means to group customers based on purchase frequency and spending creating segments like Budget Shoppers, Frequent Buyers and Big Spenders for personalised marketing.</p> <ul> <li> <p>The algorithm works by first randomly picking some central points called centroids and each data point is then assigned to the closest centroid forming a cluster.</p> </li> <li> <p>After all the points are assigned to a cluster the centroids are updated by finding the average position of the points in each cluster.</p> </li> <li> <p>This process repeats until the centroids stop changing forming clusters.</p> </li> <li> <p>The goal of clustering is to divide the data points into clusters so that similar data points belong to same group.</p> </li> </ul> \ud83d\udccc How k-means clustering works? <p>We are given a data set of items with certain features and values for these features like a vector.</p> <p>The task is to categorize those items into groups. To achieve this we will use the K-means algorithm. 'K' in the name of the algorithm represents the number of groups/clusters we want to classify our items into.</p> <p></p> <p>The algorithm will categorize the items into k groups or clusters of similarity.To calculate that similarity we will use the Euclidean distance as a measurement. The algorithm works as follows:  </p> <ol> <li> <p>First we randomly initialize k points called means or cluster centroids.</p> </li> <li> <p>We categorize each item to its closest mean and we update the mean's coordinates, which are the averages of the items categorized in that cluster so far.</p> </li> <li> <p>We repeat the process for a given number of iterations and at the end, we have our clusters.</p> </li> </ol> <p>In K-Means:</p> <ul> <li> <p>You start with some initial cluster centers (means).</p> </li> <li> <p>These means are just points in your feature space.</p> </li> <li> <p>There are two common ways to choose them:</p> <ol> <li> <p>Pick random data points as means.</p> </li> <li> <p>Pick random values within the range of the dataset.</p> </li> </ol> </li> </ul> <p>Suppose you have this dataset:</p> Customer ID Age (x1) Income (x2 in \\$K) 1 25 40 2 30 45 3 35 50 4 60 100 5 65 105 <p>This is a 2D dataset (Age and Income).</p> <p>\u2705 Method 1: Initialize means using random data points</p> <p>We randomly choose k=2 actual rows as our initial centers:</p> <ul> <li> <p>Mean 1 = (25, 40)</p> </li> <li> <p>Mean 2 = (60, 100)</p> </li> </ul> <p>These are real customers, just chosen as starting points.</p> <p>\u2705 Method 2: Initialize means using random values within feature ranges</p> <p>Here, we use the min and max of each feature:</p> <ul> <li> <p>Age: min = 25, max = 65 \u21d2 range = [25, 65]</p> </li> <li> <p>Income: min = 40, max = 105 \u21d2 range = [40, 105]</p> </li> </ul> <p>Now randomly generate any values within this box:</p> <ul> <li> <p>Mean 1 = (30.5, 80.2) \u2190 random numbers between 25\u201365 and 40\u2013105</p> </li> <li> <p>Mean 2 = (58.3, 45.0)</p> </li> </ul> <p>\ud83e\udd14 Why do we need different ways?</p> <ul> <li> <p>Random points from the dataset: safer, avoids outliers.</p> </li> <li> <p>Random values in the feature space: more flexible, but risky if the range has irrelevant areas (e.g., outliers can skew the range).</p> </li> </ul> <p>\ud83d\udd01 After Initialization</p> <p>No matter how you initialize:</p> <ul> <li> <p>K-Means will iteratively adjust the means by:</p> <ul> <li> <p>Assigning each point to the nearest mean.</p> </li> <li> <p>Recomputing each mean as the average of its assigned points.</p> </li> </ul> </li> <li> <p>The algorithm stops when the means stop changing significantly.</p> </li> </ul> <p>Summary:</p> Method Example Notes Pick random data points (25, 40), (60, 100) Easy, safe Pick random values in range (30.5, 80.2), (58.3, 45.0) More flexible, but riskier \ud83d\udccc Example: K-Means Initialization Methods with Visualization <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Sample dataset: [Age, Income]\ndata = np.array([\n    [25, 40],\n    [30, 45],\n    [35, 50],\n    [60, 100],\n    [65, 105]\n])\n\n# Function to initialize centroids using method 1 (random points from data)\ndef init_random_points(data, k=2):\n    indices = random.sample(range(len(data)), k)\n    return data[indices]\n\n# Function to initialize centroids using method 2 (random values within range)\ndef init_random_values(data, k=2):\n    mins = data.min(axis=0)\n    maxs = data.max(axis=0)\n    return np.array([np.random.uniform(mins, maxs) for _ in range(k)])\n\n# Initialize\nnp.random.seed(42)  # for reproducibility\nk = 2\nmeans_method1 = init_random_points(data, k)\nmeans_method2 = init_random_values(data, k)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.scatter(data[:, 0], data[:, 1], c='blue', label='Data Points')\nplt.scatter(means_method1[:, 0], means_method1[:, 1], c='green', marker='X', s=200, label='Method 1: Data Points')\nplt.scatter(means_method2[:, 0], means_method2[:, 1], c='red', marker='P', s=200, label='Method 2: Random Values')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Income ($K)\")\nplt.title(\"K-Means Initialization Methods\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udcca What You'll See:</p> <ul> <li> <p>Blue dots = original customer data (Age vs. Income)</p> </li> <li> <p>Green 'X' markers = initial means chosen from actual data points</p> </li> <li> <p>Red 'P' markers = initial means from random values inside feature range</p> </li> </ul> <p>This will help you visually compare how these two initialization strategies place the cluster centers.</p> \ud83d\udccc Euclidean Distance <p>Euclidean Distance is defined as the distance between two points in Euclidean space.To find the distance between two points, the length of the line segment that connects the two points should be measured.</p> <p>Euclidean distance is like measuring the straightest and shortest path between two points.</p> <p>Imagine you have a string and you stretch it tight between two points on a map; the length of that string is the Euclidean distance. It tells you how far apart the two points are without any turns or bends, just like a bird would fly directly from one spot to another.</p> <p>This metric is based on the Pythagorean theorem and is widely utilized in various fields such as machine learning, data analysis, computer vision, and more.</p> <p>Euclidean Distance Formula</p> <p>Consider two points (x1, y1) and (x2, y2) in a 2-dimensional space; the Euclidean Distance between them is given by using the formula:</p> <p></p> <p>Where,</p> <ul> <li> <p>d is Euclidean Distance,</p> </li> <li> <p>(x1, y1) is the Coordinate of the first point,</p> </li> <li> <p>(x2, y2) is the Coordinate of the second point.</p> </li> </ul> <p>Euclidean Distance in 3D</p> <p>If the two points (x1, y1, z1) and (x2, y2, z2) are in a 3-dimensional space, the Euclidean Distance between them is given by using the formula:</p> <p></p> <p>Where,</p> <ul> <li> <p>d is Euclidean Distance,</p> </li> <li> <p>(x1, y1, z1) is the Coordinate of the first point,</p> </li> <li> <p>(x2, y2, z2) is the Coordinate of the second point.</p> </li> </ul> <p>Euclidean Distance in nD</p> <p>In general, the Euclidean Distance formula between two points (x11, x12, x13, ...., x1n) and (x21, x22, x23, ...., x2n) in an n-dimensional space is given by the formula:</p> <p></p> <p>Where,</p> <ul> <li> <p>i Ranges from 1 to n,</p> </li> <li> <p>d is Euclidean distance,</p> </li> <li> <p>(x11, x12, x13, ...., x1n) is the Coordinate of the First Point,</p> </li> <li> <p>(x21, x22, x23, ...., x2n) is the Coordinate of the Second Point.</p> </li> </ul> <p>Euclidean Distance Formula Derivation</p> <p>Euclidean Distance Formula is derived by following the steps added below:</p> <ul> <li> <p>Step 1: Let us consider two points, A (x1, y1) and B (x2, y2), and d is the distance between the two points.</p> </li> <li> <p>Step 2: Join the points using a straight line (AB).</p> </li> <li> <p>Step 3: Now, let us construct a right-angled triangle whose hypotenuse is AB, as shown in the figure below.</p> </li> </ul> <p></p> <p>Step 4: Now, using Pythagoras theorem we know that,</p> <p></p> <p>Note: Selecting the right number of clusters is important for meaningful segmentation to do this we use Elbow Method for optimal value of k in KMeans which is a graphical tool used to determine the optimal number of clusters (k) in K-means.</p> \ud83d\udccc Elbow Method for optimal value of k in KMeans <p>Choosing the optimal number of clusters is a crucial step in any unsupervised learning algorithm.</p> <p>Since we don\u2019t have predefined cluster counts in unsupervised learning, we need a systematic approach to determine the best k value. The Elbow Method is a popular technique used for this purpose in K-Means clustering.</p> \ud83d\udccc Elbow Method in K-Means Clustering <p>In K-Means clustering, we start by randomly initializing k clusters and iteratively adjusting these clusters until they stabilize at an equilibrium point. However, before we can do this, we need to decide how many clusters (k) we should use.</p> <p>The Elbow Method helps us find this optimal k value. Here\u2019s how it works:</p> <ol> <li> <p>We iterate over a range of k values, typically from 1 to n (where n is a hyper-parameter you choose).</p> </li> <li> <p>For each k, we calculate the Within-Cluster Sum of Squares (WCSS).</p> </li> </ol> <p></p> <p>The Elbow Point: Optimal k Value</p> <p>The Elbow Method works in below steps:</p> <ul> <li> <p>We calculate a distance measure called WCSS (Within-Cluster Sum of Squares). This tells us how spread out the data points are within each cluster.</p> </li> <li> <p>We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS.</p> </li> <li> <p>We plot a graph with k on the X-axis and WCSS on the Y-axis.</p> </li> <li> <p>Identifying the Elbow Point: As we increase kkk, the WCSS typically decreases because we're creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an \"elbow\" shape in the graph.</p> <ul> <li> <p>Before the elbow: Increasing kkk significantly reduces WCSS, indicating that new clusters effectively capture more of the data's variability.</p> </li> <li> <p>After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting.</p> </li> </ul> </li> </ul> <p></p> <p>The goal is to identify the point where the rate of decrease in WCSS sharply changes, indicating that adding more clusters (beyond this point) yields diminishing returns. This \"elbow\" point suggests the optimal number of clusters.</p> \ud83d\udccc Understanding Distortion and Inertia in K-Means Clustering <p>In K-Means clustering, we aim to group similar data points together. To evaluate the quality of these groupings, we use two key metrics: Distortion and Inertia.</p> <p>1. Distortion</p> <p>Distortion measures the average squared distance between each data point and its assigned cluster center. It's a measure of how well the clusters represent the data. A lower distortion value indicates better clustering.</p> <p></p> <p>2. Inertia</p> <p>Inertia is the sum of squared distances of each data point to its closest cluster center. It's essentially the total squared error of the clustering. Like distortion, a lower inertia value suggests better clustering.</p> <p></p> <p>In the Elbow Method, we calculate the distortion or inertia for different values of k (number of clusters). We then plot these values to identify the \"elbow point\", where the rate of decrease in distortion or inertia starts to slow down. This elbow point often indicates the optimal number of clusters.</p> <p>A Lower Distortion or Inertia is Generally Better</p> <p>A lower distortion or inertia implies that the data points are more closely grouped around their respective cluster centers. However, it's important to balance this with the number of clusters. Too few clusters might not capture the underlying structure of the data, while too many clusters can lead to overfitting.</p> <p>By understanding distortion and inertia, we can effectively evaluate the quality of K-Means clustering and select the optimal number of clusters.</p> \ud83d\udccc Implementation of Elbow Method <p>In this section, we will demonstrate how to implement the Elbow Method to determine the optimal number of clusters (k) using Python's Scikit-learn library. We will create a random dataset, apply K-means clustering, calculate the Within-Cluster Sum of Squares (WCSS) for different values of k, and visualize the results to determine the optimal number of clusters.</p> \ud83d\udccc Step 1: Importing the required libraries <pre><code>from sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> \ud83d\udccc Step 2: Creating and Visualizing the data <p>We will create a random array and visualize its distribution</p> <pre><code># Creating the dataset\nx1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6,\n               7, 8, 9, 8, 9, 9, 8, 4, 4, 5, 4])\nx2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7,\n               1, 2, 1, 2, 3, 2, 3, 9, 10, 9, 10])\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n\n# Visualizing the data\nplt.scatter(x1, x2, marker='o')\nplt.xlim([0, 10])\nplt.ylim([0, 10])\nplt.title('Dataset Visualization')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n</code></pre> <p></p> <p>From the above visualization, we can see that the optimal number of clusters should be around 3. But visualizing the data alone cannot always give the right answer. Hence we demonstrate the following steps.</p> \ud83d\udccc Step 3: Building the Clustering Model and Calculating Distortion and Inertia <p>In this step, we will fit the K-means model for different values of k (number of clusters) and calculate both the distortion and inertia for each value.</p> <pre><code>distortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nK = range(1, 10)\n\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k, random_state=42).fit(X)\n\n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)**2) / X.shape[0])\n\n    inertias.append(kmeanModel.inertia_)\n\n    mapping1[k] = distortions[-1]\n    mapping2[k] = inertias[-1]\n</code></pre> \ud83d\udccc Step 4: Tabulating and Visualizing the Results <p>a) Displaying Distortion Values</p> <pre><code>print(\"Distortion values:\")\nfor key, val in mapping1.items():\n    print(f'{key} : {val}')\n\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Distortion')\nplt.show()\n</code></pre> <p></p> <p>b) Displaying Inertia Values:</p> <p></p> \ud83d\udccc Step 5: Clustered Data Points For Different k Values <p>We will plot images of data points clustered for different values of k. For this, we will apply the k-means algorithm on the dataset by iterating on a range of k values.</p> <pre><code>k_range = range(1, 5)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n    y_kmeans = kmeans.fit_predict(X)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', marker='o', edgecolor='k', s=100)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n                s=300, c='red', label='Centroids', edgecolor='k')\n    plt.title(f'K-means Clustering (k={k})')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.legend()\n    plt.grid()\n    plt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Key Takeaways</p> <ul> <li> <p>The Elbow Method helps you choose the optimal number of clusters (k) in KMeans clustering.</p> </li> <li> <p>It analyzes how adding more clusters (increasing k) affects the spread of data points within each cluster (WCSS).</p> </li> <li> <p>The k value corresponding to the \"elbow\" in the WCSS vs k graph is considered the optimal choice.</p> </li> <li> <p>The Elbow Method provides a good starting point, but consider your specific data and goals when finalizing k.</p> </li> </ul> \ud83d\udcca Use Case: Customer Segmentation for Marketing <p>\ud83c\udfaf Objective</p> <p>A retail company wants to segment its customers based on purchasing behavior so that it can:</p> <ul> <li> <p>Run personalized marketing campaigns,</p> </li> <li> <p>Identify high-value customers,</p> </li> <li> <p>Improve customer retention.</p> </li> </ul> <p>\ud83d\udcc1 Dataset</p> <p>[Kaggle][https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis]</p> <p>\ud83d\udcd8 Dataset Overview</p> <p>This dataset, available at Kaggle as Customer Personality Analysis by imakash3011, includes 2,240 customer records with 29 features, covering demographic info, household characteristics, spending on products, and campaign responses</p> <p>Key feature groups:</p> <ul> <li> <p>People: <code>ID, Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome, Dt_Customer, Recency, Complain</code></p> </li> <li> <p>Product Purchases (last 2 years): <code>MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds</code></p> </li> <li> <p>Promotion Behavior: <code>NumDealsPurchases, AcceptedCmp1\u2013AcceptedCmp5, Response</code></p> </li> <li> <p>Purchasing Channels: <code>NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth</code></p> </li> </ul> <p>\ud83e\udde0 Step-by-Step: Build K-Means Real-world Model</p> <p>1. Load &amp; Clean Data</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv('marketing_campaign.csv', sep='\\t')\n# Handle missing income values by median or capping\ndf['Income'].fillna(df['Income'].median(), inplace=True)\n</code></pre> <p>2. Feature Engineering</p> <pre><code>from datetime import datetime\ndf['Age'] = datetime.now().year - df['Year_Birth']\ndf['Total_Expenses'] = df[['MntWines', 'MntFruits', 'MntMeatProducts',\n                            'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum(axis=1)\ndf['Total_Accepted_Cmp'] = df[['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','Response']].sum(axis=1)\n</code></pre> <p>3. Select and Scale Features</p> <p>Choose relevant variables for segmentation:</p> <pre><code>features = ['Income', 'Age', 'Recency', 'Total_Expenses', 'Total_Accepted_Cmp', 'NumWebPurchases', 'NumStorePurchases']\nX = df[features]\nfrom sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n</code></pre> <p>4. Determine Number of Clusters</p> <p>Plot the Elbow Curve or compute silhouette scores to choose optimal k, typically k=2 to 4</p> <p>5. Run K-Means Clustering</p> <p>\u2705 Real-Time Use Case: Customer Segmentation using K-Means</p> <p>Business Scenario:</p> <p>A retail company wants to segment its customers based on demographics, spending behavior, and tenure to run personalized marketing campaigns.</p> <pre><code># Step 1: Install required packages\n!pip install -q seaborn\n\n# Step 2: Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Step 3: Load data\ndf = pd.read_csv(\"marketing_campaign.csv\", sep='\\t')\n\n# Step 4: Preprocess\ndf['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], dayfirst=True)\ndf['Customer_Tenure'] = (pd.Timestamp(\"2025-01-01\") - df['Dt_Customer']).dt.days\n\n# Drop unnecessary columns\ndf = df.drop(columns=['ID', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue'])\n\n# One-hot encode categorical\ndf = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True)\n\n# Handle missing values\nimputer = SimpleImputer(strategy=\"median\")\ndf_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_imputed)\n\n# Step 5: Fit K-Means\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ndf_imputed['Cluster'] = kmeans.fit_predict(X_scaled)\n\n# Step 6: Label clusters (optional &amp; domain-driven)\ncluster_labels = {\n    0: 'High Income, High Spend',\n    1: 'Low Income, Less Spend',\n    2: 'Middle Income, Average Spend',\n    3: 'Senior Customers'\n}\ndf_imputed['Segment_Label'] = df_imputed['Cluster'].map(cluster_labels)\n\n# Step 7: Visualize clusters\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    x=df_imputed['Income'],\n    y=df_imputed['MntWines'],\n    hue=df_imputed['Segment_Label'],\n    palette='Set2'\n)\nplt.title(\"Customer Segmentation using K-Means\")\nplt.xlabel(\"Annual Income\")\nplt.ylabel(\"Wine Spend\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Insights You Can Derive:</p> <ul> <li> <p>High Income, High Spend: Likely premium customers to upsell.</p> </li> <li> <p>Low Income, Less Spend: Might benefit from discount-based offers.</p> </li> <li> <p>Middle Income: Core customer base with moderate loyalty.</p> </li> <li> <p>Senior Customers: Segment by age and target with legacy brand values.</p> </li> </ul>"},{"location":"MachineLearning/UnsupervisedLearning/overview/","title":"Overview","text":"\u2705 Unsupervised Learning? \ud83d\udccc What is Unsupervised Learning? <p>Unsupervised Learning is a type of machine learning where the algorithm learns patterns from unlabeled data, meaning there are no predefined outputs or target variables. i.e., we don't give output to our model. The training model has only input parameter values and discovers the groups or patterns on its own.</p> <p>Key Concept:</p> <p>The goal of unsupervised learning is to discover hidden patterns, structures, or relationships in the data without any human guidance.</p> <p></p> <p>The image shows set of animals: elephants, camels, and cows that represents raw data that the unsupervised learning algorithm will process.</p> <ul> <li> <p>The \"Interpretation\" stage signifies that the algorithm doesn't have predefined labels or categories for the data. It needs to figure out how to group or organize the data based on inherent patterns.</p> </li> <li> <p>Algorithm represents the core of unsupervised learning process using techniques like clustering, dimensionality reduction, or anomaly detection to identify patterns and structures in the data.</p> </li> <li> <p>Processing stage shows the algorithm working on the data.</p> </li> </ul> <p>The output shows the results of the unsupervised learning process. In this case, the algorithm might have grouped the animals into clusters based on their species (elephants, camels, cows).</p> \ud83d\udccc How does unsupervised learning work? <p>Unsupervised learning works by analyzing unlabeled data to identify patterns and relationships. The data is not labeled with any predefined categories or outcomes, so the algorithm must find these patterns and relationships on its own. </p> <ul> <li> <p>The model tries to find similarities, clusters, or distributions in the data.</p> </li> <li> <p>It does not know in advance what the output should be.</p> </li> <li> <p>It learns from the structure or distribution of the data itself.</p> </li> </ul> \ud83d\udccc Common Techniques: Technique Description Example Use Case Clustering Group similar data points together. Customer segmentation (e.g., K-Means) Dimensionality Reduction Reduce number of features while preserving variance. Visualizing high-dimensional data (e.g., PCA) Anomaly Detection Detect unusual data points. Fraud detection, network intrusion detection Association Rules Find rules showing relationships between variables. Market basket analysis (e.g., Apriori) <p>\ud83d\udce6 Example:</p> <p>Input Data:</p> <p>Age    Income 25     40K 27     42K 50     150K 52     160K</p> <p>Output:</p> <p>The algorithm (e.g., K-Means) may group the people into below group Without being told these categories in advance.</p> <ul> <li> <p>Group 1: Younger, lower income</p> </li> <li> <p>Group 2: Older, higher income</p> </li> </ul> <p>\u2705 Applications:</p> <ul> <li> <p>Customer segmentation</p> </li> <li> <p>Recommendation systems (e.g., Netflix, Amazon)</p> </li> <li> <p>Anomaly/fraud detection</p> </li> <li> <p>Document or image clustering</p> </li> </ul> <p>Summary:</p> <p>Unsupervised learning helps machines discover the unknown in data, making it especially powerful for exploratory analysis and feature discovery.</p> \ud83d\udccc Challenges of Unsupervised Learning: <p>1. No Ground Truth (No Labels)</p> <ul> <li> <p>Why it's hard: There\u2019s no \u201ccorrect\u201d answer to evaluate against.</p> </li> <li> <p>Impact: Difficult to measure accuracy or performance objectively.</p> </li> </ul> <p>2. Choosing the Right Algorithm</p> <ul> <li> <p>Why it's hard: Many algorithms (K-Means, DBSCAN, PCA, etc.) work well on specific types of data.</p> </li> <li> <p>Impact: Poor algorithm choice can lead to meaningless results.</p> </li> </ul> <p>3. Determining the Number of Clusters or Components</p> <ul> <li> <p>Why it's hard: You often don\u2019t know how many groups (e.g., clusters) are in the data.</p> </li> <li> <p>Example: How many customer segments exist in a marketing dataset?</p> </li> </ul> <p>4. High Dimensionality</p> <ul> <li> <p>Why it's hard: More features (dimensions) make it harder to find meaningful patterns due to the curse of dimensionality.</p> </li> <li> <p>Solution: Dimensionality reduction (e.g., PCA, t-SNE) \u2014 but this adds complexity.</p> </li> </ul> <p>5. Interpreting Results</p> <ul> <li> <p>Why it's hard: Outputs like clusters or embeddings are abstract and not always clearly interpretable.</p> </li> <li> <p>Impact: Hard for stakeholders to understand or validate findings.</p> </li> </ul> <p>6. Sensitivity to Noise and Outliers</p> <ul> <li> <p>Why it's hard: Unsupervised algorithms may group noise or outliers into their own cluster or distort existing ones.</p> </li> <li> <p>Example: K-Means can be pulled off-center by outliers.</p> </li> </ul> <p>7. Scalability and Computation</p> <ul> <li>Why it's hard: Large datasets with high dimensions can make clustering and similarity computations slow or memory-intensive.</li> </ul> <p>8. Initialization Sensitivity</p> <ul> <li>Why it's hard: Some algorithms (e.g., K-Means) rely on random initialization, which can lead to different results each time.</li> </ul> <p>9. Lack of Objective Evaluation Metrics</p> <ul> <li> <p>Why it's hard: No standard metrics like accuracy or precision.</p> </li> <li> <p>Alternatives: Use metrics like Silhouette Score, Davies\u2013Bouldin Index, or visualizations \u2014 but they're approximate.</p> </li> </ul>"},{"location":"Models/Ollama/","title":"Ollama","text":"\u2705 Ollama \ud83d\udccc What is Ollama? <p>Ollama is a tool and platform that makes it easy to run and interact with large language models (LLMs) locally on your computer, especially models like <code>LLaMA</code>, <code>Mistral</code>, <code>Gemma</code>, and others \u2014 without needing cloud infrastructure or APIs like OpenAI's.</p> \ud83d\udd27 What Does Ollama Do? <ul> <li> <p>Runs LLMs locally: No internet connection or cloud server needed after downloading a model.</p> </li> <li> <p>Supports popular open-source models: Includes LLaMA 2/3, Mistral, Gemma, Code LLaMA, and others.</p> </li> <li> <p>Simple CLI tool: You can start chatting with a model using the command line.</p> </li> <li> <p>Integrates with apps: Ollama can serve models through an HTTP API, enabling developers to build local AI applications (e.g., with LangChain, CrewAI, etc.).</p> </li> </ul> \u2705 Key Features Feature Description Local model execution Uses your own CPU/GPU to run models Model management Download, update, and remove models easily Privacy-first No data is sent to external servers unless you choose to Cross-platform Works on macOS, Windows, and Linux Built-in server Starts a local server (<code>localhost:11434</code>) to access models via API \ud83e\udde0 Example Usage (Terminal) <pre><code>ollama run llama3\n</code></pre> \ud83e\udde0 Example Usage with a prompt <pre><code>ollama run mistral:7b-instruct\n&gt; What's the capital of France?\n</code></pre> \u2705 1. LLaMA Family Model Description <code>llama2</code> Meta's LLaMA 2, general-purpose LLM (7B, 13B, 70B) <code>llama3</code> Meta's latest, better quality and reasoning (8B, 70B) <code>llama3:8b</code> Smaller, fast and efficient <code>llama3:70b</code> Very powerful, large model \u2705 2. Mistral Models Model Description <code>mistral</code> Open-weight, performant 7B model <code>mixtral</code> Mixture-of-experts (MoE) version (12.9B active params) <code>mixtral:8x7b</code> Specific variant of Mixtral \u2705 3. Gemma Model Description <code>gemma</code> Google's open model, Gemma (2B, 7B) <code>gemma:2b</code> Lightweight and fast <code>gemma:7b</code> More powerful \u2705 4. Phi Model Description <code>phi</code> Microsoft's small, high-quality model <code>phi:2</code> Updated Phi-2 model \u2705 5. Code LLMs Model Description <code>codellama</code> Meta's LLaMA variant for code tasks <code>codellama:7b</code> LLaMA 2 based code model (7B) <code>codellama:13b</code> Larger version (13B) \u2705 6. Neural Chat Model Description <code>neural-chat</code> Fine-tuned for conversational tasks \u2705 7. OpenChat Model Description <code>openchat</code> Fine-tuned model with great instruction following \u2705 8. Dolphin Model Description <code>dolphin-mixtral</code> Fine-tuned Mixtral model, very chat-friendly \u2705 9. LLaVA (Multimodal) Model Description <code>llava</code> Vision + language model, image + text input \u2705 10. Solar Model Description <code>solar</code> Compact model with high performance \u2705 11. Starling Model Description <code>starling</code> Reward model fine-tuned for alignment \u2705 12. TinyLLaMA Model Description <code>tinyllama</code> Super lightweight version (1.1B) \u2705 13. Yi Model Description <code>yi</code> Open Chinese-English bilingual model \ud83d\udd0d To List All Models from CLI: <pre><code>ollama list\n</code></pre> \ud83d\udce5 To Pull a Model: <pre><code>ollama pull llama3\n</code></pre> \ud83e\udde0 To Run a Model: <pre><code>ollama run llama3\n</code></pre> \ud83d\udda5\ufe0f Hardware Requirements by Model Size Model Size Recommended RAM Recommended VRAM (GPU) CPU Disk Space Notes Tiny (1B\u20133B) \u2265 8 GB Optional (2\u20134 GB VRAM) 4-core x86 CPU \\~10 GB Runs on CPU, slow on older machines Small (7B) \u2265 16 GB \u2265 4\u20136 GB VRAM 6-core, modern CPU \\~15\u201320 GB Most popular size, runs well on modern systems Medium (13B) \u2265 24 GB \u2265 8 GB VRAM 8-core or better \\~25\u201330 GB Slower on CPU; GPU recommended Large (30B) \u2265 32 GB \u2265 16 GB VRAM 8-core+, AVX512 support helpful \u2265 50 GB GPU required for real-time chat Very Large (65B\u201370B) \u2265 64 GB \u2265 32 GB VRAM (e.g. RTX 4090) High-end workstation/server CPU \u2265 70\u2013100 GB For advanced users; long startup time on CPU \ud83e\uddf0 Software Requirements Component Requirement Operating System macOS (Intel/Apple Silicon), Linux, Windows (via WSL2 or native GUI on Win 11) Installation Via CLI (`curl https://ollama.com/install.sh sh`) or Windows GUI CUDA (GPU) NVIDIA GPU support for acceleration (CUDA &gt;= 11.7) Optional GPU Config <code>OLLAMA_FLASH_ATTENTION=1</code> to enable flash attention for faster decoding Supported Architectures x86-64, Apple M1/M2/M3 (ARM64 supported for macOS) Environment Works with Docker, CLI, or as a backend for LangChain, Python, etc. \ud83d\udce6 Model &amp; Storage Considerations Model Name Typical Size (Quantized) Model Types Use Case <code>tinyllama</code> \\~1\u20132 GB Chat/General Very fast, low resource <code>phi</code>, <code>neural-chat</code> \\~2\u20133 GB Chat, Conversational Efficient for local use <code>llama2:7b</code> \\~4\u20135 GB General-purpose Best balance of size &amp; performance <code>mistral</code>, <code>gemma:7b</code> \\~4\u20136 GB Chat, RAG, QA Faster and newer alternatives <code>codellama:13b</code> \\~7\u20138 GB Code generation GPU highly recommended <code>llama3:70b</code> \\~40\u201345 GB Top-tier reasoning and RAG Only suitable for high-end systems \u2699\ufe0f Performance Benchmarks (Approx.) Model Cold Load Time (CPU) Cold Load Time (GPU) Token Generation Speed <code>llama2:7b</code> 15\u201325 sec 3\u20135 sec \\~5\u201315 tokens/sec (CPU), \\~40\u201370 (GPU) <code>mistral</code> 10\u201320 sec 2\u20134 sec Fast, efficient <code>llama3:70b</code> 60\u201390 sec 5\u201310 sec (RTX 4090) Very fast, but huge size \ud83e\udde0 CodeLlama Models Overview Model Name Parameters Quantized Size Use Case Model Type <code>codellama</code> 7B \\~4\u20136 GB General code generation LLaMA 2 base + code fine-tuned <code>codellama:7b</code> 7B \\~4\u20136 GB Efficient for local code completion Standard <code>codellama:13b</code> 13B \\~7\u20139 GB More context, better accuracy Larger version \ud83d\udda5\ufe0f Hardware Requirements Model RAM (Recommended) GPU VRAM (Recommended) CPU (Min) Disk Space <code>codellama</code> \u2265\u202f16 GB Optional, \u2265 4 GB 4\u20136 core x86 \\~6 GB <code>codellama:7b</code> \u2265\u202f16 GB \u2265 6 GB 6-core modern CPU \\~8 GB <code>codellama:13b</code> \u2265\u202f24 GB \u2265 8\u201312 GB 8-core+ recommended \\~12 GB <p>\ud83d\udcdd Note: GPU is highly recommended for codellama:13b, especially for low-latency completions.</p> \u2699\ufe0f Software Requirements Component Details OS macOS, Linux, Windows (WSL2 or GUI for Win11) GPU Support NVIDIA CUDA 11.7+ (if using GPU acceleration) Ollama CLI/GUI <code>ollama pull codellama:7b</code> or <code>:13b</code> Usage CLI or API (<code>ollama run codellama:7b</code>) Integration Ready Works with LangChain, VS Code extensions, etc. \ud83e\uddea Performance Comparison (Approx.) Metric <code>codellama:7b</code> <code>codellama:13b</code> Load Time (CPU) \\~10\u201320 sec \\~30\u201345 sec Load Time (GPU) \\~2\u20135 sec (6GB+) \\~6\u201310 sec (12GB+) Tokens/sec (CPU) \\~8\u201315 \\~4\u20138 Tokens/sec (GPU) \\~50\u201380 \\~40\u201360 Max Context (default) 4K tokens (configurable) 4K\u20138K depending on tuning \ud83e\uddd1\u200d\ud83d\udcbb Ideal Use Cases Model Recommended For <code>codellama</code> Lightweight coding tasks, embedded in apps <code>codellama:7b</code> Local code assistants, pair programming, code explanations <code>codellama:13b</code> Advanced coding help, long function generation, full project structure output \ud83e\udde0 Understanding Model Sizes <p>The terms 7B and 13B refer to the number of parameters in the model \u2014 a critical aspect that affects performance, resource requirements, and capability.</p> Model Name Parameter Count Meaning <code>1B</code> 1 Billion Very lightweight, fast, but limited capability <code>3B</code> 3 Billion Small model for basic tasks <code>7B</code> 7 Billion Good balance of speed and performance <code>13B</code> 13 Billion Higher accuracy and reasoning; needs more resources <code>30B</code> 30 Billion Very high capability; slow on CPU <code>70B</code> 70 Billion State-of-the-art performance; needs high-end GPU or server \u2696\ufe0f 7B vs 13B \u2013 Tradeoffs Feature 7B Model 13B Model Model Size (quantized) \\~4\u20136 GB \\~7\u20139 GB Performance Fast and responsive Slower but smarter Memory Usage (RAM) 16 GB minimum 24\u201332 GB recommended GPU (VRAM) \u2265 6 GB recommended \u2265 10\u201312 GB preferred Accuracy Good for most tasks Better understanding and generation Context Window Up to 4K tokens Up to 8K tokens (configurable) Best For Lightweight apps, fast chat/code Complex coding, long-form generation Startup Time 2\u20135 sec (GPU), 10\u201320 sec (CPU) 5\u201310 sec (GPU), 30\u201345 sec (CPU) \ud83e\uddd1\u200d\ud83d\udcbb Example Ollama Models by Size Model Size Use Case <code>tinyllama</code> 1.1B Very fast, low-memory devices <code>phi</code>, <code>neural-chat</code> 2\u20133B General chat, embedded systems <code>llama2:7b</code> 7B Fast, general-purpose LLM <code>codellama:7b</code> 7B Efficient code assistant <code>codellama:13b</code> 13B Advanced code generation &amp; understanding <code>llama3:70b</code> 70B SOTA performance, massive reasoning \u2705 When to Choose What If you want... Choose Fast response, low resource usage <code>7B</code> More accurate coding and reasoning <code>13B</code> Max accuracy and deep knowledge <code>30B</code> or <code>70B</code> Ultra-lightweight chatbot <code>phi</code> or <code>tinyllama</code> \u2696\ufe0f CodeLlama: 7B vs 13B for Code Tasks Feature <code>codellama:7b</code> <code>codellama:13b</code> Parameter Count 7 Billion 13 Billion Model Size (Quantized) \\~4\u20136 GB \\~7\u20139 GB RAM Required (Minimum) 16 GB (bare minimum) 24\u201332 GB recommended GPU VRAM (Recommended) \u2265 6 GB (good performance) \u2265 10\u201312 GB (critical for usable performance) CPU-only Load Time 10\u201320 seconds 30\u201345 seconds GPU Load Time 2\u20135 seconds 6\u201310 seconds Token Generation Speed (CPU) \\~10\u201320 tokens/sec \\~5\u20138 tokens/sec Token Generation Speed (GPU) \\~40\u201380 tokens/sec \\~30\u201360 tokens/sec Context Window \\~4K tokens \\~4K\u20138K tokens Code Generation Quality \u2705 Fast, decent completions \u2705\u2705 More accurate, better long completions Handles Complex Prompts Basic\u2013Intermediate Intermediate\u2013Advanced Multilingual Code Support Yes Yes Fine-tuned For Python, JavaScript, C++, TypeScript, etc. Same, with better long-form understanding Use Cases Code snippets, function completions Long functions, class design, full solutions Real-world Example Autocompletes a Python function Writes full Flask backend with endpoints \ud83e\udde0 Which One Should You Choose? Your Setup Recommended Model CPU-only, 16 GB RAM or low VRAM GPU (\u2264 6 GB) \u2705 <code>codellama:7b</code> Decent GPU (e.g., RTX 3060+, \u226510 GB VRAM), 32 GB RAM \u2705\u2705 <code>codellama:13b</code> High-end GPU (e.g., RTX 4090), 64+ GB RAM \ud83d\ude80 <code>codellama:13b</code> or larger Want faster generation, small project completions \u2705 <code>7b</code> Need higher accuracy, deeper code reasoning \u2705\u2705 <code>13b</code> \ud83e\udde0 Let's compare codellama:7b vs codellama:13b on a real-world coding task. \ud83d\udc68\u200d\ud83d\udcbb Prompt <pre><code>\"Write a Python Flask REST API with two endpoints: /health returning {'status': 'ok'} and /predict that accepts JSON input { 'age': 30, 'income': 50000 } and returns a dummy prediction { 'approved': true }.\"\n</code></pre> \u2699\ufe0f CodeLlama:7B \u2014 Sample Output <p>(Faster response, but less structured and minimal error handling)</p> <pre><code>from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'ok'})\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.get_json()\n    age = data.get('age')\n    income = data.get('income')\n    # dummy logic\n    if age &gt; 25 and income &gt; 40000:\n        result = {'approved': True}\n    else:\n        result = {'approved': False}\n    return jsonify(result)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <p>\u2705 Strengths: Quick, to the point</p> <p>\u26a0\ufe0f Limitations: No input validation, no error handling, no docstrings.</p> \ud83d\udd0d CodeLlama:13B \u2014 Sample Output <p>(Slower response, but more robust and detailed)</p> <pre><code>from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({'status': 'ok'}), 200\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        data = request.get_json(force=True)\n\n        # Validate input\n        if 'age' not in data or 'income' not in data:\n            return jsonify({'error': 'Missing age or income'}), 400\n\n        age = data['age']\n        income = data['income']\n\n        # Dummy prediction logic\n        approved = age &gt;= 25 and income &gt;= 40000\n        return jsonify({'approved': approved}), 200\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>\u2705\u2705 Strengths: Adds validation, error handling, and cleaner structure</p> <p>\ud83d\udca1 Extras: Can also add Swagger, Pydantic, or type hints if prompted</p> \u2694\ufe0f Comparison Summary Feature <code>codellama:7b</code> <code>codellama:13b</code> Response Time \u26a1 Fast \ud83d\udd52 Slower Code Quality \ud83d\udfe8 Basic logic \u2705 Robust &amp; production-grade Validation \u274c None \u2705 Basic validation added Error Handling \u274c None \u2705 Includes <code>try-except</code> block Best For Quick prototyping Realistic codebases or full-stack scaffolds 1. Another Prompt Comparison (Docker) <p>Prompt:</p> <pre><code>Write a Dockerfile for a Python Flask app with two endpoints: /health returning {'status': 'ok'} and /predict accepting JSON and returning dummy prediction {'approved': true}.\"\n</code></pre> CodeLlama:7B \u2014 Sample Output <pre><code>FROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Flask app (<code>app.py</code>) code may be very basic, probably missing error handling or requirements details.</p> <p>Strengths: Quick and straightforward</p> <p>Limitations: Lacks automation or context-specific optimizations</p> CodeLlama:13B \u2014 Sample Output <pre><code># Use an official lightweight Python runtime\nFROM python:3.10-alpine\n\n# Set work directory\nWORKDIR /usr/src/app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy project files\nCOPY . .\n\n# Expose necessary port\nEXPOSE 5000\n\n# Start Flask server\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"app:app\"]\n</code></pre> <p>Flask app (<code>app.py</code>) likely includes validation, error handling, and production readiness.</p> <p>Strengths: More production-ready, efficient, and secure.</p> 2. Automatically Compare Outputs with Python <p>You can compare generated code from two models using Python, using tools like <code>difflib</code>. Here's a small example:</p> <pre><code>import difflib\n\ncode_7b = \"\"\"...\"\"\"  # insert 7B generated code\ncode_13b = \"\"\"...\"\"\"  # insert 13B generated code\n\nfor line in difflib.unified_diff(code_7b.splitlines(keepends=True),\n                                  code_13b.splitlines(keepends=True),\n                                  fromfile='7B', tofile='13B'):\n    print(line)\n</code></pre>"},{"location":"NLP/nlpdetails/","title":"NLP Details","text":"\u2705 Natural Language Processing (NLP) \ud83d\udccc Architecture Details Explanation"},{"location":"NLP/nlpdetails/#end-to-end-nlp-text-classification-pipeline","title":"End-to-End NLP Text Classification Pipeline","text":"<p>Project: End-to-End NLP Text Classification (Preprocessing \u2192 Training \u2192 Deployment)</p> <p>Purpose: A production-oriented pipeline for building, evaluating, and deploying text classification models. This repository includes preprocessing pipelines, model training with cross-validation and hyperparameter tuning, model serialization, and a simple inference API.</p> \ud83d\udccc Table of Contents  <ul> <li>Architecture Overview</li> <li>Features</li> <li>Folder Structure</li> <li>Requirements</li> <li>Quickstart</li> <li>Data Preprocessing Steps</li> <li>Model Training &amp; Evaluation</li> <li>Deployment &amp; Inference</li> <li>Model Monitoring &amp; MLOps</li> <li>Best Practices</li> <li>License</li> </ul> \ud83d\udccc Architecture Overview  <p>The pipeline consists of: 1. Data ingestion &amp; EDA \u2014 ingest raw text, visualize (word clouds), and inspect class balance. 2. Preprocessing \u2014 lowercasing, URL/email removal, tokenization, punctuation/digit handling, stopword removal, lemmatization/stemming. 3. Feature extraction \u2014 TF-IDF/BoW with n-gram support or contextual embeddings (BERT). 4. Model training \u2014 classifiers with StratifiedKFold CV and hyperparameter search. 5. Evaluation \u2014 accuracy, precision, recall, F1 (macro/weighted), ROC-AUC, PR-AUC, MCC. 6. Deployment \u2014 packaged pipeline served via FastAPI (ensures same preprocessing). 7. Monitoring \u2014 model versioning (MLflow/DVC), drift detection, performance tracking.</p> \ud83d\udccc Features  <ul> <li>Reproducible preprocessing pipeline</li> <li>TF-IDF and n-gram support</li> <li>Stratified K-Fold cross-validation</li> <li>Grid/Randomized hyperparameter search</li> <li>Model serialization and simple API for inference</li> <li>Guidance for monitoring and model versioning</li> </ul> \ud83d\udccc Folder Structure  <pre><code>.\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                 # raw datasets (do not edit)\n\u2502   \u2514\u2500\u2500 processed/           # processed datasets\n\u251c\u2500\u2500 notebooks/               # EDA and experiments\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 preprocess.py        # cleaning, tokenizer, lemmatizer\n\u2502   \u251c\u2500\u2500 features.py          # vectorizers, feature selection\n\u2502   \u251c\u2500\u2500 models.py            # model training and CV\n\u2502   \u2514\u2500\u2500 serve.py             # FastAPI app for inference\n\u251c\u2500\u2500 models/                  # saved model artifacts (.pkl, ONNX)\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 Dockerfile               # optional containerization\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 tests/                   # unit tests\n</code></pre> \ud83d\udccc Requirements  <p>Create a virtual environment and install dependencies:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p><code>requirements.txt</code> should include: <pre><code>scikit-learn\npandas\nnumpy\nnltk\njoblib\nfastapi\nuvicorn\nwordcloud\nmatplotlib\nimblearn\nxgboost\nlightgbm\nshap\nmlflow\n</code></pre></p> \ud83d\udccc Quickstart  <ol> <li>Prepare data: put <code>train.csv</code> into <code>data/raw/</code> with columns <code>id</code>, <code>text</code>, <code>label</code>.</li> <li>Run preprocessing + feature build: <pre><code>python src/preprocess.py --input data/raw/train.csv --output data/processed/train_clean.csv\n</code></pre></li> <li>Train model: <pre><code>python src/models.py --train data/processed/train_clean.csv --model-out models/best_model.pkl\n</code></pre></li> <li>Serve model (local): <pre><code>uvicorn src.serve:app --host 0.0.0.0 --port 8000\n# POST to http://localhost:8000/predict with {\"text\": \"your text here\"}\n</code></pre></li> </ol> \ud83d\udccc Data Preprocessing Steps  <ol> <li>Lowercase</li> <li>Remove URLs, emails, mentions</li> <li>Remove non-alphanumeric characters (configurable)</li> <li>Tokenize</li> <li>Remove stopwords (configurable language)</li> <li>Lemmatize (preferred) or stem</li> <li>Optional: numeric/emoji special handling, class balancing (SMOTE for embeddings)</li> <li>Save processed data to <code>data/processed/</code></li> </ol> \ud83d\udccc Model Training &amp; Evaluation  <ul> <li>Use <code>StratifiedKFold(n_splits=5, shuffle=True)</code> to split data.</li> <li>Use <code>Pipeline</code> from <code>sklearn</code> to chain preprocess \u2192 vectorize \u2192 classifier.</li> <li>Use <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> with scoring <code>f1_weighted</code> (or domain-specific metric).</li> <li>Log runs and artifacts to MLflow: <pre><code>import mlflow\nmlflow.start_run()\nmlflow.log_param('model', 'logistic_regression')\nmlflow.log_metric('f1', 0.92)\nmlflow.sklearn.log_model(pipeline, 'model')\nmlflow.end_run()\n</code></pre></li> </ul> \ud83d\udccc Deployment &amp; Inference  <ul> <li>Ensure the same pipeline used in training is saved and served.</li> <li>Example using FastAPI: <code>src/serve.py</code> loads <code>models/best_model.pkl</code> and exposes <code>/predict</code>.</li> <li>Consider containerizing: <pre><code>docker build -t nlp-classifier:latest .\ndocker run -p 8000:8000 nlp-classifier:latest\n</code></pre></li> </ul> \ud83d\udccc Model Monitoring &amp; MLOps  <ul> <li>Track model versions and datasets with MLflow / DVC.</li> <li>Monitor:</li> <li>Prediction distribution (class drift)</li> <li>Input feature drift</li> <li>Model performance on a labeled validation set</li> <li>Set alerts for drift thresholds; trigger retraining pipeline.</li> </ul> \ud83d\udccc Best Practices  <ul> <li>Use Pipelines to avoid leakage.</li> <li>Keep preprocessing deterministic and versioned.</li> <li>For heavy models, choose batch inference for non-critical latency tasks.</li> <li>Use feature selection if TF-IDF dims explode.</li> <li>Use SHAP for model explainability.</li> </ul> \ud83d\udccc Text Normalization <p>Steps Required for Text normalization.</p> <ul> <li> <p>Input text String</p> </li> <li> <p>Convert all letters of the string to one case(either lower or upper case)</p> </li> <li> <p>If numbers are essential to convert to words else remove all numbers</p> </li> <li> <p>Remove punctuations, other formalities of grammar</p> </li> <li> <p>Remove white spaces</p> </li> <li> <p>Remove stop words</p> </li> <li> <p>And any other computations</p> </li> </ul> <p>Text normalization with above-mentioned steps, every step can be done in some ways. So we will discuss each and everything in this whole process.</p> <p>Text String</p> <pre><code># input string \nstring = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\nprint(string)\n</code></pre> <p>Case Conversion (Lower Case)</p> <pre><code># convert to lower case\nlower_string = string.lower()\nprint(lower_string)\n</code></pre> <p>Removing Numbers Remove numbers if they're not relevant to your analyses. Usually, regular expressions are used to remove numbers.</p> <pre><code># import regex\nimport re\n\n# input string \nstring = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n\n# convert to lower case\nlower_string = string.lower()\n\n# remove numbers\nno_number_string = re.sub(r'\\d+','',lower_string)\nprint(no_number_string)\n</code></pre> <p>Removing punctuation</p> <pre><code># import regex\nimport re\n\n# input string \nstring = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n\n# convert to lower case\nlower_string = string.lower()\n\n# remove numbers\nno_number_string = re.sub(r'\\d+','',lower_string)\n\n# remove all punctuation except words and space\nno_punc_string = re.sub(r'[^\\w\\s]','', no_number_string) \nprint(no_punc_string)\n</code></pre> <p>Removing White space</p> <pre><code># import regex\nimport re\n\n# input string \nstring = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n\n# convert to lower case\nlower_string = string.lower()\n\n# remove numbers\nno_number_string = re.sub(r'\\d+','',lower_string)\n\n# remove all punctuation except words and space\nno_punc_string = re.sub(r'[^\\w\\s]','', no_number_string) \n\n# remove white spaces\nno_wspace_string = no_punc_string.strip()\nprint(no_wspace_string)\n</code></pre> <p>Removing Stop Words</p> <p>Stop words\u201d are the foremost common words during a language like \u201cthe\u201d, \u201ca\u201d, \u201con\u201d, \u201cis\u201d, \u201call\u201d. These words don't carry important meaning and are usually faraway from texts. It is possible to get rid of stop words using tongue Toolkit (NLTK), a set of libraries and programs for symbolic and statistical tongue processing.</p> <pre><code># download stopwords\nimport nltk\nnltk.download('stopwords')\n\n# import nltk for stopwords\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n# assign string\nno_wspace_string='python  released in  was a major revision of the language that is not completely backward compatible and much python  code does not run unmodified on python  with python s endoflife only python x and later are supported with older versions still supporting eg windows  and old installers not restricted to bit windows'\n\n# convert string to list of words\nlst_string = [no_wspace_string][0].split()\nprint(lst_string)\n\n# remove stopwords\nno_stpwords_string=\"\"\nfor i in lst_string:\n    if not i in stop_words:\n        no_stpwords_string += i+' '\n\n# removing last space\nno_stpwords_string = no_stpwords_string[:-1]\nprint(no_stpwords_string)\n</code></pre> <pre><code># import regex\nimport re\n\n# download stopwords\nimport nltk\nnltk.download('stopwords')\n\n# import nltk for stopwords\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n\n# input string \nstring = \"       Python 3.0, released in 2008, was a major revision of the language that is not completely backward compatible and much Python 2 code does not run unmodified on Python 3. With Python 2's end-of-life, only Python 3.6.x[30] and later are supported, with older versions still supporting e.g. Windows 7 (and old installers not restricted to 64-bit Windows).\"\n\n# convert to lower case\nlower_string = string.lower()\n\n# remove numbers\nno_number_string = re.sub(r'\\d+','',lower_string)\n\n# remove all punctuation except words and space\nno_punc_string = re.sub(r'[^\\w\\s]','', no_number_string) \n\n# remove white spaces\nno_wspace_string = no_punc_string.strip()\nno_wspace_string\n\n# convert string to list of words\nlst_string = [no_wspace_string][0].split()\nprint(lst_string)\n\n# remove stopwords\nno_stpwords_string=\"\"\nfor i in lst_string:\n    if not i in stop_words:\n        no_stpwords_string += i+' '\n\n# removing last space\nno_stpwords_string = no_stpwords_string[:-1]\n\n# output\nprint(no_stpwords_string)\n</code></pre> \ud83d\udccc Tokenization <p>Tokenization is a process of splitting text into smaller units called tokens.</p> <p>Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking down a stream of text into smaller units called tokens. These tokens can range from individual characters to full words or phrases, Based on how detailed it needs to be. By converting text into these manageable chunks, machines can more effectively analyze and understand human language.</p> <p></p> <p></p> <p></p> \ud83d\udccc Types of Tokenization <p></p> <p></p> <ul> <li> <p>Word Tokenization:This is the most common method where text is divided into individual words. It works well for languages with clear word boundaries, like English.</p> </li> <li> <p>Character Tokenization:In this method, text is split into individual characters. This is particularly useful for languages without clear word boundaries or for tasks that require a detailed analysis, such as spelling correction.</p> </li> <li> <p>Subword Tokenization:Sub-word tokenization strikes a balance between word and character tokenization by breaking down text into units that are larger than a single character but smaller than a full word.</p> </li> <li> <p>SentenceTokenization: Sentence tokenization is also a common technique used to make a division of paragraphs or large set of sentences into separated sentences as tokens.</p> </li> <li> <p>N-gram TokenizationN-gram tokenization splits words into fixed-sized chunks (size = n) of data</p> </li> </ul> \ud83d\udccc Tokenization Challenges <p></p> <p>Implementing Tokenization</p> <ul> <li> <p>NLTK (Natural Language Toolkit)</p> </li> <li> <p>SpaCy</p> </li> <li> <p>BERT Tokenizer</p> </li> <li> <p>Byte-Pair Encoding (BPE)</p> </li> <li> <p>Sentence Piece</p> </li> </ul> \ud83d\udccc Lemmatization <p>Lemmatization reduces words to their base or root form.</p> <p>Lemmatization is an important text pre-processing technique in Natural Language Processing (NLP) that reduces words to their base form known as a \"lemma.\" For example, the lemma of \"running\" is \"run\" and \"better\" becomes \"good.\" Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word. This makes lemmatization more accurate as it avoids generating non-dictionary words.</p> <p></p> <p>It is used for:</p> <ul> <li> <p>Improves accuracy: It ensures words with similar meanings like \"running\" and \"ran\" are treated as the same.</p> </li> <li> <p>Reduced Data Redundancy: By reducing words to their base forms, it reduces redundancy in the dataset. This leads to smaller datasets which makes it easier to handle and process large amounts of text for analysis or training machine learning models.</p> </li> <li> <p>Better NLP Model Performance: By treating all similar word as same, it improves the performance of NLP models by making text more consistent. For example, treating \"running,\" \"ran\" and \"runs\" as the same word improves the model's understanding of context and meaning.</p> </li> </ul> \ud83d\udccc Lemmatization Techniques <p>There are different techniques to perform lemmatization each with its own advantages and use cases:</p> <p>1. Rule Based Lemmatization</p> <p>In rule-based lemmatization, predefined rules are applied to a word to remove suffixes and get the root form. This approach works well for regular words but may not handle irregularities well.</p> <p>For example:</p> <p>Rule: <pre><code>For regular verbs ending in \"-ed,\" remove the \"-ed\" suffix.\n\nExample: \"walked\" -&gt; \"walk\"\n</code></pre></p> <p>While this method is simple and interpretable, it doesn't account for irregular word forms like \"better\" which should be lemmatized to \"good\".</p> <p>2. Dictionary-Based Lemmatization</p> <p>It uses a predefined dictionary or lexicon such as WordNet to look up the base form of a word. This method is more accurate than rule-based lemmatization because it accounts for exceptions and irregular words.</p> <p>For example:</p> <ul> <li>'running' -&gt; 'run'</li> <li>'better' -&gt; 'good'</li> <li>'went' -&gt; 'go</li> </ul> <p>\"I was running to become a better athlete and then I went home,\" -&gt; \"I was run to become a good athlete and then I go home.\"</p> <p>By using dictionaries like WordNet this method can handle a range of words effectively, especially in languages with well-established dictionaries.</p> <p>3. Machine Learning-Based Lemmatization</p> <p>It uses algorithms trained on large datasets to automatically identify the base form of words. This approach is highly flexible and can handle irregular words and linguistic nuances better than the rule-based and dictionary-based methods.</p> <p>For example:</p> <p>A trained model may deduce that \u201cwent\u201d corresponds to \u201cgo\u201d even though the suffix removal rule doesn\u2019t apply. Similarly, for 'happier' the model deduces 'happy' as the lemma. </p> <p>Machine learning-based lemmatizers are more adaptive and can generalize across different word forms which makes them ideal for complex tasks involving diverse vocabularies.</p> <p>Step 1: Installing NLTK and Downloading Necessary Resources</p> <p>In Python, the NLTK library provides an easy and efficient way to implement lemmatization. First, we need to install the NLTK library and download the necessary datasets like WordNet and the punkt tokenizer.</p> <pre><code>!pip install nltk\n</code></pre> <p>Now lets import the library and download the necessary datasets.</p> <pre><code>import nltk\nnltk.download('punkt_tab')      \nnltk.download('wordnet')    \nnltk.download('omw-1.4') \nnltk.download('averaged_perceptron_tagger_eng')\n</code></pre> <p>Step 2: Lemmatizing Text with NLTK</p> <p>Now we can tokenize the text and apply lemmatization using NLTK's WordNetLemmatizer.</p> <pre><code>from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ntext = \"The cats were running faster than the dogs.\"\n\ntokens = word_tokenize(text)\n\nlemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n\nprint(f\"Original Text: {text}\")\nprint(f\"Lemmatized Words: {lemmatized_words}\")\n</code></pre> <p>Step 3: Improving Lemmatization with Part of Speech (POS) Tagging</p> <p>To improve the accuracy of lemmatization, it\u2019s important to specify the correct Part of Speech (POS) for each word. By default, NLTK assumes that words are nouns when no POS tag is provided. However, it can be more accurate if we specify the correct POS tag for each word.</p> <p>For example:</p> <ul> <li> <p>\"running\" (as a verb) should be lemmatized to \"run\".</p> </li> <li> <p>\"better\" (as an adjective) should be lemmatized to \"good\".</p> </li> </ul> <pre><code>from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nsentence = \"The children are running towards a better place.\"\n\ntokens = word_tokenize(sentence)\n\ntagged_tokens = pos_tag(tokens)\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):  \n        return 'a'\n    elif tag.startswith('V'):  \n        return 'v'\n    elif tag.startswith('N'):  \n        return 'n'\n    elif tag.startswith('R'):  \n        return 'r'\n    else:\n        return 'n'  \n\nlemmatized_sentence = []\n\nfor word, tag in tagged_tokens:\n    if word.lower() == 'are' or word.lower() in ['is', 'am']:\n        lemmatized_sentence.append(word)  \n    else:\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n\nprint(\"Original Sentence: \", sentence)\nprint(\"Lemmatized Sentence: \", ' '.join(lemmatized_sentence))\n</code></pre> \ud83d\udccc Stemming <p>Stemming reduces works to their root by removing suffixes. Types of stemmers include:</p> <p>Stemming is an important text-processing technique that reduces words to their base or root form by removing prefixes and suffixes. This process standardizes words which helps to improve the efficiency and effectiveness of various natural language processing (NLP) tasks.</p> <p></p> <p></p> <p></p> <p></p> <p>In NLP, stemming simplifies words to their most basic form, making it easier to analyze and process text. For example, \"chocolates\" becomes \"chocolate\" and \"retrieval\" becomes \"retrieve\". This is important in the early stages of NLP tasks where words are extracted from a document and tokenized (broken into individual words).</p> <p>It helps in tasks such as <code>text classification</code>, <code>information retrieval</code> and <code>text summarization</code> by reducing words to a base form. While it is effective, it can sometimes introduce drawbacks including potential inaccuracies and a reduction in text readability.</p> <p>Examples of stemming for the word \"like\":</p> <pre><code>\"likes\" \u2192 \"like\"\n\"liked\" \u2192 \"like\"\n\"likely\" \u2192 \"like\"\n\"liking\" \u2192 \"like\"\n</code></pre> <ul> <li>Porter Stemmer    Porter's Stemmer is one of the most popular and widely used stemming algorithms. Proposed in 1980 by Martin Porter, this stemmer works by applying a series of rules to remove common suffixes from English words. It is well-known for its simplicity, speed and reliability. However, the stemmed output is not guaranteed to be a meaningful word and its applications are limited to the English language.</li> </ul> <p>Example:</p> <ul> <li> <p>'agreed' \u2192 'agree'</p> </li> <li> <p>Rule: If the word has a suffix EED (with at least one vowel and consonant) remove the suffix and change it to EE.</p> </li> </ul> <p>Advantages:</p> <ul> <li> <p>Very fast and efficient.</p> </li> <li> <p>Commonly used for tasks like information retrieval and text mining.</p> </li> </ul> <p>Limitations:</p> <ul> <li> <p>Outputs may not always be real words.</p> </li> <li> <p>Limited to English words.</p> </li> </ul> <pre><code>from nltk.stem import PorterStemmer\n\nporter_stemmer = PorterStemmer()\n\nwords = [\"running\", \"jumps\", \"happily\", \"running\", \"happily\"]\n\nstemmed_words = [porter_stemmer.stem(word) for word in words]\n\nprint(\"Original words:\", words)\nprint(\"Stemmed words:\", stemmed_words)\n</code></pre> <ul> <li>Snowball Stemmer    The Snowball Stemmer is an enhanced version of the Porter Stemmer which was introduced by Martin Porter as well. It is referred to as Porter2 and is faster and more aggressive than its predecessor. One of the key advantages of this is that it supports multiple languages, making it a multilingual stemmer.</li> </ul> <p>Example:</p> <ul> <li> <p>'running' \u2192 'run'</p> </li> <li> <p>'quickly' \u2192 'quick'</p> </li> </ul> <p>Advantages:</p> <ul> <li> <p>More efficient than Porter Stemmer.</p> </li> <li> <p>Supports multiple languages.</p> </li> </ul> <pre><code>from nltk.stem import SnowballStemmer\n\nstemmer = SnowballStemmer(language='english')\n\nwords_to_stem = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n\nstemmed_words = [stemmer.stem(word) for word in words_to_stem]\n\nprint(\"Original words:\", words_to_stem)\nprint(\"Stemmed words:\", stemmed_words)\n</code></pre> \ud83d\udccc Stopword removal <p>Stopword removal is a process to remove common words from the document. Natural language processing tasks often involve filtering out commonly occurring words that provide no or very little semantic value to text analysis. These words are known as stopwords include articles, prepositions and pronouns like \"the\", \"and\", \"is\" and \"in\". While they seem insignificant, proper stopword handling can dramatically impact the performance and accuracy of NLP applications.</p> <p>When to Remove Stopwords The decision to remove stopwords depends heavily on the specific NLP task at hand:</p> <p>Tasks that benefit from stopword removal:</p> <ul> <li> <p>Text classification and sentiment analysis</p> </li> <li> <p>Information retrieval and search engines</p> </li> <li> <p>Topic modelling and clustering</p> </li> <li> <p>Keyword extraction</p> </li> </ul> <p>Tasks that require preserving stopwords:</p> <ul> <li> <p>Machine translation (maintains grammatical structure)</p> </li> <li> <p>Text summarization (preserves sentence coherence)</p> </li> <li> <p>Question-answering systems (syntactic relationships matter)</p> </li> <li> <p>Grammar checking and parsing</p> </li> </ul> <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Sample text\ntext = \"This is a sample sentence showing stopword removal.\"\n\n# Get English stopwords and tokenize\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text.lower())\n\n# Remove stopwords\nfiltered_tokens = [word for word in tokens if word not in stop_words]\n\nprint(\"Original:\", tokens)\nprint(\"Filtered:\", filtered_tokens)\n</code></pre> \ud83d\udccc Parts of Speech (POS) Tagging <p>Parts of Speech (POS) Tagging assigns a part of speech to each word in sentence based on definition and context.</p> <ul> <li>Parts of Speech (POS) Tagging</li> </ul> \u2705 Text representation Techniques <p>It converts textual data into numerical vectors that are processed by the following methods:</p> \ud83d\udccc Parts of Speech (POS) Tagging <ul> <li> <p>One-Hot Encoding</p> </li> <li> <p>Bag of Words (BOW)</p> </li> </ul> <p>In Natural Language Processing (NLP) text data needs to be converted into numbers so that machine learning algorithms can understand it. One common method to do this is Bag of Words (BoW) model. It turns text like sentence, paragraph or document into a collection of words and counts how often each word appears but ignoring the order of the words. It does not consider the order of the words or their grammar but focuses on counting how often each word appears in the text.</p> <p>This makes it useful for tasks like text classification, sentiment analysis and clustering.</p> <p>Key Components of BoW</p> <ul> <li> <p>Vocabulary: It is a list of all unique words from the entire dataset. Each word in the vocabulary corresponds to a feature in the model.</p> </li> <li> <p>Document Representation: Each document is represented as a vector where each element shows the frequency of the words from the vocabulary in that document. The frequency of each word is used as a feature for the model.</p> </li> </ul> \ud83d\udccc Steps to Implement the Bag of Words (BoW) Model <p>Step 1: Preprocessing the Text</p> <p>Before applying the BoW model, we need to preprocess the text. This includes:</p> <ul> <li> <p>Converting the text to lowercase</p> </li> <li> <p>Removing non-word characters</p> </li> <li> <p>Removing extra spaces</p> </li> </ul> <p>Lets consider a sample text for this implementation:</p> <pre><code>Beans. I was trying to explain to somebody as we were flying in, that's corn. That's beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country and we're lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven't seen in a long time and somehow he has not aged and I have. And it's great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn't speak at the commencement.\n</code></pre> <pre><code>import nltk\nimport re\n\ntext = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that's corn.  That's beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we're lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven't seen in a long time, and somehow he has not aged and I have. And it's great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn't speak at the commencement.\"\"\"\n\ndataset = nltk.sent_tokenize(text)\n\nfor i in range(len(dataset)):\n    dataset[i] = dataset[i].lower()\n    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n\nfor i, sentence in enumerate(dataset):\n    print(f\"Sentence {i+1}: {sentence}\")\n</code></pre> <p>Step 2: Counting Word Frequencies</p> <p>In this step, we count the frequency of each word in the preprocessed text. We will store these counts in a pandas DataFrame to view them easily in a tabular format.</p> <ul> <li> <p>We initialize a dictionary to hold our word counts.</p> </li> <li> <p>Then, we tokenize each sentence into words.</p> </li> <li> <p>For each word, we check if it exists in our dictionary. If it does, we increment its count. If it doesn\u2019t, we add it to the dictionary with a count of 1.</p> </li> </ul> <pre><code>word2count = {}\n\nfor data in dataset:\n    words = nltk.word_tokenize(data)\n    for word in words:\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n\nstop_words = set(stopwords.words('english'))\n\nfiltered_word2count = {word: count for word, count in word2count.items() if word not in stop_words}\n\nword_freq_df = pd.DataFrame(list(filtered_word2count.items()), columns=['Word', 'Frequency'])\n\nword_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n\nprint(word_freq_df)\n</code></pre> <p>Step 3: Selecting the Most Frequent Words</p> <p>Now that we have counted the word frequencies, we will select the top N most frequent words (e.g top 10) to be used in the BoW model. We can visualize these frequent words using a bar chart to understand the distribution of words in our dataset.</p> <pre><code>import heapq\nimport matplotlib.pyplot as plt\n\nfreq_words = heapq.nlargest(10, word2count, key=word2count.get)\n\nprint(f\"Top 10 frequent words: {freq_words}\")\n\ntop_words = sorted(word2count.items(), key=lambda x: x[1], reverse=True)[:10]\nwords, counts = zip(*top_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts, color='skyblue')\nplt.xticks(rotation=45)\nplt.title('Top 10 Most Frequent Words')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.show()\n</code></pre> <p>Step 4: Building the Bag of Words (BoW) Model Now we will build the Bag of Words (BoW) model. This model is represented as a binary matrix where each row corresponds to a sentence and each column represents one of the top N frequent words. A 1 in the matrix shows that the word is present in the sentence and a 0 shows its absence.</p> <p>We will use a heatmap to visualize this binary matrix where green shows the presence of a word (1) and red shows its absence (0).</p> <pre><code>import numpy as np\nimport seaborn as sns\n\nX = []\n\nfor data in dataset:\n    vector = []\n    for word in freq_words:\n        if word in nltk.word_tokenize(data):\n            vector.append(1)\n        else:\n            vector.append(0)\n    X.append(vector)\n\nX = np.asarray(X)\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(X, cmap='RdYlGn', cbar=False, annot=True, fmt=\"d\", xticklabels=freq_words, yticklabels=[f\"Sentence {i+1}\" for i in range(len(dataset))])\n\nplt.title('Bag of Words Matrix')\nplt.xlabel('Frequent Words')\nplt.ylabel('Sentences')\nplt.show()\n</code></pre> <p>Step 5: Visualizing Word Frequencies with a Word Cloud</p> <p>Finally, we can create a Word Cloud to visually represent the word frequencies. In a word cloud, the size of each word is proportional to its frequency which makes it easy to identify the most common words at a glance.</p> <ul> <li>Term Frequency-Inverse Document Frequency (TF-IDF)</li> </ul> <p>TF-IDF (Term Frequency\u2013Inverse Document Frequency) is a statistical method used in natural language processing and information retrieval to evaluate how important a word is to a document in relation to a larger collection of documents. TF-IDF combines two components:</p> <p>1. Term Frequency (TF): Measures how often a word appears in a document. A higher frequency suggests greater importance. If a term appears frequently in a document, it is likely relevant to the document\u2019s content.</p> <p></p> <p>2. Inverse Document Frequency (IDF): Reduces the weight of common words across multiple documents while increasing the weight of rare words. If a term appears in fewer documents, it is more likely to be meaningful and specific.</p> <p></p> <p>This balance allows TF-IDF to highlight terms that are both frequent within a specific document and distinctive across the text document, making it a useful tool for tasks like search ranking, text classification and keyword extraction.</p> <p>Converting Text into vectors with TF-IDF</p> <p>Let's take an example where we have a corpus (a collection of documents) with three documents and our goal is to calculate the TF-IDF score for specific terms in these documents.</p> <ol> <li>Document 1: \"The cat sat on the mat.\"</li> <li>Document 2: \"The dog played in the park.\"</li> <li>Document 3: \"Cats and dogs are great pets.\"</li> </ol> <p>Our goal is to calculate the TF-IDF score for specific terms in these documents. Let\u2019s focus on the word \"cat\" and see how TF-IDF evaluates its importance.</p> <p>Step 1: Calculate Term Frequency (TF)</p> <p>For Document 1:</p> <ul> <li> <p>The word \"cat\" appears 1 time.</p> </li> <li> <p>The total number of terms in Document 1 is 6 (\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\").</p> </li> <li> <p>So, TF(cat,Document 1) = 1/6</p> </li> </ul> <p>For Document 2:</p> <ul> <li> <p>The word \"cat\" does not appear.</p> </li> <li> <p>So, TF(cat,Document 2)=0.</p> </li> </ul> <p>For Document 3:</p> <ul> <li> <p>The word \"cat\" appears 1 time.</p> </li> <li> <p>The total number of terms in Document 3 is 6 (\"cats\", \"and\", \"dogs\", \"are\", \"great\", \"pets\"). So TF (cat,Document 3)=1/6</p> </li> </ul> <p>In Document 1 and Document 3 the word \"cat\" has the same TF score. This means it appears with the same relative frequency in both documents. In Document 2 the TF score is 0 because the word \"cat\" does not appear.</p> <p>Step 2: Calculate Inverse Document Frequency (IDF)</p> <ul> <li> <p>Total number of documents in the corpus (D): 3</p> </li> <li> <p>Number of documents containing the term \"cat\": 2 (Document 1 and Document 3).</p> </li> </ul> <p></p> <p>Step 3: Calculate TF-IDF</p> <p>The TF-IDF score for \"cat\" is 0.029 in Document 1 and Document 3 and 0 in Document 2 that reflects both the frequency of the term in the document (TF) and its rarity across the corpus (IDF).</p> <p>The TF-IDF score is the product of TF and IDF:</p> <p></p> \ud83d\udccc Implementing TF-IDF in Python <p>Step 1: Import modules</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n</code></pre> <p>Step 2: Collect strings from documents and create a corpus</p> <pre><code>d0 = 'Geeks for geeks'\nd1 = 'Geeks'\nd2 = 'r2j'\nstring = [d0, d1, d2]\n</code></pre> <p>Step 3: Get TF-IDF values</p> <p>Here we are using TfidfVectorizer() from scikit learn to perform tf-idf and apply on our courpus using fit_transform.</p> <pre><code>tfidf = TfidfVectorizer()\nresult = tfidf.fit_transform(string)\n</code></pre> <p>Step 4: Display IDF values</p> <pre><code>print('\\nidf values:')\nfor ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n    print(ele1, ':', ele2)\n</code></pre> <p></p> <p>Step 5: Display TF-IDF values along with indexing</p> <pre><code>print('\\nWord indexes:')\nprint(tfidf.vocabulary_)\nprint('\\ntf-idf value:')\nprint(result)\nprint('\\ntf-idf values in matrix form:')\nprint(result.toarray())\n</code></pre> <p></p> <p></p> <ul> <li> <p>N-Gram Language Modeling with NLTK</p> </li> <li> <p>Latent Semantic Analysis (LSA)</p> </li> <li> <p>Latent Dirichlet Allocation (LDA)</p> </li> </ul> \u2705 Text Embedding Techniques <p>It refers to methods that create dense vector representations of text, capturing semantic meaning including advanced approaches like:</p> \ud83d\udccc Word Embedding <ul> <li> <p>Word2Vec (SkipGram, Continuous Bag of Words - CBOW)</p> </li> <li> <p>GloVe (Global Vectors for Word Representation)</p> </li> <li> <p>fastText</p> </li> </ul> \ud83d\udccc Pre-Trained Embedding <ul> <li> <p>ELMo (Embeddings from Language Models)</p> </li> <li> <p>BERT (Bidirectional Encoder Representations from Transformers)</p> </li> </ul> \ud83d\udccc Word Embedding <ul> <li>Doc2Vec</li> </ul> \ud83d\udccc Advanced Embeddings <ul> <li> <p>RoBERTa</p> </li> <li> <p>DistilBERT</p> </li> </ul>"},{"location":"NLP/overview/","title":"Overview","text":"\u2705 Natural Language Processing (NLP) \ud83d\udccc What is Natural Language Processing (NLP)? <p>Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that helps machines to understand and process human languages either in text or audio form. It is used across a variety of applications from speech recognition to language translation and text summarization.</p> \ud83d\udccc Natural Language Processing can be categorized into two components <ol> <li> <p>Natural Language Understanding(NLU): It involves interpreting the meaning of the text.</p> </li> <li> <p>Natural Language Generation(NLG): It involves generating human-like text based on processed data.</p> </li> </ol> \ud83d\udccc Phases of Natural Language Processing <p>It involves a series of phases that work together to process and interpret language with each phase contributing to understanding its structure and meaning.</p> <p></p> \ud83d\udccc Libraries for NLP <p>These are full-featured NLP libraries covering tokenization, tagging, parsing, embeddings, etc.</p> <ul> <li> <p>NLTK (Natural Language Toolkit)</p> </li> <li> <p>spaCy</p> </li> <li> <p>TextBlob</p> </li> <li> <p>Transformers (by Hugging Face)</p> </li> <li> <p>Gensim</p> </li> <li> <p>NLP Libraries in Python.</p> </li> </ul> \ud83d\udccc Normalizing Textual Data in NLP <p>Text Normalization transforms text into a consistent format improves the quality and makes it easier to process in NLP tasks.</p> <ol> <li> <p>Regular Expressions (RE) are sequences of characters that define search patterns.</p> <ul> <li> <p>Text Normalization</p> </li> <li> <p>Regular Expressions (RE)</p> </li> <li> <p>How to write Regular Expressions?</p> </li> <li> <p>Properties of Regular Expressions</p> </li> <li> <p>Email Extraction using RE</p> </li> </ul> </li> <li> <p>Tokenization is a process of splitting text into smaller units called tokens.</p> <ul> <li> <p>Tokenization</p> </li> <li> <p>Word Tokenization</p> </li> <li> <p>Rule-based Tokenization</p> </li> <li> <p>Subword Tokenization</p> </li> <li> <p>Dictionary-Based Tokenization</p> </li> <li> <p>Whitespace Tokenization</p> </li> <li> <p>WordPiece Tokenization</p> </li> </ul> </li> <li> <p>Lemmatization reduces words to their base or root form.</p> <ul> <li>Lemmatization</li> </ul> </li> <li> <p>Stemming reduces works to their root by removing suffixes. Types of stemmers include:</p> <ul> <li> <p>Stemming</p> </li> <li> <p>Porter Stemmer</p> </li> <li> <p>Lancaster Stemmer</p> </li> <li> <p>Snowball Stemmer</p> </li> <li> <p>Rule-based Stemming</p> </li> </ul> </li> <li> <p>Stopword removal is a process to remove common words from the document.</p> <ul> <li>Stopword removal</li> </ul> </li> <li> <p>Parts of Speech (POS) Tagging assigns a part of speech to each word in sentence based on definition and context.</p> <ul> <li>Parts of Speech (POS) Tagging</li> </ul> </li> </ol> \ud83d\udccc Text Representation and Embedding Techniques in NLP <p>Text representation Techniques</p> <p>It converts textual data into numerical vectors that are processed by the following methods:</p> <ul> <li> <p>One-Hot Encoding</p> </li> <li> <p>Bag of Words (BOW)</p> </li> <li> <p>Term Frequency-Inverse Document Frequency (TF-IDF)</p> </li> <li> <p>N-Gram Language Modeling with NLTK</p> </li> <li> <p>Latent Semantic Analysis (LSA)</p> </li> <li> <p>Latent Dirichlet Allocation (LDA)</p> </li> </ul> \ud83d\udccc Text Embedding Techniques <p>It refers to methods that create dense vector representations of text, capturing semantic meaning including advanced approaches like:</p> <ol> <li> <p>Word Embedding</p> <ul> <li> <p>Word2Vec (SkipGram, Continuous Bag of Words - CBOW)</p> </li> <li> <p>GloVe (Global Vectors for Word Representation)</p> </li> <li> <p>fastText</p> </li> </ul> </li> <li> <p>Pre-Trained Embedding</p> <ul> <li> <p>ELMo (Embeddings from Language Models)</p> </li> <li> <p>BERT (Bidirectional Encoder Representations from Transformers)</p> </li> </ul> </li> <li> <p>Document Embedding</p> <ul> <li>Doc2Vec</li> </ul> </li> <li> <p>Advanced Embeddings</p> <ul> <li> <p>RoBERTa</p> </li> <li> <p>DistilBERT</p> </li> </ul> </li> </ol> \ud83d\udccc Deep Learning Techniques for NLP <p>Deep learning has revolutionized Natural Language Processing by helping models to automatically learn complex patterns from raw text.</p> <p>Key deep learning techniques in NLP include:</p> <ul> <li> <p>Deep learning</p> </li> <li> <p>Artificial Neural Networks (ANNs)</p> </li> <li> <p>Recurrent Neural Networks (RNNs)</p> </li> <li> <p>Long Short-Term Memory (LSTM)</p> </li> <li> <p>Gated Recurrent Unit (GRU)</p> </li> <li> <p>Seq2Seq Models</p> </li> <li> <p>Transformer Models</p> </li> </ul> \ud83d\udccc Pre-Trained Language Models <p>Pre-trained models can be fine-tuned for specific tasks:</p> <ul> <li> <p>Pre-trained models</p> </li> <li> <p>GPT (Generative Pre-trained Transformer)</p> </li> <li> <p>Transformers XL</p> </li> <li> <p>T5 (Text-to-Text Transfer Transformer)</p> </li> <li> <p>Transfer Learning with Fine-tuning</p> </li> </ul> \ud83d\udccc Natural Language Processing Tasks <p>Core NLP tasks that help machines understand, interpret and generate human language.</p> <ol> <li> <p>Text Classification</p> <ul> <li> <p>Dataset for Text Classification</p> </li> <li> <p>Text Classification using Naive Bayes</p> </li> <li> <p>Text Classification using Logistic Regression</p> </li> <li> <p>Text Classification using RNNs</p> </li> <li> <p>Text Classification using CNNs</p> </li> </ul> </li> <li> <p>Information Extraction</p> <ul> <li> <p>Named Entity Recognition (NER) using SpaCy</p> </li> <li> <p>Named Entity Recognition (NER) using NLTK</p> </li> <li> <p>Relationship Extraction</p> </li> </ul> </li> <li> <p>Sentiment Analysis</p> <ul> <li> <p>What is Sentiment Analysis?</p> </li> <li> <p>Sentiment Analysis using VADER</p> </li> <li> <p>Sentiment Analysis using Recurrent Neural Networks (RNN)</p> </li> </ul> </li> <li> <p>Machine Translation</p> <ul> <li> <p>Statistical Machine Translation of Language</p> </li> <li> <p>Machine Translation with Transformer</p> </li> </ul> </li> <li> <p>Text Summarization</p> <ul> <li> <p>What is Text Summarization?</p> </li> <li> <p>Text Summarizations using Hugging Face Model</p> </li> <li> <p>Text Summarization using Sumy</p> </li> </ul> </li> <li> <p>Text Generation</p> <ul> <li> <p>Text Generation using Fnet</p> </li> <li> <p>Text Generation using Recurrent Long Short Term Memory Network</p> </li> <li> <p>Text2Text Generations using HuggingFace Model</p> </li> </ul> </li> </ol> \ud83d\udccc Natural Language Processing Chatbots <p>NLP chatbots are computer programs designed to interact with users in natural language helps in seamless communication between humans and machines. By using NLP techniques, these chatbots understand, interpret and generate human language.</p> <ul> <li>What is Natural Language Processing (NLP) Chatbots?</li> </ul> \ud83d\udccc Applications of NLP <ol> <li> <p>Voice Assistants: Alexa, Siri and Google Assistant use NLP for voice recognition and interaction.</p> </li> <li> <p>Grammar and Text Analysis: Tools like Grammarly, Microsoft Word and Google Docs apply NLP for grammar checking.</p> </li> <li> <p>Information Extraction: Search engines like Google and DuckDuckGo use NLP to extract relevant information.</p> </li> <li> <p>Chatbots: Website bots and customer support chatbots leverage NLP for automated conversations.</p> </li> </ol> \ud83d\udccc Importance of NLP <p>Natural Language Processing (NLP) plays an important role in transforming how we interact with technology and understand data. Below are reasons why it\u2019s so important:</p> <ol> <li> <p>Information Extraction: Extracts useful data from unstructured content.</p> </li> <li> <p>Sentiment Analysis: Analyzes customer opinions for businesses.</p> </li> <li> <p>Automation: Streamlines tasks like customer service and document processing.</p> </li> <li> <p>Language Translation: Breaks down language barriers with tools like Google Translate.</p> </li> <li> <p>Healthcare: Assists in analyzing medical records and research.</p> </li> </ol>"},{"location":"Notebook/allnotebook/","title":"All Notebook","text":"\u2705 Notebook Download link 1. Cross Validation Notebook <p>Cross Validation</p> 2. GridSearchCV and RandomizedSearchCV Notebook <p>GridSearchCV and RandomizedSearchCV</p>"},{"location":"OpenCV/OpenCV/","title":"OpenCV (Open Source Computer Vision Library)","text":"<p>OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning library. It allows us to process images and videos, detect objects, faces and even handwriting.</p>"},{"location":"OpenCV/OpenCV/#why-opencv","title":"Why OpenCV?","text":"<ol> <li> <p>Comprehensive Image Processing: OpenCV has a range of functions to manipulate and analyze images helps in making it ideal for various applications.</p> </li> <li> <p>Real-Time Video Processing: It supports video capture and real-time video processing.</p> </li> <li> <p>Cross-Platform: Works on multiple platforms like Windows, Linux, macOS and Android.</p> </li> <li> <p>Open-Source: It is free to use and has a large community support.</p> </li> <li> <p>Integration with Deep Learning: It integrates with popular deep learning libraries like TensorFlow and PyTorch.</p> </li> </ol> <p>OpenCV is one of the most popular computer vision libraries.</p>"},{"location":"OpenCV/OpenCV/#important-concepts-of-opencv","title":"Important concepts of OpenCV","text":"<ol> <li>Reading an image</li> <li>Extracting the RGB values of a pixel</li> <li>Extracting the Region of Interest (ROI)</li> <li>Resizing the Image</li> <li>Rotating the Image</li> <li>Drawing a Rectangle</li> <li>Displaying text</li> </ol> <p>Install the OpenCV library using the following command:</p> <p><code>pip install opencv-python</code></p>"},{"location":"OpenCV/OpenCV/#reading-an-image","title":"Reading an Image","text":"<p>First of all, we will import <code>cv2</code> module and then read the input image using cv2\u2019s <code>imread()</code> method. Then extract the <code>height</code> and <code>width</code> of the image.</p> <pre><code># Importing the OpenCV library\nimport cv2\n\n# Reading the image using imread() function\nimage = cv2.imread('/content/road.jpg')\n\n# Extracting the height and width of an image\nh, w, c = image.shape[:3]\n\n# Image shape\nprint(f\"Image shape: {image.shape}\")\n\n# Displaying the height and width\nprint(\"Height = {}, Width = {}, Channels = {}\".format(h, w, c))\n\n# pixel values\nprint(\"Image pixel details: {}\".format(image))\n</code></pre>"},{"location":"OpenCV/OpenCV/#extracting-the-rgb-values-of-a-pixel","title":"Extracting the <code>RGB</code> Values of a <code>Pixel</code>","text":"<p>Now we will focus on extracting the <code>RGB values</code> of an <code>individual pixel</code>. <code>OpenCV arranges the channels in BGR order</code>. So the <code>0th</code> <code>value</code> will correspond to the <code>Blue pixel</code> and <code>not the Red</code>.</p> <pre><code># Extracting RGB values.\n# Here we have randomly chosen a pixel\n# by passing in 100, 100 for height and width.\n\n(B, G, R) = image[100, 100]\n\n# Displaying the pixel values\nprint(\"R = {}, G = {}, B = {}\".format(R, G, B))\n\n# We can also pass the channel to extract\n# the value for a specific channel\nB = image[100, 100, 0]\nG = image[100, 100, 1]\nR = image[100, 100, 2]\n\nprint(\"B = {}\".format(B))\nprint(\"G = {}\".format(G))\nprint(\"R = {}\".format(R))\n</code></pre>"},{"location":"OpenCV/OpenCV/#extracting-the-region-of-interest-roi","title":"Extracting the <code>Region of Interest (ROI)</code>","text":"<p>Sometimes we want to extract a particular part or region of an image. This can be done by slicing the pixels of the image.</p> <pre><code># We will calculate the region of interest\n# by slicing the pixels of the image\n\nfrom google.colab.patches import cv2_imshow\n\nroi = image[100 : 500, 200 : 700]\ncv2_imshow(roi)\ncv2.waitKey(0)\n</code></pre>"},{"location":"OpenCV/OpenCV/#opencv-cv2-complete-functional-overview","title":"\ud83d\udce6 OpenCV (cv2) \u2013 Complete Functional Overview","text":""},{"location":"OpenCV/OpenCV/#1-image-video-io","title":"1\ufe0f\u20e3 Image &amp; Video I/O","text":"<p>Used to read, write, display images/videos.</p> Function Description <code>cv2.imread()</code> Read image <code>cv2.imwrite()</code> Save image <code>cv2.imshow()</code> Show image <code>cv2.waitKey()</code> Keyboard wait <code>cv2.destroyAllWindows()</code> Close windows <code>cv2.VideoCapture()</code> Read video / webcam <code>cv2.VideoWriter()</code> Save video"},{"location":"OpenCV/OpenCV/#2-image-properties-pixel-operations","title":"2\ufe0f\u20e3 Image Properties &amp; Pixel Operations","text":"Function Description <code>image.shape</code> Image dimensions <code>image.dtype</code> Data type <code>image.size</code> Total pixels <code>cv2.split()</code> Split channels <code>cv2.merge()</code> Merge channels <code>cv2.copyMakeBorder()</code> Add padding"},{"location":"OpenCV/OpenCV/#3-color-space-conversions","title":"3\ufe0f\u20e3 Color Space Conversions","text":"Conversion Function BGR \u2192 Gray <code>cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</code> BGR \u2192 RGB <code>cv2.COLOR_BGR2RGB</code> BGR \u2192 HSV <code>cv2.COLOR_BGR2HSV</code> BGR \u2192 LAB <code>cv2.COLOR_BGR2LAB</code> Gray \u2192 BGR <code>cv2.COLOR_GRAY2BGR</code>"},{"location":"OpenCV/OpenCV/#4-image-resizing-cropping-geometry","title":"4\ufe0f\u20e3 Image Resizing, Cropping &amp; Geometry","text":"Function Description <code>cv2.resize()</code> Resize image <code>cv2.getRotationMatrix2D()</code> Rotation matrix <code>cv2.warpAffine()</code> Rotate / translate <code>cv2.warpPerspective()</code> Perspective transform <code>cv2.flip()</code> Flip image ROI slicing <code>img[y1:y2, x1:x2]</code>"},{"location":"OpenCV/OpenCV/#5-image-filtering-smoothing","title":"5\ufe0f\u20e3 Image Filtering &amp; Smoothing","text":"Function Description <code>cv2.blur()</code> Average blur <code>cv2.GaussianBlur()</code> Gaussian blur <code>cv2.medianBlur()</code> Median filter <code>cv2.bilateralFilter()</code> Edge-preserving blur"},{"location":"OpenCV/OpenCV/#6-thresholding-binarization","title":"6\ufe0f\u20e3 Thresholding &amp; Binarization","text":"Function Description <code>cv2.threshold()</code> Global threshold <code>cv2.adaptiveThreshold()</code> Adaptive threshold <code>cv2.THRESH_BINARY</code> Binary <code>cv2.THRESH_OTSU</code> Otsu\u2019s method"},{"location":"OpenCV/OpenCV/#7-edge-detection","title":"7\ufe0f\u20e3 Edge Detection","text":"Function Description <code>cv2.Canny()</code> Edge detection <code>cv2.Sobel()</code> Gradient edges <code>cv2.Laplacian()</code> Second-order edges"},{"location":"OpenCV/OpenCV/#8-morphological-operations","title":"8\ufe0f\u20e3 Morphological Operations","text":"Function Description <code>cv2.erode()</code> Shrink objects <code>cv2.dilate()</code> Expand objects <code>cv2.morphologyEx()</code> Advanced ops <code>cv2.MORPH_OPEN</code> Remove noise <code>cv2.MORPH_CLOSE</code> Fill holes"},{"location":"OpenCV/OpenCV/#9-contours-shape-analysis","title":"9\ufe0f\u20e3 Contours &amp; Shape Analysis","text":"Function Description <code>cv2.findContours()</code> Detect contours <code>cv2.drawContours()</code> Draw contours <code>cv2.contourArea()</code> Area <code>cv2.arcLength()</code> Perimeter <code>cv2.boundingRect()</code> Bounding box <code>cv2.minAreaRect()</code> Rotated box <code>cv2.approxPolyDP()</code> Polygon approx"},{"location":"OpenCV/OpenCV/#feature-detection-matching","title":"\ud83d\udd1f Feature Detection &amp; Matching","text":"Algorithm Function Harris Corner <code>cv2.cornerHarris()</code> ORB <code>cv2.ORB_create()</code> SIFT <code>cv2.SIFT_create()</code> FAST <code>cv2.FastFeatureDetector_create()</code> BF Matcher <code>cv2.BFMatcher()</code> FLANN <code>cv2.FlannBasedMatcher()</code>"},{"location":"OpenCV/OpenCV/#11-object-detection-classical-cv","title":"1\ufe0f\u20e31\ufe0f\u20e3 Object Detection (Classical CV)","text":"Method Function Haar Cascade <code>cv2.CascadeClassifier()</code> HOG Detector <code>cv2.HOGDescriptor()</code> Template Matching <code>cv2.matchTemplate()</code>"},{"location":"OpenCV/OpenCV/#12-background-subtraction-motion","title":"1\ufe0f\u20e32\ufe0f\u20e3 Background Subtraction &amp; Motion","text":"Function Description <code>cv2.absdiff()</code> Frame difference <code>cv2.createBackgroundSubtractorMOG2()</code> Motion detection <code>cv2.calcOpticalFlowFarneback()</code> Optical flow"},{"location":"OpenCV/OpenCV/#13-drawing-functions","title":"1\ufe0f\u20e33\ufe0f\u20e3 Drawing Functions","text":"Function Description <code>cv2.line()</code> Draw line <code>cv2.rectangle()</code> Draw rectangle <code>cv2.circle()</code> Draw circle <code>cv2.putText()</code> Write text <code>cv2.polylines()</code> Draw polygons"},{"location":"OpenCV/OpenCV/#14-histogram-image-statistics","title":"1\ufe0f\u20e34\ufe0f\u20e3 Histogram &amp; Image Statistics","text":"Function Description <code>cv2.calcHist()</code> Histogram <code>cv2.equalizeHist()</code> Contrast <code>cv2.normalize()</code> Normalize data"},{"location":"OpenCV/OpenCV/#15-camera-calibration-geometry","title":"1\ufe0f\u20e35\ufe0f\u20e3 Camera Calibration &amp; Geometry","text":"Function Description <code>cv2.findChessboardCorners()</code> Calibration <code>cv2.calibrateCamera()</code> Camera params <code>cv2.undistort()</code> Remove distortion"},{"location":"OpenCV/OpenCV/#16-deep-learning-dnn-module","title":"1\ufe0f\u20e36\ufe0f\u20e3 Deep Learning (DNN Module)","text":"Function Description <code>cv2.dnn.readNet()</code> Load model <code>cv2.dnn.readNetFromONNX()</code> ONNX <code>cv2.dnn.blobFromImage()</code> Preprocess <code>net.forward()</code> Inference"},{"location":"OpenCV/OpenCV/#17-video-analysis-utilities","title":"1\ufe0f\u20e37\ufe0f\u20e3 Video Analysis Utilities","text":"Function Description <code>cv2.createTrackbar()</code> UI control <code>cv2.getTickCount()</code> FPS <code>cv2.getTickFrequency()</code> Timing"},{"location":"OpenCV/OpenCV/#most-used-functions-real-projects","title":"MOST USED FUNCTIONS (REAL PROJECTS)","text":"<pre><code>cv2.imread\ncv2.resize\ncv2.cvtColor\ncv2.GaussianBlur\ncv2.Canny\ncv2.findContours\ncv2.boundingRect\ncv2.putText\ncv2.VideoCapture\ncv2.dnn.readNet\n</code></pre>"},{"location":"OpenCV/OpenCV/#image-processing-and-enhancement","title":"Image Processing and Enhancement","text":""},{"location":"OpenCV/OpenCV/#blurring-an-image","title":"Blurring an Image","text":"<p>Image blurring is a technique used in image processing to reduce sharpness and detail making an image appear smoother.</p> <p>This is done by applying filters also called low-pass filters that reduce high-frequency noise and smooth finer details. Blurring is used for tasks like noise reduction, edge smoothing or creating artistic effects.</p> <p>It works by averaging the pixel values around each pixel, softening the image in the process. It\u2019s useful in scenarios where minimizing noise or reducing sharpness is necessary such as in preparing images for computer vision models or applying a soft, artistic effect.</p>"},{"location":"OpenCV/OpenCV/#types-of-blurring","title":"Types of Blurring","text":"<p>We can apply various blurring techniques based on the desired effect.</p> <p>1. Gaussian Blurring</p> <p>Gaussian blur works by applying a Gaussian function to an image, resulting in a smooth blur. It\u2019s useful for noise reduction and detail reduction in images. It is used as a preprocessing step for machine learning and deep learning models.</p> <p><code>cv2.COLOR_BGR2RGB</code></p> <pre><code>import cv2\nfrom matplotlib import pyplot as plt\n\nimage_path = '/content/cats_dogs.jpg'\nimage = cv2.imread(image_path)\nresized_image = cv2.resize(image, (1900, 800))\nresized_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\nplt.imshow(resized_image_rgb)\nplt.title('Original Image')\nplt.axis('off')\nplt.show()\n</code></pre> <p><code>cv2.GaussianBlur</code> <code>cv2.cvtColor(Gaussian, cv2.COLOR_BGR2RGB)</code></p> <pre><code>Gaussian = cv2.GaussianBlur(resized_image, (15, 15), 0)  \nGaussian_rgb = cv2.cvtColor(Gaussian, cv2.COLOR_BGR2RGB)  \nplt.imshow(Gaussian_rgb)\nplt.title('Gaussian Blurred Image')\nplt.axis('off')\nplt.show()\n</code></pre> <p>2. Median Blur</p> <p>Median blur is a non-linear filter which means it doesn't average the pixel values. Instead, it replaces each pixel with the median value of its neighboring pixels. This technique is useful for removing salt-and-pepper noise (random black and white pixels) while keeping the edges intact.</p> <p><code>cv2.medianBlur</code> <code>cv2.cvtColor(median, cv2.COLOR_BGR2RGB)</code></p> <pre><code>median = cv2.medianBlur(resized_image, 11)  \nmedian_rgb = cv2.cvtColor(median, cv2.COLOR_BGR2RGB)  \n\nplt.imshow(median_rgb)\nplt.title('Median Blurred Image')\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"OpenCV/OpenCV/#3-bilateral-blur","title":"3. Bilateral Blur","text":"<p>The bilateral filter is a more advanced technique that smooths the image while preserving edges. It calculates a weighted average based on both the spatial distance and the pixel intensity. This means that it will blur areas with similar colors and preserve sharp edges, making it useful for noise reduction without sacrificing important details.</p> <p><code>cv2.bilateralFilter</code> <code>cv2.cvtColor(bilateral, cv2.COLOR_BGR2RGB)</code></p> <pre><code>bilateral = cv2.bilateralFilter(resized_image, 15, 150, 150)  \nbilateral_rgb = cv2.cvtColor(bilateral, cv2.COLOR_BGR2RGB)  \n\nplt.imshow(bilateral_rgb)\nplt.title('Bilateral Blurred Image')\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"OpenCV/OpenCV/#challenges-of-image-blurring","title":"Challenges of Image Blurring","text":"<ol> <li> <p>Loss of Detail: While blurring can be useful, it may also blur important details in an image, reducing its overall clarity and making analysis difficult in certain contexts.</p> </li> <li> <p>Over-blurring: Overuse of blurring techniques can lead to overly soft or unrealistic images, losing sharpness and making it harder to identify key elements.</p> </li> <li> <p>Computational Cost: Advanced blurring methods such as bilateral filtering, may require significant computational resources, especially for large images or real-time applications.</p> </li> </ol>"},{"location":"OpenCV/OpenCV/#grayscaling-of-images-using-opencv","title":"Grayscaling of Images using OpenCV","text":"<p>Grayscaling is the process of converting an image from other color spaces e.g. RGB, CMYK, HSV, etc. to shades of gray. It varies between complete black and complete white.</p> <p>Importance of grayscaling</p> <ul> <li> <p>Fewer dimensions: RGB images have three channels, while grayscale images have only one.</p> </li> <li> <p>Simpler models: Less input data reduces complexity and speeds up training.</p> </li> <li> <p>Algorithm-ready: Some methods, such as Canny edge detection, work only on grayscale images.</p> </li> </ul> <p>Let's learn the different image processing methods to convert a colored image into a grayscale image.</p> <p>Method 1: Using the <code>cv2.cvtColor()</code> function</p> <p><code>cv2.COLOR_BGR2GRAY</code> <code>cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code> <code>cv2.imshow('Grayscale', gray_image)</code></p> <pre><code>import cv2\n\nimage = cv2.imread('tomatoes.jpg')\n\ngray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\ncv2.imshow('Grayscale', gray_image)\ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre> <p>Explanation:</p> <ul> <li> <p>cv2.imread(): Reads the image file (OpenCV loads color images as BGR by default).</p> </li> <li> <p>cv2.cvtColor(): Converts color spaces; <code>cv2.COLOR_BGR2GRAY</code> produces a single-channel grayscale image.</p> </li> <li> <p>cv2.imshow(): Opens a window to display the image.</p> </li> </ul> <p>Method 2: Using the cv2.imread() function with flag=zero</p> <p>In this method, we can directly load an image in grayscale mode by passing the flag 0 to cv2.imread(). This saves us from having to convert the image separately after loading.</p> <pre><code>import cv2\n\nimg = cv2.imread('tomatoes.jpg', 0)\n\ncv2.imshow('Grayscale Image', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>Explanation:</p> <ul> <li> <p>cv2.imread(path, 0): Reads the image directly in grayscale (single-channel).</p> </li> <li> <p>cv2.imshow(): Displays the single-channel image.</p> </li> <li> <p>cv2.waitKey() / cv2.destroyAllWindows(): Controls display lifetime.</p> </li> </ul> <p>Method 3.1 Weighted Method (Recommended)</p> <p>This method uses standard luminance weights (0.2989R + 0.5870G + 0.1140B) to account for human visual sensitivity\u2014more to green, less to red, least to blue. It produces a more realistic and visually accurate grayscale image.</p> <pre><code>import cv2\n\nimg_weighted = cv2.imread('tomatoes.jpg')\nrows, cols = img_weighted.shape[:2]\n\nfor i in range(rows):\n    for j in range(cols):\n        gray = 0.2989 * img_weighted[i, j][2] + 0.5870 * img_weighted[i, j][1] + 0.1140 * img_weighted[i, j][0]\n        img_weighted[i, j] = [gray, gray, gray]\n\ncv2.imshow('Grayscale Image (Weighted)', img_weighted)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>Explanation:</p> <ul> <li> <p>rows, cols = img_weighted.shape[:2]: extracts image height (rows) and width (cols) from the shape.</p> </li> <li> <p>**Nested for loops are used to iterate over every pixel in the image.</p> </li> <li> <p>Weighted formula: Gray = 0.2989R + 0.5870G + 0.1140*B (indices: R=[2], G=[1], B=[0] in OpenCV).</p> </li> <li> <p>Assigning [gray, gray, gray] preserves a 3-channel image for display.</p> </li> <li> <p>This produces visually more accurate grayscale than simple averaging, but is still slower than OpenCV\u2019s native conversions.</p> </li> </ul>"},{"location":"OpenCV/OpenCV/#method-32-using-the-pixel-manipulation-average-method","title":"Method 3.2: Using the pixel manipulation (Average method)","text":"<p>This method converts an image to grayscale by averaging the contributions of color channels (RGB). It\u2019s a simple approach but not very accurate because it treats all colors equally, ignoring how the human eye perceives brightness.</p> <pre><code>import cv2\n\nimg = cv2.imread('C:\\\\Documents\\\\full_path\\\\tomatoes.jpg')\nrows, cols = img.shape[:2]\n\nfor i in range(rows):\n    for j in range(cols):\n        gray = (img[i, j, 0] + img[i, j, 1] + img[i, j, 2]) / 3\n        img[i, j] = [gray, gray, gray]\n\ncv2.imshow('Grayscale Image (Average)', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/OpenCV/#image-processing-in-python","title":"Image Processing in Python","text":"<p>1. Image Resizing</p> <p>Image Resizing refers to the process of changing the dimensions of an image. This can involve either enlarging or reducing the size of an image while preserving its content. Resizing is often used in image processing to make images fit specific dimensions for display on different devices or for further analysis. The cv2.resize() function is used for this task. Here:</p> <ul> <li><code>cv2.resize():</code> Resizes the image to new dimensions.</li> <li><code>cv2.INTER_CUBIC:</code> Provides high-quality enlargement.</li> <li><code>cv2.INTER_AREA:</code> Works best for downscaling.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nscale_factor_1 = 3.0  \nscale_factor_2 = 1/3.0\nheight, width = image_rgb.shape[:2]\nnew_height = int(height * scale_factor_1)\nnew_width = int(width * scale_factor_1)\n\nzoomed_image = cv2.resize(src =image_rgb, \n                          dsize=(new_width, new_height), \n                          interpolation=cv2.INTER_CUBIC)\n\nnew_height1 = int(height * scale_factor_2)\nnew_width1 = int(width * scale_factor_2)\nscaled_image = cv2.resize(src= image_rgb, \n                          dsize =(new_width1, new_height1), \n                          interpolation=cv2.INTER_AREA)\n\nfig, axs = plt.subplots(1, 3, figsize=(10, 4))\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image Shape:'+str(image_rgb.shape))\naxs[1].imshow(zoomed_image)\naxs[1].set_title('Zoomed Image Shape:'+str(zoomed_image.shape))\naxs[2].imshow(scaled_image)\naxs[2].set_title('Scaled Image Shape:'+str(scaled_image.shape))\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>2. Image Rotation Images can be rotated to any degree clockwise or anticlockwise using image rotation. We just need to define rotation matrix listing rotation point, degree of rotation and the scaling factor. Here:</p> <ul> <li><code>cv2.getRotationMatrix2D() :</code> generates the transformation matrix.</li> <li><code>cv2.warpAffine() :</code> applies the rotation.</li> <li>A <code>positive angle</code> rotates the image clockwise; a <code>negative angle</code> rotates it counterclockwise.</li> <li>The <code>scale factor</code> adjusts the image size.</li> </ul> <pre><code>import cv2\nimport matplotlib.pyplot as plt\nimg = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ncenter = (image_rgb.shape[1] // 2, image_rgb.shape[0] // 2)\nangle = 30\nscale = 1\nrotation_matrix = cv2.getRotationMatrix2D(center, angle, scale)\nrotated_image = cv2.warpAffine(image_rgb, rotation_matrix, (img.shape[1], img.shape[0]))\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image')\naxs[1].imshow(rotated_image)\naxs[1].set_title('Image Rotation')\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>3. Image Translation Image Translation is the process of moving an image from one position to another within a specified frame of reference. This shift can occur along the x-axis (horizontal movement) and y-axis (vertical movement) without altering the content or orientation of the image. Here:</p> <ul> <li><code>cv2.warpAffine()</code> shifts the image based on translation values.</li> <li><code>tx</code>, <code>ty</code> define the movement along the x and y axes.</li> </ul> <pre><code>import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimg = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nwidth, height = image_rgb.shape[1], image_rgb.shape[0]\n\ntx, ty = 100, 70\ntranslation_matrix = np.array([[1, 0, tx], [0, 1, ty]], dtype=np.float32)\ntranslated_image = cv2.warpAffine(image_rgb, translation_matrix, (width, height))\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\naxs[0].imshow(image_rgb), axs[0].set_title('Original Image')\naxs[1].imshow(translated_image), axs[1].set_title('Image Translation')\n\nfor ax in axs:\n    ax.set_xticks([]), ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>4. Image Shearing</p> <p>Image Shearing is a geometric transformation that distorts or skews an image along one or both axes. This operation slants the image creating a shear effect without changing its area or shape. Shearing can be applied to make the image appear as if it\u2019s being stretched or compressed in a particular direction. Here:</p> <ul> <li><code>shear_x</code>, <code>shear_y</code> control the degree of skewing.</li> <li><code>cv2.warpAffine()</code> applies the transformation.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nwidth, height = image_rgb.shape[1], image_rgb.shape[0]\n\nshearX, shearY = -0.15, 0\ntransformation_matrix = np.array([[1, shearX, 0], [0, 1, shearY]], dtype=np.float32)\nsheared_image = cv2.warpAffine(image_rgb, transformation_matrix, (width, height))\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\naxs[0].imshow(image_rgb), axs[0].set_title('Original Image')\naxs[1].imshow(sheared_image), axs[1].set_title('Sheared Image')\n\nfor ax in axs:\n    ax.set_xticks([]), ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>5. Image Normalization Image Normalization scales pixel values to a specific range to enhance image processing tasks. Here:</p> <ul> <li><code>cv2.normalize():</code> Normalizes pixel values.</li> <li><code>cv2.NORM_MINMAX:</code> Scales values between 0 and 1.</li> <li><code>cv2.merge():</code> Combines separately normalized RGB channels.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nb, g, r = cv2.split(image_rgb)\n\nb_normalized = cv2.normalize(b.astype('float'), None, 0, 1, cv2.NORM_MINMAX)\ng_normalized = cv2.normalize(g.astype('float'), None, 0, 1, cv2.NORM_MINMAX)\nr_normalized = cv2.normalize(r.astype('float'), None, 0, 1, cv2.NORM_MINMAX)\n\nnormalized_image = cv2.merge((b_normalized, g_normalized, r_normalized))\nprint(normalized_image[:, :, 0])\n\nplt.imshow(normalized_image)\nplt.xticks([]), \nplt.yticks([]), \nplt.title('Normalized Image')\nplt.show()\n</code></pre> <p>6. Edge detection of Image Edge detection is used to find sharp edges withing image to find different objects and boundaries within a image. Canny Edge Detection is a popular edge detection method. Here:</p> <ul> <li><code>cv2.GaussianBlur():</code> Removes noise through Gaussian smoothing.</li> <li><code>cv2.Sobel():</code> Computes the gradient of the image.</li> <li><code>cv2.Canny():</code> Applies non-maximum suppression and hysteresis thresholding to detect edges.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimg = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nedges = cv2.Canny(image_rgb, 100, 700)\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\naxs[0].imshow(image_rgb), axs[0].set_title('Original Image')\naxs[1].imshow(edges), axs[1].set_title('Image Edges')\n\nfor ax in axs:\n    ax.set_xticks([]), ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>7. Image Blurring Image Blurring reduces image detail by averaging pixel values. Here:</p> <ul> <li><code>cv2.GaussianBlur():</code> Smooths using a Gaussian kernel.</li> <li><code>cv2.medianBlur():</code> Replaces pixels with the median value in a neighborhood.</li> <li><code>cv2.bilateralFilter():</code> Preserves edges while blurring.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread('Ganeshji.webp')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nblurred = cv2.GaussianBlur(image, (3, 3), 0)\nblurred_rgb = cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB)\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 4))\naxs[0].imshow(image_rgb), axs[0].set_title('Original Image')\naxs[1].imshow(blurred_rgb), axs[1].set_title('Blurred Image')\n\nfor ax in axs:\n    ax.set_xticks([]), ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>8. Morphological Image Processing Morphological Image Processing involves techniques that process the structure or shape of objects in an image. It focuses on operations like dilation, erosion, opening and closing which modify the image's geometric features. These operations work by examining the image\u2019s pixels in relation to their neighbors usually with a small mask or kernel.Here:</p> <ul> <li><code>cv2.dilate():</code> Expands object boundaries.</li> <li><code>cv2.erode():</code> Shrinks object boundaries.</li> <li><code>cv2.morphologyEx()</code> with <code>cv2.MORPH_OPEN:</code> Removes small noise.</li> <li><code>cv2.morphologyEx()</code> with <code>cv2.MORPH_CLOSE:</code> Fills small holes.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage = cv2.imread('Ganeshji.webp')\nimage_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nkernel = np.ones((3, 3), np.uint8)\n\ndilated = cv2.dilate(image_gray, kernel, iterations=2)\neroded = cv2.erode(image_gray, kernel, iterations=2)\nopening = cv2.morphologyEx(image_gray, cv2.MORPH_OPEN, kernel)\nclosing = cv2.morphologyEx(image_gray, cv2.MORPH_CLOSE, kernel)\n\nfig, axs = plt.subplots(2, 2, figsize=(7, 7))\naxs[0, 0].imshow(dilated, cmap='Greys'), axs[0, 0].set_title('Dilated Image')\naxs[0, 1].imshow(eroded, cmap='Greys'), axs[0, 1].set_title('Eroded Image')\naxs[1, 0].imshow(opening, cmap='Greys'), axs[1, 0].set_title('Opening')\naxs[1, 1].imshow(closing, cmap='Greys'), axs[1, 1].set_title('Closing')\n\nfor ax in axs.flatten():\n    ax.set_xticks([]), ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"OpenCV/OpenCV/#intensity-transformation-operations-on-images","title":"Intensity Transformation Operations on Images","text":"<p>Intensity transformations are applied on images for contrast manipulation or image thresholding. These are in the spatial domain, i.e. they are performed directly on the pixels of the image at hand, as opposed to being performed on the Fourier transform of the image. The following are commonly used intensity transformations:</p> <ol> <li>Image Negatives (Linear)</li> <li>Log Transformations</li> <li>Power-Law (Gamma) Transformations</li> <li>Piecewise-Linear Transformation Functions</li> </ol> <p></p> <p>Spatial Domain Processes - Spatial domain processes can be described using the equation:</p> <p></p> <p>Log Transformations - Mathematically, log transformations can be expressed as s = clog(1+r). Here, s is the output intensity, r&gt;=0 is the input intensity of the pixel, and c is a scaling constant. c is given by 255/(log (1 + m)), where m is the maximum pixel value in the image. It is done to ensure that the final pixel value does not exceed (L-1), or 255. Practically, log transformation maps a narrow range of low-intensity input values to a wide range of output values. Consider the following input image.</p> <pre><code>import cv2\nimport numpy as np\n\n# Open the image.\nimg = cv2.imread('sample.jpg')\n\n# Apply log transform.\nc = 255/(np.log(1 + np.max(img)))\nlog_transformed = c * np.log(1 + img)\n\n# Specify the data type.\nlog_transformed = np.array(log_transformed, dtype = np.uint8)\n\n# Save the output.\ncv2.imwrite('log_transformed.jpg', log_transformed)\n</code></pre> <p>Power-Law (Gamma) Transformation - Power-law (gamma) transformations can be mathematically expressed as s=cr\u03b3 . Gamma correction is important for displaying images on a screen correctly, to prevent bleaching or darkening of images when viewed from different types of monitors with different display settings. This is done because our eyes perceive images in a gamma-shaped curve, whereas cameras capture images in a linear fashion.</p> <pre><code>import cv2\nimport numpy as np\n\n# Open the image.\nimg = cv2.imread('sample.jpg')\n\n# Trying 4 gamma values.\nfor gamma in [0.1, 0.5, 1.2, 2.2]:\n\n    # Apply gamma correction.\n    gamma_corrected = np.array(255*(img / 255) ** gamma, dtype = 'uint8')\n\n    # Save edited images.\n    cv2.imwrite('gamma_transformed'+str(gamma)+'.jpg', gamma_corrected)\n</code></pre> <p>Below are the gamma-corrected outputs for different values of gamma. </p> <p>Gamma = 0.1:</p> <p></p> <p>Gamma = 0.5:</p> <p></p> <p>Gamma = 1.2:</p> <p></p> <p>Gamma = 2.2:</p> <p></p> <p>As can be observed from the outputs as well as the graph, gamma&gt;1 (indicated by the curve corresponding to 'nth power' label on the graph), the intensity of pixels decreases i.e. the image becomes darker. On the other hand, gamma&lt;1 (indicated by the curve corresponding to 'nth root' label on the graph), the intensity increases i.e. the image becomes lighter.</p> <p>Piecewise-Linear Transformation Functions -</p> <p>These functions, as the name suggests, are not entirely linear in nature. However, they are linear between certain x-intervals. One of the most commonly used piecewise-linear transformation functions is contrast stretching. Contrast can be defined as:</p> <p><code>Contrast =  (I_max - I_min)/(I_max + I_min)</code></p> <p>This process expands the range of intensity levels in an image so that it spans the full intensity of the camera/display. The figure below shows the graph corresponding to the contrast stretching.</p> <p></p> <p>With (r1, s1), (r2, s2) as parameters, the function stretches the intensity levels by essentially decreasing the intensity of the dark pixels and increasing the intensity of the light pixels. If r1 = s1 = 0 and r2 = s2 = L-1, the function becomes a straight dotted line in the graph (which gives no effect). The function is monotonically increasing so that the order of intensity levels between pixels is preserved. Below is the Python code to perform contrast stretching.</p> <pre><code>import cv2\nimport numpy as np\n\n# Function to map each intensity level to output intensity level.\ndef pixelVal(pix, r1, s1, r2, s2):\n    if (0 &lt;= pix and pix &lt;= r1):\n        return (s1 / r1)*pix\n    elif (r1 &lt; pix and pix &lt;= r2):\n        return ((s2 - s1)/(r2 - r1)) * (pix - r1) + s1\n    else:\n        return ((255 - s2)/(255 - r2)) * (pix - r2) + s2\n\n# Open the image.\nimg = cv2.imread('sample.jpg')\n\n# Define parameters.\nr1 = 70\ns1 = 0\nr2 = 140\ns2 = 255\n\n# Vectorize the function to apply it to each value in the Numpy array.\npixelVal_vec = np.vectorize(pixelVal)\n\n# Apply contrast stretching.\ncontrast_stretched = pixelVal_vec(img, r1, s1, r2, s2)\n\n# Save edited image.\ncv2.imwrite('contrast_stretch.jpg', contrast_stretched)\n</code></pre>"},{"location":"OpenCV/OpenCV/#image-translation-using-opencv","title":"Image Translation using OpenCV","text":"<p>Image translation is the process of shifting an image from one position to another. We simply move the entire image by a fixed number of pixels, either horizontally (along the x-axis) or vertically (along the y-axis). This technique is important in various computer vision tasks such as object tracking, image alignment and creating animations. We achieve this by using a transformation matrix which helps shift the image without distorting its content. In this article, we'll see image translation, how to perform it and other core concepts.</p> <p>Key Concepts in Image Translation Let's see key concepts in Image Translation which are as follows:</p> <p>1. Translation Matrix</p> <p>The translation matrix is used to define how much an image should be shifted. It is a 2x3 matrix that specifies the amount of horizontal and vertical shifts. The matrix looks like this:</p> <p></p> <p>where:</p> <ul> <li>Tx is the horizontal shift (in pixels).</li> <li>Ty is the vertical shift (in pixels).</li> </ul> <p>This matrix is used to move every pixel in the image by the specified amount without distorting its content.</p> <p>2. OpenCV Function OpenCV provides the <code>cv2.wrapAffine()</code> function to apply affine transformations like translation. This function uses the translation matrix to shift the image by the specified values of Tx and Ty.</p> <p>Syntax:</p> <p><code>cv2.warpAffine(img, M, (w, h))</code></p> <p>Parameters:</p> <ul> <li><code>img:</code> Image to be shifted.</li> <li><code>M:</code> The translation matrix that defines how the image will be moved.</li> <li><code>(w, h):</code> Width and height of the image after translation.</li> </ul> <pre><code>Note: A positive value for tx will shift the image to the right, while a negative value for tx will shift the image to the left and a positive value for ty will shift the image down, while a negative value for ty will shift the image up.\n</code></pre> <p>Example 1: Translating the Image Right and Down</p> <ul> <li> <p><code>height</code>, <code>width</code> = <code>image.shape[:2]: image.shape[:2]</code> gives the height and width of the image. These values are stored in height and width variables.</p> </li> <li> <p><code>quarter_height</code>, <code>quarter_width</code> = <code>height / 4, width / 4:</code> This calculates one-quarter of the image's height and width which will be used as the translation distance (in pixels).</p> </li> <li> <p><code>img_translation</code> = <code>cv2.warpAffine(image, T, (width, height)):</code> The image is shifted by the defined values using the matrix T. The resulting translated image is stored in img_translation.</p> </li> </ul> <pre><code>import cv2\nimport numpy as np\nfrom google.colab.patches import cv2_imshow\n\nimage = cv2.imread('/content/retriver.webp')\n\nheight, width = image.shape[:2]\nquarter_height, quarter_width = height / 4, width / 4\n\nT = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n\nimg_translation = cv2.warpAffine(image, T, (width, height))\n\ncv2_imshow(image) \ncv2_imshow(img_translation)\n</code></pre> <p>Example 2: Performing Multiple Translations In this example, we perform four different translations on the same image: left, right, top and bottom.</p> <ul> <li><code>rows</code>, <code>cols</code>,<code>_</code> = <code>img.shape:</code> This extracts the number of rows (height) and columns (width) from the image's shape. The third value <code>(_)</code> represents the number of color channels.</li> </ul> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimg = cv2.imread('/content/retriver.webp')\nrows, cols, _ = img.shape\n\nM_left = np.float32([[1, 0, -50], [0, 1, 0]])\nM_right = np.float32([[1, 0, 50], [0, 1, 0]])\nM_top = np.float32([[1, 0, 0], [0, 1, 50]])\nM_bottom = np.float32([[1, 0, 0], [0, 1, -50]])\n\nimg_left = cv2.warpAffine(img, M_left, (cols, rows))\nimg_right = cv2.warpAffine(img, M_right, (cols, rows))\nimg_top = cv2.warpAffine(img, M_top, (cols, rows))\nimg_bottom = cv2.warpAffine(img, M_bottom, (cols, rows))\n\nplt.subplot(221), plt.imshow(img_left), plt.title('Left')\nplt.subplot(222), plt.imshow(img_right), plt.title('Right')\nplt.subplot(223), plt.imshow(img_top), plt.title('Top')\nplt.subplot(224), plt.imshow(img_bottom), plt.title('Bottom')\nplt.show()\n</code></pre> <ul> <li>First image shows the translation to the left by 50px.</li> <li>Second image shows the translation to the right by 50px.</li> <li>Third image shows the translation to the top by 50px.</li> <li>Fourth image shows the translation to the bottom by 50px.</li> </ul>"},{"location":"OpenCV/OpenCV/#real-world-applications-of-image-translation","title":"Real-World Applications of Image Translation","text":"<ol> <li> <p>Object Tracking: It helps track moving objects in video frames by shifting the image based on the object's position, important for surveillance and motion analysis.</p> </li> <li> <p>Image Stitching: It helps align parts of images when creating panoramic views, making sure that they blend seamlessly.</p> </li> <li> <p>Augmented Reality (AR): It is used to move virtual objects within real-world environments in AR applications, helping them interact naturally with the surroundings.</p> </li> <li> <p>Image Animation: It is used to create smooth animations by shifting parts of the image step by step, simulating motion or transitions.</p> </li> </ol>"},{"location":"OpenCV/OpenCV/#advantages-of-image-translation","title":"Advantages of Image Translation","text":"<p>Let's see some common advantages of Image Translation:</p> <ol> <li> <p>Simple Implementation: Image translation is easy to implement with basic transformation matrices, making it an accessible tool for many applications.</p> </li> <li> <p>Preserves Image Integrity: Unlike other transformations, it shifts the image without distorting or altering its content, keeping the original data intact.</p> </li> <li> <p>Improves Image Alignment: It helps align images or objects in tasks like stitching, ensuring smoother integration between different visual elements.</p> </li> <li> <p>Efficient for Image Cropping: It allows precise cropping by shifting the image to focus on the desired portion, making it ideal for creating zoomed-in or focused views.</p> </li> </ol>"},{"location":"OpenCV/OpenCV/#limitations-of-image-translation","title":"Limitations of Image Translation","text":"<ol> <li> <p>Loss of Image Quality: Large translations can cause pixelation or blurriness, reducing image clarity, especially when shifting significant portions.</p> </li> <li> <p>Handling Large Translations: Shifting an image too far may crop or distort parts, losing important content or causing visual issues.</p> </li> <li> <p>Difficulty in Alignment: In tasks like stitching or tracking, translation may not perfectly align images or objects with different perspectives, leading to mismatches.</p> </li> <li> <p>Limited Real-Time Application: In real-time systems like AR or object tracking, translation can be computationally demanding, causing delays or misalignment.</p> </li> </ol>"},{"location":"OpenCV/OpenCV/#cv2cvtcolor-method","title":"cv2.cvtColor() method","text":"<p>cv2.cvtColor() is an OpenCV function that converts an image from one color space to another.</p> <p>It supports over 150 color conversion methods, but in most cases, only a few (like BGR\u2194GRAY or BGR\u2194RGB) are used frequently in real-world projects.</p> <p>Reasons for Changing Color Spaces</p> <p>Different tasks require different representations of an image:</p> <ul> <li> <p>Grayscale: Simplifies image analysis and reduces processing time.</p> </li> <li> <p>HSV (Hue, Saturation, Value): Useful for color-based segmentation and object tracking.</p> </li> <li> <p>LAB, YCrCb, RGB, etc.: Used in specialized applications like skin tone detection or advanced image enhancement.</p> </li> </ul> <p>Syntax</p> <p><code>cv2.cvtColor(src, code[, dst[, dstCn]])</code></p> <p>Parameters:</p> <ul> <li>src: input image whose color space is to be changed.</li> <li>code: color space conversion code (e.g., cv2.COLOR_BGR2GRAY).</li> <li>dst(Optional): Output image of the same size and depth as src.</li> <li>dstCn(Optional): Number of channels in destination image. If 0, it\u2019s derived automatically.</li> </ul> <p>Key Points to Remember</p> <ul> <li>OpenCV reads images in BGR format, not RGB.</li> <li>When displaying images using Matplotlib, you may need to convert BGR -&gt; RGB for correct colors.</li> <li>Always choose a color conversion code that matches your source image format.</li> </ul> <p>Examples of cv2.cvtColor() method</p> <p>Example 1: Convert BGR to Grayscale</p> <p>Here\u2019s a simple Python code using OpenCV to read an image, convert it to grayscale and display it in a window.</p> <pre><code>import cv2\nsrc = cv2.imread(r'logo.png')  # Read the image\n\n# Convert to Grayscale\ngray_image = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n\n# Display\ncv2.imshow(\"Grayscale Image\", gray_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>Explanation:</p> <ul> <li>cv2.cvtColor(src, cv2.COLOR_BGR2GRAY): Converts image to grayscale.</li> <li>cv2.imshow(\"Grayscale Image\", gray_image): Displays grayscale image in a window.</li> <li>cv2.waitKey(0): Waits for a key press to keep window open.</li> <li>cv2.destroyAllWindows(): Closes all OpenCV windows.</li> </ul> <p>Example 2: Convert BGR to HSV Here\u2019s a simple Python code using OpenCV to read an image, convert it from BGR to HSV color space and display the result.</p> <pre><code>import cv2\nsrc = cv2.imread(r'logo.png')\n\n# Convert to HSV\nhsv_image = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n\ncv2.imshow(\"HSV Image\", hsv_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>Explanation:</p> <ul> <li><code>cv2.cvtColor(src, cv2.COLOR_BGR2HSV):</code> Converts BGR image to HSV color space.</li> <li><code>cv2.imshow(\"HSV Image\", hsv_image):</code> Displays HSV image in a window titled \"HSV Image\".</li> </ul> <p>Example 3: Convert BGR to RGB (For Matplotlib)</p> <p>This Python code reads an image using OpenCV, converts it from BGR to RGB color format and then displays it using Matplotlib.</p> <pre><code>import cv2\nimport matplotlib.pyplot as plt\nsrc = cv2.imread(r'logo.png')\n\n# Convert from BGR to RGB\nrgb_image = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n\n# Display with Matplotlib\nplt.imshow(rgb_image)\nplt.title(\"RGB Image for Matplotlib\")\nplt.axis('off')\nplt.show()\n</code></pre> <p>Explanation:</p> <ul> <li><code>cv2.cvtColor(src, cv2.COLOR_BGR2RGB):</code> Converts image from BGR to RGB format for correct color display in Matplotlib.</li> <li><code>plt.imshow(rgb_image):</code> Displays the RGB image using Matplotlib.</li> <li><code>plt.axis('off'):</code> Hides the axis for a cleaner view.</li> </ul>"},{"location":"OpenCV/OpenCV/#common-conversion-codes","title":"Common Conversion Codes","text":"<p>Let\u2019s see some of the most commonly used OpenCV color conversion codes:</p> <ul> <li><code>cv2.COLOR_BGR2GRAY BGR:</code> Grayscale</li> <li><code>cv2.COLOR_BGR2RGB BGR:</code> RGB</li> <li><code>cv2.COLOR_BGR2HSV BGR:</code> HSV</li> <li><code>cv2.COLOR_BGR2LAB BGR:</code> LAB color space</li> <li><code>cv2.COLOR_BGR2YCrCb BGR:</code> YCrCb (used in compression &amp; skin detection)</li> </ul>"},{"location":"OpenCV/OpenCV/#real-world-applications","title":"Real-World Applications","text":"<p>Different color spaces are preferred for specific computer vision tasks because they highlight certain image features better. Let\u2019s see some real-world applications of these conversions:</p> <ul> <li><code>Grayscale:</code> Face detection, OCR (text recognition)</li> <li><code>HSV:</code> Object tracking (e.g., detecting a colored ball)</li> <li><code>LAB:</code> Color enhancement, skin tone detection</li> <li><code>YCrCb:</code> Video compression, human skin detection</li> </ul>"},{"location":"RAG/rag/","title":"RAG","text":""},{"location":"RAG/rag/#what-is-text-chunking-in-rag","title":"\ud83d\udd39 What is Text Chunking in RAG?","text":"<p>In Retrieval-Augmented Generation (RAG), text chunking is a foundational step that significantly impacts the performance of retrieval and generation. Choosing the right chunking strategy depends on your domain, model size, use case (e.g., question answering, summarization), and latency requirements.</p> <p>Text chunking is the process of breaking large documents into smaller pieces (chunks) before embedding and storing them in a vector database for retrieval.</p>"},{"location":"RAG/rag/#common-chunking-strategies-modes","title":"\ud83d\udd39 Common Chunking Strategies (Modes)","text":"Mode Description Use Case Model Details Fixed-size chunks Split text by fixed number of tokens (e.g., 512 tokens) General-purpose, fast and simple No model needed; tokenizers like <code>tiktoken</code>, <code>sentencepiece</code>, or <code>nltk</code> Sliding window Overlapping chunks (e.g., 512 tokens with 100-token overlap) Ensures context continuity Simple algorithmic logic with tokenizer support Sentence-based Break by sentence boundaries Preserves semantic boundaries <code>nltk.sent_tokenize</code>, <code>spacy</code>, <code>textsplit</code> Paragraph-based Keep full paragraphs as chunks Ideal for long-form documents Regex or NLP libraries like <code>nltk</code>, <code>spacy</code> Semantic chunking Split by topics or natural breakpoints using ML models Best for maintaining topic coherence <code>BERTopic</code>, <code>SBERT</code>, <code>KeyBERT</code>, LLMs with attention-based segmentation Markdown/Heading-based Use headings/subheadings in documents to split For structured docs like manuals, PDFs Regex, <code>BeautifulSoup</code>, <code>Markdown</code>, PDF parsers (e.g., <code>pdfplumber</code>, <code>PyMuPDF</code>) Recursive Character Split Hierarchical chunking (LangChain-style) from large &gt; small (section &gt; para &gt; line) Works well in structured + unstructured docs <code>LangChain</code>\u2019s <code>RecursiveCharacterTextSplitter</code>, regex Query-Aware Chunking Splits and prioritizes chunks based on relevance to a query RAG pipelines, QA systems <code>BM25</code>, <code>FAISS</code>, <code>Chroma</code>, <code>OpenAI Embeddings</code>, <code>ColBERT</code>, <code>RetrieverMixin</code> Token-Density-Based Chunks based on token density, complexity, or information richness Summarization, content prioritization Token counters, <code>OpenAI Tokenizer</code>, statistical methods Entity-Based Chunking Splits text by named entities (e.g., person, organization) Information extraction, knowledge graphs <code>spacy</code>, <code>flair</code>, <code>transformers</code> NER models Event-Based Chunking Breaks text by narrative events or transitions Story summarization, news timeline generation <code>EventBERT</code>, <code>GPT-4</code>, <code>NarrativeQA</code>, <code>BART</code> fine-tuned on events Dialogue/Turn-Based Splits by speaker turns in conversations Chatbot training, transcript analysis <code>Whisper</code>, <code>assemblyAI</code>, or transcript parsers + speaker diarization tools Table/Structure-Aware Handles structured data like tables and lists differently PDFs, spreadsheets, forms <code>Pandas</code>, <code>tabula-py</code>, <code>Camelot</code>, <code>layoutLM</code>, <code>PDFPlumber</code>, <code>Unstructured.io</code> Page-Based Chunking Chunks created per page or visual layout (often OCR-based) Invoices, scanned documents, academic papers <code>Tesseract OCR</code>, <code>PyMuPDF</code>, <code>pdf2image</code>, <code>layoutLM</code>, <code>Donut</code> Visual/Layout-Aware Uses layout cues like headers, font, or boxes to define chunks Magazines, academic PDFs, websites <code>layoutLMv3</code>, <code>Donut</code>, <code>PubLayNet</code>, <code>DocFormer</code>, <code>Unstructured.io</code> Code/Function-Based Chunks by logical programming units like functions or classes Code summarization, documentation, Copilot tools <code>tree-sitter</code>, <code>jedi</code>, <code>CodeBERT</code>, <code>PolyCoder</code>, <code>StarCoder</code>, <code>GPT-4-Code</code>"},{"location":"RAG/rag/#best-practices-for-chunking-in-rag","title":"\ud83d\udd39 Best Practices for Chunking in RAG","text":"<p>Optimal Chunk Size:</p> <ul> <li>300\u2013500 tokens is often optimal (balances semantic completeness and context window limits).</li> <li>Too small: loses context; Too large: hurts retrieval relevance.</li> </ul> <p>Use Overlap:</p> <ul> <li>Add 10\u201320% token overlap (e.g., 100 tokens) to maintain context across chunk boundaries.</li> </ul> <p>Embed with CLS or Average Pooling:</p> <ul> <li>Use models like <code>sentence-transformers, all-MiniLM, bge-base, text-embedding-ada-002</code>, etc., which are tuned for sentence-level semantics.</li> </ul> <p>Preprocessing:</p> <ul> <li>Clean HTML, remove noise, normalize whitespace.</li> <li>Keep metadata like title, section headings, page number, etc.</li> </ul>"},{"location":"RAG/rag/#models-for-chunking-semantic-splitting","title":"\ud83d\udd39 Models for Chunking / Semantic Splitting","text":"<p>These help in semantic-aware chunking:</p> Model/Library Purpose SpaCy Sentence segmentation NLTK Token/sentence splitting TextSplit Token-aware splitting LangChain RecursiveTextSplitter Recursive chunking by structure SemanticTextSplitter (OpenAI) Uses embeddings to break at semantic boundaries Unstructured.io Parsing PDFs, emails, HTMLs semantically"},{"location":"RAG/rag/#best-chunking-strategy-by-rag-type","title":"\ud83d\udd39 Best Chunking Strategy by RAG Type","text":"Use Case Best Chunking Mode Q\\&amp;A over documents Sentence-based + sliding window + overlap Legal/medical docs Paragraph + heading-based + semantic splitting Technical manuals Markdown/headings + semantic splitting Long PDFs RecursiveTextSplitter + metadata + overlap Chatbot (multi-turn) Short sentence + sliding window for continuity Enterprise search Paragraph-based with metadata indexing"},{"location":"RAG/rag/#popular-embedding-chunking-stack-in-rag-pipelines","title":"\ud83d\udd39 Popular Embedding + Chunking Stack in RAG Pipelines","text":"<p>LangChain:   - <code>RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter, TokenTextSplitter</code></p> <p>Haystack:   - <code>PreProcessor(split_by=\"sentence\", split_length=10, overlap=3)</code></p> <p>LlamaIndex:   - <code>SentenceSplitter, SemanticSplitterNodeParser</code></p>"},{"location":"RAG/rag/#toolsmodels-summary","title":"\ud83d\udd39 Tools/Models Summary","text":"Tool / Lib Use For Chunking? Notes <code>LangChain</code> \u2705 Multiple chunking classes available <code>Haystack</code> \u2705 Flexible chunking + preprocessing <code>LlamaIndex</code> \u2705 Node-based semantic chunking <code>SpaCy</code>, <code>NLTK</code> \u2705 Sentence/word segmentation <code>unstructured.io</code> \u2705 Parsing HTML, PDFs, etc. <code>textsplit</code> \u2705 Heuristics-based chunking"},{"location":"RAG/rag/#recommendation-for-best-rag-chunking-general-purpose","title":"\u2705 Recommendation for Best RAG Chunking (General Purpose)","text":"<ul> <li>Chunk Size: 350\u2013500 tokens</li> <li>Overlap: 50\u2013100 tokens</li> <li>Method: Recursive/semantic chunking with fallback to sentence-level</li> <li>Tools: LangChain RecursiveTextSplitter or LlamaIndex SemanticSplitterNodeParser</li> </ul>"},{"location":"RAG/rag/#1-langchain-recursivetextsplitter","title":"\u2705 1. LangChain \u2013 RecursiveTextSplitter","text":"<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=100,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n)\n\ntexts = text_splitter.split_text(your_document_text)\nprint(f\"Total chunks: {len(texts)}\")\n</code></pre>"},{"location":"RAG/rag/#best-for-unstructured-text-articles-reports-raw-documents","title":"\u27a1\ufe0f Best for: unstructured text (articles, reports, raw documents)","text":""},{"location":"RAG/rag/#2-langchain-markdownheadertextsplitter","title":"\u2705 2. LangChain \u2013 MarkdownHeaderTextSplitter","text":"<p>For structured markdown documents:</p> <pre><code>from langchain.text_splitter import MarkdownHeaderTextSplitter\n\nmarkdown_text = \"\"\"# Title\\nSome intro.\\n## Section 1\\nDetails here.\\n## Section 2\\nMore details.\"\"\"\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Title\"), (\"##\", \"Section\")])\ndocs = splitter.split_text(markdown_text)\n\nfor doc in docs:\n    print(doc.metadata)  # includes Title, Section\n    print(doc.page_content)\n</code></pre>"},{"location":"RAG/rag/#best-for-docs-with-clear-heading-structure-eg-technical-manuals-markdown","title":"\u27a1\ufe0f Best for: docs with clear heading structure (e.g., technical manuals, markdown)","text":""},{"location":"RAG/rag/#3-llamaindex-semanticsplitternodeparser","title":"\u2705 3. LlamaIndex \u2013 SemanticSplitterNodeParser","text":"<pre><code>from llama_index.node_parser import SemanticSplitterNodeParser\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms import OpenAI\nfrom llama_index import Document\n\n# Load your document\ndocuments = [Document(text=your_document_text)]\n\n# Semantic-aware splitting\nparser = SemanticSplitterNodeParser(\n    embed_model=OpenAIEmbedding(),\n    llm=OpenAI(),\n    chunk_size=512\n)\n\nnodes = parser.get_nodes_from_documents(documents)\n\nfor node in nodes:\n    print(node.text)\n</code></pre>"},{"location":"RAG/rag/#best-for-semantic-coherence-topic-wise-llm-assisted-splitting","title":"\u27a1\ufe0f Best for: semantic coherence (topic-wise), LLM-assisted splitting","text":""},{"location":"RAG/rag/#4-haystack-preprocessor","title":"\u2705 4. Haystack \u2013 PreProcessor","text":"<pre><code>from haystack.nodes import PreProcessor\nfrom haystack.document_stores import InMemoryDocumentStore\n\ndocument_store = InMemoryDocumentStore()\n\npreprocessor = PreProcessor(\n    clean_empty_lines=True,\n    clean_whitespace=True,\n    split_by=\"sentence\",\n    split_length=5,\n    split_overlap=1,\n    split_respect_sentence_boundary=True\n)\n\n# Load your raw text into a dict format\nraw_docs = [{\"content\": your_document_text}]\nprocessed_docs = preprocessor.process(raw_docs)\n\nprint(f\"Total processed chunks: {len(processed_docs)}\")\n</code></pre>"},{"location":"RAG/rag/#best-for-sentence-level-rag-qa-over-documents","title":"\u27a1\ufe0f Best for: sentence-level RAG, Q&amp;A over documents","text":""},{"location":"RAG/rag/#bonus-unstructured-for-parsing-raw-files","title":"\u2705 Bonus: unstructured for parsing raw files","text":"<pre><code>from unstructured.partition.auto import partition\n\nelements = partition(filename=\"sample.pdf\")\ntext = \"\\n\".join([el.text for el in elements if el.text is not None])\n</code></pre>"},{"location":"RAG/rag/#use-this-before-chunking-especially-for-pdfs-html-docx-etc","title":"\u27a1\ufe0f Use this before chunking, especially for PDFs, HTML, DOCX, etc.","text":""},{"location":"Statistic/metrics/","title":"Metrics Evaluation","text":""},{"location":"Statistic/metrics/#classification-evaluation-metrics","title":"Classification Evaluation Metrics","text":"<p>x-axis: Typically represents the independent variable.             - The x-axis is horizontal</p> <p>y-axis: Typically represents the dependent variable.             - y-axis is vertical</p> <p>Several important metrics which are used in classification algorithms under supervised learning. Although there are many metrics which can be potentially used for measuring performance of a classification model, some of the main metrics are listed below</p> <ul> <li>Confusion matrix\u2013 This is one of the most important and most commonly used metrics for evaluating the classification accuracy. Typically on the x-axis \u201ctrue classes\u201d are shown and on the y axis \u201cpredicted classes\u201d are represented. Confusion Matrix is applicable for both binary and multi class classification.</li> </ul> <p>See the cat and dog classification example listed.</p> <p></p> <p>In the below example, let\u2019s pretend that we have built a classification algorithm to identify Dogs ( Positive Class) from a total of 100 animals where in reality 70 animals are Dogs (Positive Class) and 30 are Cats (Negative Class).</p> Predicted: Positive Predicted: Negative Actual: Positive True Positive (TP) False Negative (FN) Actual: Negative False Positive (FP) True Negative (TN)"},{"location":"Statistic/metrics/#2-key-metrics-explained","title":"\u2705 2. Key Metrics Explained","text":"<ul> <li>TP = Model predicted Positive and it's actually Positive</li> <li>TN = Model predicted Negative and it's actually Negative</li> <li>FP = Model predicted Positive but it's actually Negative</li> <li>FN = Model predicted Negative but it's actually Positive</li> </ul>"},{"location":"Statistic/metrics/#accuracy","title":"\ud83d\udd39 Accuracy:","text":"<p>This measures model\u2019s overall performance in correctly identifying all classes.This metric is valid for both binary and multi-class classification however this is not very robust for the unbalanced data and we should use Precision and Recall metrics instead</p> <ul> <li>How often the model is correct overall.</li> </ul> <p></p>"},{"location":"Statistic/metrics/#precision","title":"\ud83d\udd39 Precision:","text":"<p>When a model identifies an observation as a positive, this metric measure the performance of the model in correctly identifying the true positive from the false positive. This is a very robust matrix for multiclass classification and the unbalanced data. The closer the Precision value to 1, the better the model</p> <ul> <li>Out of the predicted positives, how many are actually positive?</li> </ul> <p></p>"},{"location":"Statistic/metrics/#recall-sensitivity-or-true-positive-rate","title":"\ud83d\udd39 Recall (Sensitivity or True Positive Rate):","text":"<p>This metric measures a  model\u2019s performance in identifying the true positive out of the total true positive cases.  The closer the Recall value to 1, the better the model. As is the case with the Precision metric, this metric is a very robust matrix for multi-class classification and the unbalanced data.</p> <ul> <li>Out of all actual positives, how many did the model catch?</li> </ul> <p></p>"},{"location":"Statistic/metrics/#example","title":"\u2705 Example","text":"<p>Suppose you're building a spam classifier. You test it on 100 emails, and get:</p> <ul> <li>TP = 40 (Spam correctly identified as spam)</li> <li>TN = 50 (Not spam correctly identified as not spam)</li> <li>FP = 5 (Not spam incorrectly identified as spam)</li> <li>FN = 5 (Spam incorrectly identified as not spam)</li> </ul>"},{"location":"Statistic/metrics/#confusion-matrix","title":"\ud83d\udcca Confusion Matrix","text":"Predicted: Spam Predicted: Not Spam Actual: Spam 40 (TP) 5 (FN) Actual: Not Spam 5 (FP) 50 (TN)"},{"location":"Statistic/metrics/#calculated-metrics","title":"\ud83d\udd22 Calculated Metrics","text":"<ul> <li>Accuracy = (TP + TN) / Total = (40 + 50) / 100 = 90%</li> <li>Precision = TP / (TP + FP) = 40 / (40 + 5) = 88.9%</li> <li>Recall = TP / (TP + FN) = 40 / (40 + 5) = 88.9%</li> </ul>"},{"location":"Statistic/metrics/#when-to-use-what","title":"\u2705 When to Use What?","text":"Metric Best Used When... Accuracy Classes are balanced and cost of errors is equal. Precision False positives are costly (e.g., predicting a healthy person has diabatic). Recall False negatives are costly (e.g., missing a disease)."},{"location":"Statistic/metrics/#f1-score","title":"\u2705 F1 Score","text":"<p>The F1 Score is the harmonic mean of Precision and Recall. It balances both metrics \u2014 especially useful when you want to balance false positives and false negatives.</p>"},{"location":"Statistic/metrics/#f1-score-formula","title":"\ud83d\udd39 F1 Score Formula:","text":"<ul> <li>Precision = 40 / (40 + 5) = 0.888</li> <li>Recall = 40 / (40 + 5) = 0.888</li> </ul>"},{"location":"Statistic/metrics/#when-to-use-f1-score","title":"\u2705 When to Use F1 Score?","text":"<ul> <li> <p>Imbalanced datasets (e.g., fraud detection, disease diagnosis)</p> </li> <li> <p>When both false positives and false negatives are important</p> </li> </ul>"},{"location":"Statistic/metrics/#precision-vs-recall-vs-f1-summary","title":"\u2696\ufe0f Precision vs Recall vs F1 Summary","text":"Metric Best For Precision You want fewer false positives (FP) Recall You want fewer false negatives (FN) F1 Score You want a balance of both"},{"location":"Statistic/metrics/#example_1","title":"Example","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# True and predicted labels\ny_true = ['dog', 'dog', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat']\ny_pred = ['dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'dog', 'cat', 'cat']\n\n# Metrics for class \"dog\" as positive\nprecision = precision_score(y_true, y_pred, pos_label='dog')\nrecall = recall_score(y_true, y_pred, pos_label='dog')\nf1 = f1_score(y_true, y_pred, pos_label='dog')\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=['dog', 'cat'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['dog', 'cat'])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\n</code></pre>"},{"location":"Statistic/metrics/#explanation","title":"Explanation:","text":"<p>Note: <code>dog: positive(1) and cat: negative(0)</code></p> <ul> <li>Total count = 10</li> <li>Total actual dog count =  4</li> <li> <p>Total actual cat count =  6</p> </li> <li> <p>Correctly predicted as dog count(TP) = 3</p> </li> <li> <p>Correctly predicted as cat count(TN) = 5</p> </li> <li> <p>Predicted dog but actual is cat count(FN) = 1</p> </li> <li>Predicted cat but actual is dog count(FP) = 1</li> </ul>"},{"location":"Statistic/metrics/#confusion-matrix-interpretation-for-dog-as-positive","title":"\ud83d\udcca Confusion Matrix Interpretation (for 'dog' as positive):","text":"Predicted: dog Predicted: cat Actual: dog TP = 3 FN = 1 Actual: cat FP = 1 TN = 5 <ul> <li> <p>Precision = 3 / (3 + 1) = 0.75</p> </li> <li> <p>Recall = 3 / (3 + 1) = 0.75</p> </li> <li> <p>F1 Score = 0.75</p> </li> </ul> Index y_true y_pred Result 0 dog dog \u2705 TP 1 dog cat \u274c FN 2 cat cat \u2705 TN (not counted for F1) 3 dog dog \u2705 TP 4 cat cat \u2705 TN 5 cat cat \u2705 TN 6 dog dog \u2705 TP 7 cat dog \u274c FP 8 cat cat \u2705 TN 9 cat cat \u2705 TN <p></p>"},{"location":"Statistic/metrics/#multi-class-classification","title":"Multi-class classification","text":"<p>For multi-class classification, metrics like precision, recall, and F1 score are calculated per class and then aggregated using different methods:</p>"},{"location":"Statistic/metrics/#step-by-step","title":"\ud83c\udfaf Step-by-Step","text":"<p>Let\u2019s assume you have 3 classes: <code>'cat', 'dog', and 'rabbit'</code>.</p> <pre><code>from sklearn.metrics import precision_score, recall_score, f1_score\n\ny_true = ['cat', 'dog', 'rabbit', 'cat', 'dog', 'rabbit', 'dog', 'cat', 'rabbit']\ny_pred = ['cat', 'dog', 'cat', 'rabbit', 'dog', 'rabbit', 'rabbit', 'cat', 'rabbit']\n</code></pre>"},{"location":"Statistic/metrics/#method-1-per-class-scores","title":"\u2705 Method 1: Per-class scores","text":"<pre><code>precision_score(y_true, y_pred, average=None, labels=['cat', 'dog', 'rabbit'])\n</code></pre> <p>This gives precision for each class separately:</p> <ul> <li>Precision for <code>'cat'</code></li> <li>Precision for <code>'dog'</code></li> <li>Precision for <code>'rabbit'</code></li> </ul> <pre><code>from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\n\ny_true = ['cat', 'dog', 'rabbit', 'cat', 'dog', 'rabbit', 'dog', 'cat', 'rabbit']\ny_pred = ['cat', 'dog', 'cat', 'rabbit', 'dog', 'rabbit', 'rabbit', 'cat', 'rabbit']\n\n\nprint(classification_report(y_true, y_pred, labels=['cat', 'dog', 'rabbit']))\n</code></pre> <p></p>"},{"location":"Statistic/metrics/#confusion-matrix_1","title":"Confusion matrix","text":"<pre><code>cm = confusion_matrix(y_true, y_pred, labels=['dog', 'cat', 'rabbit'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['dog', 'cat', 'rabbit'])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\n</code></pre>"},{"location":"Statistic/metrics/#how-it-works-one-vs-rest","title":"\ud83e\udde0 How It Works (One-vs-Rest)","text":"<p>Suppose we have 3 classes: <code>cat, dog, rabbit</code>. Here's how metrics are computed <code>per class</code> using a <code>one-vs-rest</code> strategy:</p> <ol> <li>For Class cat:</li> <li>Positive class = cat</li> <li>Negative classes = dog, rabbit</li> </ol> Metric Meaning TP (cat) Predicted <code>cat</code> and actually <code>cat</code> FP (cat) Predicted <code>cat</code> but actually <code>dog</code> or <code>rabbit</code> FN (cat) Actual <code>cat</code> but predicted <code>dog</code> or <code>rabbit</code> TN (cat) All others (correct non-cat classifications) <ol> <li>For Class dog:</li> <li>Positive class = dog</li> <li>Negative classes = cat, rabbit</li> </ol> Metric Meaning TP (dog) Predicted <code>dog</code> and actually <code>dog</code> FP (dog) Predicted <code>dog</code> but actually <code>cat</code> or <code>rabbit</code> FN (dog) Actual <code>dog</code> but predicted <code>cat</code> or <code>rabbit</code> TN (dog) All others (correct non-cat classifications) <ol> <li>For Class rabbit:</li> <li>Positive class = rabbit</li> <li>Negative classes = cat, dog</li> </ol> Metric Meaning TP (rabbit) Predicted <code>rabbit</code> and actually <code>rabbit</code> FP (rabbit) Predicted <code>rabbit</code> but actually <code>cat</code> or <code>dog</code> FN (rabbit) Actual <code>rabbit</code> but predicted <code>cat</code> or <code>dog</code> TN (rabbit) All others (correct non-cat classifications)"},{"location":"Statistic/metrics/#so-in-multi-class","title":"\u2705 So, in multi-class:","text":"<ul> <li>There\u2019s no single \"positive\"/\"negative\" class.</li> <li>Instead, each class becomes the \u201cpositive\u201d class in turn, and metrics are computed accordingly.</li> </ul>"},{"location":"Statistic/statistic-details/","title":"Statistic Details","text":""},{"location":"Statistic/statistic-details/#1-foundations-of-statistical-machine-learning","title":"1. Foundations of Statistical Machine Learning:","text":"<p>Statistics and Machine Learning: Statistics provides the tools for understanding data (descriptive statistics, probability distributions), while machine learning provides the algorithms for building predictive models.</p> <p>Data Exploration and Preparation: Statistical methods help in understanding data distributions, identifying outliers, and selecting relevant features for modeling.</p> <p>Probability and Distributions: Understanding probability distributions is crucial for modeling and evaluating machine learning models. </p>"},{"location":"Statistic/statistic-details/#2-key-statistical-concepts-and-techniques","title":"2. Key Statistical Concepts and Techniques:","text":"<p>Descriptive Statistics: Measures of central tendency (mean, median, mode), variability (variance, standard deviation), and distribution shape help summarize and understand data. </p> <p>Hypothesis Testing: Used to compare populations, assess the significance of results, and evaluate model performance (e.g., t-tests, ANOVA, chi-square tests). </p> <p>Resampling Methods: Techniques like cross-validation and bootstrapping are used to estimate model performance on unseen data, especially when data is limited. </p> <p>Estimation Statistics: Focuses on estimating parameters and their uncertainty using confidence intervals, prediction intervals, etc. </p> <p>Nonparametric Methods: Used when data doesn't meet the assumptions of parametric tests (e.g., t-tests). </p>"},{"location":"Statistic/statistic-details/#3-applications-in-machine-learning","title":"3. Applications in Machine Learning:","text":"<p>Model Building: Statistics provides the foundation for building various machine learning models, such as linear regression, logistic regression, and neural networks. </p> <p>Model Evaluation: Statistical techniques like p-values, confidence intervals, and R-squared help in assessing the performance and reliability of machine learning models. </p> <p>Feature Selection and Engineering: Statistics helps in identifying relevant features and transforming data for better model performance. </p> <p>Interpreting Results: Statistical concepts help in understanding the meaning and significance of model predictions. </p>"},{"location":"Statistic/statistic-details/#4-examples","title":"4. Examples:","text":"<p>Linear Regression: Uses the method of least squares, a statistical technique, to find the best-fitting line that explains the relationship between variables. </p> <p>Hypothesis Testing: Can be used to determine if a new model significantly outperforms an existing one. </p> <p>Cross-validation: A resampling technique used to estimate how well a model will generalize to unseen data. </p>"},{"location":"Statistic/statistic-details/#statistics-for-machine-learning","title":"Statistics For Machine Learning","text":"<p>Machine Learning Statistics: In the field of machine learning (ML), statistics plays a pivotal role in extracting meaningful insights from data to make informed decisions. Statistics provides the foundation upon which various ML algorithms are built, enabling the analysis, interpretation, and prediction of complex patterns within datasets.</p> <ul> <li>Types of Statistics</li> <li>Descriptive Statistics</li> <li>Measures of Dispersion</li> <li>Measures of Shape</li> <li>Covariance and Correlation</li> <li>Visualization Techniques</li> <li>Probability Theory</li> <li>Inferential Statistics</li> <li>Population and Sample</li> <li>Estimation</li> <li>Hypothesis Testing</li> <li>ANOVA (Analysis of Variance)</li> <li>Chi-Square Tests:</li> <li>Correlation and Regression</li> <li>Bayesian Statistics</li> </ul>"},{"location":"Statistic/statistic-details/#types-of-statistics","title":"Types of Statistics","text":"<p>There are commonly two types of statistics.</p> <ul> <li> <p>Descriptive Statistics: \"De\u00adscriptive Statistics\" helps us simplify and organize big chunks of data. This makes large amounts of data easier to understand.</p> </li> <li> <p>Inferential Statistics: \"Inferential Statistics\" is a little different. It uses smaller data to draw conclusions about a larger group. It helps us predict and draw conclusions about a population.</p> </li> </ul> <p>Descriptive Statistics Descriptive statistics summarize and describe the features of a dataset, providing a foundation for further statistical analysis.</p> <p></p> <p>Measures of Dispersion</p> <ul> <li> <p>Range: The difference between the maximum and minimum values.</p> </li> <li> <p>Variance: The average squared deviation from the mean, representing data spread.</p> </li> <li> <p>Standard Deviation: The square root of variance, indicating data spread relative to the mean.</p> </li> <li> <p>Interquartile Range: The range between the first and third quartiles, measuring data spread around the median.</p> </li> </ul> <p>Measures of Shape</p> <ul> <li>Skewness: Indicates data asymmetry.</li> </ul> <p></p> <ul> <li>Kurtosis: Measures the peakedness of the data distribution.</li> </ul> <p></p>"},{"location":"Statistic/statistic-details/#covariance-and-correlation","title":"Covariance and Correlation","text":""},{"location":"Statistic/statistic-details/#visualization-techniques","title":"Visualization Techniques","text":"<ul> <li>Histograms: Show data distribution.</li> <li>Box Plots: Highlight data spread and potential outliers.</li> <li>Scatter Plots: Illustrate relationships between variables.</li> </ul>"},{"location":"Statistic/statistic-details/#probability-theory","title":"Probability Theory","text":"<p>Probability theory forms the backbone of statistical inference, aiding in quantifying uncertainty and making predictions based on data.</p> <p>Basic Concepts</p> <ul> <li> <p>Random Variables: Variables with random outcomes.</p> </li> <li> <p>Probability Distributions: Describe the likelihood of different outcomes.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#common-probability-distributions","title":"Common Probability Distributions","text":"<ul> <li> <p>Binomial Distribution: Represents the number of successes in a fixed number of trials.</p> </li> <li> <p>Poisson Distribution: Describes the number of events occurring within a fixed interval.</p> </li> <li> <p>Normal Distribution: Characterizes continuous data symmetrically distributed around the mean.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#law-of-large-numbers","title":"Law of Large Numbers:","text":"<p>States that as the sample size increases, the sample mean approaches the population mean.</p>"},{"location":"Statistic/statistic-details/#central-limit-theorem","title":"Central Limit Theorem:","text":"<p>Indicates that the distribution of sample means approximates a normal distribution as the sample size grows, regardless of the population's distribution.</p>"},{"location":"Statistic/statistic-details/#inferential-statistics","title":"Inferential Statistics","text":"<p>Inferential statistics involve making predictions or inferences about a population based on a sample of data.</p>"},{"location":"Statistic/statistic-details/#population-and-sample","title":"Population and Sample","text":"<ul> <li> <p>Population: The entire group being studied.</p> </li> <li> <p>Sample: A subset of the population used for analysis.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#estimation","title":"Estimation","text":"<ul> <li> <p>Point Estimation: Provides a single value estimate of a population parameter.</p> </li> <li> <p>Interval Estimation: Offers a range of values (confidence interval) within which the parameter likely lies.</p> </li> <li> <p>Confidence Intervals: Indicate the reliability of an estimate.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#hypothesis-testing","title":"Hypothesis Testing","text":"<ul> <li> <p>Null and Alternative Hypotheses: The null hypothesis assumes no effect or relationship, while the alternative suggests otherwise.</p> </li> <li> <p>Type I and Type II Errors: Type I error is rejecting a true null hypothesis, while Type II is failing to reject a false null hypothesis.</p> </li> <li> <p>p-Values: Measure the probability of obtaining the observed results under the null hypothesis.</p> </li> <li> <p>t-Tests and z-Tests: Compare means to assess statistical significance.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#anova-analysis-of-variance","title":"ANOVA (Analysis of Variance):","text":"<p>Compares means across multiple groups to determine if they differ significantly.</p>"},{"location":"Statistic/statistic-details/#chi-square-tests","title":"Chi-Square Tests:","text":"<p>Assess the association between categorical variables.</p>"},{"location":"Statistic/statistic-details/#correlation-and-regression","title":"Correlation and Regression:","text":"<p>Understanding relationships between variables is critical in machine learning.</p> <p>Correlation:</p> <ul> <li> <p>Pearson Correlation Coefficient: Measures linear relationship strength between two variables.</p> </li> <li> <p>Spearman Rank Correlation: Assesses the strength and direction of the monotonic relationship between variables.</p> </li> </ul> <p>Regression Analysis</p> <ul> <li> <p>Simple Linear Regression: Models the relationship between two variables.</p> </li> <li> <p>Multiple Linear Regression: Extends to multiple predictors.</p> </li> <li> <p>Assumptions of Linear Regression: Linearity, independence, homoscedasticity, normality.</p> </li> <li> <p>Interpretation of Regression Coefficients: Explains predictor influence on the response variable.</p> </li> <li> <p>Model Evaluation Metrics: R-squared, Adjusted R-squared, RMSE.</p> </li> </ul>"},{"location":"Statistic/statistic-details/#bayesian-statistics","title":"Bayesian Statistics","text":"<p>Bayesian statistics incorporate prior knowledge with current evidence to update beliefs. P(A\u2223B)=P(B)P(B\u2223A)\u22c5P(A)\u200b, where</p> <ul> <li>P(A\u2223B): The probability of event A given that event B has occurred (posterior probability).</li> <li>P(B\u2223A): The probability of event B given that event A has occurred (likelihood).</li> <li>P(A): The probability of event A occurring (prior probability).</li> <li>P(B): The probability of event B occurring.</li> </ul>"},{"location":"Statistic/statistic-details/#descriptive-statistic","title":"Descriptive Statistic","text":"<p>Statistics is the foundation of data science. Descriptive statistics are simple tools that help us understand and summarize data.They show the basic features of a dataset, like the average, highest and lowest values and how spread out the numbers are. It's the first step in making sense of information.</p> <p></p>"},{"location":"Statistic/statistic-details/#types-of-descriptive-statistics","title":"Types of Descriptive Statistics","text":"<p>There are three categories for standard classification of descriptive statistics methods, each serving different purposes in summarizing and describing data. They help us understand:</p> <ol> <li>Where the data centers (Measures of Central Tendency)</li> <li>How spread out the data is (Measure of Variability)</li> <li> <p>How the data is distributed (Measures of Frequency Distribution)</p> </li> <li> <p>Measures of Central Tendency Statistical values that describe the central position within a dataset. There are three main measures of central tendency:</p> </li> </ol> <p></p> <p>Mean: is the sum of observations divided by the total number of observations. It is also defined as average which is the sum divided by count.</p> <p>The mean() function from Python\u2019s statistics module is used to calculate the average of a set of numeric values. It adds up all the values in a list and divides the total by the number of elements.</p> <p></p> <pre><code>mean_age_data = df['age'].mean()\nmode_age_data = df['age'].mode()\nmedian_age_data = df['age'].median()\nprint(\"mean age:\",round(mean_age_data))\nprint(\"mode age:\", mode_age_data)\nprint(\"median age:\", median_age_data)\n</code></pre> <p></p> <p></p> <p>where,   - x = Observations  - n = number of terms</p> <p></p> <p>Mode: The most frequently occurring value in the dataset. It\u2019s useful for categorical data and in cases where knowing the most common choice is crucial.</p>"},{"location":"Statistic/statistic-details/#introduction-to-data-and-statistics","title":"Introduction to Data and Statistics","text":"<ul> <li>Why Statistics \u2013 The need</li> <li>Connecting dots \u2013 Stats, ML, DL</li> <li>Statistics and AI life cycle</li> <li>Data and types</li> <li>Statistical hierarchy</li> <li>Connect stats with real-world problems.</li> <li>Fundamentals:<ul> <li>Sample</li> <li>Population</li> <li>Probability &amp; Distributions</li> <li>Central limit theorem</li> </ul> </li> <li>Terminologies</li> <li>Univariate analysis</li> <li>Descriptive statistics</li> </ul>"},{"location":"Statistic/statistic-details/#statistical-foundation-of-ai","title":"Statistical Foundation of AI","text":"<ul> <li>Introduction to Stats , ML , DL</li> <li>Types of Data and Statistical Hierarchy</li> <li>Measures of Central Tendency and Dispersion</li> <li>Data Cleaning and Missing Values</li> <li>Outlier Detection and Quantiles</li> <li>Basic Visualization : Boxplot, Scatterplot</li> <li>Frequency Tables and Sorting</li> </ul>"},{"location":"Statistic/statistic-details/#what-is-statistics","title":"What is Statistics","text":"<p>Statistics is the science of collecting, analyzing, interpreting, and presenting data to make informed decisions.</p>"},{"location":"Statistic/statistic-details/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Definition: Descriptive statistics summarize and describe the main features of a dataset. Purpose: To organize, simplify, and present data in a meaningful way.</p> <p>Examples:    - Calculating the average score of students in a class.    - Creating a bar chart showing the number of people in different age groups.    - Finding median income in a city.    - Standard deviation showing how spread-out exam scores are.</p>"},{"location":"Statistic/statistic-details/#inferential-statistics_1","title":"Inferential Statistics","text":"<p>Definition: Inferential statistics use a sample of data to make predictions or generalizations about a larger population. Purpose: To draw conclusions, test hypotheses, and estimate population parameters.</p> <p>Examples:    - Using a survey of 1000 voters to predict election results for the entire country.    - Performing a t-test to compare the effectiveness of two different medicines.    - Estimating the average height of all adults in a city using a random sample.</p>"},{"location":"Statistic/statistic-details/#ai-ml-dl","title":"AI ML &amp; DL","text":""},{"location":"Statistic/statistic-details/#stats-vs-ml-vs-dl","title":"Stats vs ML vs DL","text":""},{"location":"Statistic/statistic-details/#types-of-data","title":"Types of Data","text":""},{"location":"Statistic/statistic-details/#statistical-hierarchy","title":"Statistical Hierarchy","text":""},{"location":"Statistic/statistic-details/#measures-of-central-tendency","title":"Measures of Central Tendency","text":""},{"location":"Statistic/statistic-details/#when-to-use-what","title":"When to use what","text":""},{"location":"Statistic/statistic-details/#measure-of-dispersion","title":"Measure of Dispersion","text":""},{"location":"Statistic/statistic-details/#outlier-detection","title":"Outlier Detection","text":""},{"location":"Statistic/statistic-details/#outlier-detection-techniques","title":"Outlier Detection Techniques","text":""},{"location":"Statistic/statistic-details/#data-cleaning-techniques","title":"Data Cleaning Techniques","text":""},{"location":"Statistic/statistic-details/#quantiles-and-percentiles","title":"Quantiles and Percentiles","text":""},{"location":"Statistic/statistic-details/#linear-interpolation","title":"Linear Interpolation","text":""},{"location":"Statistic/statistic-details/#visualizations","title":"Visualizations","text":""},{"location":"Statistic/statistic-details/#table-and-frequency-analysis","title":"Table and Frequency Analysis","text":""},{"location":"Statistic/statistic-details/#univariate-and-bivariate-analysis","title":"Univariate and Bivariate Analysis","text":""},{"location":"Statistic/statistic-details/#hands-on-examples","title":"Hands on Examples","text":"<ol> <li>Data Cleaning + Central Tendency &amp; Dispersion</li> </ol> <pre><code> Dataset: [Netflix Movies and TV Shows](https://www.kaggle.com/datasets/shivamb/netflix-shows)\n Task:\n\n* Load the dataset in Excel or Python.\n* Remove duplicate rows.\n* Handle missing values in `director` and `rating` columns.\n* Calculate mean, median, mode, and standard deviation of duration (after converting to numeric).\n* Comment on which measure best represents the central tendency.\n\n Topics Covered: Data Cleaning, Mean, Median, Mode, Standard Deviation\n</code></pre> <ol> <li>Outlier Detection &amp; Boxplot</li> </ol> <pre><code> Dataset: [Students Performance in Exams](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)\n Task:\n\n* Plot boxplots for `math score`, `reading score`, and `writing score`.\n* Identify and mark outliers using both boxplot and Z-score methods.\n* Discuss how outliers may affect the mean.\n\n Topics Covered: Boxplot, Z-score, IQR, Outliers\n</code></pre> <ol> <li>Dataset: Housing Prices Dataset (Ames Housing)</li> </ol> <pre><code>Task:\n\nLoad the dataset and inspect the Lot Area, SalePrice, and YearBuilt columns.\n\nIdentify and remove any impossible or suspicious values (e.g., Lot Area = 0).\n\nCalculate and interpret mean, median, standard deviation for SalePrice.\n\nUse describe() and interpret each value.\n\nBonus: Compare stats for houses built before and after 1980.\n</code></pre>"},{"location":"Statistic/statistic-details/#4-visualization-univariate-bivariate-analysis","title":"4. Visualization + Univariate &amp; Bivariate Analysis","text":"<pre><code> Dataset: [Titanic Dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset)\n Task:\n\n* Plot histogram of `Age` (univariate).\n* Create a scatterplot between `Age` and `Fare` (bivariate).\n* Interpret the relationship visually.\n* Use groupby to compare average fare for survivors and non-survivors.\n\n Topics Covered: Histogram, Scatterplot, Univariate vs Bivariate\n</code></pre>"},{"location":"Statistic/statistic-details/#5-linear-interpolation-for-missing-data","title":"5. Linear Interpolation for Missing Data","text":"<pre><code> Dataset: [Air Quality Data Set](https://archive.ics.uci.edu/ml/datasets/Air+Quality)\n Task:\n\n* Identify missing values in `CO(GT)` column.\n* Apply linear interpolation to fill gaps.\n* Compare the mean before and after interpolation.\n\n Topics Covered: Missing Value Handling, Linear Interpolation, Data Imputation\n</code></pre>"},{"location":"Statistic/statistic-details/#probability-hypothesis-testing-and-statistical-tests","title":"Probability, Hypothesis Testing, and Statistical Tests","text":"<ol> <li>Introduction to Probability and Distributions, Identifying Distributions</li> <li>Understanding Hypothesis Testing</li> <li>One Sample and 2 Sample t Tests</li> <li>ANOVA, Chi-Square Test</li> </ol>"},{"location":"Statistic/statistic-details/#what-is-probability","title":"What is Probability","text":""},{"location":"Statistic/statistic-details/#why-is-probability-important-in-statistics","title":"Why is Probability Important in Statistics?","text":""},{"location":"Statistic/statistic-details/#what-is-a-probability-distribution","title":"What is a probability distribution","text":""},{"location":"Statistic/statistic-details/#normal-distribution","title":"Normal Distribution","text":""},{"location":"Statistic/statistic-details/#binomial-distribution","title":"Binomial Distribution","text":""},{"location":"Statistic/statistic-details/#poisson-distribution","title":"Poisson Distribution","text":""},{"location":"Statistic/statistic-details/#uniform-distribution","title":"Uniform Distribution","text":""},{"location":"Statistic/statistic-details/#identifying-types-of-distributions","title":"Identifying Types of Distributions","text":""},{"location":"Statistic/statistic-details/#hypothesis-testing_1","title":"Hypothesis Testing","text":"<ul> <li>A hypothesis is simply a claim or assumption we want to test using data. Two Types of Hypotheses:</li> <li> <p>Null Hypothesis (H\u2080): \"There is no effect, no difference, nothing unusual.\"\u2192 It's like the default assumption.</p> </li> <li> <p>Alternative Hypothesis (H\u2081): \"There is an effect or difference.\"\u2192 What you're trying to prove.</p> </li> </ul> <p>Goal of Hypothesis Testing: Use sample data to decide if we have enough evidence to reject the null hypothesis. Example (Real Life):You want to test if a new teaching method improves scores. H\u2080: The new method is no better than the old one. H\u2081: The new method improves scores.</p> <p>You collect exam data and use statistics to decide if the score increase is real or due to chance.</p>"},{"location":"Statistic/statistic-details/#hypothesis-testing-example","title":"Hypothesis Testing Example","text":"<ul> <li>Think of a hypothesis test like a court trial.</li> <li>H0 (Null Hypothesis): Accused is innocent.</li> <li>H1 (Alternative Hypothesis): Accused is guilty.</li> <li>Just like in a trial, we assume the accused (H0) is innocent unless proven guilty.</li> <li>We collect sample data, like presenting evidence in court.</li> <li>The p-value tells us: If the accused were truly innocent (H0),</li> </ul> <p>what is the chance of seeing this kind of evidence?</p> <ul> <li>If the p-value is very low (e.g., &lt; 0.05), it's like strong evidence in court.</li> <li>So we reject H0 and declare the accused guilty (accept H1).</li> </ul> <p>Important: Like a real trial, we never 'prove' innocence or guilt - we assess based on evidence!</p>"},{"location":"Statistic/statistic-details/#what-is-p-value","title":"What is p value","text":""},{"location":"Statistic/statistic-details/#key-terms-hypothesis-testing","title":"Key Terms Hypothesis Testing","text":""},{"location":"Statistic/statistic-details/#steps-in-hypothesis-testing","title":"Steps in Hypothesis Testing","text":"<ol> <li>Define H0 and H1</li> <li>Choose significance level (alpha)</li> <li>Select the test (t-test, ANOVA, etc.)</li> <li>Compute test statistic and p-value</li> <li>Compare p-value to alpha and draw conclusion</li> </ol>"},{"location":"Statistic/statistic-details/#one-sample-t-test","title":"One-Sample t-test","text":"<ul> <li>Used when comparing sample mean to a known/population mean</li> <li>Example: Is average delivery time &gt; 30 minutes?</li> <li>Demo in Excel or Python</li> </ul>"},{"location":"Statistic/statistic-details/#independent-two-sample-t-test","title":"Independent Two-Sample t-test","text":"<ul> <li>Used to compare means of two independent groups</li> <li>Example: Test scores of two teaching methods</li> <li>Assumptions: Normality, Equal variances</li> </ul>"},{"location":"Statistic/statistic-details/#anova","title":"ANOVA","text":"<ul> <li>ANOVA = Analysis of Variance</li> <li>Compares means across 3 or more groups</li> <li>H0: All group means are equal</li> <li>If p &lt; 0.05, at least one group is different</li> </ul>"},{"location":"Statistic/statistic-details/#chi-square-test","title":"Chi-Square Test","text":"<ul> <li>For categorical data (e.g., Gender vs Purchase)</li> <li>Tests independence between variables</li> <li>Compares observed vs expected frequencies</li> </ul>"},{"location":"Statistic/statistic-details/#hands-on-examples_1","title":"Hands on examples","text":"<ol> <li>Probability Question</li> </ol> <pre><code>Dataset: [Students Performance in Exams (Kaggle)](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)\n\nProblem:\nCalculate the probability that a randomly selected student scored more than 80 in math.\n\nSteps for Students:\n\n Load dataset\n Count number of students with math score &gt; 80\n Divide by total number of students\n Print the probability\n</code></pre> <ol> <li>Distribution Identification Question</li> </ol> <pre><code>Dataset: [Medical Cost Personal Dataset (Kaggle)](https://www.kaggle.com/datasets/mirichoi0218/insurance)\n\nProblem:\nIdentify the distribution type of `charges` column using:\n\n Histogram\n Q-Q plot\n Shapiro-Wilk test\n\nSteps for Students:\n\n Plot histogram of `charges`\n Plot Q-Q plot\n Conduct Shapiro-Wilk test and interpret if data is normally distributed\n</code></pre> <ol> <li>One-tail t-test Question</li> </ol> <pre><code>Dataset: [Students Performance in Exams (same as above)](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)\n\nProblem:\nTest the hypothesis that mean math score is greater than 65 (use one-sample, one-tail t-test).\n\nSteps for Students:\n\n Null hypothesis: mean \u2264 65\n Alternative hypothesis: mean &gt; 65\n Conduct one-sample t-test\n Report t-statistic and p-value; conclude\n</code></pre> <ol> <li>Two-tail t-test Question</li> </ol> <pre><code>Dataset: [Students Performance in Exams (same as above)](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams)\n\nProblem:\nTest if average math score differs between male and female students (two-sample, two-tail t-test).\n\nSteps for Students:\n\n Null hypothesis: means are equal\n Alternative hypothesis: means are different\n Perform independent two-sample t-test\n Report t-statistic and p-value; conclude\n</code></pre> <ol> <li>ANOVA Question</li> </ol> <p>```  Dataset: Students Performance in Exams (same as above)</p> <p>Problem: Test if average reading score differs across parental education levels.</p> <p>Steps for Students:</p> <p>Null hypothesis: all group means are equal  Alternative hypothesis: at least one group differs  Perform ANOVA  Report F-statistic and p-value; conclude <pre><code>6. **Chi-Square Test Question**\n</code></pre> Dataset: Titanic Dataset (Kaggle)</p> <p>Problem: Test if survival status is independent of passenger class.</p> <p>Steps for Students:</p> <p>Create a contingency table (Passenger Class vs Survived)  Perform chi-square test of independence  Report chi-square statistic, p-value, and interpret <pre><code># 1 tailed t test\nfrom scipy import stats\n\n# Sample delivery times (in minutes)\ndelivery_times = [32, 35, 30, 31, 36, 33, 29, 34, 37, 28]\n\n# Population mean to test against\nmu_0 = 30\n\n# One-sample t-test (one-tailed test: greater than 30)\nt_stat, p_value_two_tailed = stats.ttest_1samp(delivery_times, mu_0)\n\n# For one-tailed test (greater than), divide p-value by 2\np_value_one_tailed = p_value_two_tailed / 2\n\n# Output results\nprint(\"Sample Mean:\", round(sum(delivery_times) / len(delivery_times), 2))\nprint(\"t-statistic:\", round(t_stat, 3))\nprint(\"p-value (one-tailed):\", round(p_value_one_tailed, 4))\n\n# Conclusion\nalpha = 0.05\nif p_value_one_tailed &lt; alpha and t_stat &gt; 0:\n    print(\"Conclusion: Reject H0 \u2014 average delivery time is greater than 30 minutes.\")\nelse:\n    print(\"Conclusion: Fail to reject H0 \u2014 not enough evidence that average is greater.\")\n\n#Independent 2 tail test\nfrom scipy import stats\n\n# Scores from two independent groups\ngroup_a = [75, 78, 74, 72, 80, 77, 73, 76, 79, 74]  # Traditional\ngroup_b = [82, 85, 84, 81, 86, 83, 80, 87, 85, 84]  # Interactive\n\n# Perform two-sample t-test (equal variances assumed)\nt_stat, p_value = stats.ttest_ind(group_a, group_b, equal_var=True)\n\n# Output results\nprint(\"Group A Mean:\", round(sum(group_a)/len(group_a), 2))\nprint(\"Group B Mean:\", round(sum(group_b)/len(group_b), 2))\nprint(\"t-statistic:\", round(t_stat, 3))\nprint(\"p-value:\", round(p_value, 4))\n\n# Interpret result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Conclusion: Reject H0 \u2014 There is a significant difference between the teaching methods.\")\nelse:\n    print(\"Conclusion: Fail to reject H0 \u2014 No significant difference detected.\")\n\n#Anova\n\nfrom scipy import stats\n\n# Step 1: Create test scores for each group\ngroup_a = [70, 72, 68, 75, 74]  # Traditional\ngroup_b = [80, 82, 85, 79, 81]  # Online\ngroup_c = [90, 88, 92, 91, 89]  # Workshop\n\n# Step 2: Perform one-way ANOVA test\nf_stat, p_value = stats.f_oneway(group_a, group_b, group_c)\n\n# Step 3: Print the result\nprint(\"F-statistic:\", round(f_stat, 2))\nprint(\"p-value:\", round(p_value, 4))\n\n# Step 4: Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Conclusion: Reject H0 \u2014 At least one group is different.\")\nelse:\n    print(\"Conclusion: Fail to reject H0 \u2014 No significant difference between groups.\")\n#Chi Square\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Observed frequency table (2x2)\n# Rows = Gender (Male, Female)\n# Columns = Purchase (Yes, No)\n\nobserved = np.array([\n    [30, 20],  # Male: 30 Yes, 20 No\n    [10, 40]   # Female: 10 Yes, 40 No\n])\n\n\n\n## Q-Q Plot Worked Example code\n\n# Q-Q Plot Worked Example: Custom Data vs Theoretical Normal Quantiles\n</code></pre> import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm <pre><code># ----------------------------\n# 1. Your dataset\n# ----------------------------\n</code></pre> data = np.array([55, 60, 62, 65, 68, 70, 75, 80, 85, 90]) n = len(data) <pre><code># ----------------------------\n# 2. Calculate percentiles for each data point\n# Formula: (i - 0.5) / n\n# ----------------------------\n</code></pre> percentiles = [(i - 0.5) / n for i in range(1, n+1)] <pre><code># ----------------------------\n# 3. Calculate theoretical normal quantiles using inverse CDF (ppf)\n# ----------------------------\n</code></pre> expected_normal = [norm.ppf(p) for p in percentiles] <pre><code># ----------------------------\n# 4. Plotting\n# ----------------------------\n</code></pre> plt.figure(figsize=(8,6)) plt.scatter(expected_normal, data, color='blue') plt.title('Custom Q-Q Plot: Data vs Theoretical Normal Quantiles') plt.xlabel('Theoretical Quantiles (Standard Normal)') plt.ylabel('Data Values (Exam Scores)') <pre><code># Add a best fit line for reference\n</code></pre> slope, intercept = np.polyfit(expected_normal, data, 1) plt.plot(expected_normal, np.array(expected_normal)slope + intercept, color='red', linestyle='--') <pre><code># Annotate each point with its percentile for teaching\n</code></pre> for x, y, p in zip(expected_normal, data, percentiles): plt.text(x, y+0.5, f\"{int(p100)}%\", fontsize=8) plt.grid(True) plt.tight_layout() plt.show() ```</p> <ul> <li>Correlation (Pearson, Spearman)</li> <li>Non-parametric tests overview</li> <li>PCA &amp; Factor Analysis</li> <li>Cluster &amp; Association Analysis</li> <li>Time Series Analysis</li> </ul>"},{"location":"Statistic/statistic-details/#multivariate-analysis-pca-clustering-time-series","title":"Multivariate Analysis, PCA, Clustering &amp; Time Series","text":""},{"location":"Statistic/statistic-details/#correlation-measuring-relationships-between-two-things","title":"Correlation: Measuring Relationships Between Two Things","text":""},{"location":"Statistic/statistic-details/#visualizing-correlation","title":"Visualizing Correlation","text":""},{"location":"Statistic/statistic-details/#visualizing-pearson-vs-spearman-correlation-example","title":"Visualizing Pearson vs. Spearman Correlation (Example)","text":""},{"location":"Statistic/statistic-details/#what-is-non-parametric-test","title":"What is Non Parametric Test","text":""},{"location":"Statistic/statistic-details/#principal-component-analysispca","title":"Principal Component Analysis(PCA)","text":""},{"location":"Statistic/statistic-details/#why-is-pca-needed","title":"Why is PCA needed?","text":""},{"location":"Statistic/statistic-details/#interpreting-pca","title":"Interpreting PCA","text":""},{"location":"Statistic/statistic-details/#pca-in-a-nutshell-with-python-code","title":"PCA in a nutshell with python code","text":"<p>Iris Dataset https://archive.ics.uci.edu/dataset/53/iris</p>"},{"location":"Statistic/statistic-details/#factor-analysis-and-cluster-analysis","title":"Factor Analysis and Cluster Analysis","text":""},{"location":"Statistic/statistic-details/#clustering-algorithms-example","title":"Clustering Algorithms Example","text":""},{"location":"Statistic/statistic-details/#association-analysis","title":"Association Analysis","text":""},{"location":"Statistic/statistic-details/#time-series-basics","title":"Time Series Basics","text":""},{"location":"Statistic/statistic-details/#assignment-title","title":"Assignment Title:","text":"<p>Hypothesis Testing: Parametric vs Non-Parametric Analysis Based on Data Distribution</p>"},{"location":"Statistic/statistic-details/#assignment-objectives","title":"Assignment Objectives:","text":"<p>By completing this assignment, you will be able to: - Understand when to use parametric vs non-parametric tests - Perform distribution checks (normality tests, plots) - Formulate and test null and alternative hypotheses - Use at least one parametric and one non-parametric test appropriately - Interpret results clearly with visuals and reasoning</p>"},{"location":"Statistic/statistic-details/#dataset","title":"Dataset:","text":"<p>Employee Attrition Dataset \u2013 Kaggle</p> <p>Dataset Features (Selected)</p> <ul> <li>Age (numeric)</li> <li>MonthlyIncome (numeric)</li> <li>JobSatisfaction (1\u20134)</li> <li>Attrition (Yes/No)</li> <li>JobRole (Sales Executive, Research Scientist, etc.)</li> </ul>"},{"location":"Statistic/statistic-details/#problem-statement","title":"Problem Statement:","text":"<p>You are an HR data analyst at a large firm. The leadership wants to understand whether: - Monthly income and age differ between employees who left vs stayed - Job satisfaction differs across job roles</p> <p>Use statistical hypothesis testing to answer the following:</p>"},{"location":"Statistic/timeseries/","title":"Time Series Old","text":""},{"location":"Statistic/timeseries/#time-series-regression-predictive-modeling","title":"Time Series, Regression &amp; Predictive Modeling","text":""},{"location":"Statistic/timeseries/#time-series-basics","title":"Time Series Basics","text":"<p>Time series is a core concept in data science and analytics, particularly when data is collected over time intervals. Let me break down the key aspects of time series, especially relevant if you're doing forecasting, anomaly detection, or trend analysis:</p>"},{"location":"Statistic/timeseries/#temporal-order","title":"Temporal Order","text":"<p>Definition: Observations are collected at specific time intervals (daily, monthly, yearly, etc.).</p> <p>Importance: Order matters; past values influence future ones.</p>"},{"location":"Statistic/timeseries/#components-of-time-series","title":"Components of Time Series","text":"<ol> <li>Trend: Long-term increase or decrease in the data.(E.g., sales increasing over years due to company growth.)</li> <li>Seasonality: Repeating short-term cycles (e.g., sales spike every December).(Can be daily, weekly, monthly, quarterly, etc.)</li> <li>Cyclic Patterns: Fluctuations over long, non-fixed periods.(Usually driven by economic conditions or business cycles.)</li> <li>Irregular (Noise): Random variation; not explained by trend/seasonality.(Often treated as residuals in models.)</li> <li>Goal: Use past data to forecast future values</li> </ol> <p>Time Series is data collected at regular time intervals. Examples: - Daily stock prices - Monthly rainfall - Hourly website traffic</p>"},{"location":"Statistic/timeseries/#stationarity-non-stationary-in-time-series","title":"Stationarity &amp; Non-stationary in Time Series","text":"<ol> <li>Stationary series: Mean, variance, autocorrelation are constant over time. </li> </ol>"},{"location":"Statistic/timeseries/#stationary-series-example","title":"Stationary Series Example","text":"<p>\u2705 Stationary Series Example \u2014 White Noise A stationary time series has:  - White noise is a random sequence of values with:     - Constant mean (usually 0)     - Constant variance     - No trend or seasonality</p> <p>\ud83d\udcca Python Code to Plot White Noise <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate white noise\nnp.random.seed(42)\nwhite_noise = np.random.normal(loc=0, scale=1, size=100)\n\n# Plot time series\nplt.figure(figsize=(12, 4))\nplt.plot(white_noise, color='blue', label='White Noise')\nplt.axhline(y=0, color='red', linestyle='--', label='Mean = 0')\nplt.title(\"White Noise - Stationary Series\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre></p> <p></p> <p>\ud83e\udde0 Explanation of the Plot - \ud83d\udcc9 The values fluctuate randomly around the horizontal red dashed line (mean = 0). - \ud83d\udfe6 There\u2019s no visible upward or downward trend. - \ud83c\udfaf The variance stays consistent (spread of data remains the same). - \u2705 This makes it a stationary time series.</p> <p>\ud83d\udccc Bonus: ADF Test to Confirm Stationarity <pre><code>from statsmodels.tsa.stattools import adfuller\nimport numpy as np\n\n# Generate white noise\nnp.random.seed(42)\nwhite_noise = np.random.normal(loc=0, scale=1, size=100)\n\nresult = adfuller(white_noise)\nprint(\"ADF Test Statistic:\", result[0])\nprint(\"p-value:\", result[1])\nprint(\"=&gt; Stationary \u2705\" if result[1] &lt; 0.05 else \"=&gt; Not Stationary \u274c\")\n</code></pre></p> <pre><code>ADF Test Statistic: -10.084425913669714\np-value: 1.1655044784188669e-17\n=&gt; Stationary \u2705\n</code></pre> <p>\ud83d\udcca Visual Comparison: Stationary vs Non-Stationary Series <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\nnp.random.seed(42)\n\n# --- White Noise (Stationary) ---\nwhite_noise = np.random.normal(loc=0, scale=1, size=100)\n\n# --- Trending Series (Non-Stationary) ---\ntrend = np.linspace(0, 10, 100) + np.random.normal(0, 1, 100)\n\n# --- Seasonal Series (Non-Stationary) ---\nseasonal = 5 * np.sin(np.linspace(0, 20, 100)) + np.random.normal(0, 0.5, 100)\n\n# ==== Plot All ====\nplt.figure(figsize=(15, 8))\n\n# White Noise\nplt.subplot(3, 1, 1)\nplt.plot(white_noise, label='White Noise', color='blue')\nplt.axhline(y=np.mean(white_noise), color='red', linestyle='--', label='Mean')\nplt.title(\"\u2705 Stationary Series: White Noise\")\nplt.legend()\nplt.grid()\n\n# Trending Series\nplt.subplot(3, 1, 2)\nplt.plot(trend, label='Trending Series', color='green')\nplt.title(\"\u274c Non-Stationary Series: Trend\")\nplt.legend()\nplt.grid()\n\n# Seasonal Series\nplt.subplot(3, 1, 3)\nplt.plot(seasonal, label='Seasonal Series', color='orange')\nplt.title(\"\u274c Non-Stationary Series: Seasonality\")\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n</code></pre></p> <p></p> <p>\ud83d\udccc Visual Summary | Series Type     | Description                                 | Stationary? | | --------------- | ------------------------------------------- | ----------- | | White Noise | Random around mean 0, constant variance     | \u2705 Yes       | | Trend       | Values increase steadily over time          | \u274c No        | | Seasonality | Regular up/down pattern over fixed interval | \u274c No        |</p> <p>\ud83e\uddea ADF Test on Each <pre><code>def adf_test(series, name):\n    result = adfuller(series)\n    print(f\"{name} - ADF p-value: {result[1]:.4f} =&gt; {'Stationary \u2705' if result[1] &lt; 0.05 else 'Not Stationary \u274c'}\")\n\nadf_test(white_noise, \"White Noise\")\nadf_test(trend, \"Trending Series\")\nadf_test(seasonal, \"Seasonal Series\")\n</code></pre></p> <pre><code>White Noise - ADF p-value: 0.0000 =&gt; Stationary \u2705\nTrending Series - ADF p-value: 0.8826 =&gt; Not Stationary \u274c\nSeasonal Series - ADF p-value: 0.0000 =&gt; Stationary \u2705\n</code></pre> <p>\ud83d\udd39 Differenced Stock Returns While stock prices are non-stationary, daily returns are often stationary. \u2705 Mean and variance stable over time, no strong trend.</p> <p>\ud83d\udd39 Temperature Anomalies (after de-trending) Global temperature anomalies after removing long-term climate trend. \u2705 Can be stationary if trend/seasonality removed.</p> <p>\ud83d\udd39 Heart Rate Variability (in controlled settings) - Measured over short intervals in healthy individuals. - Can be stable around a mean with limited variance. \u2705 Stationary under resting conditions.</p> <p>\ud83d\udd39 What is Differencing? Differencing removes trend (and sometimes seasonality) by subtracting the previous value from the current one.</p> <p></p> <p>Used when there's still a trend after first differencing (e.g., quadratic or accelerating trends).</p> <p>Step 1: Create a Trending (Non-Stationary) Series <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nnp.random.seed(42)\n\n# Original series with upward trend\ntime = np.arange(100)\ntrend_series = time + np.random.normal(0, 1, size=100)\n\n# First difference\nfirst_diff = np.diff(trend_series, n=1)\n\n# Second difference\nsecond_diff = np.diff(trend_series, n=2)\n</code></pre></p> <p>\ud83d\udcca Step 2: Plot All Three Series <pre><code>plt.figure(figsize=(14, 8))\n\n# Original\nplt.subplot(3, 1, 1)\nplt.plot(trend_series, label='Original Series (Non-Stationary)', color='orange')\nplt.title(\"\u274c Original Series with Trend\")\nplt.grid()\nplt.legend()\n\n# First difference\nplt.subplot(3, 1, 2)\nplt.plot(first_diff, label='1st Difference', color='green')\nplt.axhline(np.mean(first_diff), color='red', linestyle='--', label='Mean')\nplt.title(\"\u27a1\ufe0f First Differenced Series (Removes Linear Trend)\")\nplt.grid()\nplt.legend()\n\n# Second difference\nplt.subplot(3, 1, 3)\nplt.plot(second_diff, label='2nd Difference', color='blue')\nplt.axhline(np.mean(second_diff), color='red', linestyle='--')\nplt.title(\"\u27a1\ufe0f Second Differenced Series (Removes Quadratic Trend)\")\nplt.grid()\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre></p> <p></p> <p>\ud83e\udde0 Explanation | Step     | What Happens                              | Trend Removed? | Stationary? | | -------- | ----------------------------------------- | -------------- | ----------- | | Original | Steadily increasing \u2192 upward trend        | \u274c No           | \u274c No        | | 1st Diff | \u0394 between each value \u2192 trend removed      | \u2705 Linear       | \u2705 Usually   | | 2nd Diff | \u0394 between changes \u2192 only use if 1st fails | \u2705 Quadratic    | \u2705 Yes       |</p> <p>\ud83e\uddea ADF Stationarity Check <pre><code>from statsmodels.tsa.stattools import adfuller\n\ndef adf_check(series, name):\n    result = adfuller(series)\n    print(f\"{name} | ADF p-value: {result[1]:.4f} =&gt; {'Stationary \u2705' if result[1] &lt; 0.05 else 'Not Stationary \u274c'}\")\n\nadf_check(trend_series, \"Original\")\nadf_check(first_diff, \"1st Difference\")\nadf_check(second_diff, \"2nd Difference\")\n</code></pre></p> <pre><code>Original | ADF p-value: 0.9773 =&gt; Not Stationary \u274c\n1st Difference | ADF p-value: 0.0000 =&gt; Stationary \u2705\n2nd Difference | ADF p-value: 0.0000 =&gt; Stationary \u2705\n</code></pre> <p>\u2705 When to Stop Differencing? - Stop as soon as you get stationarity (ADF p-value &lt; 0.05). - Don't over-difference, or you'll remove useful signal.</p>"},{"location":"Statistic/timeseries/#example","title":"Example:","text":"<p>Differencing to a real-world non-stationary dataset \u2014 for example, stock prices \u2014 to show how it makes the series stationary and ready for modeling.</p> <p>\ud83d\udcca Real Dataset: Stock Prices Example (Apple Inc.)</p> <ol> <li>Load stock price data (e.g., AAPL)</li> <li>Plot original series (non-stationary)</li> <li>Apply first and second differencing</li> <li>Plot and run ADF test</li> </ol> <p>\u2705 Step 1: Install &amp; Import Required Libraries <pre><code>pip install yfinance pandas matplotlib statsmodels\n</code></pre> <pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\n</code></pre></p> <p>\u2705 Step 2: Load Apple Stock Data <pre><code># Load 2 years of daily Apple stock prices\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\nprices = df['Close']\nprices = prices.dropna()\n</code></pre></p> <p></p> <p></p> <p>\u2705 Step 3: Plot Original Series <pre><code>plt.figure(figsize=(12, 4))\nplt.plot(prices, label='Original AAPL Close Prices', color='orange')\nplt.title(\"\u274c Original Stock Prices (Likely Non-Stationary)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.grid()\nplt.show()\n</code></pre></p> <p></p> <p>\u2705 Step 4: Apply First Differencing <pre><code>first_diff = prices.diff().dropna()\n\nplt.figure(figsize=(12, 4))\nplt.plot(first_diff, label='1st Difference of AAPL Prices', color='green')\nplt.title(\"\u2705 First Differenced Series (Log Returns Approx.)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price Change\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.legend()\nplt.grid()\nplt.show()\n</code></pre></p> <p></p> <p>\u2705 Step 5: ADF Test on Each Series <pre><code>def adf_check(series, name):\n    result = adfuller(series)\n    print(f\"{name} | ADF p-value: {result[1]:.4f} =&gt; {'Stationary \u2705' if result[1] &lt; 0.05 else 'Not Stationary \u274c'}\")\n\nadf_check(prices, \"Original Series\")\nadf_check(first_diff, \"1st Difference\")\n</code></pre></p> <pre><code>Original Series | ADF p-value: 0.9504 =&gt; Not Stationary \u274c\n1st Difference | ADF p-value: 0.0000 =&gt; Stationary \u2705\n</code></pre> <p>\ud83e\udde0 What You Should Observe: - Original Series:     - Has an upward/downward trend.     - ADF p-value &gt; 0.05 \u2192 \u274c Not Stationary</p> <ul> <li>First Differenced Series:<ul> <li>Looks more like a noisy signal.</li> <li>ADF p-value &lt; 0.05 \u2192 \u2705 Stationary</li> </ul> </li> </ul> <p>\ud83d\udccc Optional: Second Differencing (only if needed) Note: But in most financial data, 1st differencing is enough.</p> <p>\u2705 Summary | Series                 | Looks Like        | ADF Test (p-value) | Stationary? | | ---------------------- | ----------------- | ------------------ | ----------- | | <code>prices</code>               | Trending          | &gt; 0.05             | \u274c No        | | <code>prices.diff()</code>        | No trend (flat)   | &lt; 0.05             | \u2705 Yes       | | <code>prices.diff().diff()</code> | Over-differenced? | usually &lt; 0.05     | \u2705 Maybe     |</p> <p>\ud83d\udcca Comparison: prices vs prices.diff() | Feature           | <code>prices</code>                       | <code>prices.diff()</code>                     | | ----------------- | ------------------------------ | ----------------------------------- | | Meaning           | Actual stock prices            | Daily change in stock price         | | Stationary?       | \u274c Typically non-stationary | \u2705 Often stationary              | | Trend/Seasonality | Contains trends                | Removes trend                       | | Use in ARIMA      | Raw data \u2192 not usable directly | Used as input for ARIMA (<code>d=1</code>)     | | First Value       | Actual closing price           | <code>NaN</code> (no previous day to subtract) |</p> <p>\ud83e\udde0 Conceptual Example Let\u2019s say we have these 5 days of Apple stock prices: Date       Price (prices)</p> <p>Day 1      150 Day 2      152 Day 3      149 Day 4      151 Day 5      154</p> <p>\u27a1\ufe0f prices.diff() = Daily change: Date       Change (prices.diff())</p> <p>Day 1      NaN        \u2190 No previous day Day 2      2    (152 - 150) Day 3     -3    (149 - 152) Day 4      2    (151 - 149) Day 5      3    (154 - 151)</p> <p>\ud83d\udcc8 Visual Comparison <pre><code>import yfinance as yf\nimport matplotlib.pyplot as plt\n\n# Download AAPL price data\ndf = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-12-31\")\nprices = df['Close']\ndaily_diff = prices.diff()\n\nplt.figure(figsize=(14, 5))\n\n# Original prices\nplt.subplot(1, 2, 1)\nplt.plot(prices, label=\"AAPL Prices\", color='orange')\nplt.title(\"\ud83d\udcc8 AAPL Stock Prices (Non-Stationary)\")\nplt.xlabel(\"Date\"); plt.ylabel(\"Price\")\nplt.grid(); plt.legend()\n\n# Differenced prices\nplt.subplot(1, 2, 2)\nplt.plot(daily_diff, label=\"Daily Price Change\", color='green')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"\ud83d\udcc9 AAPL Daily Price Change (Stationary)\")\nplt.xlabel(\"Date\"); plt.ylabel(\"\u0394 Price\")\nplt.grid(); plt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre></p> <p></p> <p>\u2705 When to Use prices.diff() You should use prices.diff() when: - You want to remove trend for modeling. - You're building ARIMA, LSTM, or other time series models. - You need a stationary signal.</p> <p>\ud83e\udde0 Summary | Task                              | Use                                  | | --------------------------------- | ------------------------------------ | | Plot/visualize trends             | <code>prices</code>                             | | Train ARIMA, forecast next values | <code>prices.diff()</code>                      | | Calculate daily returns           | <code>prices.pct_change()</code> (for % change) |</p>"},{"location":"Statistic/timeseries/#a-real-world-forecasting-example-using-pricesdiff-with-arima-based-on-stock-price-data-apple-inc-this-will-help-you-understand","title":"A real-world forecasting example using prices.diff() with ARIMA, based on stock price data (Apple Inc.). This will help you understand:","text":"<ol> <li>Why we use prices.diff()</li> <li>How to fit an ARIMA model</li> <li>How to forecast future stock price changes</li> <li>How to reconstruct actual prices from the forecasted diffs</li> </ol>"},{"location":"Statistic/timeseries/#tools-you-need","title":"\ud83d\udce6 Tools You Need","text":"<pre><code>pip install yfinance pandas matplotlib statsmodels pmdarima\n</code></pre> <p>\u2705 Step-by-Step Forecasting Using prices.diff() + ARIMA</p> <p>\ud83d\udccc Step 1: Import and Load Apple Stock Data <pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\n\n# Load stock price data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\nprices = df['Close'].dropna()\n</code></pre></p> <p>\ud83d\udccc Step 2: Make It Stationary (First Differencing) <pre><code>diff_prices = prices.diff().dropna()\n\n# Optional: ADF test\ndef adf_test(series, name):\n    result = adfuller(series)\n    print(f\"{name} ADF p-value: {result[1]:.4f} =&gt; {'Stationary \u2705' if result[1] &lt; 0.05 else 'Not Stationary \u274c'}\")\n\nadf_test(prices, \"Original Prices\")\nadf_test(diff_prices, \"Differenced Prices\")\n</code></pre></p> <pre><code>Original Prices ADF p-value: 0.9504 =&gt; Not Stationary \u274c\nDifferenced Prices ADF p-value: 0.0000 =&gt; Stationary \u2705\n</code></pre> <p>\ud83d\udccc Step 3: Fit ARIMA Model (ARIMA(1,0,1) on differenced data) We use ARIMA(1,0,1) because we\u2019ve already differenced manually (so d=0).</p> <pre><code>model = ARIMA(diff_prices, order=(1, 0, 1))\nfitted = model.fit()\nprint(fitted.summary())\n</code></pre> <p></p> <p>\ud83d\udccc Step 4: Forecast Next 30 Days of Price Changes <pre><code>forecast_diff = fitted.forecast(steps=30)\nforecast_diff.plot(title=\"Forecasted Daily Price Change (diff)\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.grid()\nplt.show()\n</code></pre></p> <p></p> <p>\ud83d\udccc Step 5: Convert Diffs Back to Real Prices To get actual prices, add the diffs to the last known price:</p> <pre><code># Step 1: Download data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\nprices = df['Close'].dropna()\n\n# \u2705 Step 2: Fit ARIMA on original data (ARIMA will handle differencing)\nmodel = ARIMA(prices, order=(1, 1, 1))  # d=1 for automatic differencing\nfitted_model = model.fit()\n\n# \u2705 Step 3: Forecast future prices directly\nforecast = fitted_model.forecast(steps=30)\n\n# \u2705 Step 4: Set future index\nforecast_index = pd.bdate_range(start=prices.index[-1] + pd.Timedelta(days=1), periods=30)\nforecast_series = pd.Series(forecast.values, index=forecast_index)\n\n# \u2705 Step 5: Plot actual + forecast\nplt.figure(figsize=(14, 6))\nplt.plot(prices[-60:], label='Actual Prices (last 60 days)', color='orange')\nplt.plot(forecast_series, label='Forecasted Prices (next 30 days)', color='green', linestyle='--', marker='o')\n\nplt.plot(\n    [prices.index[-1], forecast_series.index[0]],\n    [float(prices.iloc[-1]), float(forecast_series.iloc[0])],\n    color='gray', linestyle=':', label='Actual \u2192 Forecast Transition'\n)\n\nplt.title(\"\ud83d\udcc8 AAPL Stock Price Forecast (ARIMA on original data)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price (USD)\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Step 1: Load data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\nprices = df['Close'].dropna()\n\n# Step 2: Fit ARIMA model (let d=1 handle differencing)\nmodel = ARIMA(prices, order=(1, 1, 1))\nfitted_model = model.fit()\n\n# Step 3: Forecast future values with confidence intervals\nn_forecast = 30\nforecast_result = fitted_model.get_forecast(steps=n_forecast)\nforecast_mean = forecast_result.predicted_mean\nconf_int = forecast_result.conf_int()\n\n# Step 4: Create forecast index\nforecast_index = pd.bdate_range(start=prices.index[-1] + pd.Timedelta(days=1), periods=n_forecast)\nforecast_series = pd.Series(forecast_mean.values, index=forecast_index)\nconf_df = pd.DataFrame(conf_int.values, index=forecast_index, columns=[\"Lower Bound\", \"Upper Bound\"])\n\n# Step 5: Plot actual, forecasted, and confidence interval\nplt.figure(figsize=(14, 6))\n\n# Actual\nplt.plot(prices[-60:], label='Actual Prices', color='orange')\n\n# Forecast\nplt.plot(forecast_series, label='Forecasted Prices', color='green', linestyle='--', marker='o')\n\n# Confidence intervals\nplt.fill_between(\n    forecast_index,\n    conf_df[\"Lower Bound\"],\n    conf_df[\"Upper Bound\"],\n    color='green',\n    alpha=0.2,\n    label='95% Confidence Interval'\n)\n\n# Transition line\nplt.plot(\n    [prices.index[-1], forecast_index[0]],\n    [float(prices.iloc[-1]), float(forecast_series.iloc[0])],\n    color='gray', linestyle=':', label='Actual \u2192 Forecast Transition'\n)\n\nplt.title(\"\ud83d\udcc8 AAPL Stock Price Forecast (statsmodels ARIMA)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price (USD)\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"Statistic/timeseries/#pmdarima","title":"pmdarima","text":"<p>\u2705 Automatically selects best ARIMA parameters using pmdarima.auto_arima \u2705 Forecasts future values with confidence intervals \u2705 Plots actual, forecasted values, and 95% confidence bounds \u2705 Works on stock prices or your sales data (CSV)</p> <p>\u2705 1. \ud83d\udd27 Install pmdarima (if not already installed) <pre><code>pip install pmdarima==1.8.5 numpy==1.23.5 --force-reinstall --no-cache-dir\n</code></pre></p> <p>\u2705 2. Complete Code with Auto ARIMA + Confidence Intervals</p> <pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pmdarima as pm\n\n# Step 1: Download AAPL stock prices\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\nprices = df['Close'].dropna()\n\n# Step 2: Auto ARIMA model selection\nmodel = pm.auto_arima(\n    prices,\n    seasonal=False,\n    stepwise=True,\n    suppress_warnings=True,\n    error_action=\"ignore\",\n    trace=True\n)\n\n# Step 3: Forecast next 30 business days with confidence intervals\nn_forecast = 30\nforecast, conf_int = model.predict(n_periods=n_forecast, return_conf_int=True)\n\n# Step 4: Create future date index\nforecast_index = pd.bdate_range(start=prices.index[-1] + pd.Timedelta(days=1), periods=n_forecast)\n\nforecast_series = pd.Series(forecast, index=forecast_index)\nconf_df = pd.DataFrame(conf_int, index=forecast_index, columns=[\"Lower Bound\", \"Upper Bound\"])\n\n# Step 5: Plot actual, forecast, and confidence intervals\nplt.figure(figsize=(14, 6))\n\n# Plot actual prices\nplt.plot(prices[-60:], label=\"Actual Prices (last 60 days)\", color=\"orange\")\n\n# Plot forecast\nplt.plot(forecast_series, label=\"Forecasted Prices\", color=\"green\", linestyle=\"--\", marker='o')\n\n# Plot confidence interval (shaded area)\nplt.fill_between(\n    forecast_index,\n    conf_df[\"Lower Bound\"],\n    conf_df[\"Upper Bound\"],\n    color=\"green\",\n    alpha=0.2,\n    label=\"95% Confidence Interval\"\n)\n\n# Draw connector line\nplt.plot(\n    [prices.index[-1], forecast_series.index[0]],\n    [float(prices.iloc[-1]), float(forecast_series.iloc[0])],\n    color='gray', linestyle=':', label='Actual \u2192 Forecast Transition'\n)\n\nplt.title(\"\ud83d\udcc8 AAPL Stock Price Forecast (Auto ARIMA with Confidence Interval)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price (USD)\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Statistic/timeseries/#use-facebook-prophet-or-lstm-model-instead","title":"Use Facebook Prophet or LSTM model instead","text":"Method Pros Cons Facebook Prophet Easy to use, handles trends &amp; seasonality, interpretable Assumes additive/multiplicative components LSTM (Deep Learning) Learns complex patterns, flexible Needs more data &amp; tuning, less interpretable <ol> <li> <p>\u2705 Facebook Prophet Code (quick, interpretable)</p> </li> <li> <p>\u2705 LSTM Code (deep learning with Keras)</p> </li> </ol> <p>\u2705 OPTION 1: Facebook Prophet</p> <p>\ud83d\udd27 Install:</p> <pre><code>pip install prophet\n</code></pre> <p>\u2705 Code to Forecast AAPL with Prophet</p> <p><pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\n# Step 1: Download data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\n\n# Step 2: Prepare data for Prophet\ndf = df.reset_index()\ndf = df[['Date', 'Close']]\ndf.columns = ['ds', 'y']  # Prophet needs ds (datetime) and y (value)\n\n# Check for NaNs or data issues\ndf = df.dropna()\ndf['ds'] = pd.to_datetime(df['ds'])\n\n# Step 3: Fit the model\nmodel = Prophet(daily_seasonality=True)\nmodel.fit(df)\n\n# Step 4: Create future dates\nfuture = model.make_future_dataframe(periods=30)\nforecast = model.predict(future)\n\n# Step 5: Plot forecast\nfig1 = model.plot(forecast)\nplt.title(\"\ud83d\udcc8 AAPL Forecast - Facebook Prophet\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.grid(True)\nplt.show()\n</code></pre> </p> <p><pre><code>import yfinance as yf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\n# Step 1: Load AAPL stock data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\ndf = df.reset_index()\ndf = df[['Date', 'Close']]\ndf.columns = ['ds', 'y']\ndf.dropna(inplace=True)\ndf['ds'] = pd.to_datetime(df['ds'])\n\n# Step 2: Fit Prophet model\nmodel = Prophet(daily_seasonality=True)\nmodel.fit(df)\n\n# Step 3: Forecast next 30 days\nfuture = model.make_future_dataframe(periods=30)\nforecast = model.predict(future)\n\n# Step 4: Merge forecast with actual for overlay\nforecast_filtered = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nmerged = pd.merge(df, forecast_filtered, on='ds', how='outer')\n\n# Step 5: Plot actual and forecast overlay\nplt.figure(figsize=(14, 6))\n\n# Plot actual values\nplt.plot(merged['ds'], merged['y'], label='Actual', color='orange')\n\n# Plot forecast values\nplt.plot(merged['ds'], merged['yhat'], label='Forecast', color='green', linestyle='--')\n\n# Confidence interval shading\nplt.fill_between(\n    merged['ds'],\n    merged['yhat_lower'],\n    merged['yhat_upper'],\n    color='green',\n    alpha=0.2,\n    label='95% Confidence Interval'\n)\n\nplt.title(\"\ud83d\udcc8 AAPL Stock Price: Actual vs Forecast (Prophet)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price (USD)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> </p> <p>\u2705 OPTION 2: LSTM Model with Keras (Deep Learning)</p> <p>\ud83d\udd27 Install:</p> <pre><code>pip install tensorflow\n</code></pre> <p>\u2705 LSTM Code for Price Forecasting</p> <pre><code>import yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Load data\ndf = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-12-31\")\ndata = df['Close'].values.reshape(-1, 1)\n\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data)\n\n# Create sequences\nX, y = [], []\nseq_len = 60  # Use past 60 days to predict next\n\nfor i in range(seq_len, len(scaled_data)):\n    X.append(scaled_data[i-seq_len:i])\n    y.append(scaled_data[i])\n\nX, y = np.array(X), np.array(y)\n\n# Build model\nmodel = Sequential([\n    LSTM(50, return_sequences=False, input_shape=(X.shape[1], 1)),\n    Dense(1)\n])\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, y, epochs=10, batch_size=32, verbose=1)\n\n# Forecast next 30 days\nlast_seq = scaled_data[-seq_len:]\nforecast = []\n\nfor _ in range(30):\n    input_seq = last_seq.reshape(1, seq_len, 1)\n    pred = model.predict(input_seq, verbose=0)\n    forecast.append(pred[0][0])\n    last_seq = np.append(last_seq[1:], pred, axis=0)\n\n# Inverse transform forecast\nforecast_prices = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n\n# Create index\nforecast_index = pd.bdate_range(start=df.index[-1] + pd.Timedelta(days=1), periods=30)\n\n# Plot\nplt.figure(figsize=(14, 6))\nplt.plot(df.index[-60:], data[-60:], label='Actual Prices', color='orange')\nplt.plot(forecast_index, forecast_prices, label='LSTM Forecast', color='green', linestyle='--', marker='o')\nplt.title(\"\ud83d\udcc8 AAPL Stock Price Forecast (LSTM)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price (USD)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p></p>"},{"location":"Statistic/timeseries/#autocorrelation-cross-correlation","title":"Autocorrelation &amp; Cross-Correlation","text":""},{"location":"Statistic/timeseries/#moving-averages-smoothing","title":"Moving Averages &amp; Smoothing","text":""},{"location":"Statistic/timeseries/#holt-winters-method","title":"Holt-Winters Method","text":""},{"location":"Statistic/timeseries/#additive-vs-multiplicative-models","title":"Additive Vs Multiplicative Models","text":""},{"location":"Statistic/timeseries/#ar-auto-regression","title":"AR (Auto Regression)","text":""},{"location":"Statistic/timeseries/#arima-models","title":"ARIMA Models","text":""},{"location":"Statistic/timeseries/#arimax-vs-sarimax","title":"Arimax Vs Sarimax","text":""},{"location":"Statistic/timeseries/#smoothing-automated-forecasting","title":"Smoothing &amp; Automated Forecasting","text":""},{"location":"Statistic/timeseries/#automated-time-series-models","title":"Automated Time Series Models","text":""},{"location":"Statistic/timeseries/#uni-bi-and-multivariate","title":"Uni, Bi and Multivariate","text":""},{"location":"Statistic/timeseries/#multivariate-analysis-with-code-example","title":"Multivariate Analysis with code example","text":""},{"location":"Statistic/timeseries/#linear-regression","title":"Linear Regression","text":""},{"location":"Statistic/timeseries/#linear-regression-assumptions","title":"Linear Regression Assumptions","text":""},{"location":"Statistic/timeseries/#end-to-end-regression-pipeline","title":"End-to-End Regression Pipeline","text":""},{"location":"Statistic/timeseries/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":""},{"location":"Statistic/timeseries/#feature-engineering","title":"Feature Engineering","text":""},{"location":"Statistic/timeseries/#train-test-split-evaluation","title":"Train-Test Split &amp; Evaluation","text":""},{"location":"Statistic/timeseries/#data-storytelling-presentation","title":"Data Storytelling &amp; Presentation","text":""},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/","title":"Mean","text":"\u2705 Mean (Arithmetic Mean) \ud83d\udccc What is Mean? <p>The mean is the average of a set of numbers. It\u2019s a measure of central tendency, used to represent the typical value in a dataset.</p> \ud83d\udd22 Formula: <p>Mean = Sum\u00a0of\u00a0all\u00a0values / Number\u00a0of\u00a0values</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#real-time-example-retail-sales","title":"\ud83e\udde0 Real-Time Example: Retail Sales","text":"<p>Suppose you're a retail manager tracking daily sales for a week.</p> Day Sales (\u20b9) Monday 10,000 Tuesday 12,000 Wednesday 9,000 Thursday 11,000 Friday 10,000 Saturday 20,000 Sunday 18,000 <p>\u2795 Step 1: Sum of sales</p> <p>Total\u00a0Sales = 10,000+12,000+9,000+11,000+10,000+20,000+18,000=\u20b990,000</p> <p>\u2797 Step 2: Number of days = 7</p> <p>\ud83d\udcca Step 3: Calculate Mean</p> <p>Mean\u00a0Sales = \u20b990,000 / 7 = \u20b912,857.14</p> <p>\ud83d\udcd8 Interpretation:</p> <p>On average, your store made \u20b912,857.14 per day during the week.</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#when-mean-can-be-misleading","title":"\u26a0\ufe0f When Mean Can Be Misleading","text":"<p>If one day had an unusually high or low sale, it would affect the mean significantly.</p> <p>For example, if Saturday = \u20b9100,000, the mean would rise sharply, even though it's not typical for the week.</p> <p>\u2705 When to Use Mean</p> Use Case Use Mean? Why? Data without outliers \u2705 Yes Accurately reflects central value Symmetric distribution \u2705 Yes Mean = Median \u2248 Mode Data with outliers or skew \u274c Use Median Mean may be distorted <p>\u2705 What is a Symmetric Distribution?(Normal/Bell Curve)</p> <p>A symmetric distribution is a type of probability distribution where the left and right sides are mirror images of each other when plotted on a graph.</p> <pre><code>    |\n    |               *\n    |             *   *\n    |           *       *\n    |         *           *\n    |       *               *\n    |     *                   *\n    |---*-----------------------*---\n    |&lt;--------CENTER-----------&gt;|\n        Mean = Median = Mode\n</code></pre> <p>\ud83d\udd39 Characteristics:</p> <ul> <li> <p>The peak is in the center, and the tails decrease equally on both sides.</p> </li> <li> <p>Perfect mirror image on both sides.</p> </li> <li> <p>Mean = Median = Mode all at the center.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#key-characteristics","title":"\ud83e\udde0 Key Characteristics","text":"Feature Description Mirror-like shape Left side \u2248 Right side Mean = Median = Mode All measures of central tendency are equal No skew Skewness = 0 (perfect symmetry) Bell-shaped (often) Many symmetric distributions look like a bell"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#example-of-symmetric-distributions","title":"\ud83d\udfe2 Example of Symmetric Distributions","text":"<p>1. Normal Distribution (Gaussian)</p> <ul> <li>Classic example of symmetry</li> <li>Appears in real-world data like heights, IQ scores, blood pressure</li> </ul> <p>2. Uniform Distribution</p> <ul> <li>All values equally likely, so symmetry exists across the range</li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#real-time-example-human-heights","title":"\ud83d\udcca Real-Time Example: Human Heights","text":"<p>Suppose you measure the heights of 10,000 adult men:</p> <ul> <li>Most are around 5'9\" (175 cm)</li> <li>Fewer are below 5'6\" or above 6'2\"</li> <li>The distribution looks like a bell curve</li> </ul> <p>\u27a4 That\u2019s a symmetric distribution centered at the average height.</p> <p>\ud83d\udeab Not Symmetric? \u2192 It\u2019s Skewed</p> Type Description Left-skewed (negative) Tail longer on left; mean &lt; median Right-skewed (positive) Tail longer on right; mean &gt; median <p>\u2705 Why Symmetric Distributions Matter</p> <ul> <li>Make statistical modeling easier</li> <li>Help justify using the mean as a reliable average</li> <li>Used in many algorithms (e.g., z-scores, standardization)</li> </ul> <p>\ud83d\udd36 2. Right-Skewed Distribution (Positively Skewed)</p> <pre><code>    |\n    |           *\n    |         *  *\n    |        *    *\n    |       *      *\n    |     *         *\n    |  *             *\n    |*------------------*-----\n    |&lt;---|----CENTER----&gt;\n        Mode &lt; Median &lt; Mean\n</code></pre> <p>\ud83d\udd39 Characteristics:</p> <ul> <li>Long tail to the right (higher values).</li> <li>Mean is pulled right by outliers.</li> <li>Examples: income, housing prices, website load time.</li> </ul> <p>\ud83d\udd37 3. Left-Skewed Distribution (Negatively Skewed)</p> <pre><code>    |\n    |              *\n    |             * *\n    |            *   *\n    |           *     *\n    |         *        *\n    |       *           *\n    |-----*----------------*---\n    |        CENTER----&gt;---&gt;\n       Mean &lt; Median &lt; Mode\n</code></pre> <p>\ud83d\udd39 Characteristics:</p> <ul> <li>Long tail to the left (lower values).</li> <li>Mean is pulled left by small outliers.</li> <li>Examples: age at retirement, exam scores in easy tests.</li> </ul> <p>\ud83e\udde0 Summary Table</p> Type Shape Order of Mean, Median, Mode Symmetric Bell-shaped / Equal tails Mean = Median = Mode Right-skewed Tail on right (high values) Mean &gt; Median &gt; Mode Left-skewed Tail on left (low values) Mean &lt; Median &lt; Mode <p></p> <p>Here's a visual comparison of three types of distributions:</p> <ol> <li> <p>Symmetric (Normal Distribution) \u2013 Mean, Median, and Mode overlap at the center.</p> </li> <li> <p>Right-Skewed \u2013 Tail extends to the right; Mean &gt; Median.</p> </li> <li> <p>Left-Skewed \u2013 Tail extends to the left; Mean &lt; Median.</p> </li> </ol>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#example","title":"Example","text":"<p>Dataset of people's height, weight and shoe size</p> <pre><code>from IPython.display import Markdown\nimport pandas as pd\n\ndf = pd.read_csv(\"hight.csv\")\nmean = df[\"Hight\"].mean()\nmedian = df[\"Hight\"].median()\nmode = df[\"Hight\"].mode()\nmin = df[\"Hight\"].min()\nmax = df[\"Hight\"].max()\n\nMarkdown(f\"\"\"\n### Height Statistics\n- **Mean:** {mean}\n- **Median:** {median}\n- **Mode:** {mode.tolist()}\n- **Min:** {min}\n- **Max:** {max}\n\"\"\")\n</code></pre> <p></p> <p>\u2705 Option 1: Histogram (Basic Distribution)</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"hight.csv\")\n\nplt.figure(figsize=(8, 5))\nplt.hist(df[\"Hight\"], bins=10, color='skyblue', edgecolor='black')\nplt.title(\"Height Distribution\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\u2705 Option 2: Histogram + KDE (Smoothed Curve)</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"hight.csv\")\n\nplt.figure(figsize=(8, 5))\nsns.histplot(df[\"Hight\"], bins=10, kde=True, color='lightgreen', edgecolor='black')\nplt.title(\"Height Distribution with KDE\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\u2705 Option 3: Box Plot (to see Outliers &amp; Spread)</p> <pre><code>plt.figure(figsize=(6, 1.5))\nsns.boxplot(x=df[\"Hight\"], color='orange')\nplt.title(\"Height Box Plot\")\nplt.xlabel(\"Height\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\u2705 Histogram + KDE + Mean, Median &amp; Mode Lines</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv(\"hight.csv\")\n\n# Calculate statistics\nmean = df[\"Hight\"].mean()\nmedian = df[\"Hight\"].median()\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"Hight\"], bins=10, kde=True, color='skyblue', edgecolor='black')\n\n# Add mean and median lines\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.2f}')\nplt.axvline(median, color='green', linestyle='--', linewidth=2, label=f'Median: {median:.2f}')\n\n# Customize plot\nplt.title(\"Height Distribution with Mean and Median\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#height-distribution-with-mean-median-and-mode","title":"\ud83d\udcca Height Distribution with Mean, Median, and Mode","text":"<p>\u2705 1. Histogram (Blue Bars)</p> <ul> <li> <p>This shows the frequency distribution of height values in dataset.</p> </li> <li> <p>The x-axis represents height values.</p> </li> <li> <p>The y-axis represents how many times each range of height values occurs (frequency).</p> </li> </ul> <p>\u2705 2. KDE Line (Blue Smooth Curve)</p> <ul> <li> <p>This is a Kernel Density Estimate: a smoothed version of the histogram.</p> </li> <li> <p>It gives an idea of the probability density of the height distribution.</p> </li> <li> <p>Helps you visually assess normality and skewness.</p> </li> </ul> <p>\u2705 3. Mean Line (Red, Dashed)</p> <ul> <li> <p>Vertical red dashed line at 173.40.</p> </li> <li> <p>Represents the average height in the dataset.</p> </li> <li> <p>In symmetric distributions, the mean aligns with median and mode.</p> </li> </ul> <p>\u2705 4. Median Line (Green, Dashed)</p> <ul> <li> <p>Vertical green dashed line at 174.00.</p> </li> <li> <p>Represents the middle value when all heights are sorted.</p> </li> <li> <p>It's very close to the mean, indicating the data is fairly symmetric.</p> </li> </ul> <p>\u2705 5. Mode Lines (Purple, Dotted)</p> <ul> <li> <p>Two purple dotted lines at 172 and 185.</p> </li> <li> <p>These are the most frequently occurring height values (multi-modal).</p> </li> <li> <p>Because both occur with the same highest frequency, have 2 modes.</p> </li> <li> <p>This is common in bimodal distributions (2 peaks or groups in data).</p> </li> </ul> <p>Key Insights:</p> Metric Value Interpretation Mean 173.40 Central average of all values Median 174.00 Middle value \u2014 very close to the mean Modes 172, 185 Most frequent heights \u2014 suggests 2 groups (bimodal distribution) <p>\ud83e\udde0 Conclusion:</p> <ul> <li> <p>Height data is approximately symmetric.</p> </li> <li> <p>However, the presence of two modes hints that your dataset might have two dominant clusters or groups, possibly due to different categories (e.g., gender, age groups, etc.).</p> </li> </ul> <p>Mod</p> <pre><code>import pandas as pd\n\n# Load data\ndf = pd.read_csv(\"hight.csv\")\n\n# Group by height and count occurrences\nheight_counts = df[\"Hight\"].value_counts().sort_values(ascending=False)\n\n\n# Display as DataFrame\nheight_distribution = height_counts.reset_index()\nheight_distribution.columns = ['Height', 'Count']\nprint(height_distribution)\n</code></pre> <p></p> <p>It's clearly tell two mod with same value(172 &amp; 185) max count=6</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mean/#1-mean-average","title":"\u2705 1. Mean (Average)","text":"<p>\ud83d\udccc Use When: </p> <ul> <li> <p>Data is symmetric (normally distributed)</p> </li> <li> <p>You want the overall average or total divided by number</p> </li> <li> <p>Outliers are not significant</p> </li> </ul> <p>\u26a0\ufe0f Avoid When:</p> <ul> <li>There are extreme values (outliers), which can distort the mean</li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median/","title":"Median","text":"\u2705 Median (Arithmetic Median) \ud83d\udccc What is Median? <p>The median is the middle value in a sorted list of numbers.</p> <p>\ud83d\udcca Example 1 (Odd Count):</p> <p>Heights: [160, 165, 170, 175, 180] \u2192 Median = 170 (the middle value)</p> <p>\ud83d\udcca Example 2 (Even Count):</p> <p>Heights: [160, 165, 170, 175, 180, 185] \u2192 Median = (170 + 175) / 2 = 172.5</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median/#visualize-median-on-a-histogram","title":"\u2705 Visualize Median on a Histogram","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"hight.csv\")\n\n# Calculate the median\nmedian = df[\"Hight\"].median()\n\n# Plot histogram\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"Hight\"], bins=10, kde=True, color='lightblue', edgecolor='black')\n\n# Add median line\nplt.axvline(median, color='green', linestyle='--', linewidth=2, label=f'Median: {median:.2f}')\n\n# Customize the plot\nplt.title(\"Height Distribution with Median\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Median/#2-median-middle-value","title":"\u2705 2. Median (Middle Value)","text":"<p>\ud83d\udccc Use When:</p> <ul> <li> <p>Data is skewed (has outliers or long tail)</p> </li> <li> <p>You want a resistant measure of central tendency</p> </li> <li> <p>You're dealing with income, real estate prices, or medical costs</p> </li> </ul> <p>\ud83e\udde0 Example:</p> <p>Median house price, median income \u2014 better than mean when there's a few very high values.</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode/","title":"Mode","text":"\u2705 Mode (Arithmetic Mode) \ud83d\udccc What is Mode? <p>The mode is the value (or values) that appear most frequently in a dataset.</p> <p>\u2705 Definition:</p> <p>Mode = the most common value in the data.</p> <p>A dataset can have:</p> <ul> <li> <p>One mode \u2192 Unimodal</p> </li> <li> <p>Two modes \u2192 Bimodal</p> </li> <li> <p>Three or more modes \u2192 Multimodal</p> </li> <li> <p>No mode \u2192 if all values occur only once</p> </li> </ul> <p>\ud83d\udcca Example 1: One Mode</p> <p>Heights = [160, 162, 170, 172, 172, 175, 180] \u2192 Mode = 172 (occurs twice, others once)</p> <p>\ud83d\udcca Example 2: Two Modes</p> <p>Heights = [160, 172, 172, 175, 185, 185] \u2192 Mode = [172, 185] (both occur twice)</p> <p>\ud83d\udcca Example 3: No Mode</p> <p>Heights = [160, 165, 170, 175, 180] \u2192 All values are unique \u2192 No mode</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode/#why-mode-matters","title":"\ud83e\udde0 Why Mode Matters:","text":"<ul> <li> <p>It tells us what value is most typical or popular.</p> </li> <li> <p>Useful in categorical data and discrete numerical data.</p> </li> <li> <p>In some cases (e.g., shoe size, T-shirt size), mode is more useful than mean or median.</p> </li> </ul> <pre><code># Load data\ndf = pd.read_csv(\"hight.csv\")\n\nmode = df[\"Hight\"].mode()\nprint(f\"Mode: {mode.tolist()}\")\n</code></pre> <pre><code>Mode: [172, 185]\n</code></pre>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode/#histogram-kde-mode-lines","title":"\u2705 Histogram + KDE + Mode Line(s)","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"hight.csv\")\n\n# Calculate mode(s)\nmodes = df[\"Hight\"].mode()\n\n# Plot histogram\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"Hight\"], bins=10, kde=True, color='lightblue', edgecolor='black')\n\n# Add mode line(s)\nfor mode_value in modes:\n    plt.axvline(mode_value, color='purple', linestyle=':', linewidth=2, label=f'Mode: {mode_value}')\n\n# Handle duplicate labels\nhandles, labels = plt.gca().get_legend_handles_labels()\nunique_labels = dict(zip(labels, handles))\nplt.legend(unique_labels.values(), unique_labels.keys())\n\n# Customize plot\nplt.title(\"Height Distribution with Mode(s)\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Central-Tendency/Mode/#3-mode-most-frequent-value","title":"\u2705 3. Mode (Most Frequent Value)","text":"<p>\ud83d\udccc Use When:</p> <ul> <li> <p>You need to find the most common value</p> </li> <li> <p>You're working with categorical or discrete data (e.g., shoe size, T-shirt size)</p> </li> <li> <p>Data may have multiple peaks (multi-modal)</p> </li> </ul> <p>\ud83e\udde0 Example:</p> <p>Most common shoe size sold, most popular blood group, most frequent height</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/CofficientVariation/","title":"Cofficient of Variation","text":"\u2705 Coefficient of Variation (CV) \ud83d\udccc What is Coefficient of Variation (CV)? <p>The Coefficient of Variation (CV) is a standardized measure of dispersion of a dataset, expressed as a percentage. It tells you how much variability exists in relation to the mean.</p> <p>\ud83d\udccc Formula:</p> <p></p> <p>\u2705 When to Use:</p> <ul> <li> <p>To compare variability between datasets with different units or different means.</p> </li> <li> <p>Useful in risk analysis, finance, biology, and machine learning model comparison.</p> </li> </ul> <p>\ud83c\udfaf Real-life Example (ML Context):</p> <p>Imagine you are comparing the performance of two ML models across different datasets:</p> <ul> <li> <p>Model A:</p> <ul> <li> <p>Mean Accuracy = 90%</p> </li> <li> <p>Std Dev = 2%</p> </li> <li> <p>CV = (2 / 90) \u00d7 100 = 2.22%</p> </li> </ul> </li> <li> <p>Model B:</p> <ul> <li> <p>Mean Accuracy = 70%</p> </li> <li> <p>Std Dev = 5%</p> </li> <li> <p>CV = (5 / 70) \u00d7 100 = 7.14%</p> </li> </ul> </li> </ul> <p>\ud83e\udde0 Conclusion: Model A is more consistent (lower CV) than Model B.</p> <p>\ud83d\udcca Python Code to Compute CV:</p> <pre><code>import numpy as np\n\n# Sample data (e.g., model accuracies)\ndata = [88, 91, 89, 92, 90, 87, 93]\n\nmean = np.mean(data)\nstd_dev = np.std(data)\ncv = (std_dev / mean) * 100\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Standard Deviation: {std_dev:.2f}\")\nprint(f\"Coefficient of Variation (CV): {cv:.2f}%\")\n</code></pre> <p>Mean: 90.00 Standard Deviation: 2.00 Coefficient of Variation (CV): 2.22%</p> <p>When NOT to Use CV:</p> <ul> <li> <p>When the mean is near zero (as CV becomes unstable or undefined).</p> </li> <li> <p>For negative values where the context doesn\u2019t allow meaningful interpretation (like temperature in \u00b0C or \u00b0F).</p> </li> </ul> <p>\ud83d\udcca Python Code to Visualize CV Comparison</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Example: Accuracies of 3 ML models\nmodel_names = ['Model A', 'Model B', 'Model C']\ndata = [\n    [88, 91, 89, 92, 90, 87, 93],  # Model A\n    [68, 72, 70, 69, 71, 66, 73],  # Model B\n    [82, 80, 81, 79, 85, 78, 84],  # Model C\n]\n\nmeans = [np.mean(d) for d in data]\nstds = [np.std(d) for d in data]\ncvs = [(std / mean) * 100 for std, mean in zip(stds, means)]\n\n# Plotting\nfig, ax = plt.subplots()\nbars = ax.bar(model_names, cvs, color=['skyblue', 'salmon', 'lightgreen'])\n\n# Annotate CV values on top of bars\nfor bar, cv in zip(bars, cvs):\n    height = bar.get_height()\n    ax.annotate(f'{cv:.2f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10)\n\n# Labels and title\nax.set_ylabel('Coefficient of Variation (%)')\nax.set_title('Model Performance Variability (CV)')\nplt.ylim(0, max(cvs) + 5)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Interpretation:</p> <ul> <li> <p>Lower CV \u2192 More consistent model.</p> </li> <li> <p>Higher CV \u2192 More variation in performance.</p> </li> </ul> <p>\ud83d\udcca Python Code: CV for Accuracy, Precision, and Recall</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample model performance scores\nmodel_metrics = {\n    \"Accuracy\": {\n        \"Model A\": [88, 91, 89, 92, 90, 87, 93],\n        \"Model B\": [68, 72, 70, 69, 71, 66, 73],\n        \"Model C\": [82, 80, 81, 79, 85, 78, 84]\n    },\n    \"Precision\": {\n        \"Model A\": [85, 86, 84, 87, 83, 88, 85],\n        \"Model B\": [72, 74, 70, 71, 73, 69, 72],\n        \"Model C\": [79, 78, 77, 81, 80, 76, 82]\n    },\n    \"Recall\": {\n        \"Model A\": [82, 84, 81, 83, 85, 80, 86],\n        \"Model B\": [66, 68, 65, 69, 67, 64, 70],\n        \"Model C\": [76, 78, 75, 77, 74, 79, 73]\n    }\n}\n\n# Plotting CV for each metric\nfig, axs = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\nfig.suptitle(\"Coefficient of Variation for Accuracy, Precision, Recall\", fontsize=14)\n\nfor idx, (metric_name, scores) in enumerate(model_metrics.items()):\n    means = [np.mean(scores[model]) for model in scores]\n    stds = [np.std(scores[model]) for model in scores]\n    cvs = [(std / mean) * 100 for std, mean in zip(stds, means)]\n\n    bars = axs[idx].bar(scores.keys(), cvs, color=['skyblue', 'salmon', 'lightgreen'])\n\n    for bar, cv in zip(bars, cvs):\n        axs[idx].annotate(f'{cv:.2f}%', \n                          xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), \n                          xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10)\n\n    axs[idx].set_title(f\"{metric_name}\")\n    axs[idx].set_ylabel(\"CV (%)\" if idx == 0 else \"\")\n    axs[idx].set_ylim(0, max(cvs) + 5)\n    axs[idx].grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p></p> <p>\u2705 Insights:</p> <ul> <li> <p>This allows you to compare consistency of models across multiple performance metrics.</p> </li> <li> <p>Helps in model selection: Even if two models have similar average precision, the one with lower CV is more reliable.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange/","title":"Interquartile Range(IQR)","text":"\u2705 Interquartile Range (IQR) \ud83d\udccc What is Interquartile Range (IQR)? <p>The Interquartile Range (IQR) is a measure of statistical dispersion \u2014 it tells us how spread out the middle 50% of a dataset is.</p> <p>\ud83e\uddee Definition:</p> <p>IQR=Q3\u2212Q1</p> <ul> <li> <p>Q1 (1st Quartile): The 25th percentile \u2014 25% of data falls below this point.</p> </li> <li> <p>Q3 (3rd Quartile): The 75th percentile \u2014 75% of data falls below this point.</p> </li> <li> <p>The middle 50% of the data lies between Q1 and Q3.</p> </li> </ul> <p>\u2705 Why is IQR useful?</p> <ul> <li> <p>It is resistant to outliers, unlike the full range.</p> </li> <li> <p>Helps identify data concentration and detect outliers using the 1.5 \u00d7 IQR rule.</p> </li> </ul> <p>\ud83d\udce6 Real-world example (Salary):</p> Employee Salary (\u20b9k) A 20 B 25 C 28 D 30 E 32 F 35 G 36 H 40 I 45 <ul> <li> <p>Sorted: 20, 25, 28, 30, 32, 35, 36, 40, 45</p> </li> <li> <p>Q1 = 28, Q3 = 36</p> </li> <li> <p>IQR = Q3(36) - Q1(28) = 8</p> </li> </ul> <p>So, the middle 50% of employee salaries lie between \u20b928k and \u20b936k.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Salary data (in thousands)\nsalaries = [20, 25, 28, 30, 32, 35, 36, 40, 45]\n\n# Calculate Q1, Q3 and IQR\nQ1 = np.percentile(salaries, 25)\nQ3 = np.percentile(salaries, 75)\nIQR = Q3 - Q1\n\nprint(f\"Q1 (25th percentile): {Q1}\")\nprint(f\"Q3 (75th percentile): {Q3}\")\nprint(f\"IQR: {IQR}\")\n\n# Create a box plot\nplt.figure(figsize=(8, 2))\nsns.boxplot(x=salaries, color=\"skyblue\")\n\n# Add annotations\nplt.axvline(Q1, color='green', linestyle='--', label=f'Q1 = {Q1}')\nplt.axvline(Q3, color='red', linestyle='--', label=f'Q3 = {Q3}')\nplt.title('Salary Distribution with IQR')\nplt.xlabel('Salary (in \u20b9k)')\nplt.legend()\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Q1 = np.percentile(data, 25) = 28.5</p> <p>Q3 = np.percentile(data, 75) = 39.0</p> <p>IQR = Q3 - Q1 = 10.5</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/InterquartileRange/#determine-bounds","title":"Determine bounds","text":"<p>lower_bound = Q1 - 1.5 * IQR lower_bound = 28.5 - (1.5 * 10.5) = 12.75</p> <p>upper_bound = Q3 + 1.5 * IQR upper_bound = 39.0 + (1.5 * 10.5) = 54.75</p> <p>Outliers: [70] . Because any value &gt; 54.75 &amp; &lt; 12.75 should consider as Outliers.</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/Range/","title":"Range","text":"\u2705 Range <p>Dispersion refers to how spread out the values in a dataset are. It helps you understand the variability in your data \u2014 beyond just the average.</p> \ud83d\udccc What is Range? <p>The Range is the difference between the maximum and minimum values in a dataset.</p> <p>Range = Maximum\u00a0Value \u2212 Minimum\u00a0Value</p> <p>\u2705 Example: Consider the dataset:</p> <p>data = [10, 12, 14, 18, 25]</p> <ul> <li> <p>Maximum value = 25</p> </li> <li> <p>Minimum value = 10</p> </li> <li> <p>Range = 25 - 10 = 15</p> </li> </ul> <p>\ud83d\udcca Visualization: Range on a Number Line</p> <p>Let\u2019s visualize this using a simple number line:</p> <p>10     12     14     16     18     20     22     24     25  |------|------|------|------|------|------|------|------|  \u2191                                                 \u2191 Min (10)                                       Max (25)</p> <p>\u27a1\ufe0f  Range = Max - Min = 25 - 10 = 15 units</p> <ul> <li> <p>The left arrow points to 10 (Min)</p> </li> <li> <p>The right arrow correctly points to 25 (Max) now</p> </li> <li> <p>The total span from 10 to 25 is the range = 15</p> </li> </ul> <p></p> <p>Here is the correct matplotlib visualization for Range on a Number Line:</p> <ul> <li> <p>\ud83d\udd34 Red dots = individual data points</p> </li> <li> <p>\ud83d\udd35 Blue line = range (from Min = 10 to Max = 25)</p> </li> <li> <p>Arrows point to the Min (10) and Max (25)</p> </li> <li> <p>The total span (Range) = 25 - 10 = 15 units</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/StandardDeviation/","title":"Standard Deviation","text":"\u2705 Standard Deviation \ud83d\udccc What is Standard Deviation? <p>Standard Deviation is a statistical measure of the spread or dispersion of a set of data points relative to their mean (average).</p> <p></p> <p>Where:</p> <ul> <li> <p>xi= each data point</p> </li> <li> <p>\u03bc = mean of the dataset</p> </li> <li> <p>N = number of data points</p> </li> </ul> <p>It answers the question:</p> <p>\"On average, how far are the data points from the mean?\"</p> <p>\ud83d\udccc Key Characteristics</p> <ul> <li> <p>Low Standard Deviation(SD) \u21d2 Data points are close to the mean (less variability)</p> </li> <li> <p>High Standard Deviation(SD) \u21d2 Data points are spread out (more variability)</p> </li> </ul> <p>\ud83e\udde0 Real-Time Examples</p> <p>1. \ud83d\udce6 Inventory Management</p> <ul> <li> <p>A company tracks daily sales of a product.</p> </li> <li> <p>If the SD is low, it can forecast inventory confidently.</p> </li> <li> <p>If SD is high, sales fluctuate a lot, so safety stock needs to be higher.</p> </li> </ul> <p>2. \ud83e\ude7a Healthcare</p> <ul> <li> <p>Standard Deviation(SD) of blood pressure readings across patients helps identify normal vs. abnormal variability.</p> </li> <li> <p>A low Standard Deviation(SD) in clinical trial data shows consistent drug response.</p> </li> </ul> <p>3. \ud83d\udcc8 Machine Learning: Model Evaluation</p> <ul> <li> <p>During cross-validation, SD of model accuracy across folds shows model stability.</p> </li> <li> <p>Low SD means consistent performance \u2192 robust model.</p> </li> </ul> <p>4. \ud83c\udf93 Student Scores</p> <ul> <li> <p>Class A: Mean = 80, SD = 2 \u2192 All students score close to 80</p> </li> <li> <p>Class B: Mean = 80, SD = 15 \u2192 Scores vary widely from student to student</p> </li> </ul> <p>\ud83d\udcc5 When to Use Standard Deviation</p> Scenario Use SD? Why? Understanding data variability \u2705 Measures how consistent the data is Comparing performance consistency \u2705 E.g., which model/branch/store is more stable Outlier detection (with mean) \u2705 Points outside \u00b12 SD are potential outliers Normally distributed data \u2705 SD is most meaningful with symmetric distributions Skewed distributions \u26a0\ufe0f Better to use IQR (less affected by outliers) <p>\ud83d\udd01 Standard Deviation vs Other Measures</p> Measure Best Use Case Standard Deviation For normal-like distributions IQR For skewed/outlier-prone data Range Quick check for extreme spread Variance Square of SD, used in theoretical stats <p></p> <p>Here's a visual comparison of Standard Deviation:</p> <ul> <li> <p>\ud83d\udfe2 Low SD (\u03c3 = 5): The green curve is narrower \u2014 values are tightly clustered around the mean (50).</p> </li> <li> <p>\ud83d\udd34 High SD (\u03c3 = 15): The red curve is wider \u2014 values are more spread out from the mean.</p> </li> <li> <p>\ud83d\udd35 Blue dashed line: Indicates the mean (50) for both distributions.</p> </li> </ul> <p>Interpretation:</p> <ul> <li> <p>A smaller standard deviation means more consistency.</p> </li> <li> <p>A larger standard deviation means more variability or uncertainty.</p> </li> </ul> <p>\u2705 Python Code: Visualize Low vs High Standard Deviation</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nlow_sd = np.random.normal(loc=50, scale=5, size=1000)    # Mean=50, SD=5\nhigh_sd = np.random.normal(loc=50, scale=15, size=1000)  # Mean=50, SD=15\n\n# Create the plot\nplt.figure(figsize=(10, 5))\n\n# Plot histograms\nplt.hist(low_sd, bins=30, alpha=0.6, label='Low SD (\u03c3=5)', color='green', density=True)\nplt.hist(high_sd, bins=30, alpha=0.6, label='High SD (\u03c3=15)', color='red', density=True)\n\n# Plot the mean line\nplt.axvline(50, color='blue', linestyle='--', linewidth=2, label='Mean = 50')\n\n# Add labels, title, legend\nplt.title('\ud83d\udcca Standard Deviation Comparison: Low vs High')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\n# Show plot\nplt.show()\n</code></pre>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Dispersion/Variance/","title":"Variance","text":"\u2705 Variance \ud83d\udccc What is Variance? <p>Variance is a statistical measure that tells us how far data points are spread out from the mean. It is the average of the squared deviations from the mean.</p> <p></p> <ul> <li> <p>xi: individual data point</p> </li> <li> <p>\u03bc: mean of the dataset</p> </li> <li> <p>N: total number of data points</p> </li> </ul> <p>In simple terms: Variance measures how much the values in a dataset \"vary\" from the mean.</p> <p>\ud83d\udccc Key Concepts:</p> <ul> <li> <p>Always non-negative</p> </li> <li> <p>If all values are the same, variance is zero</p> </li> <li> <p>Units are squared (e.g., if values are in meters, variance is in square meters)</p> </li> </ul> <p>\ud83d\udd01 Variance vs. Standard Deviation</p> Metric Description Variance Average of squared deviations Standard Deviation Square root of variance (more interpretable) <p>\ud83e\udde0 Real-World Examples</p> <p>1. \ud83c\udf93 Student Scores</p> <ul> <li> <p>Two classes have the same average score (e.g., 80), but:</p> <ul> <li> <p>Class A has low variance: all students scored close to 80.</p> </li> <li> <p>Class B has high variance: scores range from 50 to 100.</p> </li> </ul> </li> </ul> <p>2. \ud83d\udcc8 Stock Market</p> <ul> <li> <p>Low variance: stable stock</p> </li> <li> <p>High variance: volatile stock</p> </li> </ul> <p>3. \ud83e\uddea Medical Testing</p> <ul> <li> <p>Low variance in lab results = consistent equipment</p> </li> <li> <p>High variance = possible errors or patient variation</p> </li> </ul> <p>\ud83d\udcc5 When to Use Variance</p> Use Case Use Variance? Reason Feature Selection in ML \u2705 Variance threshold to remove low-information features Comparing model stability \u2705 Variance of accuracy across folds Data consistency check \u2705 Small variance = reliable system <p>\ud83e\uddee Example Calculation</p> <p>data = [10, 12, 14, 18, 25]</p> <ol> <li> <p>Mean \u03bc = (10+12+14+18+25)/5 =15.8</p> </li> <li> <p>Squared deviations:</p> <ul> <li> <p>(10 - 15.8)\u00b2 = 33.64</p> </li> <li> <p>(12 - 15.8)\u00b2 = 14.44</p> </li> <li> <p>(14 - 15.8)\u00b2 = 3.24</p> </li> <li> <p>(18 - 15.8)\u00b2 = 4.84</p> </li> <li> <p>(25 - 15.8)\u00b2 = 84.64</p> </li> </ul> </li> <li> <p>Sum = 140.8</p> </li> <li> <p>Variance = 140.8 / 5 = 28.16</p> </li> </ol> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = [10, 12, 14, 16, 18, 20, 22, 30]\nmean = np.mean(data)\nvariance = np.var(data)\n\n# Create plot\nplt.figure(figsize=(10, 5))\nplt.scatter(data, [1]*len(data), color='red', label='Data Points')\n\n# Plot mean line\nplt.axvline(mean, color='green', linestyle='--', label=f'Mean = {mean:.2f}')\n\n# Annotate deviations\nfor x in data:\n    plt.plot([x, mean], [1, 1], color='blue', linestyle='--')\n    plt.text(x, 1.05, f'({x}-{mean:.1f})\u00b2', ha='center', fontsize=8, rotation=45)\n\n# Labels and title\nplt.yticks([])\nplt.title(f'\ud83d\udd27 Variance Visualization (Variance = {variance:.2f})')\nplt.xlabel('Data Points')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p></p> <p>Here's a visual explanation of Variance:</p> <ul> <li> <p>\ud83d\udd34 Red dots = individual data points</p> </li> <li> <p>\ud83d\udfe2 Green dashed line = mean of the dataset</p> </li> <li> <p>\ud83d\udd35 Double-headed blue arrows = distance from each point to the mean</p> </li> <li> <p>\ud83d\udcd8 Each label (e.g., (x - mean)\u00b2) shows the squared deviation from the mean</p> </li> </ul> <p>\ud83e\uddee Summary:</p> <ul> <li> <p>The squared deviations are used to calculate the variance.</p> </li> <li> <p>Variance is the average of these squared deviations, which in this case is:</p> </li> </ul> <p></p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Deciles/","title":"Deciles","text":"\u2705 Deciles \ud83d\udccc What Are Deciles? <p>Deciles are statistical measures that divide a dataset into 10 equal parts, each containing 10% of the data after sorting in ascending order.</p> <ul> <li> <p>Deciles are nine values (D\u2081 to D\u2089) that split the data into ten equal parts.</p> </li> <li> <p>Each decile represents a 10% increment in the distribution.</p> </li> </ul> Decile Meaning D\u2081 10% of data is below this point D\u2082 20% of data is below this point D\u2085 50% of data is below this point \u2192 Median D\u2089 90% of data is below this point <p>\ud83e\uddee Example</p> <p>Suppose we have the following sorted data of 20 students\u2019 marks:</p> <p>10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105</p> <ul> <li> <p>D\u2081 (10%) \u2192 2nd value = 15</p> </li> <li> <p>D\u2082 (20%) \u2192 4th value = 25</p> </li> <li> <p>D\u2085 (50%) \u2192 10th value = 55 (Median)</p> </li> <li> <p>D\u2089 (90%) \u2192 18th value = 95</p> </li> </ul> <p>Note: For more accurate results, you can use interpolation if percentiles fall between ranks.</p> <p>\ud83d\udcca Visualization using Seaborn</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, \n                 60, 65, 70, 75, 80, 85, 90, 95, 100, 105])\n\n# Calculate deciles\ndeciles = np.percentile(data, [10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n# Plot distribution\nsns.histplot(data, bins=10, kde=True, color='skyblue')\nfor i, d in enumerate(deciles, start=1):\n    plt.axvline(d, color='red', linestyle='--')\n    plt.text(d, 1, f\"D{i}\", rotation=90, verticalalignment='bottom', color='red')\n\nplt.title(\"Deciles in a Dataset\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> \ud83e\udde0 When to Use Deciles: <ul> <li> <p>In education, to rank students into top 10%, bottom 10%, etc.</p> </li> <li> <p>In income distribution, to understand how income is spread.</p> </li> <li> <p>In marketing, for customer segmentation based on spending or frequency.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/","title":"Percentiles","text":"\u2705 Percentiles \ud83d\udccc What Are Percentiles? <p>Percentiles are measures that divide a dataset into 100 equal parts. Each percentile tells you the relative position of a value within the data distribution.</p> <ul> <li>The k-th percentile is the value below which k% of the data falls</li> </ul> <p>Example:</p> <ul> <li> <p>25th percentile (P25) \u2192 25% of data is below this value.</p> </li> <li> <p>50th percentile (P50) \u2192 Median (middle value).</p> </li> <li> <p>75th percentile (P75) \u2192 75% of data falls below this value.</p> </li> <li> <p>90th percentile (P90) \u2192 90% of data falls below this value.</p> </li> <li> <p>100th percentile (P100) \u2192 100% of data falls below this value.</p> </li> </ul> \ud83e\udde0 Why Use Percentiles?: <p>Percentiles help answer:</p> <ul> <li> <p>How extreme or typical a value is</p> </li> <li> <p>Where a value stands relative to others</p> </li> <li> <p>Useful for outlier detection, ranking, and cut-off thresholds</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#real-world-examples","title":"\ud83d\udce6 Real-World Examples","text":"<p>\ud83c\udfeb Exam Scores:</p> <ul> <li>If your score is in the 90th percentile, you did better than 90% of the students.</li> </ul> <p>\ud83c\udfe5 Medical Growth Charts:</p> <ul> <li>A baby in the 40th percentile for height is taller than 40% of babies the same age.</li> </ul> <p>\ud83d\udcbb Website Load Time:</p> <ul> <li>\"P95 latency = 3 seconds\" means 95% of pages load faster than 3 seconds.</li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#how-to-calculate-percentiles-in-python-with-visualization","title":"\u2705 How to Calculate Percentiles in Python (with Visualization)","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example dataset\ndf = pd.read_csv(\"hight.csv\")\nheights = df[\"Hight\"]\n\n# Calculate percentiles\np25 = np.percentile(heights, 25)\np50 = np.percentile(heights, 50)  # Median\np75 = np.percentile(heights, 75)\np90 = np.percentile(heights, 90)\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.histplot(heights, bins=10, kde=True, color='lightblue', edgecolor='black')\n\n# Draw percentile lines\nplt.axvline(p25, color='green', linestyle='--', label=f'25th Percentile: {p25:.2f}')\nplt.axvline(p50, color='blue', linestyle='--', label=f'50th Percentile (Median): {p50:.2f}')\nplt.axvline(p75, color='orange', linestyle='--', label=f'75th Percentile: {p75:.2f}')\nplt.axvline(p90, color='red', linestyle='--', label=f'90th Percentile: {p90:.2f}')\n\n# Final touches\nplt.title(\"Height Distribution with Percentiles\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output Interpretation:</p> <ul> <li> <p>Left of P25: 25% of heights</p> </li> <li> <p>Between P25 and P75: Middle 50% (interquartile range)</p> </li> <li> <p>Above P90: Top 10% tallest individuals \u2014 possibly outliers or elite group</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#box-plot-to-visualize-percentiles-q1-q2-q3","title":"\u2705 Box Plot to Visualize Percentiles (Q1, Q2, Q3)","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load your dataset\ndf = pd.read_csv(\"hight.csv\")\nheights = df[\"Hight\"]\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=heights, color=\"skyblue\", width=0.4)\n\n# Label percentiles\np25 = heights.quantile(0.25)\np50 = heights.median()\np75 = heights.quantile(0.75)\n\nplt.axvline(p25, color='green', linestyle='--', label=f'25th Percentile (Q1): {p25:.2f}')\nplt.axvline(p50, color='blue', linestyle='--', label=f'50th Percentile (Median, Q2): {p50:.2f}')\nplt.axvline(p75, color='orange', linestyle='--', label=f'75th Percentile (Q3): {p75:.2f}')\n\n# Title &amp; Legend\nplt.title(\"\ud83d\udce6 Box Plot of Heights with Percentiles\")\nplt.xlabel(\"Height\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\ud83d\udccc Interpretation of Box Plot:</p> <ul> <li> <p>Left whisker: Minimum value (excluding outliers)</p> </li> <li> <p>Box start: Q1 (25th percentile)</p> </li> <li> <p>Line inside box: Median (Q2 or 50th percentile)</p> </li> <li> <p>Box end: Q3 (75th percentile)</p> </li> <li> <p>Right whisker: Maximum value (excluding outliers)</p> </li> <li> <p>Dots outside whiskers: Outliers</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#grouped-box-plot-eg-by-gender","title":"\u2705 Grouped Box Plot (e.g., by Gender)","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"hight.csv\")\n\n# Check if 'Gender' column exists\nif 'Gender' in df.columns:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Gender', y='Hight', data=df, palette='pastel')\n\n    # Titles and labels\n    plt.title('\ud83d\udcca Height Distribution by Gender with Percentiles')\n    plt.xlabel('Gender')\n    plt.ylabel('Height')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"\u274c 'Gender' column not found in the dataset. Please check your data.\")\n</code></pre> <p>\ud83d\udccc What You\u2019ll See in This Plot:</p> <ul> <li> <p>Each box shows the distribution for one gender</p> </li> <li> <p>Median line inside the box: 50th percentile</p> </li> <li> <p>Box edges: 25th and 75th percentiles</p> </li> <li> <p>Whiskers: Range of typical values</p> </li> <li> <p>Dots beyond whiskers: Outliers</p> </li> </ul> <p>\u2139\ufe0f Use Case Example:</p> Scenario Best Metric Comparing central value Median (robust to outliers) Analyzing spread &amp; shape Box Plot with percentiles Detecting outliers or skewness Box Plot &amp; Mode"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#step-by-step-outlier-detection-using-iqr","title":"\u2705 Step-by-Step: Outlier Detection Using IQR","text":"<p>Formula:</p> <ol> <li> <p>IQR = Q3 \u2212 Q1</p> </li> <li> <p>Lower Bound = Q1 \u2212 1.5 \u00d7 IQR</p> </li> <li> <p>Upper Bound = Q3 + 1.5 \u00d7 IQR</p> </li> <li> <p>Outliers: Any value &lt; Lower Bound or &gt; Upper Bound</p> </li> </ol>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#example-calculate-outliers-from-height-data","title":"\ud83d\udd22 Example: Calculate Outliers from Height Data","text":"<pre><code>import pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"hight.csv\")\nheights = df[\"Hight\"]\n\n# Calculate Q1, Q3, and IQR\nQ1 = heights.quantile(0.25)\nQ3 = heights.quantile(0.75)\nIQR = Q3 - Q1\n\n# Calculate bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identify outliers\noutliers = heights[(heights &lt; lower_bound) | (heights &gt; upper_bound)]\n\nprint(\"Q1 (25th percentile):\", Q1)\nprint(\"Q3 (75th percentile):\", Q3)\nprint(\"IQR:\", IQR)\nprint(\"Lower Bound:\", lower_bound)\nprint(\"Upper Bound:\", upper_bound)\nprint(\"\\n\ud83d\udea8 Outliers:\\n\", outliers)\n</code></pre> <p>Q1 (25th percentile): 164.0 Q3 (75th percentile): 185.0 IQR: 21.0 Lower Bound: 132.5 Upper Bound: 216.5</p> <p>\ud83d\udea8 Outliers:  Series([], Name: Hight, dtype: int64)</p> <p>\ud83d\udce6 Visualization with Outliers</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=heights, color=\"lightblue\", width=0.3)\nplt.axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\nplt.axvline(upper_bound, color='red', linestyle='--', label='Upper Bound')\nplt.title(\"\ud83d\udce6 Box Plot with Outliers\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Percentiles/#real-time-use-case","title":"\ud83c\udfaf Real-Time Use Case:","text":"<p>Imagine this is a student height dataset in school:</p> <ul> <li> <p>Most students are between 130\u2013160 cm (normal distribution)</p> </li> <li> <p>A few entries like 90 cm (too short) or 200 cm (too tall) would be flagged as outliers (maybe measurement errors or rare cases)</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles/","title":"Quartiles","text":"\u2705 Quartiles \ud83d\udccc What Are Quartiles? <p>Quartiles divide a dataset into four equal parts, each containing 25% of the data. They are key components of descriptive statistics and are often used in box plots.</p> \ud83d\udd22 Quartile Definitions: <ul> <li> <p>Q1 (First Quartile / 25th percentile): 25% of the data falls below this value.</p> </li> <li> <p>Q2 (Second Quartile / Median / 50th percentile): 50% of the data falls below this value.</p> </li> <li> <p>Q3 (Third Quartile / 75th percentile): 75% of the data falls below this value.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles/#real-life-example-heights-of-students","title":"\ud83d\udce6 Real-life Example: Heights of Students","text":"<p>Suppose you have the following heights (in cm):</p> <p>[150, 152, 155, 157, 160, 162, 165, 168, 170, 172, 175, 178, 180, 182, 185, 188, 190, 192, 195, 200]</p> <p>There are 20 values in total.</p> <ul> <li> <p>Q1 = 162 \u2192 25% of students are \u2264 162 cm</p> </li> <li> <p>Q2 = 172 \u2192 50% of students are \u2264 172 cm (Median)</p> </li> <li> <p>Q3 = 185 \u2192 75% of students are \u2264 185 cm</p> </li> </ul> <p>\ud83d\udcc8 Visual: Box Plot</p> <p>Box Plot of Heights, which visually represents:</p> <ul> <li> <p>Q1 (25th percentile): Left edge of the box</p> </li> <li> <p>Q2 (50th percentile / Median): Line inside the box</p> </li> <li> <p>Q3 (75th percentile): Right edge of the box</p> </li> <li> <p>Whiskers: Range of non-outlier values</p> </li> <li> <p>Dots beyond whiskers: Outliers (if any)</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Quartiles/#real-time-example","title":"\ud83d\udcca Real-Time Example","text":"<p>Suppose you have the following sorted test scores of 11 students:</p> <p>45, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100</p> <ul> <li> <p>Q2 (Median) = 75 (middle value)</p> </li> <li> <p>Q1 = Median of the lower half \u2192 45, 55, 60, 65, 70 \u2192 Q1 = 60</p> </li> <li> <p>Q3 = Median of the upper half \u2192 80, 85, 90, 95, 100 \u2192 Q3 = 90</p> </li> </ul> <p>\ud83d\udcc8 Visualization \u2013 Box Plot</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data\ndata = [45, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n\n# Create box plot\nsns.boxplot(data=data, color=\"skyblue\")\nplt.title(\"Box Plot of Student Scores\")\nplt.xlabel(\"Test Scores\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>This will generate a Box Plot showing:</p> <ul> <li> <p>The box from Q1 to Q3 (60 to 90)</p> </li> <li> <p>The line in the box at Q2 (75, median)</p> </li> <li> <p>The whiskers extending to min and max (45 to 100)</p> </li> <li> <p>Outliers (if any) shown as dots beyond whiskers</p> </li> </ul> <p>\ud83d\udccc Key Uses of Quartiles</p> <ul> <li> <p>Identify spread and skewness</p> </li> <li> <p>Detect outliers using IQR (Interquartile Range):</p> </li> </ul> <p>IQR=Q3\u2212Q1</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score/","title":"Z-Score","text":"\u2705 Z-Score \ud83d\udccc What Are Z-Score? <p>The Z-score (also known as standard score) tells you how many standard deviations a data point is from the mean of a dataset.</p>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score/#z-score-method-standard-score-method","title":"Z-Score Method (Standard Score Method)","text":"<p>\u2705 When to Use:</p> <ul> <li> <p>When your data is normally distributed.</p> </li> <li> <p>Works well for univariate (single variable) analysis.</p> </li> </ul> <p>\ud83d\udcd8 Formula:</p> <ul> <li> <p>X = individual value</p> </li> <li> <p>\u03bc = mean of the dataset</p> </li> <li> <p>\u03c3 = standard deviation</p> </li> <li> <p>Z = 0 \u2192 value is exactly the mean</p> </li> <li> <p>Z &gt; 0 \u2192 value is above the mean</p> </li> <li> <p>Z &lt; 0 \u2192 value is below the mean</p> </li> <li> <p>Z &gt; 3 or Z &lt; -3 \u2192 considered an outlier (in many applications)</p> </li> </ul> <p>Z= (X\u2212\u03bc)/\u03c3</p> <ul> <li> <p>\u03bc = mean of the data</p> </li> <li> <p>\u03c3 = standard deviation</p> </li> <li> <p>Z-score &gt; 3 or &lt; -3 is usually considered an outlier</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score/#real-time-example","title":"\u2705 Real-Time Example:","text":"<p>Let\u2019s say the average height of students in a class is 170 cm with a standard deviation of 5 cm. If a student is 180 cm tall:</p> <p>Z=(180\u2212170)/5 = 2</p> <p>This means the student's height is 2 standard deviations above the average.</p> <p>\ud83d\udcca Visualization:</p> <p>If you plot a bell-shaped curve (normal distribution):</p> <ul> <li> <p>Most values lie between Z = -1 and Z = +1</p> </li> <li> <p>Around 95% of data lies between Z = -2 and Z = +2</p> </li> <li> <p>Extreme Z-scores (e.g., &lt; -3 or &gt; +3) indicate potential outliers</p> </li> </ul> <p>\ud83e\udde0 When to Use:</p> <ul> <li> <p>To detect outliers</p> </li> <li> <p>To standardize different datasets</p> </li> <li> <p>To compare scores from different distributions</p> </li> <li> <p>In machine learning for feature scaling (standardization)</p> </li> </ul> <p>\ud83d\udccc Example in Code (Height):</p> <pre><code>from scipy.stats import zscore\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"hight.csv\")\ndf['Z_Score'] = zscore(df[\"Hight\"])\n\n# Identify outliers using Z-score threshold\noutliers_z = df[df['Z_Score'].abs() &gt; 3]\n\nprint(\"\ud83d\udea8 Z-Score Outliers:\\n\", outliers_z[['Hight', 'Z_Score']])\n</code></pre> <p>\ud83d\udea8 Z-Score Outliers:  Empty DataFrame Columns: [Hight, Z_Score] Index: []</p> <p></p> <p>The plot above highlights outliers in red using the Z-score method:</p> <p>How Outliers Are Detected:</p> <ul> <li>Z-score measures how far a value is from the mean in terms of standard deviations.</li> </ul> <p>A common threshold is:</p> <ul> <li>Z &gt; 3 or Z &lt; -3 \u21d2 considered an outlier.</li> </ul> <p>\ud83d\udcca Example Insight:</p> <ul> <li> <p>Most heights cluster around the mean (green dashed line).</p> </li> <li> <p>Two points far from this central cluster (e.g., 120 cm and 250 cm) are marked in red as outliers.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Measures-of-Position-Relative-Standing/Z-Score/#mahalanobis-distance-for-multivariate-outliers","title":"\ud83d\udd01 Mahalanobis Distance (for Multivariate Outliers)","text":"<p>\u200b\u2705 When to Use:</p> <ul> <li> <p>When you have multiple features/columns (multivariate data)</p> </li> <li> <p>Takes correlation between variables into account</p> </li> </ul> <p>\ud83d\udccc Example with Multiple Features (e.g., Height &amp; Weight):</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import mahalanobis\nfrom numpy.linalg import inv\n\n# Sample multivariate dataset\ndf = pd.read_csv(\"height_weight.csv\")  # assume columns: Hight, Weight\ndata = df[['Hight', 'Weight']]\n\n# Mean vector &amp; Covariance matrix\nmean_vec = data.mean().values\ncov_matrix = np.cov(data.T)\ninv_cov_matrix = inv(cov_matrix)\n\n# Calculate Mahalanobis distance for each point\ndf['Mahalanobis'] = data.apply(lambda x: mahalanobis(x, mean_vec, inv_cov_matrix), axis=1)\n\n# Set threshold (e.g., &gt;3 or &gt;5 based on degrees of freedom)\nthreshold = 3\noutliers_maha = df[df['Mahalanobis'] &gt; threshold]\n\nprint(\"\ud83d\udea8 Mahalanobis Outliers:\\n\", outliers_maha[['Hight', 'Weight', 'Mahalanobis']])\n</code></pre> <p>\ud83d\udd2c Summary Table</p> Method Best For Multivariate Assumes Normality Threshold IQR General use \u274c \u274c 1.5\u00d7IQR Z-score Normal data \u274c \u2705 Z &gt; 3 Mahalanobis Multivariate outliers \u2705 \u2705 (ideally) D\u00b2 &gt; 3 or 5"},{"location":"Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Kurtosis/","title":"Kurtosis","text":"\u2705 Kurtosis \ud83d\udccc What is Kurtosis? <p>Kurtosis is a statistical measure that describes the \u201ctailedness\u201d of a data distribution \u2014 that is, how heavily the tails (extreme values) differ from a normal distribution.</p> <p>Definition</p> <p>Kurtosis tells us:</p> <ul> <li> <p>How peaked the distribution is.</p> </li> <li> <p>How much data lies in the tails (outliers).</p> </li> </ul> <p>Mathematically, it's based on the fourth standardized moment about the mean.</p> <p>\ud83e\uddea Types of Kurtosis</p> Type Description Visual Shape Mesokurtic Normal distribution; moderate tails and peak. Bell-shaped Leptokurtic High peak, fat tails \u2192 more extreme values. Tall and thin Platykurtic Flat peak, thin tails \u2192 fewer extreme values. Wide and flat <p>\ud83d\udcca Visual Representation</p> <p>1. Mesokurtic (Normal)</p> <pre><code>    *\n  *   *\n*       *\n</code></pre> <ul> <li>*</li> <li>*</li> </ul> <p>2. Leptokurtic (Heavy tails)</p> <pre><code>    *\n    *\n   * *\n  *   *\n *     *\n</code></pre> <ul> <li>*</li> </ul> <p>3. Platykurtic (Light tails)</p> <pre><code>  *         *\n</code></pre> <ul> <li> <ul> <li> <ul> <li>*</li> </ul> </li> </ul> </li> <li>*</li> </ul> <p>\ud83d\udcd0 Kurtosis Value Interpretation</p> <ul> <li> <p>Kurtosis \u2248 3 \u2192 Mesokurtic (normal)</p> </li> <li> <p>Kurtosis &gt; 3 \u2192 Leptokurtic (heavy tails, more outliers)</p> </li> <li> <p>Kurtosis &lt; 3 \u2192 Platykurtic (light tails, fewer outliers)</p> </li> </ul> <p>Some software (like Python\u2019s scipy.stats.kurtosis) reports \u201cexcess kurtosis\u201d, which subtracts 3:</p> Excess Kurtosis Shape 0 Normal &gt; 0 Leptokurtic &lt; 0 Platykurtic <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis\n\n# Sample distributions\nnormal = np.random.normal(0, 1, 1000)\nlepto = np.random.laplace(0, 1, 1000)    # Heavy tails\nplaty = np.random.uniform(-3, 3, 1000)   # Light tails\n\n# Calculate kurtosis\nprint(\"Normal Kurtosis:\", kurtosis(normal))  # ~0\nprint(\"Leptokurtic Kurtosis:\", kurtosis(lepto))  # &gt; 0\nprint(\"Platykurtic Kurtosis:\", kurtosis(platy))  # &lt; 0\n\n# Plot\nsns.kdeplot(normal, label='Normal (Mesokurtic)')\nsns.kdeplot(lepto, label='Leptokurtic')\nsns.kdeplot(platy, label='Platykurtic')\nplt.legend()\nplt.title(\"Comparison of Kurtosis Types\")\nplt.show()\n</code></pre> <p></p> <p>\u2705 Why Kurtosis Matters</p> <ul> <li> <p>Helps detect outliers or extreme risks in finance, health data, etc.</p> </li> <li> <p>Important in quality control, risk modeling, and machine learning to understand distributions better.</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Shape-of-the-Distribution/Skewness/","title":"Skewness","text":"\u2705 Shape of the Distribution in Statistics <p>The shape of a distribution describes how data is spread or clustered across a range of values. It gives insights into central tendency, variability, and skewness.</p> \ud83d\udccc Main Types of Distribution Shapes <ol> <li> <p>Symmetrical Distribution (Normal or Bell-shaped)</p> </li> <li> <p>Skewed Distribution</p> </li> <li> <p>Uniform Distribution</p> </li> <li> <p>Bimodal Distribution</p> </li> <li> <p>Symmetrical Distribution (Normal or Bell-shaped)</p> </li> <li> <p>Mean = Median = Mode</p> </li> <li> <p>Both sides of the center are mirror images.</p> </li> <li> <p>Classic example: Height, IQ scores</p> </li> </ol> <p>\ud83d\udcc8 Example:</p> <pre><code>   *\n *   *\n</code></pre> <ul> <li>*</li> <li>*</li> <li> <p>*</p> </li> <li> <p>Skewed Distribution</p> </li> </ul> <p>a. Right-Skewed (Positively Skewed)</p> <ul> <li> <p>Tail is longer on the right</p> </li> <li> <p>Mean &gt; Median &gt; Mode</p> </li> <li> <p>Examples: Income, house prices</p> </li> </ul> <p>\ud83d\udcc8 Shape:</p> <ul> <li></li> <li>*</li> <li> <ul> <li></li> </ul> </li> <li> <ul> <li></li> </ul> </li> <li> <ul> <li></li> </ul> </li> <li> <ul> <li>*              *                 *</li> </ul> </li> </ul> <p>b. Left-Skewed (Negatively Skewed)</p> <ul> <li> <p>Tail is longer on the left</p> </li> <li> <p>Mean &lt; Median &lt; Mode</p> </li> <li> <p>Examples: Retirement age, exam scores with most students scoring high</p> </li> </ul> <p>\ud83d\udcc8 Shape:</p> <pre><code>          *\n        *\n      *\n    *\n  *\n*\n</code></pre> <p>*  </p> <ol> <li> <p>Uniform Distribution</p> </li> <li> <p>All values have equal frequency.</p> </li> <li> <p>No peak.</p> </li> </ol> <p>\ud83d\udcc8 Shape:</p> <ol> <li> <p>Bimodal Distribution</p> </li> <li> <p>Two clear peaks.</p> </li> <li> <p>Suggests two different subgroups in data.</p> </li> </ol> <p>\ud83d\udcc8 Shape:</p> <pre><code>*       *\n</code></pre> <ul> <li> <ul> <li> <ul> <li>*</li> </ul> </li> </ul> </li> <li> <ul> <li> <ul> <li>*</li> </ul> </li> </ul> </li> <li> <ul> <li> <ul> <li>*</li> </ul> </li> </ul> </li> <li> <ul> <li>*</li> </ul> </li> </ul> <p>\ud83d\udcca Python Code to Visualize</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create sample data\nnormal = np.random.normal(loc=50, scale=10, size=1000)\nright_skew = np.random.exponential(scale=10, size=1000)\nleft_skew = -np.random.exponential(scale=10, size=1000) + 50\nuniform = np.random.uniform(0, 100, 1000)\nbimodal = np.concatenate([np.random.normal(40, 5, 500), np.random.normal(70, 5, 500)])\n\n# Plot\nfig, axes = plt.subplots(3, 2, figsize=(14, 10))\nsns.histplot(normal, kde=True, ax=axes[0, 0]).set(title='Normal Distribution')\nsns.histplot(right_skew, kde=True, ax=axes[0, 1]).set(title='Right Skewed')\nsns.histplot(left_skew, kde=True, ax=axes[1, 0]).set(title='Left Skewed')\nsns.histplot(uniform, kde=True, ax=axes[1, 1]).set(title='Uniform Distribution')\nsns.histplot(bimodal, kde=True, ax=axes[2, 0]).set(title='Bimodal Distribution')\naxes[2, 1].axis('off')  # Hide the last empty plot\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Normal Distribution:</p> <p></p> <p>Right Skewed:</p> <p></p> <p>Left Skewed:</p> <p></p> <p>Uniform Distribution:</p> <p></p> <p>Bimodal Distribution:</p> <p></p>"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/BarChart/","title":"Bar Chart","text":"\u2705 Bar Chart \ud83d\udccc What is Bar Chart? <p>A Bar Chart is a visual representation used to compare categories of data using rectangular bars. Each bar's length or height is proportional to the value it represents.</p> <p>\ud83e\udde0 Key Features of a Bar Chart</p> Element Description X-axis Categories (e.g., Products, Departments, Genders) Y-axis Numeric values (e.g., sales, counts) Bars Represent values of each category Orientation Vertical (default) or horizontal <p>\ud83d\udcca Bar Chart Example in Python</p> <pre><code>import matplotlib.pyplot as plt\n\n# Example: Sales of different products\nproducts = ['Laptop', 'Tablet', 'Smartphone', 'Smartwatch']\nsales = [150, 90, 300, 120]\n\n# Create bar chart\nplt.bar(products, sales, color='teal')\nplt.title('Sales by Product Category')\nplt.xlabel('Product')\nplt.ylabel('Units Sold')\nplt.grid(axis='y')\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udd04 Horizontal Bar Chart Example</p> <pre><code>plt.barh(products, sales, color='coral')\nplt.title('Sales by Product Category (Horizontal)')\nplt.xlabel('Units Sold')\nplt.ylabel('Product')\nplt.grid(axis='x')\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udccc When to Use a Bar Chart</p> <ul> <li> <p>Comparing discrete categories (e.g., survey responses, department performance).</p> </li> <li> <p>Displaying counts, frequencies, or aggregated data.</p> </li> <li> <p>Showing ranking or distribution across categories.</p> </li> </ul> <p>\ud83c\udd9a Bar Chart vs Histogram</p> Feature Bar Chart Histogram Data Type Categorical Continuous numeric Bars Touching? No (space between bars) Yes (adjacent bars for intervals) Purpose Compare category values Show data distribution (frequencies)"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/BarChart/#example-use-cases","title":"\ud83d\udcc8 Example Use Cases","text":"<ul> <li> <p>\ud83c\udfe2 Number of employees per department</p> </li> <li> <p>\ud83d\udcf1 App downloads by platform</p> </li> <li> <p>\ud83c\udfc6 Medal count by country</p> </li> <li> <p>\ud83d\udcac Customer feedback categories</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/BoxPlot/","title":"Box Plot","text":"\u2705 Box Plot \ud83d\udccc What is Box Plot? <p>A Box Plot is a powerful visualization tool that summarizes the distribution, central tendency, and variability of a dataset using five-number summary:</p> <p>\ud83e\udde0 Five-Number Summary</p> Term Meaning Minimum Lowest value (excluding outliers) Q1 1st Quartile (25th percentile) Median (Q2) 2nd Quartile (50th percentile) Q3 3rd Quartile (75th percentile) Maximum Highest value (excluding outliers) <p>\ud83d\udd0d Box Plot Structure</p> <pre><code>|---------|=========|---------|\nmin      Q1       Q2       Q3       max\n</code></pre> <ul> <li> <p>Box: From Q1 to Q3 \u2192 represents the Interquartile Range (IQR)</p> </li> <li> <p>Line inside box: Median (Q2)</p> </li> <li> <p>Whiskers: Extend to min and max (not including outliers)</p> </li> <li> <p>Dots outside whiskers: Outliers</p> </li> </ul> <p>\ud83d\udcc8 Example: Box Plot in Python</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample data: test scores\ndata = [45, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n\n# Create box plot\nsns.boxplot(data=data, color=\"lightblue\")\nplt.title(\"Box Plot of Test Scores\")\nplt.xlabel(\"Test Scores\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\u2705 What Can You Learn from a Box Plot?</p> Insight How to Spot it Central Tendency Median line in the box (Q2) Spread (IQR) Width of the box (Q3 - Q1) Outliers Points outside the whiskers Skewness Median not centered in the box <p>\ud83d\udccc When to Use a Box Plot</p> <ul> <li> <p>Comparing distributions across groups (e.g., salary by department)</p> </li> <li> <p>Detecting outliers</p> </li> <li> <p>Visualizing data spread and symmetry</p> </li> <li> <p>Excellent for Exploratory Data Analysis (EDA)</p> </li> </ul> <p>\ud83d\udcca Example with Group Comparison</p> <pre><code>import pandas as pd\n\n# Example dataset\ndf = pd.DataFrame({\n    \"Department\": [\"Sales\"]*5 + [\"Tech\"]*5 + [\"HR\"]*5,\n    \"Salary\": [45, 50, 47, 48, 55, 75, 80, 85, 70, 90, 35, 40, 38, 42, 37]\n})\n\n# Box plot by group\nsns.boxplot(x=\"Department\", y=\"Salary\", data=df, palette=\"pastel\")\nplt.title(\"Salary Distribution by Department\")\nplt.show()\n</code></pre> <p></p>"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/DotPlot/","title":"Dot Plot","text":"\u2705 Dot Plot \ud83d\udccc What is Dot Plot? <p>A Dot Plot is a graph used to display discrete data where each dot represents one observation. It\u2019s particularly useful for small datasets or showing frequencies in a visually intuitive way.</p> <p>\ud83e\udde0 Key Characteristics</p> Feature Description Data Type Usually categorical or discrete numeric X-axis Categories or numeric values Dots Each dot represents one occurrence or a count Stacking Dots are stacked vertically when values repeat <p>\ud83d\udcc8 Dot Plot Example in Python</p> <pre><code>import matplotlib.pyplot as plt\n\n# Example data: Number of students scoring each grade\ngrades = ['A', 'B', 'C', 'D', 'F']\ncounts = [5, 8, 4, 2, 1]\n\n# Plot dot plot manually\nfor i in range(len(grades)):\n    plt.plot([grades[i]] * counts[i], list(range(1, counts[i]+1)), 'ko')\n\nplt.title(\"Dot Plot of Student Grades\")\nplt.xlabel(\"Grades\")\nplt.ylabel(\"Frequency\")\nplt.grid(True, axis='y')\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udccc When to Use a Dot Plot</p> <ul> <li> <p>To show exact frequency counts for a small dataset.</p> </li> <li> <p>To compare individual values without aggregating into bars (like bar chart).</p> </li> <li> <p>When clarity is more important than compactness.</p> </li> </ul> <p>\u2705 Dot Plot vs Bar Chart</p> Feature Dot Plot Bar Chart Representation Individual dots Bars representing total count Best for Small datasets Large categorical datasets Insight Exact count of items Quick comparison of group totals <p>\ud83e\uddea Example Use Cases</p> <ul> <li> <p>\ud83d\udc68\u200d\ud83c\udfeb Test scores in a small class</p> </li> <li> <p>\ud83e\uddee Number of items sold per day (if few days)</p> </li> <li> <p>\ud83d\udcac Survey responses (agree/disagree/neutral)</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/Histogram/","title":"Histogram","text":"\u2705 Histogram \ud83d\udccc What is Histogram? <p>A Histogram is a graphical representation used to visualize the distribution of numerical data. It groups data into intervals (bins) and shows the frequency (count) of data points in each interval.</p> <p>\ud83e\udde0 Key Features of a Histogram</p> Element Description Bins Intervals into which data is grouped (e.g., 0\u201310, 10\u201320) Frequency Number of data points in each bin X-axis Data intervals (e.g., age, income) Y-axis Frequency (how many times a value occurs) <p>\ud83d\udcc8 Example Visualization in Python</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Example: Random test scores of 100 students\ndata = np.random.normal(loc=70, scale=10, size=100)\n\n# Plot histogram\nplt.hist(data, bins=10, color='skyblue', edgecolor='black')\nplt.title('Histogram of Test Scores')\nplt.xlabel('Score Range')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>What You Can Observe from a Histogram</p> <ul> <li> <p>Shape of distribution (e.g., normal, skewed, bimodal)</p> </li> <li> <p>Spread of data</p> </li> <li> <p>Central tendency (where most data is clustered)</p> </li> <li> <p>Outliers or gaps</p> </li> </ul> <p>\ud83d\udccc When to Use a Histogram</p> <ul> <li> <p>Understanding the distribution of continuous data.</p> </li> <li> <p>Detecting skewness or symmetry.</p> </li> <li> <p>Identifying mode(s) (most frequent values).</p> </li> <li> <p>Performing exploratory data analysis (EDA) in ML/AI pipelines.</p> </li> </ul> <p>\ud83d\udcca Histogram vs Bar Chart</p> Feature Histogram Bar Chart Data Type Continuous (numerical) Categorical Bars Touching? Yes (bins are adjacent) No (bars are separated) X-Axis Intervals or ranges Categories (e.g., Apple, Banana)"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/LinePlot/","title":"Line Plot","text":"\u2705 Line Plot \ud83d\udccc What is Line Plot? <p>A Line Plot (or Line Graph) is used to show changes over a continuous variable, typically time. It connects data points with a line, making it ideal for spotting trends, patterns, and fluctuations.</p> <p>\ud83e\udde0 Key Features of a Line Plot</p> Element Description X-axis Independent variable (usually time: days, months, years) Y-axis Dependent variable (e.g., temperature, revenue, score) Line Connects data points to reveal trends or movement Markers Optional dots to highlight data points <p>\ud83d\udcca Python Example \u2013 Line Plot with Matplotlib</p> <pre><code>import matplotlib.pyplot as plt\n\n# Example data: Monthly sales\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nsales = [120, 135, 150, 145, 160, 180]\n\n# Create line plot\nplt.plot(months, sales, marker='o', linestyle='-', color='blue')\nplt.title(\"Monthly Sales Trend\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Sales (in units)\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\u2705 What Can You Learn from a Line Plot?</p> Insight How to Interpret Increasing trend Line slopes upward Decreasing trend Line slopes downward Fluctuations Line goes up and down irregularly Peaks and valleys Highest and lowest points on the line <p>\ud83d\udccc When to Use a Line Plot</p> <ul> <li> <p>To visualize trends over time (time series)</p> </li> <li> <p>To compare multiple series (e.g., different products or countries)</p> </li> <li> <p>To forecast future trends using models</p> </li> </ul> <p>\ud83d\udcc8 Multi-Line Plot Example</p> <pre><code>months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\nproduct_A = [120, 135, 150, 145, 160, 180]\nproduct_B = [100, 120, 140, 135, 150, 170]\n\nplt.plot(months, product_A, marker='o', label='Product A')\nplt.plot(months, product_B, marker='s', label='Product B')\nplt.title(\"Sales Comparison\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Sales\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udcca Use Cases for Line Plots</p> <ul> <li> <p>\ud83d\udcc5 Stock prices over time</p> </li> <li> <p>\ud83c\udf21\ufe0f Temperature variations per month</p> </li> <li> <p>\ud83e\udde0 Model performance over training epochs</p> </li> <li> <p>\ud83d\udcc8 Sales/Revenue trends</p> </li> </ul>"},{"location":"Statistic/DescriptiveStatistics/Visualization-Tools/PieChart/","title":"Pie Chart","text":"\u2705 Pie Chart \ud83d\udccc What is Pie Chart? <p>A Pie Chart displays data in a circular graph, where each slice represents a part of the whole. It\u2019s ideal for showing percentage or proportional data.</p> <p>\ud83e\udde0 Key Features of a Pie Chart</p> Feature Description Whole Circle Represents 100% of the data Slices Each slice shows the contribution of a category to the total Angle Determined by the proportion of the category Labels Often includes category names and percentages <p>\ud83d\udcca Python Example \u2013 Simple Pie Chart</p> <pre><code>import matplotlib.pyplot as plt\n\n# Example data: Market share by company\nlabels = ['Company A', 'Company B', 'Company C', 'Company D']\nsizes = [40, 30, 20, 10]\ncolors = ['skyblue', 'lightgreen', 'salmon', 'gold']\n\n# Plot pie chart\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.title(\"Market Share Distribution\")\nplt.axis('equal')  # Equal aspect ratio makes the pie circular\nplt.show()\n</code></pre> <p></p> <p>\u2705 What Can You Learn from a Pie Chart?</p> Insight Example Largest share The biggest slice (e.g., \"Company A\") Relative comparison How one category compares to another Proportion Exact percentage shown with <code>autopct</code> <p>\u26a0\ufe0f Limitations of Pie Charts</p> <ul> <li> <p>Not ideal for comparing many categories ( &gt; 5 is hard to read).</p> </li> <li> <p>Less precise than bar charts for comparing exact values.</p> </li> <li> <p>Hard to read if the slices are similar in size.</p> </li> </ul> <p>\ud83d\udccc When to Use a Pie Chart</p> <ul> <li> <p>You want to show composition or breakdown (e.g., budget allocation).</p> </li> <li> <p>Only a few categories to display.</p> </li> <li> <p>You want to highlight one dominant segment.</p> </li> </ul> <p>\ud83e\uddea Real-World Use Cases</p> <ul> <li> <p>\ud83d\udcb0 Expense distribution (e.g., rent, food, travel)</p> </li> <li> <p>\ud83d\udc65 Population by religion/gender/region</p> </li> <li> <p>\ud83d\uded2 Product sales by category</p> </li> <li> <p>\ud83d\udce7 Email open rates by device</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Hypothesis-Testing/overview/","title":"Overview","text":"\u2705 Hypothesis Testing <p>Hypothesis Testing is a statistical method used to make inferences or draw conclusions about a population based on sample data.</p> <p>It helps determine whether there's enough evidence in a sample to support a specific claim about the population.</p> <p>\ud83e\uddea Core Idea</p> <p>You begin with two competing hypotheses:</p> <ul> <li> <p>Null Hypothesis (H\u2080): No effect, no difference, or status quo.</p> </li> <li> <p>Alternative Hypothesis (H\u2081 or H\u2090): There is an effect, a difference, or a change.</p> </li> </ul> <p>You collect data and use a statistical test to decide whether to reject H\u2080.</p> <p>Common Terminology</p> Term Meaning Null Hypothesis (H\u2080) Assumes no difference or effect. Alternative Hypothesis What you aim to support \u2014 there is a difference or effect. p-value Probability of getting test results as extreme as observed if H\u2080 true. Significance level (\u03b1) Common threshold (e.g., 0.05) to decide whether to reject H\u2080. Reject H\u2080 If p &lt; \u03b1 \u2192 statistically significant \u2192 support H\u2081. <p>\ud83e\udde0 Types of Hypothesis Tests</p> Test Type Purpose Assumptions/Use Case Z-Test Test population mean (known variance) \u2705 Parametric T-Test Test population mean (unknown variance) \u2705 Parametric ANOVA Compare means of 3+ groups \u2705 Parametric F-Test Compare variances of two populations \u2705 Parametric Chi-Square Test Test relationships between categorical variables \u274c Non-Parametric Mann-Whitney U Compare two independent samples (ordinal/scale) \u274c Non-Parametric Wilcoxon Signed-Rank Compare paired samples (ordinal/scale) \u274c Non-Parametric Kruskal-Wallis Test Compare 3+ groups without assuming normality \u274c Non-Parametric Dunn's Test Post-hoc for Kruskal-Wallis \u274c Non-Parametric Tukey\u2019s HSD Post-hoc for ANOVA \u2705 Parametric <p>\ud83c\udfaf Real Example: Do Students Improve After Training?</p> <p>Let's say a group of students took a pre-test, attended a training course, then took a post-test.</p> <pre><code>import numpy as np\nfrom scipy.stats import ttest_rel\nimport matplotlib.pyplot as plt\n\n# Sample scores\nbefore = np.array([55, 60, 52, 58, 57, 59, 61])\nafter = np.array([65, 67, 62, 66, 63, 68, 69])\n\n# Perform paired t-test (parametric)\nstat, p = ttest_rel(after, before)\nprint(f\"T-statistic: {stat:.3f}\")\nprint(f\"P-value: {p:.3f}\")\n\n# Decision\nalpha = 0.05\nif p &lt; alpha:\n    print(\"\u2705 Significant improvement after training (reject H\u2080).\")\nelse:\n    print(\"\u274c No significant improvement (fail to reject H\u2080).\")\n</code></pre> <p>T-statistic: 14.653 P-value: 0.000 \u2705 Significant improvement after training (reject H\u2080).</p> <p>\ud83d\udcca Visualization</p> <pre><code>plt.figure(figsize=(8,5))\nplt.plot(before, label='Before Training', marker='o')\nplt.plot(after, label='After Training', marker='s')\nplt.title(\"Student Scores: Before vs After Training\")\nplt.xlabel(\"Student Index\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udd04 Hypothesis Testing Process</p> <ol> <li> <p>State the hypotheses (H\u2080 &amp; H\u2081)</p> </li> <li> <p>Choose significance level (\u03b1 = 0.05)</p> </li> <li> <p>Select the appropriate test (e.g., t-test, ANOVA)</p> </li> <li> <p>Calculate test statistic &amp; p-value</p> </li> <li> <p>Compare p-value with \u03b1</p> </li> <li> <p>Draw conclusion: reject or fail to reject H\u2080</p> </li> </ol> <p>\ud83d\udccc Summary</p> Step Description H\u2080 No change (e.g., mean_before = mean_after) H\u2081 There is a change (e.g., mean_after &gt; mean_before) p &lt; 0.05 Reject H\u2080 \u2192 statistically significant p &gt;= 0.05 Fail to reject H\u2080"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/Chi-square/","title":"Chi-square","text":"\u2705 Chi-Square Test <p>\ud83d\udcca Real-Time Example 3: Chi-Square Test for Independence</p> <pre><code>import pandas as pd\nfrom scipy.stats import chi2_contingency\n\n# Survey results: Gender vs Preferred Learning Style\ndata = pd.DataFrame({\n    \"Visual\": [25, 30],\n    \"Auditory\": [20, 15],\n    \"Kinesthetic\": [15, 20]\n}, index=[\"Male\", \"Female\"])\n\nchi2, p, dof, expected = chi2_contingency(data)\nprint(f\"Chi-Square: {chi2:.3f}, p-value: {p:.3f}\")\n</code></pre> <p>Chi-Square: 1.686, p-value: 0.430</p> <p>\ud83d\udccc Summary Table</p> Test Best For Python Function Mann\u2013Whitney U 2 independent groups <code>mannwhitneyu</code> Wilcoxon 2 related groups <code>wilcoxon</code> Kruskal\u2013Wallis 3+ independent groups <code>kruskal</code> Friedman 3+ related groups <code>friedmanchisquare</code> Chi-Square Categorical variables <code>chi2_contingency</code>"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis/","title":"Kruskal-Wallis","text":"\u2705 Kruskal\u2013Wallis H Test <p>\ud83d\udcca Real-Time Example 2: Kruskal\u2013Wallis H Test Compare 3+ groups with non-normal distributions.</p> <pre><code>from scipy.stats import kruskal\n\ngroup1 = [88, 90, 85, 95, 92]\ngroup2 = [75, 78, 72, 70, 80]\ngroup3 = [60, 65, 63, 62, 68]\n\nstat, p = kruskal(group1, group2, group3)\nprint(f\"H-Statistic: {stat}, p-value: {p:.3f}\")\n</code></pre> <p>H-Statistic: 12.5, p-value: 0.002</p>"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/Kruskal-Wallis/#dunns-test-post-hoc-for-kruskal-wallis","title":"\u2705 Dunn\u2019s Test (Post-hoc for Kruskal-Wallis)","text":"<p>After running a Kruskal\u2013Wallis test (non-parametric ANOVA) and finding a significant result, you can use Dunn\u2019s test to figure out which specific groups differ.</p> <p>\ud83e\uddea When to Use Dunn\u2019s Test?</p> <ul> <li> <p>You\u2019ve used Kruskal\u2013Wallis to compare 3+ independent groups</p> </li> <li> <p>You found a significant p-value</p> </li> <li> <p>Now you want to compare each pair of groups</p> </li> </ul> <p>\ud83d\udcca Real-Time Example: Kruskal-Wallis + Dunn's Test</p> <pre><code>import numpy as np\nimport scikit_posthocs as sp\nimport pandas as pd\nfrom scipy.stats import kruskal\n\n# Example scores from 3 groups\ngroup1 = [88, 90, 85, 95, 92]\ngroup2 = [75, 78, 72, 70, 80]\ngroup3 = [60, 65, 63, 62, 68]\n\n# Combine into a single array\ndata = group1 + group2 + group3\ngroups = (['Group1'] * len(group1)) + (['Group2'] * len(group2)) + (['Group3'] * len(group3))\n\n# Kruskal-Wallis Test\nstat, p = kruskal(group1, group2, group3)\nprint(f\"Kruskal-Wallis H-statistic: {stat:.3f}, p-value: {p:.3f}\")\n\n# If significant, run Dunn\u2019s post-hoc test\nif p &lt; 0.05:\n    df = pd.DataFrame({'score': data, 'group': groups})\n    dunn_result = sp.posthoc_dunn(df, val_col='score', group_col='group', p_adjust='bonferroni')\n    print(\"\\nDunn's Test Pairwise Comparisons:\\n\", dunn_result)\n</code></pre> <p>Kruskal-Wallis H-statistic: 12.500, p-value: 0.002</p> <p>Dunn's Test Pairwise Comparisons:            Group1  Group2    Group3 Group1  1.000000  0.2313  0.001221 Group2  0.231300  1.0000  0.231300 Group3  0.001221  0.2313  1.000000</p> <p>Visualization</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(x=\"group\", y=\"score\", data=df)\nplt.title(\"Group Comparison - Dunn's Test\")\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udccc Notes:</p> <ul> <li> <p>p_adjust='bonferroni' is used to correct for multiple comparisons (you can also use 'holm', 'fdr_bh', etc.)</p> </li> <li> <p>Dunn\u2019s test helps identify which group pairs differ significantly (like Tukey\u2019s HSD in parametric ANOVA)</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/Mann-WhitneyU/","title":"Mann-Whitney U","text":"\u2705 Mann\u2013Whitney U Test <p>\ud83d\udcca Real-Time Example 1: Mann\u2013Whitney U Test</p> <p>Compare test scores between 2 teaching methods (non-normally distributed).</p> <pre><code>import numpy as np\nfrom scipy.stats import mannwhitneyu\n\ngroup1 = [88, 90, 85, 95, 92]\ngroup2 = [75, 78, 72, 70, 80]\n\nstat, p = mannwhitneyu(group1, group2, alternative='two-sided')\nprint(f\"U-Statistic: {stat}, p-value: {p:.3f}\")\n</code></pre> <p>U-Statistic: 25.0, p-value: 0.008</p>"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/Wilcoxon/","title":"Wilcoxon","text":"\u2705 Wilcoxon Signed-Rank Test <p>The Wilcoxon Signed-Rank Test is a non-parametric alternative to the paired t-test. It compares two related (paired/matched) samples to assess whether their population mean ranks differ.</p> <p>\ud83e\uddea When to Use Wilcoxon Test?</p> <ul> <li> <p>You have two related groups (e.g., before and after treatment)</p> </li> <li> <p>Data is not normally distributed</p> </li> <li> <p>Alternative to the paired t-test</p> </li> </ul> <p>\ud83d\udcca Real-Life Example (Python): Before vs After Scores</p> <pre><code>import numpy as np\nfrom scipy.stats import wilcoxon\n\n# Paired sample data (before and after)\nbefore = np.array([72, 75, 78, 76, 74, 77, 73])\nafter =  np.array([74, 78, 79, 77, 75, 78, 76])\n\n# Wilcoxon Signed-Rank Test\nstat, p = wilcoxon(before, after)\nprint(f\"Wilcoxon statistic: {stat:.3f}\")\nprint(f\"p-value: {p:.3f}\")\n\n# Interpretation\nif p &lt; 0.05:\n    print(\"\u2705 Statistically significant difference between paired samples.\")\nelse:\n    print(\"\u274c No significant difference between paired samples.\")\n</code></pre> <p>Wilcoxon statistic: 0.000 p-value: 0.016 \u2705 Statistically significant difference between paired samples.</p> <p>Visualization</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.plot(before, label=\"Before\", marker='o')\nplt.plot(after, label=\"After\", marker='s')\nplt.title(\"Before vs After Comparison\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udccc Summary</p> Test Use Case Assumptions Wilcoxon Two related groups Ordinal or continuous data vs Non-parametric alternative Paired t Assumes normality"},{"location":"Statistic/InferentialStatistics/Non-Parametric-Tests/overview/","title":"Overview","text":"\u2705 Non-Parametric Test \ud83d\udccc What is Non-Parametric Test? <p>Non-parametric tests are statistical tests that do not assume a specific distribution (like normality) for the data. They're ideal for:</p> <ul> <li> <p>Small sample sizes</p> </li> <li> <p>Ordinal/ranked data</p> </li> <li> <p>Data that violates assumptions of parametric tests</p> </li> </ul> <p>\ud83e\uddea Common Non-Parametric Tests</p> Test Parametric Equivalent Use Case Mann\u2013Whitney U Independent t-test Compare 2 independent groups Wilcoxon Signed-Rank Paired t-test Compare 2 related groups Kruskal\u2013Wallis H One-way ANOVA Compare 3+ independent groups Friedman Test Repeated Measures ANOVA Compare 3+ related groups Chi-Square Test \u2014 Test for independence or goodness-of-fit"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/ANOVA/","title":"ANOVA","text":"\u2705 ANOVA (Analysis of Variance) \ud83d\udccc What is ANOVA (Analysis of Variance)? <p>ANOVA (Analysis of Variance) is a parametric test used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.</p> <p>\ud83d\udccc When to Use ANOVA:</p> <ul> <li> <p>Comparing more than 2 groups (e.g., test scores of students from 3 different schools).</p> </li> <li> <p>Data should be normally distributed and have equal variances.</p> </li> <li> <p>The independent variable is categorical, and the dependent variable is continuous.</p> </li> </ul> <p>\ud83d\udd2c Real-Time Example:</p> <p>Suppose a researcher wants to test whether students from three different schools (A, B, and C) perform differently in a mathematics exam.</p> <p>\ud83e\uddea Python Code with Graph</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Random seed for reproducibility\nnp.random.seed(42)\n\n# Sample data for 3 schools\nschool_A = np.random.normal(loc=70, scale=5, size=30)\nschool_B = np.random.normal(loc=75, scale=5, size=30)\nschool_C = np.random.normal(loc=80, scale=5, size=30)\n\n# Create DataFrame\ndata = pd.DataFrame({\n    'Score': np.concatenate([school_A, school_B, school_C]),\n    'School': ['A'] * 30 + ['B'] * 30 + ['C'] * 30\n})\n\n# \ud83d\udcca Boxplot for Visualization\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='School', y='Score', data=data, palette=\"Set2\")\nplt.title('Scores by School')\nplt.ylabel('Math Exam Score')\nplt.show()\n\n# \u2697\ufe0f One-Way ANOVA Test\nf_stat, p_val = stats.f_oneway(school_A, school_B, school_C)\n\n# \ud83d\udccb Results\nprint(\"ANOVA F-statistic:\", round(f_stat, 2))\nprint(\"ANOVA p-value:\", round(p_val, 4))\n\n# \ud83d\udd0d Interpretation\nif p_val &lt; 0.05:\n    print(\"\u2705 At least one group mean is significantly different (p &lt; 0.05)\")\nelse:\n    print(\"\u274c No significant difference between group means (p \u2265 0.05)\")\n</code></pre> <p></p> <p>\u2705 Summary:</p> <ul> <li> <p>Test Used: One-Way ANOVA</p> </li> <li> <p>Goal: Compare the mean scores of 3 different schools</p> </li> <li> <p>Output: F-statistic &amp; p-value</p> </li> <li> <p>Interpretation: If p &lt; 0.05, at least one group is different</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/ANOVA/#tukey-hsd-post-hoc-test-after-anova","title":"\u2705 Tukey HSD Post-Hoc Test (After ANOVA)","text":"<p>After finding a significant result in ANOVA, you can run Tukey\u2019s Honest Significant Difference (HSD) test to pinpoint which groups are significantly different from each other.</p> <p>\ud83d\udccc Why Use Tukey HSD?</p> <ul> <li> <p>ANOVA tells if there is a difference among groups.</p> </li> <li> <p>Tukey HSD tells which specific groups differ.</p> </li> <li> <p>It controls for Type I error across multiple comparisons.</p> </li> </ul> <p>\ud83e\uddea Real-Time Example (continued from ANOVA)</p> <pre><code>from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# \ud83e\uddea Tukey HSD Test\ntukey = pairwise_tukeyhsd(endog=data['Score'], groups=data['School'], alpha=0.05)\n\n# \ud83d\udccb Results\nprint(tukey)\n\n# \ud83d\udcca Visualization of Tukey Test\ntukey.plot_simultaneous(comparison_name='A', xlabel='Mean Score Difference')\nplt.title(\"Tukey HSD: Mean Score Difference Between Schools\")\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udccc Interpretation:</p> <ul> <li> <p><code>meandiff:</code> Difference in group means</p> </li> <li> <p><code>p-adj:</code> Adjusted p-value (controls for multiple comparisons)</p> </li> <li> <p><code>reject:</code> True means significant difference</p> </li> <li> <p>Example: School A vs School C \u2192 Significant difference (p &lt; 0.05)</p> </li> </ul> <p>\ud83d\udcc9 Visualization:</p> <p>The plot shows confidence intervals for group mean differences. If the CI doesn\u2019t cross zero \u2192 significant difference.</p> <p>\ud83e\uddfe Summary:</p> <ul> <li> <p>Run Tukey HSD after significant ANOVA</p> </li> <li> <p>Helps identify which pairs are significantly different</p> </li> <li> <p>Visual and statistical output makes conclusions easier</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/overview/","title":"Overview","text":"\u2705 Parametric Tests \ud83d\udccc What is Parametric Tests? <p>Parametric tests are statistical tests that assume the data follows a known distribution \u2014 usually a normal distribution.</p> <p>\u2705 Key Assumptions of Parametric Tests:</p> <ol> <li> <p>Normal distribution of the data (bell curve).</p> </li> <li> <p>Homogeneity of variances (equal variance across groups).</p> </li> <li> <p>Interval or ratio scale data (numeric, continuous).</p> </li> <li> <p>Observations are independent.</p> </li> </ol> <p>\ud83e\udde0 Why Use Parametric Tests?</p> <ul> <li> <p>\u2705 More powerful and sensitive</p> </li> <li> <p>\u2705 Provide precise estimates</p> </li> <li> <p>\u2705 Widely used in scientific and business analysis</p> </li> </ul> <p>\ud83d\udcda Types of Parametric Tests</p> Test Name Purpose Use Case Example Z-test Compare means or proportions (large sample) Compare sample mean to known population mean T-test Compare means of one/two/paired groups Test if two drugs have different effects ANOVA Compare means of 3 or more groups Compare exam scores across 3 schools F-test Compare variances between groups Check if two machines have equal variability Pearson Correlation Test linear relationship between variables Is height linearly related to weight? Linear Regression Predict one variable from another Predict house price based on area and location <p>Breakdown of T-Tests (most used parametric tests)</p> T-test Type Use Case One-sample t-test Sample mean vs known population mean Two-sample t-test Compare means of two independent groups Paired t-test Compare means of the same group (before/after)"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/t-test/","title":"t-test","text":"\u2705 T-test \ud83d\udccc What is T-test? <p>A t-test is a parametric statistical test used to compare means between groups and determine whether the difference is statistically significant.</p> <p>\u2705 When to Use a t-test? Use a t-test when:</p> <ul> <li> <p>Your data is approximately normally distributed.</p> </li> <li> <p>You are comparing means.</p> </li> <li> <p>Your data is interval or ratio scale.</p> </li> <li> <p>You have independent or paired observations.</p> </li> </ul> <p>\ud83e\uddea Types of t-tests</p> Type Use Case Python Function Example 1. One-sample t-test Compare sample mean to known/population mean <code>ttest_1samp()</code> Test if average height = 170cm 2. Two-sample (Independent) t-test Compare means of two independent groups <code>ttest_ind()</code> Compare test scores of boys vs girls 3. Paired-sample (Dependent) t-test Compare means of same group at different times <code>ttest_rel()</code> Compare weight before and after a diet <p>\ud83d\udccc 1. One-Sample t-test</p> <p>A one-sample t-test is a statistical test used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean.</p> <p>\ud83d\udccc When to Use? Use it when:</p> <ul> <li> <p>You have one group of observations.</p> </li> <li> <p>You want to compare its mean to a specific known value.</p> </li> </ul> <p>\ud83d\udca1 Real-Life Example Scenario</p> <p>Suppose a fitness coach claims that the average weight of gym members is 70 kg. You collect a sample of 15 members and want to test if their average weight differs significantly from 70 kg.</p> <p>\ud83e\uddee Hypotheses</p> <ul> <li> <p>Null Hypothesis (H\u2080): \u03bc = 70 \u2192 Mean weight is 70 kg.</p> </li> <li> <p>Alternative Hypothesis (H\u2081): \u03bc \u2260 70 \u2192 Mean weight is not 70 kg.</p> </li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Sample weights of 15 gym members\nsample_weights = [72, 69, 75, 71, 68, 74, 70, 73, 76, 68, 72, 69, 71, 67, 70]\n\n# Known population mean (hypothesized mean)\npopulation_mean = 70\n\n# One-sample t-test\nt_stat, p_value = stats.ttest_1samp(sample_weights, population_mean)\n\nprint(\"T-statistic:\", round(t_stat, 3))\nprint(\"P-value:\", round(p_value, 3))\n\n# Interpretation\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"\u2705 Reject the null hypothesis: The average weight is significantly different from 70 kg.\")\nelse:\n    print(\"\u274c Fail to reject the null hypothesis: No significant difference from 70 kg.\")\n\n# Visualization\nplt.figure(figsize=(8, 5))\nplt.hist(sample_weights, bins=8, color='skyblue', edgecolor='black')\nplt.axvline(population_mean, color='red', linestyle='--', label='Population Mean (70kg)')\nplt.axvline(np.mean(sample_weights), color='green', linestyle='--', label=f'Sample Mean ({round(np.mean(sample_weights),2)}kg)')\nplt.title(\"Distribution of Sample Weights\")\nplt.xlabel(\"Weight (kg)\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Conclusion</p> <p>Since the p-value is greater than 0.05, we do not have enough evidence to say the sample mean is significantly different from 70 kg.</p> <p>\ud83d\udccc 2. Two-Sample t-test</p> <p>\ud83d\udcd8 What is a Two-Sample T-Test?</p> <p>A two-sample t-test (also called independent t-test) compares the means of two independent groups to determine if they are significantly different from each other.</p> <p>\ud83d\udccc When to Use It?</p> <p>Use a two-sample t-test when:</p> <ul> <li>You are comparing two different groups.</li> <li>Each group is independent (no repeated measures or pairing).</li> <li>Your variable is numerical and approximately normally distributed.</li> </ul> <p>\ud83d\udca1 Real-Life Example Scenario</p> <p>A teacher wants to know if two different teaching methods result in different average test scores. Group A used traditional methods, and Group B used online interactive methods.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Sample test scores\ngroup_A = [78, 74, 80, 79, 72, 77, 76, 75]\ngroup_B = [82, 85, 88, 90, 84, 83, 87, 86]\n\n# Two-sample (independent) t-test\nt_stat, p_value = stats.ttest_ind(group_A, group_B)\n\nprint(\"T-statistic:\", round(t_stat, 3))\nprint(\"P-value:\", round(p_value, 3))\n\n# Interpretation\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"\u2705 Reject the null hypothesis: There is a significant difference between the two groups.\")\nelse:\n    print(\"\u274c Fail to reject the null hypothesis: No significant difference between the two groups.\")\n\n# Visualization\nplt.figure(figsize=(8, 5))\nplt.hist(group_A, bins=5, alpha=0.6, label='Group A (Traditional)', color='skyblue', edgecolor='black')\nplt.hist(group_B, bins=5, alpha=0.6, label='Group B (Online)', color='salmon', edgecolor='black')\nplt.axvline(np.mean(group_A), color='blue', linestyle='--', label=f'Mean A: {np.mean(group_A):.1f}')\nplt.axvline(np.mean(group_B), color='red', linestyle='--', label=f'Mean B: {np.mean(group_B):.1f}')\nplt.title(\"Distribution of Test Scores by Teaching Method\")\nplt.xlabel(\"Test Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Conclusion</p> <p>Since the p-value &lt; 0.05, the result is statistically significant \u2014 the average test scores differ between Group A and Group B.</p>"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/t-test/#two-sample-t-test","title":"Two-sample t-test","text":"<p>A two-sample t-test, also known as an independent t-test, is a statistical method used to determine whether the means of two independent groups are significantly different from each other.</p> <p>Key Characteristics</p> Feature Description \ud83d\udc6b Groups Two independent groups (e.g., Group A vs. Group B) \ud83c\udfaf Goal Compare means \ud83d\udd22 Data Type Continuous numerical data \ud83e\udde0 Assumptions Normal distribution, equal/unequal variance (can be tested) <p>\ud83d\udccc When to Use</p> <ul> <li> <p>Comparing test scores of students taught with two different teaching methods.</p> </li> <li> <p>Comparing blood pressure between two different drug groups.</p> </li> <li> <p>Comparing customer satisfaction between two branches of a store.</p> </li> </ul> <p>\ud83e\uddee Hypotheses</p> <p>Null Hypothesis (H\u2080):</p> <ul> <li>\u03bc\u2081 = \u03bc\u2082 \u2192 The means of the two groups are equal.</li> </ul> <p>Alternative Hypothesis (H\u2081):</p> <ul> <li> <p>\u03bc\u2081 \u2260 \u03bc\u2082 \u2192 The means of the two groups are not equal.</p> </li> <li> <p>You can also do one-tailed tests if you expect one group to be higher/lower than the other.</p> </li> </ul> <p>\u2705 Types of Two-Sample T-Test</p> Test Type When to Use Equal Variance (pooled) If both groups have similar variance Unequal Variance (Welch\u2019s) If the variance between groups is different (default in <code>scipy</code>) <p>\u26a0\ufe0f Assumptions</p> <ol> <li> <p>The two groups are independent.</p> </li> <li> <p>The data in each group is normally distributed.</p> </li> <li> <p>Variances of the two groups are either equal or unequal (handle using Welch's t-test).</p> </li> </ol> <p>\ud83d\udcca Formula (for equal variances)</p> <p></p> <p>\ud83d\udcc8 Example Use Case</p> <p>A school wants to compare math scores between boys and girls in 10th grade. If boys and girls are sampled independently, use a two-sample t-test.</p> <p>Example:</p> <p>We want to compare the average test scores of two classes (Class A and Class B) who were taught using different teaching methods.</p> <p>\ud83d\udc0d Python Code: Two-Sample T-Test with Visualization</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind\n\n# Sample test scores for two independent classes\nclass_A_scores = [78, 74, 80, 79, 72, 77, 76, 75]\nclass_B_scores = [82, 85, 88, 90, 84, 83, 87, 86]\n\n# Perform two-sample (independent) t-test\nt_stat, p_val = ttest_ind(class_A_scores, class_B_scores)\n\nprint(\"Class A Mean:\", np.mean(class_A_scores))\nprint(\"Class B Mean:\", np.mean(class_B_scores))\nprint(\"T-statistic:\", round(t_stat, 3))\nprint(\"P-value:\", round(p_val, 3))\n\n# Interpretation\nalpha = 0.05\nif p_val &lt; alpha:\n    print(\"\u2705 Significant difference between Class A and Class B scores.\")\nelse:\n    print(\"\u274c No significant difference between Class A and Class B scores.\")\n\n# \ud83d\udcca Visualization\nplt.figure(figsize=(8, 5))\nplt.hist(class_A_scores, bins=5, alpha=0.7, label='Class A', color='skyblue', edgecolor='black')\nplt.hist(class_B_scores, bins=5, alpha=0.7, label='Class B', color='salmon', edgecolor='black')\nplt.axvline(np.mean(class_A_scores), color='blue', linestyle='--', label=f'Class A Mean: {np.mean(class_A_scores):.1f}')\nplt.axvline(np.mean(class_B_scores), color='red', linestyle='--', label=f'Class B Mean: {np.mean(class_B_scores):.1f}')\nplt.title(\"Distribution of Test Scores: Class A vs Class B\")\nplt.xlabel(\"Test Scores\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Interpretation Since the p-value &lt; 0.05, we reject the null hypothesis. This means:</p> <p>\ud83d\udce2 There is a statistically significant difference between the average scores of Class A and Class B.</p>"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/t-test/#paired-t-test","title":"Paired T-Test","text":"<p>A paired t-test compares the means of two related (paired) samples, like:</p> <ul> <li> <p>Before vs. After results from the same individuals.</p> </li> <li> <p>Two related measurements from the same subjects (e.g., left vs. right hand grip strength).</p> </li> </ul> <p>\ud83e\uddee Hypotheses:</p> <ul> <li>Null Hypothesis (H\u2080):</li> </ul> <p>The mean difference between the paired observations is zero.</p> <ul> <li>Alternative Hypothesis (H\u2081):</li> </ul> <p>The mean difference is not zero.</p> <p>Real-Life Example Scenario</p> <p>A coach wants to check if a fitness training program improved the running speed of 10 athletes. We have their running times before and after the training.</p> <p>\ud83d\udc0d Python Code with Visualization</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_rel\n\n# Running times (in seconds) before and after training for 10 athletes\nbefore_training = [15.2, 14.8, 16.0, 15.5, 15.7, 16.1, 15.0, 14.9, 15.6, 16.3]\nafter_training =  [14.6, 14.4, 15.2, 15.0, 15.1, 15.4, 14.5, 14.2, 14.9, 15.7]\n\n# Paired t-test\nt_stat, p_val = ttest_rel(before_training, after_training)\n\nprint(\"T-statistic:\", round(t_stat, 3))\nprint(\"P-value:\", round(p_val, 3))\n\n# Interpretation\nalpha = 0.05\nif p_val &lt; alpha:\n    print(\"\u2705 Training significantly changed running time.\")\nelse:\n    print(\"\u274c No significant change in running time due to training.\")\n\n# Visualization of before vs after\nx = np.arange(len(before_training))\nwidth = 0.35\n\nplt.figure(figsize=(10, 5))\nplt.bar(x - width/2, before_training, width, label='Before', color='skyblue')\nplt.bar(x + width/2, after_training, width, label='After', color='lightgreen')\nplt.xticks(x, [f\"Athlete {i+1}\" for i in x])\nplt.ylabel(\"Running Time (seconds)\")\nplt.title(\"Running Times Before and After Training\")\nplt.legend()\nplt.grid(True, axis='y')\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>\ud83e\udde0 Interpretation</p> <p>Since p-value &lt; 0.05, we reject the null hypothesis.</p> <p>The training program significantly improved the athletes' running times.</p> <p>\ud83e\uddfe Comparison: Types of T-Tests</p> Feature \u2705 One-Sample T-Test \u2705 Two-Sample T-Test \u2705 Paired T-Test Purpose Compare sample mean to a known population mean Compare means of two independent groups Compare means of two related (paired) samples Data Type One group of numeric data Two groups (independent) of numeric data Two sets of numeric data from same subjects Sample Relationship Single sample Independent samples Dependent/paired samples Typical Use Case Is average weight \u2260 70 kg? Do male and female students score differently? Did training improve the same person\u2019s speed? Null Hypothesis (H\u2080) \u03bc = \u03bc\u2080 (sample mean = known mean) \u03bc\u2081 = \u03bc\u2082 (group means are equal) \u03bc_d = 0 (mean of differences = 0) Python Function <code>ttest_1samp()</code> <code>ttest_ind()</code> <code>ttest_rel()</code> Real-Life Example Compare avg. height of a class to 165 cm Compare test scores of two classes Compare BP before and after medication <p>Visual Summary</p> <p>1\ufe0f\u20e3 One-Sample:     [Sample A]  vs  [Known Mean]</p> <p>2\ufe0f\u20e3 Two-Sample:     [Group A]   vs  [Group B]     \u2190 Independent</p> <p>3\ufe0f\u20e3 Paired:     [Before]    vs  [After]       \u2190 Same subject</p>"},{"location":"Statistic/InferentialStatistics/Parametric-Tests/z-test/","title":"z-test","text":"\u2705 Z-Test \ud83d\udccc What is Z-Test? <p>A Z-test is a parametric statistical test used to determine whether there is a significant difference between sample and population means (or between two sample means) when the population variance is known and the sample size is large (typically n &gt; 30).</p> <p>When to Use a Z-Test?</p> Condition Description \u2705 Sample size is large (n &gt; 30) \u2705 Population standard deviation (\u03c3) is known \u2705 Data follows a normal distribution <p>\ud83d\udce6 Types of Z-Test</p> Z-Test Type Purpose One-sample Z-test Compare sample mean to population mean Two-sample Z-test Compare means from two independent samples Z-test for proportions Compare population proportions <p>\ud83e\uddea One-Sample Z-Test Formula</p> <p></p> <p>\ud83d\udcca Real-Life Example: Test if new teaching method improves scores</p> <p>Suppose:</p> <ul> <li> <p>Population mean (\u03bc) = 70</p> </li> <li> <p>Population std. deviation (\u03c3) = 10</p> </li> <li> <p>Sample scores from new method: [72, 74, 69, 68, 71, 73, 75, 70]</p> </li> </ul> <p>\u2705 Python Example:</p> <pre><code>import numpy as np\nfrom scipy.stats import norm\n\n# Given\npopulation_mean = 70\npopulation_std = 10\nsample = np.array([72, 74, 69, 68, 71, 73, 75, 70])\nsample_mean = np.mean(sample)\nn = len(sample)\n\n# Z-statistic\nz = (sample_mean - population_mean) / (population_std / np.sqrt(n))\n\n# P-value (two-tailed)\np_value = 2 * (1 - norm.cdf(abs(z)))\n\nprint(f\"Z-score: {z:.3f}\")\nprint(f\"P-value: {p_value:.3f}\")\n</code></pre> <p>Z-score: 0.424 P-value: 0.671</p> <ul> <li>Since p &gt; 0.05 \u2192 Fail to reject null hypothesis.   Conclusion: The new teaching method does not significantly change scores.</li> </ul> <p>\ud83d\udccc Summary:</p> Aspect Value Test Type Parametric Use When \u03c3 is known, n &gt; 30 Distribution Normal Test Statistic Z-score Common Use Cases A/B Testing, Proportions <p>two-sample z-test</p> <p>z-test for proportions</p>"},{"location":"Statistic/InferentialStatistics/Population-and-Sample/Population/","title":"Population","text":"\u2705 Population \ud83d\udccc What is Population? <p>In statistics, a population refers to the entire set of individuals or items that you're interested in studying.</p> <p>Population = The complete group of elements (people, objects, transactions, etc.) that have at least one characteristic in common.</p> <p>\ud83e\udde0 Real Example:</p> <p>Imagine you're analyzing the heights of all students in a university.</p> <ul> <li> <p>Population = All 5,000 students in the university.</p> </li> <li> <p>But it's difficult to collect height data from all 5,000.</p> </li> <li> <p>So we collect data from a smaller group (Sample), e.g., 100 students.</p> </li> </ul> <p>\ud83d\udc0d Python Example with Visual Diagram</p> <p></p> <p>\ud83d\udcc8 What This Shows:</p> <ul> <li> <p>A bell-shaped curve (normal distribution) representing the heights of all 5,000 students (the entire population).</p> </li> <li> <p>The red dashed line indicates the mean height of the population.</p> </li> </ul> <p>\u2705 When to Use Population Data</p> <ul> <li> <p>When you have access to the entire dataset.</p> </li> <li> <p>In small groups (e.g., all employees in a company).</p> </li> <li> <p>In census studies (like national population surveys).</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Population-and-Sample/Sample/","title":"Sample","text":"\u2705 Sample \ud83d\udccc What is Sample? <p>A sample is a subset of individuals, items, or data points selected from a population for analysis.</p> <p>A sample represents a small part of the population, used to draw conclusions (inferences) about the whole.</p> <p>\ud83e\udde0 Why Use a Sample?</p> <ul> <li> <p>It is often impractical or impossible to measure the entire population.</p> </li> <li> <p>Sampling is cheaper, faster, and more efficient.</p> </li> <li> <p>If done correctly, a sample can provide accurate insights about the population.</p> </li> </ul> <p>\ud83d\udccc Real-World Examples:</p> Study Goal Population Sample Example Average income of people in India All Indian citizens 2,000 people randomly selected Satisfaction level of Amazon customers All Amazon users 500 customers who recently ordered Average height of university students All 5,000 university students 100 randomly chosen students <p>\ud83d\udc0d Python Example \u2013 Sampling from a Population</p> <p>Let\u2019s build on the previous population of student heights, and now take a sample:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assume previous population is already generated\npopulation = np.random.normal(loc=170, scale=10, size=5000)\n\n# Step 1: Draw a random sample from population (e.g., 100 students)\nsample = np.random.choice(population, size=100, replace=False)\n\n# Step 2: Plot both population and sample\nplt.figure(figsize=(12, 6))\n\n# Population histogram\nsns.histplot(population, bins=50, kde=True, color='skyblue', label='Population', stat='density', alpha=0.4)\n\n# Sample histogram\nsns.histplot(sample, bins=20, kde=True, color='orange', label='Sample', stat='density', alpha=0.6)\n\n# Mean lines\nplt.axvline(np.mean(population), color='blue', linestyle='--', label='Population Mean')\nplt.axvline(np.mean(sample), color='orange', linestyle='--', label='Sample Mean')\n\nplt.title('\ud83d\udcca Population vs Sample: Heights Distribution')\nplt.xlabel('Height (cm)')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udcca Interpretation:</p> <ul> <li> <p>The blue curve shows the full population.</p> </li> <li> <p>The orange curve shows the sample drawn from the population.</p> </li> <li> <p>Both curves should have similar shapes, and the sample mean should be close to the population mean.</p> </li> </ul> <p>\ud83d\udd01 Summary</p> Term Description Symbol Population Entire group (e.g., all 5,000 students) <code>N</code> Sample Subset of the population (e.g., 100 students) <code>n</code> <p>\ud83c\udfaf Why Sampling Techniques Matter</p> <p>Choosing the right sampling method ensures that your sample:</p> <ul> <li> <p>Represents the population fairly.</p> </li> <li> <p>Reduces bias.</p> </li> <li> <p>Improves accuracy of statistical inferences.</p> </li> </ul> <p>\ud83d\udcda Types of Sampling Techniques</p> <p>1. \ud83c\udfb2 Simple Random Sampling (SRS)</p> <p>Every member of the population has an equal chance of being selected.</p> <p>\u2705 When to Use: When the population is homogeneous.</p> <p>\ud83d\udccc Python Example:</p> <pre><code>sample_srs = np.random.choice(population, size=100, replace=False)\n</code></pre> <p>2. \ud83d\udcca Stratified Sampling</p> <p>The population is divided into strata (groups), and random samples are taken from each group.</p> <p>\u2705 When to Use: When the population has distinct subgroups (e.g., gender, age group).</p> <p>\ud83d\udccc Python Example:</p> <pre><code># Simulate population with gender\nimport pandas as pd\npopulation_df = pd.DataFrame({\n    'height': population,\n    'gender': np.random.choice(['Male', 'Female'], size=5000, p=[0.5, 0.5])\n})\n\n# Stratified sampling: 50 males and 50 females\nsample_stratified = population_df.groupby('gender').sample(n=50, random_state=42)\n</code></pre> <p>3. \ud83e\uddfe Systematic Sampling</p> <p>Select every k-th element from a list after a random start.</p> <p>\u2705 When to Use: When data is ordered or evenly distributed.</p> <p>\ud83d\udccc Python Example:</p> <pre><code>k = 50  # Select every 50th person\nstart = np.random.randint(0, k)\nsample_systematic = population[start::k][:100]\n</code></pre> <p>4. \ud83d\udccd Cluster Sampling</p> <p>The population is divided into clusters (e.g., schools, cities), and entire clusters are randomly selected.</p> <p>\u2705 When to Use: When the population is naturally grouped and listing all elements is difficult.</p> <p>\ud83d\udccc Python-style Pseudocode:</p> <pre><code># Imagine schools as clusters and select few whole schools\n# Not implemented without full data structure of clusters\n</code></pre>"},{"location":"Statistic/InferentialStatistics/Population-and-Sample/Sample/#estimating-population-parameters-from-a-sample","title":"\ud83d\udcd0 Estimating Population Parameters from a Sample","text":"<p>1. \ud83d\udccf Sample Mean \u2248 Population Mean</p> <pre><code>sample_mean = np.mean(sample)\npopulation_mean = np.mean(population)\n</code></pre> <p>2. \ud83d\udcca Sample Standard Deviation</p> <p>Use <code>ddof=1</code> to get the unbiased estimate.</p> <pre><code>sample_std = np.std(sample, ddof=1)\n</code></pre> <p>3. \ud83d\udcc8 Confidence Interval for the Mean</p> <p>Estimate population mean using a confidence interval (e.g., 95%):</p> <pre><code>import scipy.stats as stats\n\nconfidence = 0.95\nn = len(sample)\nmean = np.mean(sample)\nstd_err = stats.sem(sample)  # Standard error\nmargin = stats.t.ppf((1 + confidence) / 2, df=n-1) * std_err\n\nlower_bound = mean - margin\nupper_bound = mean + margin\n\nprint(f\"95% Confidence Interval for Population Mean: ({lower_bound:.2f}, {upper_bound:.2f})\")\n</code></pre> <p>\u2705 Summary Table</p> Technique Use Case Bias Risk Simple Random Equal chance for all Low Stratified Population has meaningful subgroups Very Low Systematic Regularly spaced selection Medium Cluster Population naturally in groups Higher"},{"location":"Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods/","title":"Sampling Methods","text":"\u2705 Sampling Methods \ud83d\udccc What is Sampling Methods? <p>A sampling method is the strategy or approach used to select a subset (sample) from a larger population for analysis.</p> <p>Since it\u2019s often impractical to study an entire population, sampling methods allow you to collect representative data efficiently, while ensuring accuracy and minimizing bias.</p> <p>\ud83e\udde0 Purpose of Sampling Methods</p> <ul> <li> <p>To reduce cost and effort.</p> </li> <li> <p>To ensure the sample represents the population well.</p> </li> <li> <p>To make valid statistical inferences (e.g., mean, proportion, variance).</p> </li> </ul>"},{"location":"Statistic/InferentialStatistics/Population-and-Sample/SamplingMethods/#two-major-categories-of-sampling-methods","title":"\ud83d\udd30 Two Major Categories of Sampling Methods","text":"<p>1. \ud83e\uddea Probability Sampling</p> <p>Every member of the population has a known, non-zero chance of being selected.</p> <p>\u2705 Best for reducing bias and making strong statistical conclusions.</p> <p>Common Techniques:</p> Method Description Simple Random Every member has equal chance of selection Stratified Sampling Population divided into subgroups (strata); sample from each Systematic Sampling Select every k-th item after a random start Cluster Sampling Divide into groups (clusters), then randomly select entire groups <p>2. \ud83e\uddfe Non-Probability Sampling</p> <p>Not every member has a known or equal chance of selection.</p> <p>\u2705 Easier and faster, but more prone to bias.</p> <p>Common Techniques:</p> Method Description Convenience Sampling Choose whoever is easiest to reach Judgmental Sampling Use expert judgment to pick samples Quota Sampling Select samples to meet quotas for specific traits Snowball Sampling Existing subjects recruit future subjects (common in social research) <p>\ud83d\uddbc\ufe0f Sampling Methods Flowchart:</p> <p>Sampling Method \u251c\u2500\u2500 Probability Sampling \u2502   \u251c\u2500\u2500 Simple Random \u2502   \u251c\u2500\u2500 Stratified \u2502   \u251c\u2500\u2500 Systematic \u2502   \u2514\u2500\u2500 Cluster \u2514\u2500\u2500 Non-Probability Sampling     \u251c\u2500\u2500 Convenience     \u251c\u2500\u2500 Judgmental     \u251c\u2500\u2500 Quota     \u2514\u2500\u2500 Snowball</p> <p>\ud83d\udccc Example:</p> <p>Goal: Survey students about exam stress.</p> Sampling Method Example Simple Random Sampling Randomly select 100 students from the entire list. Stratified Sampling Select 50 males and 50 females randomly from each group. Convenience Sampling Ask students who are near the library. Snowball Sampling Ask one student to refer other friends."},{"location":"Statistic/InferentialStatistics/Probability-Theory/ProbabilityDistributions/","title":"Probability Distributions","text":"\u2705 Probability Distribution <p>A probability distribution describes how the probabilities are distributed over the values of a random variable.</p> <ul> <li> <p>For discrete variables, it assigns probabilities to individual values.</p> </li> <li> <p>For continuous variables, it describes a probability density over a range of values.</p> </li> </ul> <p>\ud83d\udd00 Classification of Probability Distributions</p> Type Description Example Discrete Finite/countable outcomes Binomial, Poisson Continuous Infinite/uncountable outcomes Normal, Exponential <p>\ud83c\udfb2 Discrete Probability Distributions</p> <p>1. Bernoulli Distribution</p> <ul> <li> <p>Use: Single trial with 2 outcomes (success/failure)</p> </li> <li> <p>PMF: </p> </li> </ul> <pre><code>from scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\np = 0.6\nx = [0, 1]\npmf = bernoulli.pmf(x, p)\n\nplt.bar(x, pmf)\nplt.title('Bernoulli Distribution (p=0.6)')\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.show()\n</code></pre> <p></p> <p>2. Binomial Distribution</p> <ul> <li> <p>Use: Number of successes in n independent Bernoulli trials</p> </li> <li> <p>PMF: </p> </li> </ul> <pre><code>from scipy.stats import binom\n\nn, p = 10, 0.5\nx = range(n+1)\npmf = binom.pmf(x, n, p)\n\nplt.bar(x, pmf)\nplt.title(\"Binomial Distribution (n=10, p=0.5)\")\nplt.xlabel(\"Successes\")\nplt.ylabel(\"Probability\")\nplt.show()\n</code></pre> <p></p> <p>3. Poisson Distribution</p> <ul> <li> <p>Use: Events occurring in a fixed interval</p> </li> <li> <p>PMF: </p> </li> </ul> <pre><code>from scipy.stats import poisson\n\nmu = 4\nx = range(0, 15)\npmf = poisson.pmf(x, mu)\n\nplt.bar(x, pmf)\nplt.title(\"Poisson Distribution (\u03bb=4)\")\nplt.xlabel(\"Number of Events\")\nplt.ylabel(\"Probability\")\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udcc8 Continuous Probability Distributions</p> <p>1. Uniform Distribution</p> <ul> <li> <p>Use: All outcomes equally likely</p> </li> <li> <p>PDF: </p> </li> </ul> <pre><code>from scipy.stats import uniform\nimport numpy as np\n\na, b = 0, 1\nx = np.linspace(a, b, 100)\npdf = uniform.pdf(x, a, b-a)\n\nplt.plot(x, pdf)\nplt.title(\"Uniform Distribution [0,1]\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.show()\n</code></pre> <p></p> <p>2. Normal (Gaussian) Distribution</p> <ul> <li>Use: Natural phenomena, bell-shaped curve</li> </ul> <p>PDF: </p> <pre><code>from scipy.stats import norm\n\nmu, sigma = 0, 1\nx = np.linspace(-4, 4, 100)\npdf = norm.pdf(x, mu, sigma)\n\nplt.plot(x, pdf)\nplt.title(\"Normal Distribution (\u03bc=0, \u03c3=1)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.grid()\nplt.show()\n</code></pre> <p></p> <p>3. Exponential Distribution</p> <p>Use: Time between Poisson events</p> <p>PDF: </p> <pre><code>from scipy.stats import expon\n\nlam = 1\nx = np.linspace(0, 10, 100)\npdf = expon.pdf(x, scale=1/lam)\n\nplt.plot(x, pdf)\nplt.title(\"Exponential Distribution (\u03bb=1)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.grid()\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udcda Summary Table</p> Distribution Type Use Case Example Bernoulli Discrete Single coin toss Binomial Discrete Number of heads in multiple tosses Poisson Discrete Number of arrivals in an hour Uniform Continuous Roll of a fair die Normal (Gaussian) Continuous Heights, test scores Exponential Continuous Time to next event"},{"location":"Statistic/InferentialStatistics/Probability-Theory/overview/","title":"Overview","text":"\u2705 Probability Theory <p>Probability Theory is the branch of mathematics that deals with quantifying uncertainty. It provides the foundation for making inferences and predictions about random events.</p> <p>Key Definitions</p> Term Meaning Experiment A process that produces an outcome (e.g., tossing a coin) Sample Space (S) The set of all possible outcomes (e.g., {Heads, Tails}) Event (E) A subset of the sample space (e.g., getting Heads) Probability (P) A measure between 0 and 1 representing how likely an event is <p>\ud83d\udcd8 Types of Probability</p> Type Description Example Theoretical Based on logical reasoning P(Head) = 1/2 Experimental Based on observed data Flip a coin 100 times: Head shows 47 times Subjective Based on personal belief \"I think India has a 70% chance to win\" <p>\ud83e\udde0 Probability Rules</p> <ol> <li> <p>0 \u2264 P(E) \u2264 1</p> </li> <li> <p>P(Sample Space) = 1</p> </li> <li> <p>P(A \u222a B) = P(A) + P(B) - P(A \u2229 B)</p> </li> <li> <p>Complement Rule: P(A\u1d9c) = 1 - P(A)</p> </li> </ol> <p>\ud83d\udcca Types of Events</p> Event Type Description Example Independent Events One event does not affect the other Tossing two coins Dependent Events One event affects the other Drawing cards without replacement Mutually Exclusive Events that cannot happen at the same time Getting Head or Tail in one toss Exhaustive Events All possible outcomes covered Rolling a 6-sided die (1-6) <p>\ud83c\udfb2 Real-World Example in Python</p> <p>Scenario:</p> <p>You flip a fair coin 1000 times. Estimate the probability of getting heads.</p> <pre><code>import numpy as np\n\n# Simulate 1000 coin tosses\nnp.random.seed(42)\ncoin_tosses = np.random.choice(['Heads', 'Tails'], size=1000)\n\n# Count heads\nheads_prob = np.sum(coin_tosses == 'Heads') / 1000\nprint(f\"Estimated Probability of Heads: {heads_prob:.2f}\")\n</code></pre> <p>Estimated Probability of Heads: 0.49</p> <p>\ud83d\udcc8 Visualization: Coin Toss Simulation</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(x=coin_tosses)\nplt.title(\"Coin Toss Outcomes (1000 Trials)\")\nplt.xlabel(\"Outcome\")\nplt.ylabel(\"Frequency\")\nplt.show()\n</code></pre> <p></p> <p>\ud83d\udd01 Probability Distributions (Preview)</p> <p>Probability theory leads into Probability Distributions, such as:</p> <ul> <li> <p>Discrete: Bernoulli, Binomial, Poisson</p> </li> <li> <p>Continuous: Uniform, Normal, Exponential</p> </li> </ul>"},{"location":"Temporal/Temporal-client/","title":"Temporal Client - Python SDK","text":"<p>A <code>Temporal Client</code> enables you to communicate with the Temporal Service. Communication with a Temporal Service lets you perform actions such as starting Workflow Executions, sending Signals and Queries to Workflow Executions, getting Workflow results, and more.</p>"},{"location":"Temporal/Temporal-client/#ref-document","title":"Ref document","text":"<p>https://docs.temporal.io/develop/python/temporal-client</p> <p>https://github.com/temporalio/documentation/tree/main</p>"},{"location":"Temporal/core-application/","title":"Core application - Python SDK","text":""},{"location":"Temporal/core-application/#develop-a-basic-workflow","title":"Develop a basic Workflow","text":"<p>How to develop a basic Workflow using the Temporal Python SDK.</p> <p>Workflows are the fundamental unit of a Temporal Application, and it all starts with the development of a Workflow Definition.</p> <p>In the Temporal Python SDK programming model, <code>Workflows are defined as classes</code>.</p> <p>Specify the <code>@workflow.defn</code> decorator on the Workflow class to identify a Workflow.</p> <p>Use the <code>@workflow.run</code> to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as @workflow.defn. Run methods have positional parameters.</p> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from your_activities_dacx import your_activity\n    from your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nTo spawn an Activity Execution, use the [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) operation from within your Workflow Definition.\n\n`execute_activity()` is a shortcut for [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) that waits on its result.\n\nTo get just the handle to wait and cancel separately, use `start_activity()`.\nIn most cases, use `execute_activity()` unless advanced task capabilities are needed.\n\nA single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of `start_activity()` or `execute_activity()` and must be supplied by the `args` keyword argument.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Workflow name with a custom name in the decorator argument. For example, `@workflow.defn(name=\"your-workflow-name\")`. If the name parameter is not specified, the Workflow name defaults to the function name.\ndacx\"\"\"\n\n\"\"\"dacx\nIn the Temporal Python SDK programming model, Workflows are defined as classes.\n\nSpecify the `@workflow.defn` decorator on the Workflow class to identify a Workflow.\n\nUse the `@workflow.run` to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as `@workflow.defn`. Run methods have positional parameters.\ndacx\"\"\"\n\n\"\"\"dacx\nTo return a value of the Workflow, use `return` to return an object.\n\nTo return the results of a Workflow Execution, use either `start_workflow()` or `execute_workflow()` asynchronous methods.\ndacx\"\"\"\n\n\"\"\"dacx\nUse [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) to start an Activity and return its handle, [`ActivityHandle`](https://python.temporal.io/temporalio.workflow.ActivityHandle.html). Use [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) to return the results.\n\nYou must provide either `schedule_to_close_timeout` or `start_to_close_timeout`.\n\n`execute_activity()` is a shortcut for `await start_activity()`. An asynchronous `execute_activity()` helper is provided which takes the same arguments as `start_activity()` and `await`s on the result. `execute_activity()` should be used in most cases unless advanced task capabilities are needed.\ndacx\"\"\"\n\n\n@workflow.defn(name=\"YourWorkflow\")\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, name: str) -&gt; str:\n        return await workflow.execute_activity(\n            your_activity,\n            YourParams(\"Hello\", name),\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n\n\"\"\" @dacx\nid: how-to-spawn-an-activity-execution-in-python\ntitle: How to spawn an Activity Execution in Python\nlabel: Activity Execution\ndescription: Use the `execute_activity()` operation from within your Workflow Definition.\nlines: 3, 9-18, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-customize-workflow-type-in-python\ntitle: How to customize Workflow types in Python\nlabel: Customize Workflow types\ndescription: Customize Workflow types.\nlines: 3, 20-22, 47-55\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-develop-a-workflow-definition-in-python\ntitle: How to develop a Workflow Definition in Python\nlabel: Develop a Workflow Definition\ndescription: To develop a Workflow Definition, specify the `@workflow.defn` decorator on the Workflow class and use `@workflow.run` to mark the entry point.\nlines: 3, 24-30, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-define-workflow-return-values-in-python\ntitle: How to define Workflow return values\nlabel: Define Workflow return values\ndescription: Define Workflow return values.\ntags:\n - workflow return values\nlines: 3, 32-36, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-get-the-result-of-an-activity-execution-in-python\ntitle: How to get the result of an Activity Execution in Python\nlabel: Get the result of an Activity Execution\ndescription: Get the result of an Activity Execution.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 3, 38-44, 47-55\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#define-workflow-parameters","title":"Define Workflow parameters","text":"<p>How to define Workflow parameters using the Temporal Python SDK.</p> <p>Temporal Workflows may have any number of custom parameters. However, we strongly recommend that objects are used as parameters, so that the object's individual fields may be altered without breaking the signature of the Workflow. All Workflow Definition parameters must be serializable.</p> <p><code>Temporal Workflows can take any number of parameters But you should pass a single object (DTO), not many primitive arguments And everything must be serializable</code></p>"},{"location":"Temporal/core-application/#1-what-are-workflow-parameters","title":"1\ufe0f\u20e3 What are \u201cWorkflow parameters\u201d?","text":"<p>In Temporal, when you start a Workflow, you pass input parameters.</p> <p>Example (Python):</p> <pre><code>@workflow.defn\nclass ScanWorkflow:\n    @workflow.run\n    async def run(self, account_id: str, region: str):\n        ...\n</code></pre> <p>Here:</p> <ul> <li><code>account_id</code></li> <li><code>region</code></li> </ul> <p>are workflow parameters.</p>"},{"location":"Temporal/core-application/#2-what-does-signature-breaking-mean","title":"2\ufe0f\u20e3 What does \u201csignature breaking\u201d mean?","text":"<p>Let\u2019s say you deploy this workflow:</p> <p><code>async def run(self, account_id: str, region: str):</code></p> <p>Now <code>later</code>, you decide you need one more parameter:</p> <p><code>async def run(self, account_id: str, region: str, severity: str):</code></p> <p>\ud83d\udea8 Problem:</p> <ul> <li>Existing running workflows</li> <li>Stored execution history</li> <li>Replays</li> </ul> <p>\ud83d\udc49 Signature changed \u2192 old executions can break</p> <p>This is what <code>\u201cbreaking the signature\u201d</code> means.</p>"},{"location":"Temporal/core-application/#3-why-passing-many-parameters-is-risky","title":"3\ufe0f\u20e3 Why passing many parameters is risky \u274c","text":"<p>Bad practice:</p> <pre><code>async def run(\n    self,\n    account_id: str,\n    region: str,\n    severity: str,\n    notify: bool,\n    approval_required: bool,\n):\n</code></pre> <p>If you:</p> <ul> <li>Add</li> <li>Remove</li> <li>Reorder</li> <li>parameters</li> </ul> <p>You risk:</p> <ul> <li>Workflow replay failures</li> <li>Versioning headaches</li> <li>Backward incompatibility</li> </ul>"},{"location":"Temporal/core-application/#4-recommended-approach-use-an-object-dto","title":"4\ufe0f\u20e3 Recommended approach: use an object (DTO) \u2705","text":"<p>Instead, use one object parameter.</p> <p>Example (Python dataclass)</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ScanInput:\n    account_id: str\n    region: str\n    severity: str | None = None\n    notify: bool = False\n    approval_required: bool = True\n</code></pre> <p>Workflow:</p> <pre><code>@workflow.defn\nclass ScanWorkflow:\n    @workflow.run\n    async def run(self, input: ScanInput):\n        ...\n</code></pre>"},{"location":"Temporal/core-application/#5-why-this-is-better-key-reason","title":"5\ufe0f\u20e3 Why this is better (key reason)","text":"<p>Now, in the future, you can:</p> <pre><code>@dataclass\nclass ScanInput:\n    account_id: str\n    region: str\n    severity: str | None = None\n    notify: bool = False\n    approval_required: bool = True\n    remediation_strategy: str | None = None  # NEW FIELD\n</code></pre> <ul> <li>\u2705 Existing workflows still work</li> <li>\u2705 No signature change</li> <li>\u2705 Safe evolution</li> <li>\u2705 Backward compatibility</li> </ul>"},{"location":"Temporal/core-application/#6-what-does-all-workflow-parameters-must-be-serializable-mean","title":"6\ufe0f\u20e3 What does \u201cAll Workflow parameters must be serializable\u201d mean?","text":"<p>Temporal:</p> <ul> <li>Stores workflow inputs</li> <li>Replays workflows</li> <li>Persists state</li> </ul> <p>So parameters must be:</p> <ul> <li>JSON-serializable</li> <li>Or Protobuf-serializable</li> </ul> <p>\u2705 Good types</p> <ul> <li><code>str</code></li> <li><code>int</code></li> <li><code>float</code></li> <li><code>bool</code></li> <li><code>dict</code></li> <li><code>list</code></li> <li><code>dataclass</code></li> <li><code>Pydantic model</code></li> </ul> <p>\u274c Bad (NOT serializable)</p> <ul> <li>Open file handles</li> <li>Database connections</li> <li>Thread locks</li> <li>Sockets</li> <li>Lambdas / functions</li> <li>Class instances with live state</li> </ul>"},{"location":"Temporal/core-application/#8-why-temporal-is-strict-about-this","title":"8\ufe0f\u20e3 Why Temporal is strict about this","text":"<p>Temporal guarantees:</p> <ul> <li>Deterministic execution</li> <li>Crash recovery</li> <li>Replay correctness</li> </ul> <p>That\u2019s why:</p> <ul> <li>Inputs must be immutable data</li> <li>No external side effects in Workflow code</li> <li>No non-serializable objects</li> </ul>"},{"location":"Temporal/core-application/#11-best-practice-checklist","title":"1\ufe0f\u20e31\ufe0f\u20e3 Best practice checklist","text":"<p>\u2714 One input object (DTO) \u2714 Default values for new fields \u2714 Only serializable data \u2714 No live connections \u2714 Activities do the real work \u2714 Workflow = orchestration only</p>"},{"location":"Temporal/core-application/#examples-of-non-serializable-objects-do-not-use","title":"\u274c Examples of NON-serializable objects (DO NOT USE)","text":"<p>1\ufe0f\u20e3 Database connections</p> <pre><code>async def run(self, db_conn):\n    ...\n</code></pre> <p>Why \u274c</p> <ul> <li>Live network connection</li> <li>Cannot be serialized</li> <li>Breaks replay</li> </ul> <p>2\ufe0f\u20e3 Cloud SDK clients (AWS, Azure, GCP)</p> <pre><code>import boto3\n\nclient = boto3.client(\"ec2\")\n\nasync def run(self, ec2_client):\n    ...\n</code></pre> <p>Why \u274c</p> <ul> <li>Holds sockets, credentials, state</li> <li>Non-deterministic</li> </ul> <p>3\ufe0f\u20e3 Open file handles</p> <pre><code>file = open(\"data.txt\")\n\nasync def run(self, f):\n    ...\n</code></pre> <p>Why \u274c</p> <ul> <li>OS resource</li> <li>Cannot be restored on replay</li> </ul> <p>4\ufe0f\u20e3 Threads, locks, executors</p> <pre><code>from threading import Lock\n\nlock = Lock()\n\nasync def run(self, lock):\n    ...\n</code></pre> <p>Why \u274c</p> <ul> <li>Runtime-only objects</li> <li>No serialization format</li> </ul> <p>5\ufe0f\u20e3 Functions / lambdas</p> <pre><code>async def run(self, callback):\n    callback()\n</code></pre> <p>Why \u274c</p> <ul> <li>Code references cannot be serialized</li> <li>Replay unsafe</li> </ul> <p>6\ufe0f\u20e3 Sockets / HTTP sessions</p> <pre><code>import requests\n\nsession = requests.Session()\n\nasync def run(self, session):\n    ...\n</code></pre> <p>Why \u274c</p> <ul> <li>Connection state</li> <li>Side effects</li> </ul> <p>7\ufe0f\u20e3 ORM sessions (SQLAlchemy)</p> <pre><code>async def run(self, session):\n    session.query(...)\n</code></pre> <p>Why \u274c</p> <ul> <li>Connection + transaction state</li> <li>Replay impossible</li> </ul>"},{"location":"Temporal/core-application/#serializable-objects-safe","title":"\u2705 Serializable objects (SAFE)","text":"<p>1\ufe0f\u20e3 Primitives</p> <pre><code>async def run(self, account_id: str, region: str):\n    ...\n</code></pre> <p>2\ufe0f\u20e3 Dict / List</p> <pre><code>async def run(self, config: dict):\n    ...\n</code></pre> <p>3\ufe0f\u20e3 Dataclasses (BEST PRACTICE)</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ScanInput:\n    account_id: str\n    region: str\n    severity: str | None = None\n</code></pre> <pre><code>async def run(self, input: ScanInput):\n    ...\n</code></pre> <p>4\ufe0f\u20e3 Pydantic models</p> <pre><code>from pydantic import BaseModel\n\nclass ScanInput(BaseModel):\n    account_id: str\n    region: str\n</code></pre> <p>5\ufe0f\u20e3 Enums</p> <pre><code>from enum import Enum\n\nclass ScanType(str, Enum):\n    FULL = \"full\"\n    QUICK = \"quick\"\n</code></pre>"},{"location":"Temporal/core-application/#wrong-logic-side-effects-inside-workflow","title":"\u274c WRONG (logic + side effects inside Workflow)","text":"<pre><code>@workflow.run\nasync def run(self, input):\n    client = boto3.client(\"ec2\")\n    client.describe_instances()\n</code></pre>"},{"location":"Temporal/core-application/#right-workflow-orchestrates-activity-executes","title":"\u2705 RIGHT (Workflow orchestrates, Activity executes)","text":"<pre><code>@workflow.run\nasync def run(self, input: ScanInput):\n    await workflow.execute_activity(\n        scan_ec2_activity,\n        input.account_id,\n        input.region,\n        start_to_close_timeout=timedelta(minutes=5),\n    )\n</code></pre> <pre><code>@activity.defn\nasync def scan_ec2_activity(account_id: str, region: str):\n    client = boto3.client(\"ec2\")\n    return client.describe_instances()\n</code></pre> <p>Workflow parameters are the method parameters of the singular method decorated with <code>@workflow.run</code>. These can be any data type Temporal can convert, including <code>dataclasses</code> when properly type-annotated. Technically this can be multiple parameters, but Temporal strongly encourages a single <code>dataclass</code> parameter containing all input fields.</p>"},{"location":"Temporal/core-application/#define-workflow-return-parameters","title":"Define Workflow return parameters","text":"<p>How to define Workflow return parameters using the Temporal Python SDK.</p> <p>Workflow return values must also be serializable. Returning results, returning errors, or throwing exceptions is fairly idiomatic in each language that is supported. However, Temporal APIs that must be used to get the result of a Workflow Execution will only ever receive one of either the result or the error.</p> <p>To return a value of the Workflow, use <code>return</code> to return an object.</p> <p>To return the results of a Workflow Execution, use either <code>start_workflow()</code> or <code>execute_workflow()</code> asynchronous methods.</p> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from your_activities_dacx import your_activity\n    from your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nTo spawn an Activity Execution, use the [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) operation from within your Workflow Definition.\n\n`execute_activity()` is a shortcut for [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) that waits on its result.\n\nTo get just the handle to wait and cancel separately, use `start_activity()`.\nIn most cases, use `execute_activity()` unless advanced task capabilities are needed.\n\nA single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of `start_activity()` or `execute_activity()` and must be supplied by the `args` keyword argument.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Workflow name with a custom name in the decorator argument. For example, `@workflow.defn(name=\"your-workflow-name\")`. If the name parameter is not specified, the Workflow name defaults to the function name.\ndacx\"\"\"\n\n\"\"\"dacx\nIn the Temporal Python SDK programming model, Workflows are defined as classes.\n\nSpecify the `@workflow.defn` decorator on the Workflow class to identify a Workflow.\n\nUse the `@workflow.run` to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as `@workflow.defn`. Run methods have positional parameters.\ndacx\"\"\"\n\n\"\"\"dacx\nTo return a value of the Workflow, use `return` to return an object.\n\nTo return the results of a Workflow Execution, use either `start_workflow()` or `execute_workflow()` asynchronous methods.\ndacx\"\"\"\n\n\"\"\"dacx\nUse [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) to start an Activity and return its handle, [`ActivityHandle`](https://python.temporal.io/temporalio.workflow.ActivityHandle.html). Use [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) to return the results.\n\nYou must provide either `schedule_to_close_timeout` or `start_to_close_timeout`.\n\n`execute_activity()` is a shortcut for `await start_activity()`. An asynchronous `execute_activity()` helper is provided which takes the same arguments as `start_activity()` and `await`s on the result. `execute_activity()` should be used in most cases unless advanced task capabilities are needed.\ndacx\"\"\"\n\n\n@workflow.defn(name=\"YourWorkflow\")\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, name: str) -&gt; str:\n        return await workflow.execute_activity(\n            your_activity,\n            YourParams(\"Hello\", name),\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n\n\"\"\" @dacx\nid: how-to-spawn-an-activity-execution-in-python\ntitle: How to spawn an Activity Execution in Python\nlabel: Activity Execution\ndescription: Use the `execute_activity()` operation from within your Workflow Definition.\nlines: 3, 9-18, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-customize-workflow-type-in-python\ntitle: How to customize Workflow types in Python\nlabel: Customize Workflow types\ndescription: Customize Workflow types.\nlines: 3, 20-22, 47-55\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-develop-a-workflow-definition-in-python\ntitle: How to develop a Workflow Definition in Python\nlabel: Develop a Workflow Definition\ndescription: To develop a Workflow Definition, specify the `@workflow.defn` decorator on the Workflow class and use `@workflow.run` to mark the entry point.\nlines: 3, 24-30, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-define-workflow-return-values-in-python\ntitle: How to define Workflow return values\nlabel: Define Workflow return values\ndescription: Define Workflow return values.\ntags:\n - workflow return values\nlines: 3, 32-36, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-get-the-result-of-an-activity-execution-in-python\ntitle: How to get the result of an Activity Execution in Python\nlabel: Get the result of an Activity Execution\ndescription: Get the result of an Activity Execution.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 3, 38-44, 47-55\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#customize-your-workflow-type","title":"Customize your Workflow Type","text":"<p>How to customize your Workflow Type using the Temporal Python SDK.</p> <p>Workflows have a Type that are referred to as the Workflow name.</p> <p>The following examples demonstrate how to set a custom name for your Workflow Type.</p> <p>You can customize the Workflow name with a custom name in the decorator argument. For example, <code>@workflow.defn(name=\"your-workflow-name\")</code>. If the name parameter is not specified, the Workflow name defaults to the unqualified class name.</p> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from your_activities_dacx import your_activity\n    from your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nTo spawn an Activity Execution, use the [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) operation from within your Workflow Definition.\n\n`execute_activity()` is a shortcut for [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) that waits on its result.\n\nTo get just the handle to wait and cancel separately, use `start_activity()`.\nIn most cases, use `execute_activity()` unless advanced task capabilities are needed.\n\nA single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of `start_activity()` or `execute_activity()` and must be supplied by the `args` keyword argument.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Workflow name with a custom name in the decorator argument. For example, `@workflow.defn(name=\"your-workflow-name\")`. If the name parameter is not specified, the Workflow name defaults to the function name.\ndacx\"\"\"\n\n\"\"\"dacx\nIn the Temporal Python SDK programming model, Workflows are defined as classes.\n\nSpecify the `@workflow.defn` decorator on the Workflow class to identify a Workflow.\n\nUse the `@workflow.run` to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as `@workflow.defn`. Run methods have positional parameters.\ndacx\"\"\"\n\n\"\"\"dacx\nTo return a value of the Workflow, use `return` to return an object.\n\nTo return the results of a Workflow Execution, use either `start_workflow()` or `execute_workflow()` asynchronous methods.\ndacx\"\"\"\n\n\"\"\"dacx\nUse [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) to start an Activity and return its handle, [`ActivityHandle`](https://python.temporal.io/temporalio.workflow.ActivityHandle.html). Use [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) to return the results.\n\nYou must provide either `schedule_to_close_timeout` or `start_to_close_timeout`.\n\n`execute_activity()` is a shortcut for `await start_activity()`. An asynchronous `execute_activity()` helper is provided which takes the same arguments as `start_activity()` and `await`s on the result. `execute_activity()` should be used in most cases unless advanced task capabilities are needed.\ndacx\"\"\"\n\n\n@workflow.defn(name=\"YourWorkflow\")\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, name: str) -&gt; str:\n        return await workflow.execute_activity(\n            your_activity,\n            YourParams(\"Hello\", name),\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n\n\"\"\" @dacx\nid: how-to-spawn-an-activity-execution-in-python\ntitle: How to spawn an Activity Execution in Python\nlabel: Activity Execution\ndescription: Use the `execute_activity()` operation from within your Workflow Definition.\nlines: 3, 9-18, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-customize-workflow-type-in-python\ntitle: How to customize Workflow types in Python\nlabel: Customize Workflow types\ndescription: Customize Workflow types.\nlines: 3, 20-22, 47-55\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-develop-a-workflow-definition-in-python\ntitle: How to develop a Workflow Definition in Python\nlabel: Develop a Workflow Definition\ndescription: To develop a Workflow Definition, specify the `@workflow.defn` decorator on the Workflow class and use `@workflow.run` to mark the entry point.\nlines: 3, 24-30, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-define-workflow-return-values-in-python\ntitle: How to define Workflow return values\nlabel: Define Workflow return values\ndescription: Define Workflow return values.\ntags:\n - workflow return values\nlines: 3, 32-36, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-get-the-result-of-an-activity-execution-in-python\ntitle: How to get the result of an Activity Execution in Python\nlabel: Get the result of an Activity Execution\ndescription: Get the result of an Activity Execution.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 3, 38-44, 47-55\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#develop-workflow-logic","title":"Develop Workflow logic","text":"<p>How to develop Workflow logic using the Temporal Python SDK.</p> <p>Workflow logic is constrained by deterministic execution requirements. Therefore, each language is limited to the use of certain idiomatic techniques. However, each Temporal SDK provides a set of APIs that can be used inside your Workflow to interact with external (to the Workflow) application code.</p> <p>Workflow code must be deterministic. This means:</p> <ul> <li>no threading</li> <li>no randomness</li> <li>no external calls to processes</li> <li>no network I/O</li> <li>no global state mutation</li> <li>no system date or time</li> </ul> <p>All API safe for Workflows used in the <code>temporalio.workflow</code> must run in the implicit <code>asyncio event loop</code> and be deterministic.</p>"},{"location":"Temporal/core-application/#develop-a-basic-activity","title":"Develop a basic Activity","text":"<p>How to develop a basic Activity using the Temporal Python SDK.</p> <p>One of the primary things that Workflows do is orchestrate the execution of Activities. An Activity is a normal function or method execution that's intended to execute a single, well-defined action (either short or long-running), such as querying a database, calling a third-party API, or transcoding a media file. An Activity can interact with world outside the Temporal Platform or use a Temporal Client to interact with a Temporal Service. For the Workflow to be able to execute the Activity, we must define the <code>Activity Definition</code>.</p> <p>You can develop an Activity Definition by using the <code>@activity.defn decorator</code>. Register the function as an Activity with a custom name through a decorator argument, for example <code>@activity.defn(name=\"your_activity\")</code>.</p> <p>The Temporal Python SDK supports multiple ways of implementing an Activity:</p> <ul> <li>Asynchronously using <code>asyncio</code></li> <li>Synchronously multithreaded using <code>concurrent.futures.ThreadPoolExecutor</code></li> <li>Synchronously multiprocess using <code>concurrent.futures.ProcessPoolExecutor and multiprocessing.managers.SyncManager</code></li> </ul> <pre><code>from temporalio import activity\nfrom your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nYou can develop an Activity Definition by using the `@activity.defn` decorator.\nRegister the function as an Activity with a custom name through a decorator argument, for example `@activity.defn(name=\"your_activity\")`.\n\n:::note\n\nThe Temporal Python SDK supports multiple ways of implementing an Activity:\n- Asynchronously using [`asyncio`](https://docs.python.org/3/library/asyncio.html)\n- Synchronously multithreaded using [`concurrent.futures.ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)\n- Synchronously multiprocess using [`concurrent.futures.ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) and [`multiprocessing.managers.SyncManager`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.SyncManager)\n\nBlocking the async event loop in Python would turn your asynchronous program into a synchronous program that executes serially, defeating the entire purpose of using `asyncio`.\nThis can also lead to potential deadlock, and unpredictable behavior that causes tasks to be unable to execute.\nDebugging these issues can be difficult and time consuming, as locating the source of the blocking call might not always be immediately obvious.\n\nDue to this, consider not make blocking calls from within an asynchronous Activity, or use an async safe library to perform\nthese actions.\nIf you must use a blocking library, consider using a synchronous Activity instead.\n\n:::\ndacx\"\"\"\n\n\"\"\"dacx\nActivity parameters are the function parameters of the function decorated with `@activity.defn`.\nThese can be any data type Temporal can convert, including dataclasses when properly type-annotated.\nTechnically this can be multiple parameters, but Temporal strongly encourages a single dataclass parameter containing all input fields.\ndacx\"\"\"\n\n\"\"\"dacx\nAn Activity Execution can return inputs and other Activity values.\n\nThe following example defines an Activity that takes a string as input and returns a string.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Activity name with a custom name in the decorator argument. For example, `@activity.defn(name=\"your-activity\")`.\nIf the name parameter is not specified, the Activity name defaults to the function name.\ndacx\"\"\"\n\n\n@activity.defn(name=\"your_activity\")\nasync def your_activity(input: YourParams) -&gt; str:\n    return f\"{input.greeting}, {input.name}!\"\n\n\n\"\"\" @dacx\nid: how-to-develop-an-activity-definition-in-python\ntitle: How to develop an Activity Definition in Python\nlabel: Activity Definition\ndescription: You can develop an Activity Definition by using the `@activity.defn` decorator.\ntags:\n - python sdk\n - code sample\n - activity definition\nlines: 1, 4-24, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-parameters-in-python\ntitle: How to do define Activity parameters in Python\nlabel: Activity parameters\ndescription: Activity parameters are the function parameters of the function decorated with `@activity.defn`.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 1-3, 26-30, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-return-values-in-python\ntitle: How to define Activity return values in Python\nlabel: Activity return values\ndescription: To return a value of the Workflow, use `return` to return an object.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 32-36, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-customize-activity-type-in-python\ntitle: How to customize Activity Type in Python\nlabel: Customize Activity Type\ndescription: Customize your Activity Type.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 38-41, 44-46\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#develop-activity-parameters","title":"Develop Activity Parameters","text":"<p>How to develop Activity Parameters using the Temporal Python SDK.</p> <p>There is no explicit limit to the total number of parameters that an Activity Definition may support. However, there is a limit to the total size of the data that ends up encoded into a gRPC message Payload.</p> <p>A single argument is limited to a maximum size of 2 MB. And the total size of a gRPC message, which includes all the arguments, is limited to a maximum of 4 MB.</p> <p>Also, keep in mind that all Payload data is recorded in the <code>Workflow Execution Event History</code> and large Event Histories can affect Worker performance. This is because the entire Event History could be transferred to a Worker Process with a <code>Workflow Task</code>.</p> <p>Some SDKs require that you pass context objects, others do not. When it comes to your application data\u2014that is, data that is serialized and encoded into a Payload\u2014we recommend that you use a single object as an argument that wraps the application data passed to Activities. This is so that you can change what data is passed to the Activity without breaking a function or method signature.</p> <p>Activity parameters are the function parameters of the function decorated with `@activity.defn```. These can be any data type Temporal can convert, including dataclasses when properly type-annotated. Technically this can be multiple parameters, but Temporal strongly encourages a single dataclass parameter containing all input fields.</p> <pre><code>from temporalio import activity\nfrom your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nYou can develop an Activity Definition by using the `@activity.defn` decorator.\nRegister the function as an Activity with a custom name through a decorator argument, for example `@activity.defn(name=\"your_activity\")`.\n\n:::note\n\nThe Temporal Python SDK supports multiple ways of implementing an Activity:\n- Asynchronously using [`asyncio`](https://docs.python.org/3/library/asyncio.html)\n- Synchronously multithreaded using [`concurrent.futures.ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)\n- Synchronously multiprocess using [`concurrent.futures.ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) and [`multiprocessing.managers.SyncManager`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.SyncManager)\n\nBlocking the async event loop in Python would turn your asynchronous program into a synchronous program that executes serially, defeating the entire purpose of using `asyncio`.\nThis can also lead to potential deadlock, and unpredictable behavior that causes tasks to be unable to execute.\nDebugging these issues can be difficult and time consuming, as locating the source of the blocking call might not always be immediately obvious.\n\nDue to this, consider not make blocking calls from within an asynchronous Activity, or use an async safe library to perform\nthese actions.\nIf you must use a blocking library, consider using a synchronous Activity instead.\n\n:::\ndacx\"\"\"\n\n\"\"\"dacx\nActivity parameters are the function parameters of the function decorated with `@activity.defn`.\nThese can be any data type Temporal can convert, including dataclasses when properly type-annotated.\nTechnically this can be multiple parameters, but Temporal strongly encourages a single dataclass parameter containing all input fields.\ndacx\"\"\"\n\n\"\"\"dacx\nAn Activity Execution can return inputs and other Activity values.\n\nThe following example defines an Activity that takes a string as input and returns a string.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Activity name with a custom name in the decorator argument. For example, `@activity.defn(name=\"your-activity\")`.\nIf the name parameter is not specified, the Activity name defaults to the function name.\ndacx\"\"\"\n\n\n@activity.defn(name=\"your_activity\")\nasync def your_activity(input: YourParams) -&gt; str:\n    return f\"{input.greeting}, {input.name}!\"\n\n\n\"\"\" @dacx\nid: how-to-develop-an-activity-definition-in-python\ntitle: How to develop an Activity Definition in Python\nlabel: Activity Definition\ndescription: You can develop an Activity Definition by using the `@activity.defn` decorator.\ntags:\n - python sdk\n - code sample\n - activity definition\nlines: 1, 4-24, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-parameters-in-python\ntitle: How to do define Activity parameters in Python\nlabel: Activity parameters\ndescription: Activity parameters are the function parameters of the function decorated with `@activity.defn`.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 1-3, 26-30, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-return-values-in-python\ntitle: How to define Activity return values in Python\nlabel: Activity return values\ndescription: To return a value of the Workflow, use `return` to return an object.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 32-36, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-customize-activity-type-in-python\ntitle: How to customize Activity Type in Python\nlabel: Customize Activity Type\ndescription: Customize your Activity Type.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 38-41, 44-46\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#define-activity-return-values","title":"Define Activity return values","text":"<p>How to define Activity return values using the Temporal Python SDK.</p> <p>All data returned from an Activity must be serializable.</p> <p>Activity return values are subject to payload size limits in Temporal. The default payload size limit is 2MB, and there is a hard limit of 4MB for any gRPC message size in the Event History transaction (see Cloud limits here). Keep in mind that all return values are recorded in a Workflow Execution Event History.</p> <p>An Activity Execution can return inputs and other Activity values.</p> <p>The following example defines an Activity that takes a string as input and returns a string.</p> <pre><code>from temporalio import activity\nfrom your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nYou can develop an Activity Definition by using the `@activity.defn` decorator.\nRegister the function as an Activity with a custom name through a decorator argument, for example `@activity.defn(name=\"your_activity\")`.\n\n:::note\n\nThe Temporal Python SDK supports multiple ways of implementing an Activity:\n- Asynchronously using [`asyncio`](https://docs.python.org/3/library/asyncio.html)\n- Synchronously multithreaded using [`concurrent.futures.ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)\n- Synchronously multiprocess using [`concurrent.futures.ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) and [`multiprocessing.managers.SyncManager`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.SyncManager)\n\nBlocking the async event loop in Python would turn your asynchronous program into a synchronous program that executes serially, defeating the entire purpose of using `asyncio`.\nThis can also lead to potential deadlock, and unpredictable behavior that causes tasks to be unable to execute.\nDebugging these issues can be difficult and time consuming, as locating the source of the blocking call might not always be immediately obvious.\n\nDue to this, consider not make blocking calls from within an asynchronous Activity, or use an async safe library to perform\nthese actions.\nIf you must use a blocking library, consider using a synchronous Activity instead.\n\n:::\ndacx\"\"\"\n\n\"\"\"dacx\nActivity parameters are the function parameters of the function decorated with `@activity.defn`.\nThese can be any data type Temporal can convert, including dataclasses when properly type-annotated.\nTechnically this can be multiple parameters, but Temporal strongly encourages a single dataclass parameter containing all input fields.\ndacx\"\"\"\n\n\"\"\"dacx\nAn Activity Execution can return inputs and other Activity values.\n\nThe following example defines an Activity that takes a string as input and returns a string.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Activity name with a custom name in the decorator argument. For example, `@activity.defn(name=\"your-activity\")`.\nIf the name parameter is not specified, the Activity name defaults to the function name.\ndacx\"\"\"\n\n\n@activity.defn(name=\"your_activity\")\nasync def your_activity(input: YourParams) -&gt; str:\n    return f\"{input.greeting}, {input.name}!\"\n\n\n\"\"\" @dacx\nid: how-to-develop-an-activity-definition-in-python\ntitle: How to develop an Activity Definition in Python\nlabel: Activity Definition\ndescription: You can develop an Activity Definition by using the `@activity.defn` decorator.\ntags:\n - python sdk\n - code sample\n - activity definition\nlines: 1, 4-24, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-parameters-in-python\ntitle: How to do define Activity parameters in Python\nlabel: Activity parameters\ndescription: Activity parameters are the function parameters of the function decorated with `@activity.defn`.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 1-3, 26-30, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-return-values-in-python\ntitle: How to define Activity return values in Python\nlabel: Activity return values\ndescription: To return a value of the Workflow, use `return` to return an object.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 32-36, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-customize-activity-type-in-python\ntitle: How to customize Activity Type in Python\nlabel: Customize Activity Type\ndescription: Customize your Activity Type.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 38-41, 44-46\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#customize-your-activity-type","title":"Customize your Activity Type","text":"<p>How to customize your Activity Type</p> <p>Activities have a Type that are referred to as the Activity name. The following examples demonstrate how to set a custom name for your Activity Type.</p> <p>You can customize the Activity name with a custom name in the decorator argument. For example, <code>@activity.defn(name=\"your-activity\")</code>. If the name parameter is not specified, the Activity name defaults to the function name.</p> <pre><code>from temporalio import activity\nfrom your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nYou can develop an Activity Definition by using the `@activity.defn` decorator.\nRegister the function as an Activity with a custom name through a decorator argument, for example `@activity.defn(name=\"your_activity\")`.\n\n:::note\n\nThe Temporal Python SDK supports multiple ways of implementing an Activity:\n- Asynchronously using [`asyncio`](https://docs.python.org/3/library/asyncio.html)\n- Synchronously multithreaded using [`concurrent.futures.ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)\n- Synchronously multiprocess using [`concurrent.futures.ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) and [`multiprocessing.managers.SyncManager`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.SyncManager)\n\nBlocking the async event loop in Python would turn your asynchronous program into a synchronous program that executes serially, defeating the entire purpose of using `asyncio`.\nThis can also lead to potential deadlock, and unpredictable behavior that causes tasks to be unable to execute.\nDebugging these issues can be difficult and time consuming, as locating the source of the blocking call might not always be immediately obvious.\n\nDue to this, consider not make blocking calls from within an asynchronous Activity, or use an async safe library to perform\nthese actions.\nIf you must use a blocking library, consider using a synchronous Activity instead.\n\n:::\ndacx\"\"\"\n\n\"\"\"dacx\nActivity parameters are the function parameters of the function decorated with `@activity.defn`.\nThese can be any data type Temporal can convert, including dataclasses when properly type-annotated.\nTechnically this can be multiple parameters, but Temporal strongly encourages a single dataclass parameter containing all input fields.\ndacx\"\"\"\n\n\"\"\"dacx\nAn Activity Execution can return inputs and other Activity values.\n\nThe following example defines an Activity that takes a string as input and returns a string.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Activity name with a custom name in the decorator argument. For example, `@activity.defn(name=\"your-activity\")`.\nIf the name parameter is not specified, the Activity name defaults to the function name.\ndacx\"\"\"\n\n\n@activity.defn(name=\"your_activity\")\nasync def your_activity(input: YourParams) -&gt; str:\n    return f\"{input.greeting}, {input.name}!\"\n\n\n\"\"\" @dacx\nid: how-to-develop-an-activity-definition-in-python\ntitle: How to develop an Activity Definition in Python\nlabel: Activity Definition\ndescription: You can develop an Activity Definition by using the `@activity.defn` decorator.\ntags:\n - python sdk\n - code sample\n - activity definition\nlines: 1, 4-24, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-parameters-in-python\ntitle: How to do define Activity parameters in Python\nlabel: Activity parameters\ndescription: Activity parameters are the function parameters of the function decorated with `@activity.defn`.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 1-3, 26-30, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-define-activity-return-values-in-python\ntitle: How to define Activity return values in Python\nlabel: Activity return values\ndescription: To return a value of the Workflow, use `return` to return an object.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 32-36, 44-46\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-customize-activity-type-in-python\ntitle: How to customize Activity Type in Python\nlabel: Customize Activity Type\ndescription: Customize your Activity Type.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 38-41, 44-46\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#start-an-activity-execution","title":"Start an Activity Execution","text":"<p>How to start an Activity Execution using the Temporal Python SDK.</p> <p>Calls to spawn <code>Activity Executions</code> are written within a <code>Workflow Definition</code>. The call to spawn an Activity Execution generates the <code>ScheduleActivityTask</code> Command. This results in the set of three <code>Activity Task</code> related Events (<code>ActivityTaskScheduled</code>, <code>ActivityTaskStarted</code>, and <code>ActivityTask[Closed]</code>)in your Workflow Execution Event History.</p> <p>To spawn an Activity Execution, use the <code>execute_activity()</code> operation from within your Workflow Definition.</p> <p><code>execute_activity()</code> is a shortcut for <code>start_activity()</code> that waits on its result.</p> <p>To get just the handle to wait and cancel separately, use <code>start_activity()</code>. In most cases, use <code>execute_activity()</code> unless advanced task capabilities are needed.</p> <p>A single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of <code>start_activity()</code> or <code>execute_activity()</code> and must be supplied by the args keyword argument.</p> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from your_activities_dacx import your_activity\n    from your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nTo spawn an Activity Execution, use the [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) operation from within your Workflow Definition.\n\n`execute_activity()` is a shortcut for [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) that waits on its result.\n\nTo get just the handle to wait and cancel separately, use `start_activity()`.\nIn most cases, use `execute_activity()` unless advanced task capabilities are needed.\n\nA single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of `start_activity()` or `execute_activity()` and must be supplied by the `args` keyword argument.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Workflow name with a custom name in the decorator argument. For example, `@workflow.defn(name=\"your-workflow-name\")`. If the name parameter is not specified, the Workflow name defaults to the function name.\ndacx\"\"\"\n\n\"\"\"dacx\nIn the Temporal Python SDK programming model, Workflows are defined as classes.\n\nSpecify the `@workflow.defn` decorator on the Workflow class to identify a Workflow.\n\nUse the `@workflow.run` to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as `@workflow.defn`. Run methods have positional parameters.\ndacx\"\"\"\n\n\"\"\"dacx\nTo return a value of the Workflow, use `return` to return an object.\n\nTo return the results of a Workflow Execution, use either `start_workflow()` or `execute_workflow()` asynchronous methods.\ndacx\"\"\"\n\n\"\"\"dacx\nUse [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) to start an Activity and return its handle, [`ActivityHandle`](https://python.temporal.io/temporalio.workflow.ActivityHandle.html). Use [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) to return the results.\n\nYou must provide either `schedule_to_close_timeout` or `start_to_close_timeout`.\n\n`execute_activity()` is a shortcut for `await start_activity()`. An asynchronous `execute_activity()` helper is provided which takes the same arguments as `start_activity()` and `await`s on the result. `execute_activity()` should be used in most cases unless advanced task capabilities are needed.\ndacx\"\"\"\n\n\n@workflow.defn(name=\"YourWorkflow\")\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, name: str) -&gt; str:\n        return await workflow.execute_activity(\n            your_activity,\n            YourParams(\"Hello\", name),\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n\n\"\"\" @dacx\nid: how-to-spawn-an-activity-execution-in-python\ntitle: How to spawn an Activity Execution in Python\nlabel: Activity Execution\ndescription: Use the `execute_activity()` operation from within your Workflow Definition.\nlines: 3, 9-18, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-customize-workflow-type-in-python\ntitle: How to customize Workflow types in Python\nlabel: Customize Workflow types\ndescription: Customize Workflow types.\nlines: 3, 20-22, 47-55\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-develop-a-workflow-definition-in-python\ntitle: How to develop a Workflow Definition in Python\nlabel: Develop a Workflow Definition\ndescription: To develop a Workflow Definition, specify the `@workflow.defn` decorator on the Workflow class and use `@workflow.run` to mark the entry point.\nlines: 3, 24-30, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-define-workflow-return-values-in-python\ntitle: How to define Workflow return values\nlabel: Define Workflow return values\ndescription: Define Workflow return values.\ntags:\n - workflow return values\nlines: 3, 32-36, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-get-the-result-of-an-activity-execution-in-python\ntitle: How to get the result of an Activity Execution in Python\nlabel: Get the result of an Activity Execution\ndescription: Get the result of an Activity Execution.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 3, 38-44, 47-55\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#set-the-required-activity-timeouts","title":"Set the required Activity Timeouts","text":"<p>How to set the required Activity Timeouts using the Temporal Python SDK.</p> <p>Activity Execution semantics rely on several parameters. The only required value that needs to be set is either a <code>Schedule-To-Close Timeout</code> or a <code>Start-To-Close Timeout</code>. These values are set in the Activity Options.</p> <p>Activity options are set as keyword arguments after the Activity arguments.</p> <p>Available timeouts are:</p> <ul> <li>schedule_to_close_timeout</li> <li>schedule_to_start_timeout</li> <li>start_to_close_timeout</li> </ul> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\nfrom temporalio.common import RetryPolicy\n\nwith workflow.unsafe.imports_passed_through():\n    from activities import your_activity, YourParams\n\n\"\"\"dacx\nActivity options are set as keyword arguments after the Activity arguments.\n\nAvailable timeouts are:\n\n- schedule_to_close_timeout\n- schedule_to_start_timeout\n- start_to_close_timeout\ndacx\"\"\"\n\n\"\"\"dacx\nTo create an Activity Retry Policy in Python, set the [RetryPolicy](https://python.temporal.io/temporalio.common.RetryPolicy.html) class within the [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) or [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) function.\ndacx\"\"\"\n\n\n@workflow.defn\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, greeting: str) -&gt; list[str]:\n        activity_timeout_result = await workflow.execute_activity(\n            your_activity,\n            YourParams(greeting, \"Activity Timeout option\"),\n            # Activity Execution Timeout\n            start_to_close_timeout=timedelta(seconds=10),\n            # schedule_to_start_timeout=timedelta(seconds=10),\n            # schedule_to_close_timeout=timedelta(seconds=10),\n        )\n        activity_result = await workflow.execute_activity(\n            your_activity,\n            YourParams(greeting, \"Retry Policy options\"),\n            start_to_close_timeout=timedelta(seconds=10),\n            # Retry Policy\n            retry_policy=RetryPolicy(\n                backoff_coefficient=2.0,\n                maximum_attempts=5,\n                initial_interval=timedelta(seconds=1),\n                maximum_interval=timedelta(seconds=2),\n                # non_retryable_error_types=[\"ValueError\"],\n            ),\n        )\n        return activity_timeout_result, activity_result\n\n\n\"\"\" @dacx\nid: how-to-set-activity-timeouts-in-python\ntitle: How to set Activity Timeouts in Python\nlabel: Set Activity Timeouts\ndescription: Set Activity timeouts from within your Workflow Definition.\ntags:\n - activity\n - timeout\n - python sdk\n - code sample\nlines: 9-17, 28-35\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-set-an-activity-retry-policy-in-python\ntitle: How to set an Activity Retry Policy in Python\nlabel: Retry Policy\ndescription: Create an instance of an Activity Retry Policy in Python.\ntags:\n - activity\n - retry policy\n - python sdk\n - code sample\nlines: 19-21, 4, 36-48\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#get-the-results-of-an-activity-execution","title":"Get the results of an Activity Execution","text":"<p>How to get the results of an Activity Execution using the Temporal Python SDK.</p> <p>The call to spawn an Activity Execution generates the ScheduleActivityTask Command and provides the Workflow with an Awaitable. Workflow Executions can either block progress until the result is available through the Awaitable or continue progressing, making use of the result when it becomes available.</p> <p>Use <code>start_activity()</code> to start an Activity and return its handle, <code>ActivityHandle</code>. Use <code>execute_activity()</code> to return the results.</p> <p>You must provide either <code>schedule_to_close_timeout</code> or <code>start_to_close_timeout</code>.</p> <p><code>execute_activity()</code> is a shortcut for await <code>start_activity()</code>. An asynchronous <code>execute_activity()</code> helper is provided which takes the same arguments as <code>start_activity()</code> and awaits on the <code>result. execute_activity()</code> should be used in most cases unless advanced task capabilities are needed.</p> <pre><code>from datetime import timedelta\n\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    from your_activities_dacx import your_activity\n    from your_dataobject_dacx import YourParams\n\n\"\"\"dacx\nTo spawn an Activity Execution, use the [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) operation from within your Workflow Definition.\n\n`execute_activity()` is a shortcut for [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) that waits on its result.\n\nTo get just the handle to wait and cancel separately, use `start_activity()`.\nIn most cases, use `execute_activity()` unless advanced task capabilities are needed.\n\nA single argument to the Activity is positional. Multiple arguments are not supported in the type-safe form of `start_activity()` or `execute_activity()` and must be supplied by the `args` keyword argument.\ndacx\"\"\"\n\n\"\"\"dacx\nYou can customize the Workflow name with a custom name in the decorator argument. For example, `@workflow.defn(name=\"your-workflow-name\")`. If the name parameter is not specified, the Workflow name defaults to the function name.\ndacx\"\"\"\n\n\"\"\"dacx\nIn the Temporal Python SDK programming model, Workflows are defined as classes.\n\nSpecify the `@workflow.defn` decorator on the Workflow class to identify a Workflow.\n\nUse the `@workflow.run` to mark the entry point method to be invoked. This must be set on one asynchronous method defined on the same class as `@workflow.defn`. Run methods have positional parameters.\ndacx\"\"\"\n\n\"\"\"dacx\nTo return a value of the Workflow, use `return` to return an object.\n\nTo return the results of a Workflow Execution, use either `start_workflow()` or `execute_workflow()` asynchronous methods.\ndacx\"\"\"\n\n\"\"\"dacx\nUse [`start_activity()`](https://python.temporal.io/temporalio.workflow.html#start_activity) to start an Activity and return its handle, [`ActivityHandle`](https://python.temporal.io/temporalio.workflow.ActivityHandle.html). Use [`execute_activity()`](https://python.temporal.io/temporalio.workflow.html#execute_activity) to return the results.\n\nYou must provide either `schedule_to_close_timeout` or `start_to_close_timeout`.\n\n`execute_activity()` is a shortcut for `await start_activity()`. An asynchronous `execute_activity()` helper is provided which takes the same arguments as `start_activity()` and `await`s on the result. `execute_activity()` should be used in most cases unless advanced task capabilities are needed.\ndacx\"\"\"\n\n\n@workflow.defn(name=\"YourWorkflow\")\nclass YourWorkflow:\n    @workflow.run\n    async def run(self, name: str) -&gt; str:\n        return await workflow.execute_activity(\n            your_activity,\n            YourParams(\"Hello\", name),\n            start_to_close_timeout=timedelta(seconds=10),\n        )\n\n\n\"\"\" @dacx\nid: how-to-spawn-an-activity-execution-in-python\ntitle: How to spawn an Activity Execution in Python\nlabel: Activity Execution\ndescription: Use the `execute_activity()` operation from within your Workflow Definition.\nlines: 3, 9-18, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-customize-workflow-type-in-python\ntitle: How to customize Workflow types in Python\nlabel: Customize Workflow types\ndescription: Customize Workflow types.\nlines: 3, 20-22, 47-55\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-develop-a-workflow-definition-in-python\ntitle: How to develop a Workflow Definition in Python\nlabel: Develop a Workflow Definition\ndescription: To develop a Workflow Definition, specify the `@workflow.defn` decorator on the Workflow class and use `@workflow.run` to mark the entry point.\nlines: 3, 24-30, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-define-workflow-return-values-in-python\ntitle: How to define Workflow return values\nlabel: Define Workflow return values\ndescription: Define Workflow return values.\ntags:\n - workflow return values\nlines: 3, 32-36, 47-55\n@dacx \"\"\"\n\n\n\"\"\" @dacx\nid: how-to-get-the-result-of-an-activity-execution-in-python\ntitle: How to get the result of an Activity Execution in Python\nlabel: Get the result of an Activity Execution\ndescription: Get the result of an Activity Execution.\ntags:\n - activity execution\n - python sdk\n - code sample\nlines: 3, 38-44, 47-55\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#run-a-worker-process","title":"Run a Worker Process","text":"<p>How to run a Worker Process using the Temporal Python SDK.</p> <p>The <code>Worker Process</code> is where Workflow Functions and Activity Functions are executed.</p> <ul> <li>Each <code>Worker Entity</code> in the Worker Process must register the exact Workflow Types and Activity Types it may execute.</li> <li>Each Worker Entity must also associate itself with exactly one <code>Task Queue</code>.</li> <li>Each Worker Entity polling the same Task Queue must be registered with the same Workflow Types and Activity Types.</li> <li>A Worker Entity is the component within a Worker Process that listens to a specific Task Queue.</li> <li>Although multiple Worker Entities can be in a single Worker Process, a single Worker Entity Worker Process may be perfectly sufficient. </li> <li>A Worker Entity contains a Workflow Worker and/or an Activity Worker, which makes progress on Workflow Executions and Activity Executions, respectively.</li> <li>To develop a Worker, use the Worker() constructor and add your Client, Task Queue, Workflows, and Activities as arguments. The following code example creates a Worker that polls for tasks from the Task Queue and executes the Workflow. When a Worker is created, it accepts a list of Workflows in the workflows parameter, a list of Activities in the activities parameter, or both.</li> </ul> <pre><code>import asyncio\n\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\nfrom your_activities_dacx import your_activity\nfrom your_workflows_dacx import YourWorkflow\n\n\"\"\"dacx\nTo develop a Worker, use the `Worker()` constructor and add your Client, Task Queue, Workflows, and Activities as arguments.\nThe following code example creates a Worker that polls for tasks from the Task Queue and executes the Workflow.\nWhen a Worker is created, it accepts a list of Workflows in the workflows parameter, a list of Activities in the activities parameter, or both.\ndacx\"\"\"\n\n\"\"\"dacx\nWhen a `Worker` is created, it accepts a list of Workflows in the `workflows` parameter, a list of Activities in the `activities` parameter, or both.\ndacx\"\"\"\n\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"your-task-queue\",\n        workflows=[YourWorkflow],\n        activities=[your_activity],\n    )\n    await worker.run()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\"\"\" @dacx\nid: how-to-develop-a-worker-program-in-python\ntitle: How to develop a Worker Program in Python\nlabel: Worker Program\ndescription: Create a new instance of a Worker.\ntags:\n - worker\n - python sdk\n - code sample\nlines: 3-4, 8-12, 19-31\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-register-types-with-a-worker-in-python\ntitle: How to register types with a Worker in Python\nlabel: Register types with a Worker\ndescription: Register types with a Worker.\ntags:\n - worker\n - python sdk\n - code sample\nlines: 14-16, 19-31\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/core-application/#register-types","title":"Register types","text":"<p>How to register types using the Temporal Python SDK.</p> <p>All Workers listening to the same Task Queue name must be registered to handle the exact same Workflows Types and Activity Types.</p> <p>When a <code>Worker</code> is created, it accepts a list of <code>Workflows</code> in the <code>workflows</code> parameter, a list of Activities in the activities parameter, or both.</p> <pre><code>import asyncio\n\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\nfrom your_activities_dacx import your_activity\nfrom your_workflows_dacx import YourWorkflow\n\n\"\"\"dacx\nTo develop a Worker, use the `Worker()` constructor and add your Client, Task Queue, Workflows, and Activities as arguments.\nThe following code example creates a Worker that polls for tasks from the Task Queue and executes the Workflow.\nWhen a Worker is created, it accepts a list of Workflows in the workflows parameter, a list of Activities in the activities parameter, or both.\ndacx\"\"\"\n\n\"\"\"dacx\nWhen a `Worker` is created, it accepts a list of Workflows in the `workflows` parameter, a list of Activities in the `activities` parameter, or both.\ndacx\"\"\"\n\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"your-task-queue\",\n        workflows=[YourWorkflow],\n        activities=[your_activity],\n    )\n    await worker.run()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\"\"\" @dacx\nid: how-to-develop-a-worker-program-in-python\ntitle: How to develop a Worker Program in Python\nlabel: Worker Program\ndescription: Create a new instance of a Worker.\ntags:\n - worker\n - python sdk\n - code sample\nlines: 3-4, 8-12, 19-31\n@dacx \"\"\"\n\n\"\"\" @dacx\nid: how-to-register-types-with-a-worker-in-python\ntitle: How to register types with a Worker in Python\nlabel: Register types with a Worker\ndescription: Register types with a Worker.\ntags:\n - worker\n - python sdk\n - code sample\nlines: 14-16, 19-31\n@dacx \"\"\"\n</code></pre>"},{"location":"Temporal/setup/","title":"Temporal setup","text":""},{"location":"Temporal/setup/#install-python","title":"Install Python","text":"<p>Make sure you have Python installed. Check your version of Python with the following command.</p> <p><code>python3 -V</code> -&gt; <code>python 3.13.3</code></p>"},{"location":"Temporal/setup/#install-the-temporal-python-sdk","title":"Install the Temporal Python SDK","text":"<p>You should install the Temporal Python SDK in your project using a virtual environment. Create a directory for your Temporal project, switch to the new directory, create a Python virtual environment, activate it, and then install the Temporal SDK.</p> <p><code>mkdir temporal-project</code> <code>cd temporal-project</code> <code>python3 -m venv env</code> <code>source env/bin/activate</code> <code>pip install temporalio</code></p>"},{"location":"Temporal/setup/#install-temporal-cli","title":"Install Temporal CLI","text":"<p>Configure a local Temporal Service for development.</p> <p>The fastest way to get a development version of the Temporal Service running on your local machine is to use <code>Temporal CLI</code>.</p> <p>macOS:  Install the Temporal CLI using Homebrew:</p> <p><code>brew install temporal</code></p> <p>Linux:  Download the Temporal CLI for your architecture:</p> <p>https://temporal.download/cli/archive/latest?platform=linux&amp;arch=amd64 https://temporal.download/cli/archive/latest?platform=linux&amp;arch=arm64</p> <p>Extract the archive and move the temporal binary into your PATH, for example:</p> <p><code>sudo mv temporal /usr/local/bin</code></p>"},{"location":"Temporal/setup/#start-the-development-server","title":"Start the development server","text":"<p>Once you've installed Temporal CLI and added it to your PATH, open a new Terminal window and run the following command.</p> <p>This command starts a local Temporal Service. It starts the Web UI, creates the default Namespace, and uses an in-memory database.</p> <p>The Temporal Service will be available on <code>localhost:7233</code>. The Temporal Web UI will be available at <code>http://localhost:8233</code>.</p> <p>Leave the local Temporal Service running as you work through tutorials and other projects. You can stop the Temporal Service at any time by pressing CTRL+C.</p> <p>Once you have everything installed, you're ready to build apps with Temporal on your local machine.</p> <p>Change the Web UI port The Temporal Web UI may be on a different port in some examples or tutorials. To change the port for the Web UI, use the <code>--ui-port</code> option when starting the server:</p> <p><code>temporal server start-dev --ui-port 8080</code></p> <p>The Temporal Web UI will now be available at http://localhost:8080.</p> <p></p>"},{"location":"Temporal/setup/#run-hello-world-test-your-installation","title":"Run Hello World: Test Your Installation","text":"<p>Now let's verify your setup is working by creating and running a complete Temporal application with both a Workflow and Activity.</p> <p>This test will confirm that:</p> <ul> <li> <p>The Temporal Python SDK is properly installed</p> </li> <li> <p>Your local Temporal Service is running</p> </li> <li> <p>You can successfully create and execute Workflows and Activities</p> </li> <li> <p>The communication between components is functioning correctly</p> </li> </ul>"},{"location":"Temporal/setup/#1-create-the-activity","title":"1. Create the Activity","text":"<ul> <li>Create an Activity file (<code>activities.py</code>):</li> </ul> <pre><code>from temporalio import activity\n\n@activity.defn\nasync def greet(name: str) -&gt; str:\n    return f\"Hello {name}\"\n</code></pre> <p>An Activity is a normal function or method that executes a single, well-defined action (either short or long running), which often involve interacting with the outside world, such as sending emails, making network requests, writing to a database, or calling an API, which are prone to failure. If an Activity fails, Temporal automatically retries it based on your configuration.</p>"},{"location":"Temporal/setup/#2-create-the-workflow","title":"2. Create the Workflow","text":"<p>Create a Workflow file (<code>workflows.py</code>):</p> <p>Workflows orchestrate Activities and contain the application logic. Temporal Workflows are resilient. They can run and keep running for years, even if the underlying infrastructure fails. If the application itself crashes, Temporal will automatically recreate its pre-failure state so it can continue right where it left off.</p>"},{"location":"Temporal/setup/#3-create-the-worker","title":"3. Create the Worker","text":"<p>Create a Worker file (<code>worker.py</code>):</p> <pre><code>import asyncio\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\nfrom workflows import SayHelloWorkflow\nfrom activities import greet\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    worker = Worker(\n        client,\n        task_queue=\"my-task-queue\",\n        workflows=[SayHelloWorkflow],\n        activities=[greet],\n    )\n    print(\"Worker started.\")\n    await worker.run()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run the Worker by opening up a new terminal:</p> <p>Keep this terminal running - you should see \"Worker started\" displayed.</p> <p></p> <p>With your Activity and Workflow defined, you need a Worker to execute them. A Worker polls a Task Queue, that you configure it to poll, looking for work to do. Once the Worker dequeues the Workflow or Activity task from the Task Queue, it then executes that task.</p> <p>Workers are a crucial part of your Temporal application as they're what actually execute the tasks defined in your Workflows and Activities. </p>"},{"location":"Temporal/setup/#4-execute-the-workflow","title":"4. Execute the Workflow","text":"<p>Now that your Worker is running, it's time to start a Workflow Execution.</p> <p>This final step will validate that everything is working correctly with your file labeled <code>starter.py</code>.</p> <p>Create a separate file called <code>starter.py</code>:</p> <pre><code>import asyncio\nimport uuid\nfrom temporalio.client import Client\n\nasync def main():\n    client = await Client.connect(\"localhost:7233\")\n    result = await client.execute_workflow(\n        \"SayHelloWorkflow\",\n        \"Temporal\",\n        id=f\"say-hello-workflow-{uuid.uuid4()}\",\n        task_queue=\"my-task-queue\",\n    )\n    print(\"Workflow result:\", result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>While the Worker is still running, run the following command in a new terminal:</p> <p></p> <p>Verify Success</p> <p>If everything is working correctly, you should see:</p> <ul> <li> <p>Worker processing the Workflow and Activity</p> </li> <li> <p>Output: <code>Workflow result: Hello Temporal</code></p> </li> <li> <p>Workflow Execution details in the <code>Temporal Web UI</code></p> </li> </ul> <p></p> <p></p> <p></p>"},{"location":"Temporal/temporal-overview/","title":"What is Temporal?","text":"<p>Temporal is a workflow orchestration platform that lets you run long-running, reliable, fault-tolerant workflows in distributed systems.</p> <p><code>Temporal guarantees that your code continues exactly where it left off \u2014 even if your service crashes, restarts, or runs for days.</code></p>"},{"location":"Temporal/temporal-overview/#why-temporal-exists-the-real-problem","title":"Why Temporal exists (the real problem)","text":"<p>Modern systems (microservices, cloud, AI agents) have problems like:</p> <ul> <li>Processes crash</li> <li>Networks fail</li> <li>APIs timeout</li> <li>Pods restart</li> <li>Workflows take hours/days</li> <li>Humans need to approve steps</li> </ul> <p>Traditional tools <code>lose state</code> when this happens. </p> <p>Temporal was built to solve <code>this exact reliability gap</code>.</p> <p></p> <p></p> <p></p> <p>It records every step of your workflow so it can:</p> <ul> <li>Retry safely</li> <li>Resume after failure</li> <li>Wait indefinitely</li> <li>Guarantee no duplicate execution</li> </ul>"},{"location":"Temporal/temporal-overview/#core-components","title":"Core components","text":"<p>1\ufe0f\u20e3 Temporal Server (runs separately)</p> <ul> <li> <p>Manages workflow state</p> </li> <li> <p>Stores execution history</p> </li> <li> <p>Handles retries, timers, signals</p> </li> </ul> <p>2\ufe0f\u20e3 Workers (your code)</p> <ul> <li> <p>Execute workflow logic</p> </li> <li> <p>Execute activities (real work)</p> </li> </ul> <p>3\ufe0f\u20e3 Database</p> <ul> <li>Persists workflow history (Postgres, MySQL, Cassandra)</li> </ul>"},{"location":"Temporal/temporal-overview/#what-makes-temporal-special","title":"What makes Temporal special?","text":"<p>\ud83e\udde0 Durable execution</p> <p>If your app crashes at step 7 of 20:</p> <p>\u27a1\ufe0f <code>Temporal restarts from step 7</code>, not step 1</p> <p>\ud83d\udd01 Automatic retries</p> <ul> <li>No custom retry logic needed.</li> </ul> <p>\u23f1 Long waits (hours / days / months)</p> <ul> <li>This does NOT block memory or CPU.</li> </ul> <p>\ud83e\uddcd Human-in-the-loop</p>"},{"location":"Temporal/temporal-overview/#exactly-once-execution","title":"\u2705 Exactly-once execution","text":"<p>Even if:</p> <ul> <li>Worker crashes</li> <li>Network flakes</li> <li>Retry happens</li> </ul> <p>The action is executed <code>once and only once</code>.</p>"},{"location":"Temporal/temporal-overview/#what-temporal-is-not","title":"What Temporal is NOT","text":"<p>\u274c Not an AI framework</p> <p>\u274c Not a message queue</p> <p>\u274c Not Kubernetes</p> <p>\u274c Not a replacement for Terraform/Ansible</p> <p>\u274c Not a scheduler like cron</p> <p>Temporal is about correctness and durability, not intelligence.</p>"},{"location":"Temporal/temporal-overview/#where-temporal-is-used","title":"Where Temporal is used","text":"<p>Common real-world use cases</p> <ul> <li> <p>Microservice orchestration</p> </li> <li> <p>Payment processing</p> </li> <li> <p>Order fulfillment</p> </li> <li> <p>Cloud provisioning workflows</p> </li> <li> <p>Incident remediation</p> </li> <li> <p>Agentic AI orchestration</p> </li> </ul> <p>Companies using it include <code>Uber</code>, <code>Netflix</code>, <code>Stripe</code>, <code>Datadog</code>, etc.</p>"},{"location":"Temporal/temporal-overview/#temporal-vs-traditional-approaches","title":"Temporal vs traditional approaches","text":"Problem Without Temporal With Temporal Crash recovery Manual Automatic Retries Custom code Built-in Long waits Dangerous Safe State persistence Ad-hoc DB Native Observability Logs only Full UI"},{"location":"Temporal/temporal-overview/#temporal-in-agentic-ai-why-it-comes-up-so-often","title":"Temporal in Agentic AI (why it comes up so often)","text":"<p>Agentic AI workflows:</p> <ul> <li> <p>Run for long time</p> </li> <li> <p>Call many tools</p> </li> <li> <p>Fail often</p> </li> <li> <p>Need approval</p> </li> <li> <p>Must resume safely</p> </li> </ul> <p>Temporal provides the reliability layer for agents.</p> <p><code>Agents reason</code> \u2014 <code>Temporal remembers</code>.</p>"},{"location":"Temporal/temporal-overview/#temporal-vs-airflow-vs-step-functions","title":"Temporal vs Airflow vs Step Functions","text":"<p>High-level comparison</p> Dimension Temporal Apache Airflow AWS Step Functions Core purpose Durable workflow execution Batch job scheduling Cloud-native orchestration Primary design Stateful, fault-tolerant workflows DAG-based schedulers Managed state machine Typical users Platform, backend, agentic AI teams Data engineering AWS-centric teams Runs where Self-hosted / Cloud Self-hosted / Managed Fully managed (AWS) <p>Execution &amp; reliability</p> Capability Temporal Airflow Step Functions Crash recovery \u2705 Automatic, exact resume \u274c Task retry only \u26a0\ufe0f State retry, limited Long-running (days/months) \u2705 Native \u274c Not safe \u26a0\ufe0f Limited (timeouts) Exactly-once execution \u2705 Guaranteed \u274c No \u26a0\ufe0f Partial Automatic retries \u2705 Built-in, deterministic \u26a0\ufe0f Task-level only \u26a0\ufe0f Config-based Timers / sleeps \u2705 Native (no resources) \u274c Not designed \u26a0\ufe0f Limited <p>State &amp; workflow model</p> Aspect Temporal Airflow Step Functions State persistence Durable execution history Metadata only JSON state Workflow definition Code (Python/Go/Java) DAG (Python) JSON / YAML Dynamic workflows \u2705 Yes \u274c No \u26a0\ufe0f Limited Human-in-the-loop \u2705 First-class \u274c No \u26a0\ufe0f Workarounds Versioning workflows \u2705 Safe versioning \u274c Painful \u26a0\ufe0f Manual <p>Agentic AI suitability (important)</p> Requirement Temporal Airflow Step Functions Long-running agents \u2705 Excellent \u274c No \u26a0\ufe0f Limited Tool orchestration \u2705 Yes \u274c No \u26a0\ufe0f AWS-only Human approvals \u2705 Native \u274c No \u26a0\ufe0f SNS/Lambda hacks Multi-agent coordination \u2705 Strong \u274c No \u274c Weak Failure recovery \u2705 Deterministic \u274c Manual \u26a0\ufe0f Partial <p>Cloud &amp; ecosystem</p> Aspect Temporal Airflow Step Functions Cloud neutrality \u2705 Multi-cloud \u2705 Multi-cloud \u274c AWS only Vendor lock encouraging \u274c No \u274c No \u2705 Yes Infra control Full control Full control AWS-managed Cost model Infra-based Infra-based Pay per transition <p>Complexity &amp; operations</p> Factor Temporal Airflow Step Functions Learning curve \u26a0\ufe0f Medium\u2013High \u26a0\ufe0f Medium \u2705 Low Operational overhead \u26a0\ufe0f Medium \u26a0\ufe0f High \u2705 None Debuggability \u2705 Excellent UI \u26a0\ufe0f Logs-heavy \u26a0\ufe0f CloudWatch Best for teams Platform / SRE Data teams App teams Use case Winner Agentic AI \ud83c\udfc6 Temporal Data pipelines \ud83c\udfc6 Airflow AWS-native glue \ud83c\udfc6 Step Functions Enterprise remediation \ud83c\udfc6 Temporal Simple orchestration \ud83c\udfc6 Step Functions"}]}